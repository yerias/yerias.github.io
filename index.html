<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="BigData Developer"><meta name="keywords" content="yerias,TUNANのBlog,BigData"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017 | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">129</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">30</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">26</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">TUNANのBlog</div><div id="site-sub-title">感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017</div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2020/05/07/spark/14/">Spark性能优化之开发调优</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-07</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul>
<li>调优概述</li>
<li>原则一：避免创建重复的RDD</li>
<li>原则二：尽可能复用同一个RDD</li>
<li>原则三：对多次使用的RDD进行持久化</li>
<li>原则四：尽量避免使用shuffle类算子</li>
<li>原则五：使用map-side预聚合的shuffle操作</li>
<li>原则六：使用高性能的算子</li>
<li>原则七：广播大变量</li>
<li>原则八：使用Kryo优化序列化性能</li>
<li>原则九：优化数据结构</li>
<li>原则十：Data Locality本地化级别</li>
</ul>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。</p>
<p>然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。</p>
<p>Spark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。</p>
<p>笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。</p>
<p>本文作为Spark性能优化指南的基础篇，主要讲解开发调优以及资源调优。</p>
<h2 id="调优概述"><a href="#调优概述" class="headerlink" title="调优概述"></a>调优概述</h2><p>Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。</p>
<h2 id="原则一：避免创建重复的RDD"><a href="#原则一：避免创建重复的RDD" class="headerlink" title="原则一：避免创建重复的RDD"></a>原则一：避免创建重复的RDD</h2><p>通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。</p>
<p>我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。</p>
<p>一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。</p>
<p>一个简单的例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。</span></span><br><span class="line"><span class="comment">// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。</span></span><br><span class="line"><span class="comment">// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd2.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。</span></span><br><span class="line"><span class="comment">// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。</span></span><br><span class="line"><span class="comment">// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。</span></span><br><span class="line"><span class="comment">// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure>

<h2 id="原则二：尽可能复用同一个RDD"><a href="#原则二：尽可能复用同一个RDD" class="headerlink" title="原则二：尽可能复用同一个RDD"></a>原则二：尽可能复用同一个RDD</h2><p>除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。</p>
<p>一个简单的例子</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 错误的做法。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。</span></span><br><span class="line"><span class="comment">// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line"><span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; rdd2 = rdd1.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分别对rdd1和rdd2执行了不同的算子操作。</span></span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd2.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。</span></span><br><span class="line"><span class="comment">// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 其实在这种情况下完全可以复用同一个RDD。</span></span><br><span class="line"><span class="comment">// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。</span></span><br><span class="line"><span class="comment">// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd1.map(tuple._2...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。</span></span><br><span class="line"><span class="comment">// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。</span></span><br><span class="line"><span class="comment">// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。</span></span><br></pre></td></tr></table></figure>

<h2 id="原则三：对多次使用的RDD进行持久化"><a href="#原则三：对多次使用的RDD进行持久化" class="headerlink" title="原则三：对多次使用的RDD进行持久化"></a>原则三：对多次使用的RDD进行持久化</h2><p>当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。</p>
<p>Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。</p>
<p>因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。</p>
<p>对多次使用的RDD进行持久化的代码示例</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"><span class="comment">// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。</span></span><br><span class="line"><span class="comment">// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。</span></span><br><span class="line"><span class="comment">// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).cache()</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。</span></span><br><span class="line"><span class="comment">// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。</span></span><br><span class="line"><span class="comment">// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。</span></span><br><span class="line"><span class="comment">// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure>

<p>对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。</p>
<h3 id="Spark的持久化级别"><a href="#Spark的持久化级别" class="headerlink" title="Spark的持久化级别"></a>Spark的持久化级别</h3><table>
<thead>
<tr>
<th>持久化级别</th>
<th>含义解释</th>
</tr>
</thead>
<tbody><tr>
<td>MEMORY_ONLY</td>
<td>使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。</td>
</tr>
<tr>
<td>MEMORY_AND_DISK</td>
<td>使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</td>
</tr>
<tr>
<td>MEMORY_ONLY_SER</td>
<td>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td>
</tr>
<tr>
<td>MEMORY_AND_DISK_SER</td>
<td>基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td>
</tr>
<tr>
<td>DISK_ONLY</td>
<td>使用未序列化的Java对象格式，将数据全部写入磁盘文件中。</td>
</tr>
<tr>
<td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等.</td>
<td>对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。</td>
</tr>
</tbody></table>
<h3 id="如何选择一种最合适的持久化策略"><a href="#如何选择一种最合适的持久化策略" class="headerlink" title="如何选择一种最合适的持久化策略"></a>如何选择一种最合适的持久化策略</h3><ul>
<li>默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。</li>
<li>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</li>
<li>如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。</li>
<li>通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</li>
</ul>
<h2 id="原则四：尽量避免使用shuffle类算子"><a href="#原则四：尽量避免使用shuffle类算子" class="headerlink" title="原则四：尽量避免使用shuffle类算子"></a>原则四：尽量避免使用shuffle类算子</h2><p>如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。</p>
<p>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。</p>
<p>因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p>
<p>Broadcast与map进行join代码示例</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 传统的join操作会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Broadcast+map的join操作，不会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 使用Broadcast将一个数据量较小的RDD作为广播变量。</span></span><br><span class="line"><span class="keyword">val</span> rdd2Data = rdd2.collect()</span><br><span class="line"><span class="keyword">val</span> rdd2DataBroadcast = sc.broadcast(rdd2Data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span></span><br><span class="line"><span class="comment">// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。</span></span><br><span class="line"><span class="comment">// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.map(rdd2DataBroadcast...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span></span><br><span class="line"><span class="comment">// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span></span><br></pre></td></tr></table></figure>

<h2 id="原则五：使用map-side预聚合的shuffle操作"><a href="#原则五：使用map-side预聚合的shuffle操作" class="headerlink" title="原则五：使用map-side预聚合的shuffle操作"></a>原则五：使用map-side预聚合的shuffle操作</h2><p>如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。</p>
<p>所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。</p>
<p>比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</p>
<p><img src="https://yerias.github.io/spark_img/shuffle1.png" alt="shuffle1"></p>
<p><img src="https://yerias.github.io/spark_img/shuffle2.png" alt="shuffle2"></p>
<h2 id="原则六：使用高性能的算子"><a href="#原则六：使用高性能的算子" class="headerlink" title="原则六：使用高性能的算子"></a>原则六：使用高性能的算子</h2><p>除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。</p>
<h3 id="使用reduceByKey-aggregateByKey替代groupByKey"><a href="#使用reduceByKey-aggregateByKey替代groupByKey" class="headerlink" title="使用reduceByKey/aggregateByKey替代groupByKey"></a>使用reduceByKey/aggregateByKey替代groupByKey</h3><p>详情见“原则五：使用map-side预聚合的shuffle操作”。</p>
<h3 id="使用mapPartitions替代普通map"><a href="#使用mapPartitions替代普通map" class="headerlink" title="使用mapPartitions替代普通map"></a>使用mapPartitions替代普通map</h3><p>mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！</p>
<h3 id="使用foreachPartitions替代foreach"><a href="#使用foreachPartitions替代foreach" class="headerlink" title="使用foreachPartitions替代foreach"></a>使用foreachPartitions替代foreach</h3><p>原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。</p>
<h3 id="使用filter之后进行coalesce操作"><a href="#使用filter之后进行coalesce操作" class="headerlink" title="使用filter之后进行coalesce操作"></a>使用filter之后进行coalesce操作</h3><p>通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。</p>
<h3 id="使用repartitionAndSortWithinPartitions替代repartition与sort类操作"><a href="#使用repartitionAndSortWithinPartitions替代repartition与sort类操作" class="headerlink" title="使用repartitionAndSortWithinPartitions替代repartition与sort类操作"></a>使用repartitionAndSortWithinPartitions替代repartition与sort类操作</h3><p>repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。</p>
<h2 id="原则七：广播大变量"><a href="#原则七：广播大变量" class="headerlink" title="原则七：广播大变量"></a>原则七：广播大变量</h2><p>有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。</p>
<p>在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。</p>
<p>因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。</p>
<p>广播大变量的代码示例</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 以下代码在算子函数中，使用了外部的变量。</span></span><br><span class="line"><span class="comment">// 此时没有做任何特殊操作，每个task都会有一份list1的副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line">rdd1.map(list1...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下代码将list1封装成了Broadcast类型的广播变量。</span></span><br><span class="line"><span class="comment">// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。</span></span><br><span class="line"><span class="comment">// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。</span></span><br><span class="line"><span class="comment">// 每个Executor内存中，就只会驻留一份广播变量副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line"><span class="keyword">val</span> list1Broadcast = sc.broadcast(list1)</span><br><span class="line">rdd1.map(list1Broadcast...)</span><br></pre></td></tr></table></figure>

<h2 id="原则八：使用Kryo优化序列化性能"><a href="#原则八：使用Kryo优化序列化性能" class="headerlink" title="原则八：使用Kryo优化序列化性能"></a>原则八：使用Kryo优化序列化性能</h2><p>在Spark中，主要有三个地方涉及到了序列化：</p>
<ul>
<li>在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。</li>
<li>将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。</li>
<li>使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。</li>
</ul>
<p>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。</p>
<p>以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建SparkConf对象。</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class="line"><span class="comment">// 设置序列化器为KryoSerializer。</span></span><br><span class="line">conf.set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line"><span class="comment">// 注册要序列化的自定义类型。</span></span><br><span class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</span><br></pre></td></tr></table></figure>

<h2 id="原则九：优化数据结构"><a href="#原则九：优化数据结构" class="headerlink" title="原则九：优化数据结构"></a>原则九：优化数据结构</h2><p>Java中，有三种类型比较耗费内存：</p>
<ul>
<li>对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。</li>
<li>字符串，每个字符串内部都有一个字符数组以及长度等额外信息。</li>
<li>集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。</li>
</ul>
<p>因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。</p>
<p>但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。</p>
<h2 id="原则十：Data-Locality本地化级别"><a href="#原则十：Data-Locality本地化级别" class="headerlink" title="原则十：Data Locality本地化级别"></a>原则十：Data Locality本地化级别</h2><p><strong>PROCESS_LOCAL</strong>：进程本地化，代码和数据在同一个进程中，也就是在同一个executor中；计算数据的task由executor执行，数据在executor的BlockManager中；性能最好</p>
<p><strong>NODE_LOCAL</strong>：节点本地化，代码和数据在同一个节点中；比如说，数据作为一个HDFS block块，就在节点上，而task在节点上某个executor中运行；或者是，数据和task在一个节点上的不同executor中；数据需要在进程间进行传输<br><strong>NO_PREF</strong>：对于task来说，数据从哪里获取都一样，没有好坏之分<br><strong>RACK_LOCAL</strong>：机架本地化，数据和task在一个机架的两个节点上；数据需要通过网络在节点之间进行传输<br><strong>ANY</strong>：数据和task可能在集群中的任何地方，而且不在一个机架中，性能最差</p>
<p>spark.locality.wait，默认是3s</p>
<p>Spark在Driver上，对Application的每一个stage的task，进行分配之前，都会计算出每个task要计算的是哪个分片数据，RDD的某个partition；Spark的task分配算法，优先，会希望每个task正好分配到它要计算的数据所在的节点，这样的话，就不用在网络间传输数据；</p>
<p>但是可能task没有机会分配到它的数据所在的节点，因为可能那个节点的计算资源和计算能力都满了；所以呢，这种时候，通常来说，Spark会等待一段时间，默认情况下是3s钟（不是绝对的，还有很多种情况，对不同的本地化级别，都会去等待），到最后，实在是等待不了了，就会选择一个比较差的本地化级别，比如说，将task分配到靠它要计算的数据所在节点，比较近的一个节点，然后进行计算。</p>
<p>但是对于第二种情况，通常来说，肯定是要发生数据传输，task会通过其所在节点的BlockManager来获取数据，BlockManager发现自己本地没有数据，会通过一个getRemote()方法，通过TransferService（网络数据传输组件）从数据所在节点的BlockManager中，获取数据，通过网络传输回task所在节点。</p>
<p>对于我们来说，当然不希望是类似于第二种情况的了。最好的，当然是task和数据在一个节点上，直接从本地executor的BlockManager中获取数据，纯内存，或者带一点磁盘IO；如果要通过网络传输数据的话，那么实在是，性能肯定会下降的，大量网络传输，以及磁盘IO，都是性能的杀手。</p>
<p><strong>什么时候要调节这个参数？</strong><br>观察日志，spark作业的运行日志，推荐大家在测试的时候，先用client模式，在本地就直接可以看到比较全的日志。<br>日志里面会显示，starting task。。。，PROCESS LOCAL、NODE LOCAL，观察大部分task的数据本地化级别。</p>
<p>如果大多都是PROCESS_LOCAL，那就不用调节了<br>如果是发现，好多的级别都是NODE_LOCAL、ANY，那么最好就去调节一下数据本地化的等待时长<br>调节完，应该是要反复调节，每次调节完以后，再来运行，观察日志<br>看看大部分的task的本地化级别有没有提升；看看，整个spark作业的运行时间有没有缩短</p>
<p>但是注意别本末倒置，本地化级别倒是提升了，但是因为大量的等待时长，spark作业的运行时间反而增加了，那就还是不要调节了。</p>
<p>spark.locality.wait，默认是3s；可以改成6s，10s</p>
<p>默认情况下，下面3个的等待时长，都是跟上面那个是一样的，都是3s</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">spark.locality.wait.process<span class="comment">//建议60s</span></span><br><span class="line">spark.locality.wait.node<span class="comment">//建议30s</span></span><br><span class="line">spark.locality.wait.rack<span class="comment">//建议20s</span></span><br></pre></td></tr></table></figure>

<hr>
<p>转载自: <a href="https://tech.meituan.com/spark-tuning-basic.html" target="_blank" rel="noopener">https://tech.meituan.com/spark-tuning-basic.html</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/05/06/spark/25/">Spark各个版本特性</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-06</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><p>源博：<a href="https://www.maxinhong.com/2020/04/03/68.spark各个版本特性/#more" target="_blank" rel="noopener">https://www.maxinhong.com/2020/04/03/68.spark%E5%90%84%E4%B8%AA%E7%89%88%E6%9C%AC%E7%89%B9%E6%80%A7/#more</a></p>
<hr>
<h1 id="各个版本特性（官方文档）"><a href="#各个版本特性（官方文档）" class="headerlink" title="各个版本特性（官方文档）"></a>各个版本特性（官方文档）</h1><p><a href="https://spark.apache.org/releases/" target="_blank" rel="noopener">https://spark.apache.org/releases/</a><br><a href="https://spark.apache.org/news/index.html" target="_blank" rel="noopener">https://spark.apache.org/news/index.html</a></p>
<h2 id="Spark-0-6-x"><a href="#Spark-0-6-x" class="headerlink" title="Spark 0.6.x"></a>Spark 0.6.x</h2><ul>
<li>Standalone部署模式进行了简化</li>
</ul>
<h2 id="Spark-0-7"><a href="#Spark-0-7" class="headerlink" title="Spark 0.7"></a>Spark 0.7</h2><ul>
<li>Python API</li>
<li>增加Spark Streaming</li>
<li>支持maven build</li>
</ul>
<h2 id="Spark-0-8"><a href="#Spark-0-8" class="headerlink" title="Spark 0.8"></a>Spark 0.8</h2><ul>
<li>支持MLlib库</li>
<li>hadoop yarn正式支持</li>
</ul>
<h2 id="Spark-0-9"><a href="#Spark-0-9" class="headerlink" title="Spark 0.9"></a>Spark 0.9</h2><ul>
<li>用SparkConf类来配置SparkContext</li>
<li>spark streaming正式版发布</li>
<li>GraphX的测试版出现</li>
<li>mllib库升级，支持python</li>
<li>core升级</li>
</ul>
<h2 id="Spark-1-0"><a href="#Spark-1-0" class="headerlink" title="Spark 1.0"></a>Spark 1.0</h2><ul>
<li>提出spark-submit脚本和history-server</li>
<li>yarn安全模式整合</li>
<li>spark sql被提出</li>
<li>java8的支持</li>
</ul>
<h2 id="Spark-1-1"><a href="#Spark-1-1" class="headerlink" title="Spark 1.1"></a>Spark 1.1</h2><ul>
<li>spark增强了磁盘（非内存）的排序的速率</li>
</ul>
<h2 id="Spark-1-2"><a href="#Spark-1-2" class="headerlink" title="Spark 1.2"></a>Spark 1.2</h2><ul>
<li>shuffle大升级</li>
<li>Graphx正式版发布</li>
</ul>
<h2 id="Spark-1-3"><a href="#Spark-1-3" class="headerlink" title="Spark 1.3"></a>Spark 1.3</h2><ul>
<li>新增DataFrame API</li>
<li>Spark SQL正式脱离alpha版本</li>
</ul>
<h2 id="Spark-1-4"><a href="#Spark-1-4" class="headerlink" title="Spark 1.4"></a>Spark 1.4</h2><ul>
<li>正式引入SparkR</li>
<li>Spark Core为应用提供了REST API来获取各种信息</li>
</ul>
<h2 id="Spark-1-5"><a href="#Spark-1-5" class="headerlink" title="Spark 1.5"></a>Spark 1.5</h2><ul>
<li>Spark1.5重点是对性能的提升，引入钨丝项目，该项目通过对几个底层框架的重构进一步优化Spark性能</li>
</ul>
<h2 id="Spark-1-6"><a href="#Spark-1-6" class="headerlink" title="Spark 1.6"></a>Spark 1.6</h2><ul>
<li>新增Dataset API</li>
</ul>
<h2 id="Spark-2-0"><a href="#Spark-2-0" class="headerlink" title="Spark 2.0"></a>Spark 2.0</h2><ul>
<li>用sparksession实现hivecontext和sqlcontext统一</li>
<li>合并dataframe和datasets</li>
</ul>
<h2 id="Spark-2-1"><a href="#Spark-2-1" class="headerlink" title="Spark 2.1"></a>Spark 2.1</h2><ul>
<li>提升ORC格式文件的读写性能</li>
</ul>
<h2 id="Spark-2-2"><a href="#Spark-2-2" class="headerlink" title="Spark 2.2"></a>Spark 2.2</h2><ul>
<li>Structured Streaming的生产环境支持已经就绪</li>
</ul>
<h2 id="Spark-2-3"><a href="#Spark-2-3" class="headerlink" title="Spark 2.3"></a>Spark 2.3</h2><ul>
<li>Structured Streaming 引入了低延迟的连续处理</li>
<li>支持 stream-to-stream joins</li>
</ul>
<h2 id="Spark-2-4"><a href="#Spark-2-4" class="headerlink" title="Spark 2.4"></a>Spark 2.4</h2><ul>
<li>Scala 2.12</li>
<li>添加了35个高阶函数</li>
</ul>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/05/04/presto/3/">PrestoUDF开发</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-04</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Presto/">Presto</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Presto/">Presto</a></span><div class="content"><h2 id="Presto函数"><a href="#Presto函数" class="headerlink" title="Presto函数"></a>Presto函数</h2><p>在 Presto 中，函数大体分为三种：scalar，aggregation 和 window 类型。分别如下：</p>
<p>1）scalar标量函数，简单来说就是 Java 中的一个静态方法，本身没有任何状态。</p>
<p>2）aggregation累积状态的函数，或聚集函数，如count，avg。如果只是单节点，单机状态可以直接用一个变量存储即可，但是presto是分布式计算引擎，状态数据会在多个节点之间传输，因此状态数据需要被序列化成 Presto 的内部格式才可以被传输。</p>
<p>3）window 窗口函数，如同sparkSQL中的窗口函数类似</p>
<p>官网地址：<a href="https://prestodb.github.io/docs/current/develop/functions.html" target="_blank" rel="noopener">https://prestodb.github.io/docs/current/develop/functions.html</a></p>
<h2 id="自定义Scalar函数的实现"><a href="#自定义Scalar函数的实现" class="headerlink" title="自定义Scalar函数的实现"></a>自定义Scalar函数的实现</h2><h3 id="定义一个java类"><a href="#定义一个java类" class="headerlink" title="定义一个java类"></a>定义一个java类</h3><ol>
<li><p>用 @ScalarFunction 的 Annotation 标记实现业务逻辑的静态方法。</p>
</li>
<li><p>用 @Description 描述函数的作用，这里的内容会在 SHOW FUNCTIONS 中显示。</p>
</li>
<li><p>用@SqlType 标记函数的返回值类型，如返回字符串，因此是 StandardTypes.VARCHAR。</p>
</li>
<li><p>Java 方法的返回值必须使用 Presto 内部的序列化方式，因此字符串类型必须返回 Slice， 使用 Slices.utf8Slice 方法可以方便的将 String 类型转换成 Slice 类型</p>
</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunna.spark.presto.udf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.function.Description;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.function.ScalarFunction;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.function.SqlType;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.type.StandardTypes;</span><br><span class="line"><span class="keyword">import</span> io.airlift.slice.Slice;</span><br><span class="line"><span class="keyword">import</span> io.airlift.slice.Slices;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PrefixUDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Description</span>(<span class="string">"输入的数据加上前缀"</span>)      <span class="comment">//描述</span></span><br><span class="line">    <span class="meta">@ScalarFunction</span>(<span class="string">"tunan_prefix"</span>)       <span class="comment">//方法名称</span></span><br><span class="line">    <span class="meta">@SqlType</span>(StandardTypes.VARCHAR)       <span class="comment">//返回类型</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Slice <span class="title">prefix</span><span class="params">(@SqlType(StandardTypes.VARCHAR)</span>Slice input)</span>&#123;</span><br><span class="line">        <span class="comment">// Slices.utf8Slice 方法可以方便的将 String 类型转换成 Slice 类型</span></span><br><span class="line">        <span class="keyword">return</span> Slices.utf8Slice(<span class="string">"tunan_prefix_"</span>+input.toStringUtf8());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Presto插件机制"><a href="#Presto插件机制" class="headerlink" title="Presto插件机制"></a>Presto插件机制</h3><p>presto不能像hive那样配置自定义的udf，要采用这种插件机制实现。Presto 的插件(Plugin)机制，是 Presto 能够整合多种数据源的核心。通过实现不同的 Plugin，Presto 允许用户在不同类型的数据源之间进行 JOIN 等计算。Presto 内部的所有数据源都是通过插件机制实现， 例如 MySQL、Hive、HBase等。Presto 插件机制不仅通过加载 Connector 来实现不同数据源的访问，还通过加载 FunctionFactory 来实现 UDF 的加载。 Presto 的 Plugin 遵循 Java 中的 ServiceLoader 规范， 实现非常简单。</p>
<p>实现一个plugin接口如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunna.spark.presto.udf;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.Plugin;</span><br><span class="line"><span class="keyword">import</span> com.google.common.collect.ImmutableSet;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Set;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProstoUDFPlugin</span> <span class="keyword">implements</span> <span class="title">Plugin</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Set&lt;Class&lt;?&gt;&gt; getFunctions() &#123;</span><br><span class="line">        <span class="keyword">return</span> ImmutableSet.&lt;Class&lt;?&gt;&gt;builder()</span><br><span class="line">                .add(PrefixUDF<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">                .<span class="title">build</span>()</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="注册函数"><a href="#注册函数" class="headerlink" title="注册函数"></a>注册函数</h3><p>在resources下创建META-INF/services目录，创建文件com.facebook.presto.spi.Plugin，拷贝Plugin的全限定名</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">com.tunna.spark.presto.udf.ProstoUDFPlugin</span><br></pre></td></tr></table></figure>

<p>最后在presto的plugin目录下创建我们自己的目录，并且打包上传到我们自己的目录下，需要重启presto才能将jar中的自定义函数加载进去，如果有多个依赖，都需要放在我们创建的目录下</p>
<h2 id="自定义Aggregation函数的实现"><a href="#自定义Aggregation函数的实现" class="headerlink" title="自定义Aggregation函数的实现"></a>自定义Aggregation函数的实现</h2><h3 id="实现原理步骤"><a href="#实现原理步骤" class="headerlink" title="实现原理步骤"></a>实现原理步骤</h3><p>Presto 把 Aggregation 函数分解成三个步骤执行：</p>
<ol>
<li><p>input(state, data): 针对每条数据，执行 input 函数。这个过程是并行执行的，因此在每个有数据的节点都会执行，最终得到多个累积的状态数据。</p>
</li>
<li><p>combine(state1, state2)：将所有节点的状态数据聚合起来，多次执行，直至所有状态数据被聚合成一个最终状态，也就是 Aggregation 函数的输出结果。</p>
</li>
<li><p>output(final_state, out)：最终输出结果到一个 BlockBuilder。</p>
</li>
</ol>
<h3 id="具体代码实现过程"><a href="#具体代码实现过程" class="headerlink" title="具体代码实现过程"></a>具体代码实现过程</h3><ol>
<li>一个继承AccumulatorState的State接口，自定义get和set方法</li>
<li>定义一个 Java 类，使用 @AggregationFunction 标记为 Aggregation 函数</li>
<li>使用 @InputFunction、 @CombineFunction、@OutputFunction 分别标记计算函数、合并结果函数和最终输出函数在 Plugin 处注册 Aggregation 函数</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunna.spark.presto.aggudf;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.block.BlockBuilder;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.function.*;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.type.StandardTypes;</span><br><span class="line"><span class="keyword">import</span> io.airlift.slice.Slice;</span><br><span class="line"><span class="keyword">import</span> io.airlift.slice.Slices;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> com.facebook.presto.spi.type.VarcharType.VARCHAR;</span><br><span class="line"></span><br><span class="line"><span class="meta">@AggregationFunction</span>(<span class="string">"tunan_concat"</span>)    <span class="comment">// Agg方法名</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TunanAggregationUDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@InputFunction</span>  <span class="comment">//输入函数</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">input</span><span class="params">(StringValueState state,@SqlType(StandardTypes.VARCHAR)</span> Slice value)</span>&#123;</span><br><span class="line"></span><br><span class="line">        state.setStringValue(Slices.utf8Slice(isNull(state.getStringValue())+<span class="string">"|"</span>+value.toStringUtf8()));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@CombineFunction</span>    <span class="comment">//合并函数</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">combine</span><span class="params">(StringValueState state1,StringValueState state2)</span></span>&#123;</span><br><span class="line">        state1.setStringValue(Slices.utf8Slice(isNull(state1.getStringValue())+<span class="string">"|"</span>+isNull(state2.getStringValue())));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@OutputFunction</span>(StandardTypes.VARCHAR)  <span class="comment">//输出函数</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">output</span><span class="params">(StringValueState state, BlockBuilder builder)</span></span>&#123;</span><br><span class="line">        VARCHAR.writeSlice(builder,state.getStringValue());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断null值</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">isNull</span><span class="params">(Slice slice)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> slice ==<span class="keyword">null</span>?<span class="string">""</span>:slice.toStringUtf8();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="注册函数-1"><a href="#注册函数-1" class="headerlink" title="注册函数"></a>注册函数</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProstoUDFPlugin</span> <span class="keyword">implements</span> <span class="title">Plugin</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Set&lt;Class&lt;?&gt;&gt; getFunctions() &#123;</span><br><span class="line">        <span class="keyword">return</span> ImmutableSet.&lt;Class&lt;?&gt;&gt;builder()</span><br><span class="line">                .add(PrefixUDF<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">                .<span class="title">add</span>(<span class="title">TunanAggregationUDF</span>.<span class="title">class</span>) // 新加的</span></span><br><span class="line"><span class="class">                .<span class="title">build</span>()</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://yerias.github.io/presto/udf.jpg" alt=""></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/05/03/presto/2/">Presto部署</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-03</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Presto/">Presto</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Presto/">Presto</a></span><div class="content"><h2 id="安装Presto"><a href="#安装Presto" class="headerlink" title="安装Presto"></a>安装Presto</h2><p>1.下载</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">wget</span> <span class="string">https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.200/presto-server-0.200.tar.gz</span></span><br></pre></td></tr></table></figure>

<p>官网下载最新版本: <a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-server" target="_blank" rel="noopener">点我进入官网下载</a> ，注意选择presto-server</p>
<p>2.解压</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">tar</span> <span class="string">-zxvf presto-server-0.200.tar.gz -C /usr/local/</span></span><br></pre></td></tr></table></figure>

<p>/usr/local/presto-server-0.200则为安装目录，另外Presto还需要数据目录，数据目录最好不要在安装目录里面，方便后面Presto的版本升级。</p>
<h2 id="配置Presto"><a href="#配置Presto" class="headerlink" title="配置Presto"></a>配置Presto</h2><p>在安装目录里创建etc目录。这目录会有以下配置：</p>
<ul>
<li>结点属性（Node Properties）：每个结点的环境配置</li>
<li>JVM配置（JVM Config）：Java虚拟机的命令行选项</li>
<li>配置属性（Config Properties）：Persto server的配置</li>
<li>Catelog属性（Catalog Properties）：配置Connector（数据源）</li>
</ul>
<h3 id="结点属性（Node-Properties）"><a href="#结点属性（Node-Properties）" class="headerlink" title="结点属性（Node Properties）"></a>结点属性（Node Properties）</h3><p>结点属性文件etc/node.properties，包含每个结点的配置。一个结点是一个Presto实例。这文件一般是在Presto第一次安装时创建的。以下是最小配置：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">node.environment</span>=<span class="string">production</span></span><br><span class="line"><span class="meta">node.id</span>=<span class="string">ffffffff-ffff-ffff-ffff-ffffffffffff</span></span><br><span class="line"><span class="meta">node.data-dir</span>=<span class="string">/var/presto/data</span></span><br></pre></td></tr></table></figure>

<p><code>node.environment</code>: 环境名字，Presto集群中的结点的环境名字都必须是一样的。<br><code>node.id</code>: 唯一标识，每个结点的标识都必须是为一的。就算重启或升级Presto都必须还保持原来的标识。<br><code>node.data-dir</code>: 数据目录，Presto用它来保存log和其他数据，<strong>建议放在自己定义的目录下</strong>。</p>
<h3 id="JVM配置（JVM-Config）"><a href="#JVM配置（JVM-Config）" class="headerlink" title="JVM配置（JVM Config）"></a>JVM配置（JVM Config）</h3><p>JVM配置文件etc/jvm.config，包含启动Java虚拟机时的命令行选项。格式是每一行是一个命令行选项。此文件数据是由shell解析，所以选项中包含空格或特殊字符会被忽略。</p>
<p>以下是参考配置：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">-server</span></span><br><span class="line"><span class="attr">-Xmx16G</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+UseG1GC</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">G1HeapRegionSize=32M</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+UseGCOverheadLimit</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+ExplicitGCInvokesConcurrent</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+HeapDumpOnOutOfMemoryError</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+ExitOnOutOfMemoryError</span></span><br></pre></td></tr></table></figure>

<p>注意：如果<code>ExitOnOutOfMemoryError</code>报错，注释即可</p>
<p>因为<code>OutOfMemoryError</code>会导致JVM存在不一致状态，所以用heap dump来debug，来找出进程为什么崩溃的原因。</p>
<h3 id="配置属性（Config-Properties）"><a href="#配置属性（Config-Properties）" class="headerlink" title="配置属性（Config Properties）"></a>配置属性（Config Properties）</h3><p>配置属性文件etc/config.properties，包含Presto server的配置。Presto server可以同时为coordinator和worker，但一个大集群里最好就是只指定一台机器为coordinator。<br>以下是coordinator的最小配置：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">coordinator</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">node-scheduler.include-coordinator</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">http-server.http.port</span>=<span class="string">8080</span></span><br><span class="line"><span class="meta">query.max-memory</span>=<span class="string">50GB</span></span><br><span class="line"><span class="meta">query.max-memory-per-node</span>=<span class="string">1GB</span></span><br><span class="line"><span class="meta">discovery-server.enabled</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">discovery.uri</span>=<span class="string">http://example.net:8080</span></span><br></pre></td></tr></table></figure>

<p>以下是worker的最小配置：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">coordinator</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">http-server.http.port</span>=<span class="string">8080</span></span><br><span class="line"><span class="meta">query.max-memory</span>=<span class="string">50GB</span></span><br><span class="line"><span class="meta">query.max-memory-per-node</span>=<span class="string">1GB</span></span><br><span class="line"><span class="meta">discovery.uri</span>=<span class="string">http://example.net:8080</span></span><br></pre></td></tr></table></figure>

<p>如果适用于测试目的，需要将一台机器同时配置为coordinator和worker，则使用以下配置：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">coordinator</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">node-scheduler.include-coordinator</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">http-server.http.port</span>=<span class="string">8080</span></span><br><span class="line"><span class="meta">query.max-memory</span>=<span class="string">2GB</span></span><br><span class="line"><span class="meta">query.max-memory-per-node</span>=<span class="string">512MB</span></span><br><span class="line"><span class="meta">discovery-server.enabled</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">discovery.uri</span>=<span class="string">http://example.net:8080</span></span><br></pre></td></tr></table></figure>

<p><code>coordinator</code>： 是否运行该实例为coordinator（接受client的查询和管理查询执行）。<br><code>node-scheduler.include-coordinator:coordinator</code>: 是否也作为work。对于大型集群来说，在coordinator里做worker的工作会影响查询性能。<br><code>http-server.http.port</code>:指定HTTP端口。Presto使用HTTP来与外部和内部进行交流。<br><code>query.max-memory</code>: 查询能用到的最大总内存<br><code>query.max-memory-per-node</code>: 查询能用到的最大单结点内存<br><code>discovery-server.enabled</code>: Presto使用Discovery服务去找到集群中的所有结点。每个Presto实例在启动时都会在Discovery服务里注册。这样可以简化部署，不需要额外的服务，Presto的coordinator内置一个Discovery服务。也是使用HTTP端口。<br><code>discovery.uri</code>: Discovery服务的URI。将example.net:8080替换为coordinator的host和端口。<strong>这个URI不能以斜杠结尾，这个错误需特别注意，不然会报404错误</strong>。</p>
<p>另外还有以下属性：<br><code>jmx.rmiregistry.port</code>: 指定JMX RMI的注册。JMX client可以连接此端口<br><code>jmx.rmiserver.port</code>: 指定JXM RMI的服务器。可通过JMX监听。</p>
<p>详情请查看<a href="https://prestodb.io/docs/current/admin/resource-groups.html" target="_blank" rel="noopener">Resource Groups</a></p>
<h3 id="Catelog属性（Catalog-Properties）"><a href="#Catelog属性（Catalog-Properties）" class="headerlink" title="Catelog属性（Catalog Properties）"></a>Catelog属性（Catalog Properties）</h3><p>Presto通过connector访问数据。而connector是挂载（mount）在catelog中。connector支持catelog里所有的schema和table。举个例子，Hive connector映射每个Hive数据库到schema，因此Hive connector挂载在hive catelog（所以可以把catelog理解为目录，挂载），而且Hive包含table clicks在数据库web，所以这个table在Presto是hive.web.clicks。<br>Catalog的注册是通过etc/catalog目录下的catalog属性文件。例如，创建etc/catalog/jmx.properties，将jmxconnector挂载在jmx catelog：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">connector.name</span>=<span class="string">jmx</span></span><br></pre></td></tr></table></figure>

<p>查看<a href="https://prestodb.io/docs/current/connector.html" target="_blank" rel="noopener">Connectors</a>查看更多信息。</p>
<p>启动命令：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">bin/launcher</span> <span class="string">start</span></span><br></pre></td></tr></table></figure>

<p>日志在val/log目录下：<br><code>launcher.log</code>: 记录服务初始化情况和一些JVM的诊断。<br><code>server.log: Presto</code>: 的主要日志文件。会自动被压缩。<br><code>http-request.log</code>: 记录HTTP请求。会自动被压缩。</p>
<h2 id="配置MySQL-Connector"><a href="#配置MySQL-Connector" class="headerlink" title="配置MySQL Connector"></a>配置<a href="https://prestodb.io/docs/current/connector/mysql.html" target="_blank" rel="noopener">MySQL Connector</a></h2><p>创建<code>etc/catalog/mysql.properties</code></p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">connector.name</span>=<span class="string">mysql</span></span><br><span class="line"><span class="meta">connection-url</span>=<span class="string">jdbc:mysql://example.net:3306</span></span><br><span class="line"><span class="meta">connection-user</span>=<span class="string">root</span></span><br><span class="line"><span class="meta">connection-password</span>=<span class="string">secret</span></span><br></pre></td></tr></table></figure>

<h2 id="配置Hive-Connector"><a href="#配置Hive-Connector" class="headerlink" title="配置Hive Connector"></a>配置<a href="https://prestodb.io/docs/current/connector/hive.html" target="_blank" rel="noopener">Hive Connector</a></h2><p>创建<code>etc/catalog/hive.properties</code></p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">connector.name</span>=<span class="string">hive-hadoop2</span></span><br><span class="line"><span class="meta">hive.metastore.uri</span>=<span class="string">thrift://example.net:9083</span></span><br><span class="line"><span class="meta">hive.config.resources</span>=<span class="string">/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml</span></span><br></pre></td></tr></table></figure>

<p>还可以在<code>jvm.config</code>中Hadoop的代理用户</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">-DHADOOP_USER_NAME</span>=<span class="string">hdfs_user</span></span><br></pre></td></tr></table></figure>

<h2 id="运行Presto命令行界面"><a href="#运行Presto命令行界面" class="headerlink" title="运行Presto命令行界面"></a>运行Presto命令行界面</h2><p>1.下载 presto-cli-0.200-executable.jar(<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/" target="_blank" rel="noopener">下载最新版</a>),<br>2.修改名字 presto-cli-0.200-executable.jar为 presto<br>3.修改执行权限chmod +x<br>4.运行</p>
<p>指定catelog</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">./presto</span> <span class="string">--server localhost:8080 --catalog hive --schema default</span></span><br></pre></td></tr></table></figure>

<p>不指定catelog，可在命令行使用多个catalog</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">./presto</span> <span class="string">--server localhost:8080</span></span><br></pre></td></tr></table></figure>

<p>注意: JDK必须大于<code>jdk-8u151</code></p>
<p><strong>hive join mysql</strong></p>
<p><code>select * from hive.default.emp a join mysql.tunan.dept b on a.deptno = b.deptno;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"> empno | ename  |    job    | jno  |    date    |  sal   | prize  | deptno | deptno |   dname    | level </span><br><span class="line"><span class="comment">-------+--------+-----------+------+------------+--------+--------+--------+--------+------------+-------</span></span><br><span class="line"> 7369  | SMITH  | CLERK     | 7902 | 1980-12-17 |  800.0 | NULL   | 20     | 20     | RESEARCH   |  1800 </span><br><span class="line"> 7499  | ALLEN  | SALESMAN  | 7698 | 1981-2-20  | 1600.0 |  300.0 | 30     | 30     | SALES      |  1900 </span><br><span class="line"> 7521  | WARD   | SALESMAN  | 7698 | 1981-2-22  | 1250.0 |  500.0 | 10     | 10     | ACCOUNTING |  1700 </span><br><span class="line"> 7566  | JONES  | MANAGER   | 7839 | 1981-4-2   | 2975.0 | NULL   | 10     | 10     | ACCOUNTING |  1700 </span><br><span class="line"> 7654  | MARTIN | SALESMAN  | 7698 | 1981-9-28  | 1250.0 | 1400.0 | 30     | 30     | SALES      |  1900</span><br></pre></td></tr></table></figure>

<p>命令帮助: help</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/05/02/spark/24/">SS+Kafka提交服务器&amp;窗口函数&amp;SS调优</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-02</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>SS+Kafka提交服务器</li>
<li>窗口函数</li>
<li>SS调优</li>
</ol>
<h2 id="SS-Kafka提交服务器"><a href="#SS-Kafka提交服务器" class="headerlink" title="SS+Kafka提交服务器"></a>SS+Kafka提交服务器</h2><p>由于Spark自身没有spark-streaming-kafka的依赖，所以Spark Streaming+Kafka的Application跑在服务器上需要添加spark-streaming-kafka的依赖，共有三种添加依赖的方式</p>
<ol>
<li>直接在IDEA中打胖包，但是服务器上有的东西需要标识为privated，不然依赖重复了，这种方式不推介</li>
<li>提交Application的时候使用–packages参数，格式为: <code>groupId:artifactId:version</code>，这种方式需要在有网络的情况下才能使用</li>
<li>使用–jars 传入依赖，推介，这里有个技巧，可以将需要的 jar 包放在固定目录下，需要传入依赖的时候只需要使用 $(echo /home/hadoop/lib/*.jar | tr ‘ ‘ ‘,’)即可以将目录下的 jar 包全都拼接上去</li>
</ol>
<p>简单的WC案例</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">wc_ss_kafka</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> groupId:<span class="type">String</span> = _</span><br><span class="line">  <span class="keyword">var</span> topic:<span class="type">String</span> = _</span><br><span class="line">  <span class="keyword">var</span> brokers:<span class="type">String</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (args.length&lt;<span class="number">3</span>)&#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Usage: com.tunan.spark.streming.kafka.wc.wc_ss_kafka &lt;groupId&gt; &lt;topic&gt; &lt;brokers&gt;"</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    groupId = args(<span class="number">0</span>)</span><br><span class="line">    topic = args(<span class="number">1</span>)</span><br><span class="line">    brokers = args(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="string">"bootstrap.servers"</span> -&gt; brokers,</span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">"group.id"</span> -&gt; groupId,</span><br><span class="line">      <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"earliest"</span>, <span class="comment">//latest</span></span><br><span class="line">      <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(topic)</span><br><span class="line">    <span class="keyword">val</span> stream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    stream.map(x=&gt;(x.value(),<span class="number">1</span>)).reduceByKey(_+_).foreachRDD(rdd =&gt;&#123;</span><br><span class="line">      rdd.foreach(println)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>作业提交</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">  --class com.tunan.spark.streming.kafka.wc.wc_ss_kafka \</span><br><span class="line">  --master yarn \</span><br><span class="line">  --deploy-mode client \</span><br><span class="line">  --jars $(echo /home/hadoop/lib/*.jar | tr ' ' ',') \</span><br><span class="line">  /home/hadoop/jar/tunan-spark-streaming-kafka-1.0.jar \</span><br><span class="line">  wc_group_id_for_each_stream test hadoop:9090,hadoop:9091,hadoop:9092</span><br></pre></td></tr></table></figure>

<p>查看结果<img src="https://yerias.github.io/spark_img/ss_kafka_submit.png" alt=""></p>
<h2 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h2><p>前面有介绍窗口函数，现在我们来看一下如何使用</p>
<p>在工作中常常有这样的需求:</p>
<ol>
<li>每隔5秒钟统计前10秒钟的数据</li>
<li>每隔1分钟统计前05分钟的数据</li>
</ol>
<p>这类每隔多少统计前多少时间的操作就时窗口操作</p>
<p>我们以一个例子来说明窗口操作。 对之前的单词计数的示例进行扩展，每10秒钟对过去30秒的数据进行wordcount。为此，我们必须在最近30秒的DStream数据中对键值对应用<code>reduceByKey</code>操作。这是通过使用<code>reduceByKeyAndWindow</code>操作完成的。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 每隔10秒统计前30秒的数据</span></span><br><span class="line">stream.map(x=&gt;(x.value(),<span class="number">1</span>)).reduceByKeyAndWindow((a:<span class="type">Int</span>,b:<span class="type">Int</span>)=&gt;a+b,<span class="type">Seconds</span>(<span class="number">30</span>),<span class="type">Seconds</span>(<span class="number">10</span>))</span><br><span class="line">.foreachRDD(rdd =&gt;&#123;</span><br><span class="line">    rdd.foreach(println)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p>一些常见的窗口操作如下表所示。所有这些操作都用到了上述两个参数：<code>windowLength</code>和<code>slideInterval</code>。</p>
<ul>
<li><p><code>window(windowLength, slideInterval)</code></p>
<p>基于源DStream产生的窗口化的批数据计算一个新的DStream</p>
</li>
<li><p><code>countByWindow(windowLength, slideInterval)</code></p>
<p>返回流中元素的一个滑动窗口数</p>
</li>
<li><p><code>reduceByWindow(func, windowLength, slideInterval)</code></p>
<p>返回一个单元素流。利用函数func聚集滑动时间间隔的流的元素创建这个单元素流。函数必须是相关联的以使计算能够正确的并行计算。</p>
</li>
<li><p><code>reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])</code></p>
<p>应用到一个(K,V)对组成的DStream上，返回一个由(K,V)对组成的新的DStream。每一个key的值均由给定的reduce函数聚集起来。注意：在默认情况下，这个算子利用了Spark默认的并发任务数去分组。你可以用numTasks参数设置不同的任务数</p>
</li>
<li><p><code>reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])</code></p>
<p>上述<code>reduceByKeyAndWindow()</code>的更高效的版本，其中使用前一窗口的reduce计算结果递增地计算每个窗口的reduce值。这是通过对进入滑动窗口的新数据进行reduce操作，以及“逆减（inverse reducing）”离开窗口的旧数据来完成的。一个例子是当窗口滑动时对键对应的值进行“一加一减”操作。但是，它仅适用于“可逆减函数（invertible reduce functions）”，即具有相应“反减”功能的减函数（作为参数invFunc）。 像<code>reduceByKeyAndWindow</code>一样，通过可选参数可以配置reduce任务的数量。 请注意，使用此操作必须启用检查点。</p>
</li>
<li><p><code>countByValueAndWindow(windowLength, slideInterval, [numTasks])</code></p>
<p>应用到一个(K,V)对组成的DStream上，返回一个由(K,V)对组成的新的DStream。每个key的值都是它们在滑动窗口中出现的频率。</p>
</li>
</ul>
<h2 id="SS调优"><a href="#SS调优" class="headerlink" title="SS调优"></a>SS调优</h2><p>通过spark-shell运行Spark Streaming + Kafka程序，查看Web UI界面</p>
<p><img src="https://yerias.github.io/spark_img/Web_UI_2.jpg" alt=""></p>
<p>我们需要作业的性能最高，那么需要一个最佳时间</p>
<ol>
<li>在下一个批次启动作业之前一定要运行完前一个批次数据的处理</li>
<li>batch time: 根据需求来定的</li>
</ol>
<p>影响任务运行时长的要素：</p>
<ol>
<li>数据规模(增加kafka分区数==&gt;增加Spark分区==&gt;增加task)</li>
<li>batch time</li>
<li>业务复杂度</li>
</ol>
<p>kafka限速</p>
<p><img src="https://yerias.github.io/spark_img/Web_UI_1.jpg" alt=""></p>
<p>我们看到在设置auto.offset.reset = earliest后，即从头消费，如果累积的数据量特别大，那么在第一次消费的就会撑爆Kafka，必须限制每秒多少条数据</p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Default</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td><code>spark.streaming.kafka.maxRatePerPartition</code></td>
<td>not set</td>
<td>每个Kafka分区读取数据的最大速率</td>
</tr>
</tbody></table>
<p>在设置maxRatePerPartition的值时，数据量=设置的值*分区数*读取时间，加入设置的值为10,分区为3，读取时间为10s，那么每次出来的数据量: 10*3*10=300</p>
<p><strong>优点</strong></p>
<ol>
<li>如果有很多数据量没有处理，并且从头开始，为了防止过载</li>
<li>高峰期限速，防止Kafka处理能力不够挂掉</li>
</ol>
<p><strong>缺点</strong></p>
<ol>
<li>是个固定值 ==&gt; 背压(backpressure 1.5版本引入)</li>
</ol>
<p>背压(backpressure )，在Spark1.5引入，它可以在运行时根据前一个批次数据的运行情况动态调整后续批次读入的数据量</p>
<p>打开参数：spark.streaming.backpressure.enabled </p>
<p>上限参数：spark.streaming.kafka.maxRatePerPartition</p>
<p>初始参数：spark.streaming.backpressure.initialRate</p>
<table>
<thead>
<tr>
<th align="left">Property Name</th>
<th align="left">Default</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>spark.streaming.backpressure.enabled</code></td>
<td align="left">false</td>
<td align="left">使Spark流能够根据当前的批调度延迟和处理时间来控制接收速率，从而使系统接收的速度只取决于系统能够处理的速度。</td>
</tr>
<tr>
<td align="left"><code>spark.streaming.kafka.maxRatePerPartition</code></td>
<td align="left">not set</td>
<td align="left">每个Kafka分区读取数据的最大速率</td>
</tr>
<tr>
<td align="left"><code>spark.streaming.backpressure.initialRate</code></td>
<td align="left">not set</td>
<td align="left">启用背压机制时每个接收器接收第一批数据的初始最大接收速率</td>
</tr>
</tbody></table>
<p>到此，Kafka数据量过载的问题完全解决</p>
<p>最后引入一个关于StreamingContext关闭时的参数</p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Default</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td><code>spark.streaming.stopGracefullyOnShutdown</code></td>
<td>false</td>
<td>如果是“true”，Spark会在JVM关闭时优雅地关闭“StreamingContext”，而不是立即关闭。</td>
</tr>
</tbody></table>
<p>其他调优可参考官网或者<a href="https://blog.csdn.net/Johnson8702/article/details/88944368" target="_blank" rel="noopener">博客</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/05/02/presto/1/">Presto扫盲</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-02</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Presto/">Presto</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Presto/">Presto</a></span><div class="content"><h2 id="Presto简介"><a href="#Presto简介" class="headerlink" title="Presto简介"></a>Presto简介</h2><h3 id="不是什么"><a href="#不是什么" class="headerlink" title="不是什么"></a>不是什么</h3><p>虽然Presto可以解析SQL，但它不是一个标准的数据库。不是MySQL、PostgreSQL或者Oracle的代替品，也不能用来处理在线事务（OLTP）</p>
<h3 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h3><p>Presto通过使用分布式查询，可以快速高效的完成海量数据的查询。作为Hive和Pig的补充，Presto不仅能访问HDFS，也能访问不同的数据源，包括：RDBMS和其他数据源（如Cassandra）。</p>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><img src="https://yerias.github.io/presto/20200502161939.jpg" alt=""></p>
<p>图中各个组件的概念及作用会在下文讲述。</p>
<h3 id="Presto中SQL运行过程：MapReduce-vs-Presto"><a href="#Presto中SQL运行过程：MapReduce-vs-Presto" class="headerlink" title="Presto中SQL运行过程：MapReduce vs Presto"></a>Presto中SQL运行过程：MapReduce vs Presto</h3><p><img src="https://yerias.github.io/presto/3280890894-5af69697e1249_articlex.png" alt=""></p>
<p>使用内存计算，减少与硬盘交互。</p>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol>
<li>Presto与hive对比，都能够处理PB级别的海量数据分析，但Presto是基于内存运算，减少没必要的硬盘IO，所以更快。</li>
<li>能够连接多个数据源，跨数据源连表查，如从hive查询大量网站访问记录，然后从mysql中匹配出设备信息。</li>
<li>部署也比hive简单，因为hive是基于HDFS的，需要先部署HDFS。</li>
</ol>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol>
<li>虽然能够处理PB级别的海量数据分析，但不是代表Presto把PB级别都放在内存中计算的。而是根据场景，如count，avg等聚合运算，是边读数据边计算，再清内存，再读数据再计算，这种耗的内存并不高。但是连表查，就可能产生大量的临时数据，因此速度会变慢，反而hive此时会更擅长。</li>
<li>为了达到实时查询，可能会想到用它直连MySQL来操作查询，这效率并不会提升，瓶颈依然在MySQL，此时还引入网络瓶颈，所以会比原本直接操作数据库要慢。</li>
</ol>
<h2 id="Presto概念"><a href="#Presto概念" class="headerlink" title="Presto概念"></a>Presto概念</h2><h3 id="服务器类型（Server-Types）"><a href="#服务器类型（Server-Types）" class="headerlink" title="服务器类型（Server Types）"></a>服务器类型（Server Types）</h3><p>Presto有两类服务器：coordinator和worker。</p>
<h4 id="Coordinator"><a href="#Coordinator" class="headerlink" title="Coordinator"></a>Coordinator</h4><p>Coordinator服务器是用来解析语句，执行计划分析和管理Presto的worker结点。Presto安装必须有一个Coordinator和多个worker。如果用于开发环境和测试，则一个Presto实例可以同时担任这两个角色。</p>
<p>Coordinator跟踪每个work的活动情况并协调查询语句的执行。 Coordinator为每个查询建立模型，模型包含多个stage，每个stage再转为task分发到不同的worker上执行。</p>
<p>Coordinator与Worker、client通信是通过REST API。</p>
<h4 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h4><p>Worker是负责执行任务和处理数据。Worker从connector获取数据。Worker之间会交换中间数据。Coordinator是负责从Worker获取结果并返回最终结果给client。</p>
<p>当Worker启动时，会广播自己去发现 Coordinator，并告知 Coordinator它是可用，随时可以接受task。</p>
<p>Worker与Coordinator、Worker通信是通过REST API。</p>
<h3 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h3><p>贯穿全文，你会看到一些术语：connector、catelog、schema和table。这些是Presto特定的数据源</p>
<h4 id="Connector"><a href="#Connector" class="headerlink" title="Connector"></a>Connector</h4><p>Connector是适配器，用于Presto和数据源（如Hive、RDBMS）的连接。你可以认为类似JDBC那样，但却是Presto的SPI的实现，使用标准的API来与不同的数据源交互。</p>
<p>Presto有几个内建Connector：JMX的Connector、System Connector（用于访问内建的System table）、Hive的Connector、TPCH（用于TPC-H基准数据）。还有很多第三方的Connector，所以Presto可以访问不同数据源的数据。</p>
<p>每个catalog都有一个特定的Connector。如果你使用catelog配置文件，你会发现每个文件都必须包含connector.name属性，用于指定catelog管理器（创建特定的Connector使用）。一个或多个catelog用同样的connector是访问同样的数据库。例如，你有两个Hive集群。你可以在一个Presto集群上配置两个catelog，两个catelog都是用Hive Connector，从而达到可以查询两个Hive集群。</p>
<h4 id="Catelog"><a href="#Catelog" class="headerlink" title="Catelog"></a>Catelog</h4><p>一个Catelog包含Schema和Connector。例如，你配置JMX的catelog，通过JXM Connector访问JXM信息。当你执行一条SQL语句时，可以同时运行在多个catelog。</p>
<p>Presto处理table时，是通过表的完全限定（fully-qualified）名来找到catelog。例如，一个表的权限定名是hive.test_data.test，则test是表名，test_data是schema，hive是catelog。</p>
<p>Catelog的定义文件是在Presto的配置目录中。</p>
<h4 id="Schema"><a href="#Schema" class="headerlink" title="Schema"></a>Schema</h4><p>Schema是用于组织table。把catelog好schema结合在一起来包含一组的表。当通过Presto访问hive或Mysq时，一个schema会同时转为hive和mysql的同等概念。</p>
<h4 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h4><p>Table跟关系型的表定义一样，但数据和表的映射是交给Connector。</p>
<h3 id="执行查询的模型（Query-Execution-Model）"><a href="#执行查询的模型（Query-Execution-Model）" class="headerlink" title="执行查询的模型（Query Execution Model）"></a>执行查询的模型（Query Execution Model）</h3><h4 id="语句（Statement）"><a href="#语句（Statement）" class="headerlink" title="语句（Statement）"></a>语句（Statement）</h4><p>Presto执行ANSI兼容的SQL语句。当Presto提起语句时，指的就是ANSI标准的SQL语句，包含着列名、表达式和谓词。</p>
<p>之所以要把语句和查询分开说，是因为Presto里，语句只是简单的文本SQL语句。而当语句执行时，Presto则会创建查询和分布式查询计划并在Worker上运行。</p>
<h4 id="查询（Query）"><a href="#查询（Query）" class="headerlink" title="查询（Query）"></a>查询（Query）</h4><p>当Presto解析一个语句时，它将其转换为一个查询，并创建一个分布式查询计划（多个互信连接的stage，运行在Worker上）。如果想获取Presto的查询情况，则获取每个组件（正在执行这语句的结点）的快照。</p>
<p>查询和语句的区别是，语句是存SQL文本，而查询是配置和实例化的组件。一个查询包含：stage、task、split、connector、其他组件和数据源。</p>
<h4 id="Stage"><a href="#Stage" class="headerlink" title="Stage"></a>Stage</h4><p>当Presto执行查询时，会将执行拆分为有层次结构的stage。例如，从hive中的10亿行数据中聚合数据，此时会创建一个用于聚合的根stage，用于聚合其他stage的数据。</p>
<p>层次结构的stage类似一棵树。每个查询都由一个根stage，用于聚合其他stage的数据。stage是Coordinator的分布式查询计划（distributed query plan）的模型，stage不是在worker上运行。</p>
<h4 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h4><p>由于stage不是在worker上运行。stage又会被分为多个task，在不同的work上执行。<br>Task是Presto结构里是“work horse”。一个分布式查询计划会被拆分为多个stage，并再转为task，然后task就运行或处理split。Task有输入和输出，一个stage可以分为多个并行执行的task，一个task可以分为多个并行执行的driver。</p>
<h4 id="Split"><a href="#Split" class="headerlink" title="Split"></a>Split</h4><p>Task运行在split上。split是一个大数据集合中的一块。分布式查询计划最底层的stage是通过split从connector上获取数据，分布式查询计划中间层或顶层则是从它们下层的stage获取数据。</p>
<p>Presto调度查询，coordinator跟踪每个机器运行什么任务，那些split正在被处理。</p>
<h4 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h4><p>Task包含一个或多个并行的driver。Driver在数据上处理，并生成输出，然后由Task聚合，最后传送给stage的其他task。一个driver是Operator的序列。driver是Presto最最低层的并行机制。一个driver有一个输出和一个输入。</p>
<h4 id="Operator"><a href="#Operator" class="headerlink" title="Operator"></a>Operator</h4><p>Operator用来消费，传送和生产数据。如一个Operator从connector中扫表获取数据，然后生产数据给其他Operator消费。一个过滤Operator消费数据，并应用谓词，最后生产出子集数据。</p>
<h4 id="Exchange"><a href="#Exchange" class="headerlink" title="Exchange"></a>Exchange</h4><p>Exchange在Presto结点的不同stage之间传送数据。Task生产和消费数据是通过Exchange客户端。</p>
<hr>
<p>参考：<a href="https://prestodb.io/docs/current/overview/concepts.html" target="_blank" rel="noopener">https://prestodb.io/docs/current/overview/concepts.html</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/04/29/spark/23/">Spark 读写压缩文件的一次简单尝试</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-29</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>我认为以节省存储空间为角度出发，Spark作业中的读写压缩文件是必不可少的话题，当然这在MR作业中也有体现和实际解决这种问题，现在我们就要在Spark中解决这种问题。</p>
<p>如果需要安装Lzo可以看我的其他<a href="https://yerias.github.io/2018/10/15/hadoop/13/">文章</a></p>
<p>源文件是一份access的原始数据</p>
<p><img src="https://yerias.github.io/spark_img/access.log.png" alt=""></p>
<p>我们在上传到服务器上的时候，使用lzop命令压缩该文件，得到压缩后的文件，上传HDFSF并对该文件创建索引，得到我们代码中需要处理的源文件</p>
<p><img src="https://yerias.github.io/spark_img/compress-1.png" alt=""></p>
<p>需要注意的是未压缩的文件是1G，压缩后为314M</p>
<p>下图是我们想要做的事情</p>
<p><img src="https://yerias.github.io/spark_img/format%E8%AF%BB%E5%86%99.jpg" alt=""></p>
<p><strong>各种压缩格式的性能比较</strong></p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>工具</th>
<th>算法</th>
<th>扩展名</th>
<th>是否支持分割</th>
<th>Hadoop编码/解码器</th>
<th>hadoop自带</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>N/A</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>No</td>
<td>org.apache.hadoop.io.compress.DefalutCodec</td>
<td>是</td>
</tr>
<tr>
<td>gzip</td>
<td>gzip</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>No</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
<td>是</td>
</tr>
<tr>
<td>bzip2</td>
<td>bzip2</td>
<td>bzip2</td>
<td>.bz2</td>
<td>yes</td>
<td>org.apache.hadoop.io.compress.Bzip2Codec</td>
<td>是</td>
</tr>
<tr>
<td>LZO</td>
<td>Lzop</td>
<td>LZO</td>
<td>.lzo</td>
<td>yes(建索引)</td>
<td>com.hadoop.compression.lzo.LzoCodec</td>
<td>是</td>
</tr>
<tr>
<td>LZ4</td>
<td>N/A</td>
<td>LZ4</td>
<td>.lz4</td>
<td>No</td>
<td>org.apache.hadoop.io.compress.Lz4Codec</td>
<td>否</td>
</tr>
<tr>
<td>Snappy</td>
<td>N/A</td>
<td>Snappy</td>
<td>.snappy</td>
<td>No</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
<td>否</td>
</tr>
</tbody></table>
<p>压缩比：Snappy&lt;LZ4&lt;LZO&lt;GZIP&lt;BZIP2</p>
<p><strong>各种压缩格式的性能优缺点</strong></p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td>lzo</td>
<td>优点：压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便<br/>缺点：不支持split</td>
</tr>
<tr>
<td>snappy</td>
<td>优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便<br/>缺点：压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）</td>
</tr>
<tr>
<td>gzip</td>
<td>优点：压缩速度快；支持hadoop native库<br/>缺点：不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令</td>
</tr>
<tr>
<td>bzip2</td>
<td>优点：支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便<br/>缺点：压缩/解压速度慢；不支持native</td>
</tr>
</tbody></table>
<h3 id="读LZO文件写HDFS"><a href="#读LZO文件写HDFS" class="headerlink" title="读LZO文件写HDFS"></a>读LZO文件写HDFS</h3><p>需要注意的是在Spark中读取HDFS上的压缩文件，需要使用newAPIHadoopFile接口，并且传入LzoTextInputFormat，这个依赖不好解决，仓库是twitter的，下载不了，最好的办法是去github下载源码，install到本地</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.hadoop.gplcompression<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-lzo<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.4.20<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>读取文件的具体代码</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> linesRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.newAPIHadoopFile(in, classOf[<span class="type">LzoTextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>])  .map(x =&gt; x._2.toString)</span><br></pre></td></tr></table></figure>

<p>现在我们已经读进来了，可以实现我们的业务逻辑，那么最后在Spark Core中如何写出去呢? 只需要使用saveAsTextFile</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">etl(linesRDD,hashMap)  .filter(x =&gt; x.responseSize != <span class="number">0</span>).saveAsTextFile(out)</span><br></pre></td></tr></table></figure>

<h3 id="读LZO文件写HDFS也为LZO文件"><a href="#读LZO文件写HDFS也为LZO文件" class="headerlink" title="读LZO文件写HDFS也为LZO文件"></a>读LZO文件写HDFS也为LZO文件</h3><p>读LZO文件上面的方法同样适用，而写为LZO文件只需要在saveAsTextFile中指定LzopCodec即可，适用于所有的格式压缩</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">etl(linesRDD,hashMap)  .filter(x =&gt; x.responseSize != <span class="number">0</span>).saveAsTextFile(out,classOf[<span class="type">LzopCodec</span>])</span><br></pre></td></tr></table></figure>

<h3 id="读LZO文件写MYSQL"><a href="#读LZO文件写MYSQL" class="headerlink" title="读LZO文件写MYSQL"></a>读LZO文件写MYSQL</h3><p>读写表和读写文件不同，读写表更适合用Spark SQL来实现，现在我们的表的字段非常多，不适用于使用tuple的实现方式，从而自定义外部数据源，在TableScan中也是实现与上面相同代码，就能将lzo文件分片读写</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">    logError(<span class="string">"进入buildScan方法"</span>)</span><br><span class="line">    <span class="comment">// 使用RDD拿到Lzo本文内容</span></span><br><span class="line">    <span class="keyword">val</span> lines = sqlContext.sparkContext.newAPIHadoopFile(path, classOf[<span class="type">LzoTextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>])</span><br><span class="line">    .map(_._2.toString)</span><br><span class="line">    <span class="comment">// 拿到响应的schema信息</span></span><br><span class="line">    <span class="keyword">val</span> fields = schema.fields</span><br><span class="line">    <span class="comment">// 拿到每行数据，做简单处理，返回RDD[Row]</span></span><br><span class="line">    lines.map(_.split(<span class="string">","</span>).map(_.trim)).map(_.zipWithIndex.map &#123;</span><br><span class="line">        <span class="keyword">case</span> (value, index) =&gt;</span><br><span class="line">        <span class="type">Utils</span>.caseTo(value, fields(index).dataType)</span><br><span class="line">    &#125;).map(x =&gt; <span class="type">Row</span>.fromSeq(x))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我在这里尝试了压缩写和普通写，都可以查询到数据</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">        properties.put(<span class="string">"user"</span>,<span class="string">"root"</span>)</span><br><span class="line">        properties.put(<span class="string">"password"</span>,<span class="string">"root"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> url = <span class="string">"jdbc:mysql://hadoop:3306/tunan?useUnicode=true&amp;characterEncoding=utf-8"</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!flat)&#123;</span><br><span class="line">            <span class="comment">// 保存到MySQL</span></span><br><span class="line">            result.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">              .jdbc(url,<span class="string">"domain_group"</span>,properties)</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="comment">// 保存到MySQL,压缩格式为lzo</span></span><br><span class="line">            result.write.format(<span class="string">"orc"</span>).option(<span class="string">"compression"</span>,<span class="string">"lzo"</span>).mode(<span class="string">"overwrite"</span>)</span><br><span class="line">              .jdbc(url,<span class="string">"domain_group"</span>,properties)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>

<p>查询：</p>
<p><img src="https://yerias.github.io/spark_img/compress-2.png" alt=""></p>
<h3 id="读LZO文件写HIVE"><a href="#读LZO文件写HIVE" class="headerlink" title="读LZO文件写HIVE"></a>读LZO文件写HIVE</h3><p>读的方式也是一样的，使用外部数据源的方式直接得到DataFrame，在写的时候指定了存储格式，而不指定压缩格式的话，会默认指定压缩格式为Snappy</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">textDf.write.format(<span class="string">"parquet"</span>).mode(<span class="string">"overwrite"</span>).saveAsTable(<span class="string">"store_format.parquet_tb"</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://yerias.github.io/spark_img/compress-3.png" alt=""></p>
<h3 id="读LZO文件压缩写HIVE"><a href="#读LZO文件压缩写HIVE" class="headerlink" title="读LZO文件压缩写HIVE"></a>读LZO文件压缩写HIVE</h3><p>读Lzo的方式如上，在这里我们尝试了几种格式的压缩写</p>
<ol>
<li><p>指定了存储格式为parquet，压缩格式为lzo</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">textDf.write.format(<span class="string">"parquet"</span>).option(<span class="string">"compression"</span>,<span class="string">"lzo"</span>).mode(<span class="string">"overwrite"</span>).saveAsTable(<span class="string">"store_format.parquet_tb"</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://yerias.github.io/spark_img/compress-4.png" alt=""></p>
<p>可以看得到压缩格式和存储格式发生了变化，我们继续查表</p>
<p><img src="https://yerias.github.io/spark_img/compress-5.png" alt=""></p>
</li>
<li><p>现在我们将压缩格式改为orc，存储格式还是使用lzo</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">textDf.write.format(<span class="string">"orc"</span>).option(<span class="string">"compression"</span>,<span class="string">"lzo"</span>).mode(<span class="string">"overwrite"</span>).saveAsTable(<span class="string">"store_format.parquet_tb"</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://yerias.github.io/spark_img/compress-6.png" alt=""></p>
<p>我们发现orc和lzo格式并不能一起使用，查出来的是无效的结果</p>
</li>
<li><p>继续使用压缩格式为orc，存储格式改为snappy</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">textDf.write.format(<span class="string">"orc"</span>).option(<span class="string">"compression"</span>,<span class="string">"snappy"</span>).mode(<span class="string">"overwrite"</span>).saveAsTable(<span class="string">"store_format.parquet_tb"</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://yerias.github.io/spark_img/compress-7.png" alt=""></p>
<p>查询结果：</p>
<p><img src="https://yerias.github.io/spark_img/compress-5.png" alt=""></p>
</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol>
<li>使用Lzo压缩的压缩比相比于Snappy较高</li>
<li>无论哪种压缩格式在MySQL中都是无效的</li>
<li>Parquet可以和Snappy或者Lzo搭配使用，ORC可以和Snappy或者Bzip搭配使用</li>
<li>ORC不能和Lzo搭配使用</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/04/29/kafka/3/">kafka eagle安装部署</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-29</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Kakfa/">Kakfa</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Kakfa/">Kakfa</a></span><div class="content"><h5 id="1-kafka-zookeeper准备"><a href="#1-kafka-zookeeper准备" class="headerlink" title="1.kafka+zookeeper准备"></a>1.kafka+zookeeper准备</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">这里假设你已经把kafka+zookeeper安装完成，但是需要注意的几点是：</span><br><span class="line">1.kafka需要开启JMX端口</span><br><span class="line">    找到kafka安装路径，进入到bin文件夹，修改下面的地方。</span><br><span class="line">    vi kafka-server-start.sh</span><br><span class="line">...</span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$KAFKA_HEAP_OPTS</span>"</span> = <span class="string">"x"</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">"-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70"</span></span><br><span class="line">    <span class="built_in">export</span> JMX_PORT=<span class="string">"9999"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">...</span><br><span class="line">    参考链接：[kafka添加jmx端口]:https://ke.smartloli.org/3.Manuals/11.Metrics.html</span><br><span class="line"></span><br><span class="line">2.了解kafka在zookeeper配置</span><br><span class="line">    需要查看kafka的server.properties配置</span><br><span class="line">    找到zookeeper.connect此项配置，这个是要配置到eagle里面的</span><br><span class="line">    此处假设zookeeper.connect=192.168.18.11:2181,192.168.18.12:2181,192.168.18.13:2181</span><br><span class="line">    ！！！PS:此处踩了坑，如果说这里的zookeeper地址后面加了其他路径，在kafka-eagle里面也要配置，</span><br><span class="line">    否则在kafka-eagle的Dashboard中无法读取到kafka的信息。比如我们有人安装的kafka集群里面就有    </span><br><span class="line">    192.168.18.11:2181/kafka1或者192.168.18.11:2181/kafka2这种地址。</span><br><span class="line">    如果你在安装kafka的时候没有配置多余路径，这样是最好的，如果有一定要加上。</span><br><span class="line">3.连通性测试</span><br><span class="line">  安装kafka-eagle的服务器，一定要提前测试是否能连接kafka注册的zookeeper端口</span><br><span class="line">  telnet 端口进行测试</span><br></pre></td></tr></table></figure>

<h5 id="2-JDK环境准备"><a href="#2-JDK环境准备" class="headerlink" title="2.JDK环境准备"></a>2.JDK环境准备</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">此处就忽略不说了，kafka既然会安装，也是依赖JDK环境的。版本没要求，但是最好是1.7以上。</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$JAVA_HOME</span></span><br><span class="line">测试一下JDK环境是否安装成功</span><br></pre></td></tr></table></figure>

<h5 id="3-开始安装kafka-eagle"><a href="#3-开始安装kafka-eagle" class="headerlink" title="3.开始安装kafka-eagle"></a>3.开始安装kafka-eagle</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">1.下载安装包</span><br><span class="line">软件安装目录建议按照自己的规范来，以后好找</span><br><span class="line"><span class="built_in">cd</span> /tmp</span><br><span class="line">wget https://github.com/smartloli/kafka-eagle-bin/archive/v1.2.2.tar.gz</span><br><span class="line">tar zxf v1.2.2.tar.gz</span><br><span class="line"><span class="built_in">cd</span> kafka-eagle-bin-1.2.2</span><br><span class="line">tar zxf kafka-eagle-web-1.2.2-bin.tar.gz -C /data/app/</span><br><span class="line"><span class="built_in">cd</span> /data/app</span><br><span class="line">mv kafka-eagle-web-1.2.2 kafka-eagle</span><br><span class="line"></span><br><span class="line">2.环境配置</span><br><span class="line">vi /etc/profile</span><br><span class="line"><span class="built_in">export</span> KE_HOME=/data/app/kafka-eagle</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$KE_HOME</span>/bin</span><br><span class="line">ps:此处的KE_HOME按照自己实际安装的目录来，我安装在/data/app/kafka-eagle下面</span><br><span class="line">如果你是安装的其他目录，别忘了修改。</span><br><span class="line"></span><br><span class="line">3.配置修改</span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;KE_HOME&#125;</span>/conf</span><br><span class="line">vi system-config.properties</span><br><span class="line"></span><br><span class="line"><span class="comment"># multi zookeeper&amp;kafka cluster list -- The client connection address of the Zookeeper cluster is set here</span></span><br><span class="line"><span class="comment">#如果只有一个集群的话，就写一个cluster1就行了</span></span><br><span class="line">kafka.eagle.zk.cluster.alias=cluster1,cluster2   </span><br><span class="line"><span class="comment">#这里填上刚才上准备工作中的zookeeper.connect地址</span></span><br><span class="line">cluster1.zk.list=192.168.18.11:2181,192.168.18.12:2181,192.168.18.13:2181</span><br><span class="line"><span class="comment">#如果多个集群，继续写，如果没有注释掉</span></span><br><span class="line">cluster2.zk.list=192.168.18.21:2181,192.168.18.22:2181,192.168.18.23:2181/kafka </span><br><span class="line"></span><br><span class="line"><span class="comment"># zk limit -- Zookeeper cluster allows the number of clients to connect to</span></span><br><span class="line">kafka.zk.limit.size=25</span><br><span class="line"></span><br><span class="line"><span class="comment"># kafka eagel webui port -- WebConsole port access address</span></span><br><span class="line">kafka.eagle.webui.port=8048     <span class="comment">###web界面地址端口</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kafka offset storage -- Offset stored in a Kafka cluster, if stored in the zookeeper, you can not use this option</span></span><br><span class="line">kafka.eagle.offset.storage=kafka</span><br><span class="line"></span><br><span class="line"><span class="comment"># delete kafka topic token -- Set to delete the topic token, so that administrators can have the right to delete</span></span><br><span class="line">kafka.eagle.topic.token=keadmin</span><br><span class="line"></span><br><span class="line"><span class="comment"># kafka sasl authenticate, current support SASL_PLAINTEXT</span></span><br><span class="line"><span class="comment">#如果kafka开启了sasl认证，需要在这个地方配置sasl认证文件</span></span><br><span class="line">kafka.eagle.sasl.enable=<span class="literal">false</span></span><br><span class="line">kafka.eagle.sasl.protocol=SASL_PLAINTEXT</span><br><span class="line">kafka.eagle.sasl.mechanism=PLAIN</span><br><span class="line">kafka.eagle.sasl.client=/data/kafka-eagle/conf/kafka_client_jaas.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">#下面两项是配置数据库的，默认使用sqlite，如果量大，建议使用mysql，这里我使用的是sqlit</span></span><br><span class="line"><span class="comment">#如果想使用mysql建议在文末查看官方文档</span></span><br><span class="line"><span class="comment"># Default use sqlite to store data</span></span><br><span class="line">kafka.eagle.driver=org.sqlite.JDBC</span><br><span class="line"><span class="comment"># It is important to note that the '/hadoop/kafka-eagle/db' path must exist.</span></span><br><span class="line">kafka.eagle.url=jdbc:sqlite:/data/app/kafka-eagle/db/ke.db   <span class="comment">#这个地址，按照安装目录进行配置</span></span><br><span class="line">kafka.eagle.username=root</span><br><span class="line">kafka.eagle.password=smartloli</span><br><span class="line"></span><br><span class="line"><span class="comment"># &lt;Optional&gt; set mysql address</span></span><br><span class="line"><span class="comment">#kafka.eagle.driver=com.mysql.jdbc.Driver</span></span><br><span class="line"><span class="comment">#kafka.eagle.url=jdbc:mysql://127.0.0.1:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span></span><br><span class="line"><span class="comment">#kafka.eagle.username=root</span></span><br><span class="line"><span class="comment">#kafka.eagle.password=smartloli</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">如果开启了sasl认证，需要自己去修改kafka-eagle目录下的conf/kafka_client_jaas.conf</span><br><span class="line">此处不多说</span><br></pre></td></tr></table></figure>

<h5 id="4-启动kafka-eagle"><a href="#4-启动kafka-eagle" class="headerlink" title="4.启动kafka-eagle"></a>4.启动kafka-eagle</h5><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">cd $&#123;KE_HOME&#125;/bin</span><br><span class="line">chmod +x ke.sh</span><br><span class="line">./ke.sh start</span><br><span class="line">查看日志是否出问题</span><br><span class="line">tailf ../<span class="built_in">log</span>/<span class="built_in">log</span>.<span class="built_in">log</span></span><br><span class="line">如果没问题，则直接登录</span><br><span class="line">http:<span class="comment">//host:8048/ke</span></span><br><span class="line">默认用户名:admin</span><br><span class="line">默认密码:<span class="number">12345</span></span><br><span class="line">如果进入到一下界面，就说明你安装成功了！</span><br></pre></td></tr></table></figure>

<p><img src="https://yerias.github.io/kafka/eagle.jpg" alt="eagle"></p>
<h5 id="5-问题汇总"><a href="#5-问题汇总" class="headerlink" title="5.问题汇总"></a>5.问题汇总</h5><ol>
<li><p>ZKPoolUtils.localhost-startStop-1 - ERROR - Unable to connect to zookeeper server within timeout: 100000</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">这个是网络问题，在kafka-eagle服务器上自己测试一下能否能telnet通配置的zk地址。</span><br><span class="line">cat system-config.properties|grep cluster1.zk.<span class="built_in">list</span></span><br><span class="line">测试这个配置的端口</span><br></pre></td></tr></table></figure>
</li>
<li><p>ERROR - Get kafka consumer has error,msg is No resolvable bootstrap urls given in bootstrap.servers</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">这个问题是配置的zk地址有问题，看看kafka配置的zk地址</span><br><span class="line">跟自己在eagle上配置的地址是否相同，有没有少目录或者端口配置。</span><br><span class="line">可以看看文章开头，kafka+zookeeper准备-了解kafka在zookeeper配置</span><br></pre></td></tr></table></figure>
</li>
<li><p>zookeeper state changed (AuthFailed)</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">这个问题有两种情况</span><br><span class="line">1.认证的问题，确认你配置的认证文件是否正确</span><br><span class="line">2<span class="selector-class">.zk</span>地址问题，看看<span class="selector-tag">kafka</span>配置的<span class="selector-tag">zk</span>地址，跟自己在<span class="selector-tag">eagle</span>上配置的地址是否相同</span><br><span class="line">可以看看文章开头，<span class="selector-tag">kafka</span>+<span class="selector-tag">zookeeper</span>准备<span class="selector-tag">-</span>了解<span class="selector-tag">kafka</span>在<span class="selector-tag">zookeeper</span>配置</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h5 id="6-kafka-eagle使用"><a href="#6-kafka-eagle使用" class="headerlink" title="6.kafka-eagle使用"></a>6.kafka-eagle使用</h5><p>kafka-eagle官方文档: <a href="https://ke.smartloli.org/2.Install/2.Installing.html" target="_blank" rel="noopener">https://ke.smartloli.org/2.Install/2.Installing.html</a></p>
<p>kafka-eagle下载地址: <a href="http://download.smartloli.org/" target="_blank" rel="noopener">http://download.smartloli.org/</a></p>
<p>kafka-eagle git地址: <a href="https://github.com/smartloli/kafka-eagle" target="_blank" rel="noopener">https://github.com/smartloli/kafka-eagle</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/04/28/error/8/">FileNotFountException: file:/home/hadoop/lib/tunan-spark-core-1.0.jar!/ip2region.db</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-28</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Error/">Error</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Error/">Error</a></span><div class="content"><p>以后看到标题这种Error，先别管其他的，首先看看代码中有没有把Master注释掉，不然jar包中的文件永远到不了服务器的环境中去，就算把文件在服务器上的路径写死都没用。</p>
<p>由于Spark不会自动清理–files和–jars传到服务器中的文件，因此只要我们传上去的jar包运行通一次，后面不管代码中有没有指定Master，都能找到服务器中的文件。</p>
<p>报错图示1：</p>
<p><img src="https://yerias.github.io/error/notfount1.png" alt="notfount1"></p>
<p>报错图示2：</p>
<p><img src="https://yerias.github.io/error/notfount2.png" alt="notfount2"></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/04/27/error/7/">error: object hadoop is not a member of packee com</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-27</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Error/">Error</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Error/">Error</a></span><div class="content"><p>这个问题是在Spark读取Lzo压缩文件的时候碰见的，Spark读取Lzo压缩文件的时候，就算文件添加了索引，也不能分片，原因是要在获取文件的时候使用newAPIHadoopFile算子读取文件获取rdd</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.newAPIHadoopFile(in, classOf[<span class="type">LzoTextInputFormat</span>], classOf[<span class="type">LongWritable</span>],</span><br><span class="line">                              classOf[<span class="type">Text</span>]).map(x =&gt; x._2.toString)</span><br></pre></td></tr></table></figure>

<p>而这里最重要的就是LzoTextInputFormat类，这个类是Twitter的，但是添加了Twitter<a href="https://maven.twttr.com/" target="_blank" rel="noopener">仓库</a>后，能进仓库，但是不能下载，最后向朋友要了jar包，关键在于只给了我hadoop-lzo-0.4.20.jar，我通过添加外部依赖的方式，加到了项目里，运行的时候就报了error: object hadoop is not a member of packee com的错误</p>
<p><img src="E:%5Chexo%5Cyeriasblog%5Cthemes%5Cmelody%5Csource%5Cerror%5Chadoop-lzo.png" alt="hadoop-lzo"></p>
<p>其原因是只有jar包，没有pom文件，最后将jar包和pom文件一起放入maven仓库中，解决问题</p>
<hr>
<p><strong>终极解决办法是在github上下载源码，通过编译maven install到本地仓库</strong></p>
</div><hr></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/2/">&gt;&gt;</a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>