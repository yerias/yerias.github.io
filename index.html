<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="BigData Developer"><meta name="keywords" content="yerias,TUNANのBlog,BigData"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017 | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">133</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">30</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">26</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">TUNANのBlog</div><div id="site-sub-title">感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017</div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2020/05/06/spark/25/">Spark各个版本特性</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-06</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><p>参考博客：<a href="https://www.maxinhong.com/2020/04/03/68.spark各个版本特性/#more" target="_blank" rel="noopener">https://www.maxinhong.com/2020/04/03/68.spark%E5%90%84%E4%B8%AA%E7%89%88%E6%9C%AC%E7%89%B9%E6%80%A7/#more</a></p>
<hr>
<h1 id="各个版本特性（官方文档）"><a href="#各个版本特性（官方文档）" class="headerlink" title="各个版本特性（官方文档）"></a>各个版本特性（官方文档）</h1><p><a href="https://spark.apache.org/releases/" target="_blank" rel="noopener">https://spark.apache.org/releases/</a><br><a href="https://spark.apache.org/news/index.html" target="_blank" rel="noopener">https://spark.apache.org/news/index.html</a></p>
<h2 id="Spark-0-6-x"><a href="#Spark-0-6-x" class="headerlink" title="Spark 0.6.x"></a>Spark 0.6.x</h2><ul>
<li>Standalone部署模式进行了简化</li>
</ul>
<h2 id="Spark-0-7"><a href="#Spark-0-7" class="headerlink" title="Spark 0.7"></a>Spark 0.7</h2><ul>
<li>Python API</li>
<li>增加Spark Streaming</li>
<li>支持maven build</li>
</ul>
<h2 id="Spark-0-8"><a href="#Spark-0-8" class="headerlink" title="Spark 0.8"></a>Spark 0.8</h2><ul>
<li>支持MLlib库</li>
<li>hadoop yarn正式支持</li>
</ul>
<h2 id="Spark-0-9"><a href="#Spark-0-9" class="headerlink" title="Spark 0.9"></a>Spark 0.9</h2><ul>
<li>用SparkConf类来配置SparkContext</li>
<li>spark streaming正式版发布</li>
<li>GraphX的测试版出现</li>
<li>mllib库升级，支持python</li>
<li>core升级</li>
</ul>
<h2 id="Spark-1-0"><a href="#Spark-1-0" class="headerlink" title="Spark 1.0"></a>Spark 1.0</h2><ul>
<li>提出spark-submit脚本和history-server</li>
<li>yarn安全模式整合</li>
<li>spark sql被提出</li>
<li>java8的支持</li>
</ul>
<h2 id="Spark-1-1"><a href="#Spark-1-1" class="headerlink" title="Spark 1.1"></a>Spark 1.1</h2><ul>
<li>spark增强了磁盘（非内存）的排序的速率</li>
</ul>
<h2 id="Spark-1-2"><a href="#Spark-1-2" class="headerlink" title="Spark 1.2"></a>Spark 1.2</h2><ul>
<li>shuffle大升级</li>
<li>Graphx正式版发布</li>
</ul>
<h2 id="Spark-1-3"><a href="#Spark-1-3" class="headerlink" title="Spark 1.3"></a>Spark 1.3</h2><ul>
<li>新增DataFrame API</li>
<li>Spark SQL正式脱离alpha版本</li>
</ul>
<h2 id="Spark-1-4"><a href="#Spark-1-4" class="headerlink" title="Spark 1.4"></a>Spark 1.4</h2><ul>
<li>正式引入SparkR</li>
<li>Spark Core为应用提供了REST API来获取各种信息</li>
</ul>
<h2 id="Spark-1-5"><a href="#Spark-1-5" class="headerlink" title="Spark 1.5"></a>Spark 1.5</h2><ul>
<li>Spark1.5重点是对性能的提升，引入钨丝项目，该项目通过对几个底层框架的重构进一步优化Spark性能</li>
</ul>
<h2 id="Spark-1-6"><a href="#Spark-1-6" class="headerlink" title="Spark 1.6"></a>Spark 1.6</h2><ul>
<li>新增Dataset API</li>
</ul>
<h2 id="Spark-2-0"><a href="#Spark-2-0" class="headerlink" title="Spark 2.0"></a>Spark 2.0</h2><ul>
<li>用sparksession实现hivecontext和sqlcontext统一</li>
<li>合并dataframe和datasets</li>
</ul>
<h2 id="Spark-2-1"><a href="#Spark-2-1" class="headerlink" title="Spark 2.1"></a>Spark 2.1</h2><ul>
<li>提升ORC格式文件的读写性能</li>
</ul>
<h2 id="Spark-2-2"><a href="#Spark-2-2" class="headerlink" title="Spark 2.2"></a>Spark 2.2</h2><ul>
<li>Structured Streaming的生产环境支持已经就绪</li>
</ul>
<h2 id="Spark-2-3"><a href="#Spark-2-3" class="headerlink" title="Spark 2.3"></a>Spark 2.3</h2><ul>
<li>Structured Streaming 引入了低延迟的连续处理</li>
<li>支持 stream-to-stream joins</li>
</ul>
<h2 id="Spark-2-4"><a href="#Spark-2-4" class="headerlink" title="Spark 2.4"></a>Spark 2.4</h2><ul>
<li>Scala 2.12</li>
<li>添加了35个高阶函数</li>
</ul>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/05/04/presto/3/">PrestoUDF开发</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-04</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Presto/">Presto</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Presto/">Presto</a></span><div class="content"><h2 id="Presto函数"><a href="#Presto函数" class="headerlink" title="Presto函数"></a>Presto函数</h2><p>在 Presto 中，函数大体分为三种：scalar，aggregation 和 window 类型。分别如下：</p>
<p>1）scalar标量函数，简单来说就是 Java 中的一个静态方法，本身没有任何状态。</p>
<p>2）aggregation累积状态的函数，或聚集函数，如count，avg。如果只是单节点，单机状态可以直接用一个变量存储即可，但是presto是分布式计算引擎，状态数据会在多个节点之间传输，因此状态数据需要被序列化成 Presto 的内部格式才可以被传输。</p>
<p>3）window 窗口函数，如同sparkSQL中的窗口函数类似</p>
<p>官网地址：<a href="https://prestodb.github.io/docs/current/develop/functions.html" target="_blank" rel="noopener">https://prestodb.github.io/docs/current/develop/functions.html</a></p>
<h2 id="自定义Scalar函数的实现"><a href="#自定义Scalar函数的实现" class="headerlink" title="自定义Scalar函数的实现"></a>自定义Scalar函数的实现</h2><h3 id="定义一个java类"><a href="#定义一个java类" class="headerlink" title="定义一个java类"></a>定义一个java类</h3><ol>
<li><p>用 @ScalarFunction 的 Annotation 标记实现业务逻辑的静态方法。</p>
</li>
<li><p>用 @Description 描述函数的作用，这里的内容会在 SHOW FUNCTIONS 中显示。</p>
</li>
<li><p>用@SqlType 标记函数的返回值类型，如返回字符串，因此是 StandardTypes.VARCHAR。</p>
</li>
<li><p>Java 方法的返回值必须使用 Presto 内部的序列化方式，因此字符串类型必须返回 Slice， 使用 Slices.utf8Slice 方法可以方便的将 String 类型转换成 Slice 类型</p>
</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunna.spark.presto.udf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.function.Description;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.function.ScalarFunction;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.function.SqlType;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.type.StandardTypes;</span><br><span class="line"><span class="keyword">import</span> io.airlift.slice.Slice;</span><br><span class="line"><span class="keyword">import</span> io.airlift.slice.Slices;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PrefixUDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Description</span>(<span class="string">"输入的数据加上前缀"</span>)      <span class="comment">//描述</span></span><br><span class="line">    <span class="meta">@ScalarFunction</span>(<span class="string">"tunan_prefix"</span>)       <span class="comment">//方法名称</span></span><br><span class="line">    <span class="meta">@SqlType</span>(StandardTypes.VARCHAR)       <span class="comment">//返回类型</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Slice <span class="title">prefix</span><span class="params">(@SqlType(StandardTypes.VARCHAR)</span>Slice input)</span>&#123;</span><br><span class="line">        <span class="comment">// Slices.utf8Slice 方法可以方便的将 String 类型转换成 Slice 类型</span></span><br><span class="line">        <span class="keyword">return</span> Slices.utf8Slice(<span class="string">"tunan_prefix_"</span>+input.toStringUtf8());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Presto插件机制"><a href="#Presto插件机制" class="headerlink" title="Presto插件机制"></a>Presto插件机制</h3><p>presto不能像hive那样配置自定义的udf，要采用这种插件机制实现。Presto 的插件(Plugin)机制，是 Presto 能够整合多种数据源的核心。通过实现不同的 Plugin，Presto 允许用户在不同类型的数据源之间进行 JOIN 等计算。Presto 内部的所有数据源都是通过插件机制实现， 例如 MySQL、Hive、HBase等。Presto 插件机制不仅通过加载 Connector 来实现不同数据源的访问，还通过加载 FunctionFactory 来实现 UDF 的加载。 Presto 的 Plugin 遵循 Java 中的 ServiceLoader 规范， 实现非常简单。</p>
<p>实现一个plugin接口如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunna.spark.presto.udf;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.Plugin;</span><br><span class="line"><span class="keyword">import</span> com.google.common.collect.ImmutableSet;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Set;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProstoUDFPlugin</span> <span class="keyword">implements</span> <span class="title">Plugin</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Set&lt;Class&lt;?&gt;&gt; getFunctions() &#123;</span><br><span class="line">        <span class="keyword">return</span> ImmutableSet.&lt;Class&lt;?&gt;&gt;builder()</span><br><span class="line">                .add(PrefixUDF<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">                .<span class="title">build</span>()</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="注册函数"><a href="#注册函数" class="headerlink" title="注册函数"></a>注册函数</h3><p>在resources下创建META-INF/services目录，创建文件com.facebook.presto.spi.Plugin，拷贝Plugin的全限定名</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">com.tunna.spark.presto.udf.ProstoUDFPlugin</span><br></pre></td></tr></table></figure>

<p>最后在presto的plugin目录下创建我们自己的目录，并且打包上传到我们自己的目录下，需要重启presto才能将jar中的自定义函数加载进去，如果有多个依赖，都需要放在我们创建的目录下</p>
<h2 id="自定义Aggregation函数的实现"><a href="#自定义Aggregation函数的实现" class="headerlink" title="自定义Aggregation函数的实现"></a>自定义Aggregation函数的实现</h2><h3 id="实现原理步骤"><a href="#实现原理步骤" class="headerlink" title="实现原理步骤"></a>实现原理步骤</h3><p>Presto 把 Aggregation 函数分解成三个步骤执行：</p>
<ol>
<li><p>input(state, data): 针对每条数据，执行 input 函数。这个过程是并行执行的，因此在每个有数据的节点都会执行，最终得到多个累积的状态数据。</p>
</li>
<li><p>combine(state1, state2)：将所有节点的状态数据聚合起来，多次执行，直至所有状态数据被聚合成一个最终状态，也就是 Aggregation 函数的输出结果。</p>
</li>
<li><p>output(final_state, out)：最终输出结果到一个 BlockBuilder。</p>
</li>
</ol>
<h3 id="具体代码实现过程"><a href="#具体代码实现过程" class="headerlink" title="具体代码实现过程"></a>具体代码实现过程</h3><ol>
<li>一个继承AccumulatorState的State接口，自定义get和set方法</li>
<li>定义一个 Java 类，使用 @AggregationFunction 标记为 Aggregation 函数</li>
<li>使用 @InputFunction、 @CombineFunction、@OutputFunction 分别标记计算函数、合并结果函数和最终输出函数在 Plugin 处注册 Aggregation 函数</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunna.spark.presto.aggudf;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.block.BlockBuilder;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.function.*;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.type.StandardTypes;</span><br><span class="line"><span class="keyword">import</span> io.airlift.slice.Slice;</span><br><span class="line"><span class="keyword">import</span> io.airlift.slice.Slices;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> com.facebook.presto.spi.type.VarcharType.VARCHAR;</span><br><span class="line"></span><br><span class="line"><span class="meta">@AggregationFunction</span>(<span class="string">"tunan_concat"</span>)    <span class="comment">// Agg方法名</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TunanAggregationUDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@InputFunction</span>  <span class="comment">//输入函数</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">input</span><span class="params">(StringValueState state,@SqlType(StandardTypes.VARCHAR)</span> Slice value)</span>&#123;</span><br><span class="line"></span><br><span class="line">        state.setStringValue(Slices.utf8Slice(isNull(state.getStringValue())+<span class="string">"|"</span>+value.toStringUtf8()));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@CombineFunction</span>    <span class="comment">//合并函数</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">combine</span><span class="params">(StringValueState state1,StringValueState state2)</span></span>&#123;</span><br><span class="line">        state1.setStringValue(Slices.utf8Slice(isNull(state1.getStringValue())+<span class="string">"|"</span>+isNull(state2.getStringValue())));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@OutputFunction</span>(StandardTypes.VARCHAR)  <span class="comment">//输出函数</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">output</span><span class="params">(StringValueState state, BlockBuilder builder)</span></span>&#123;</span><br><span class="line">        VARCHAR.writeSlice(builder,state.getStringValue());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断null值</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">isNull</span><span class="params">(Slice slice)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> slice ==<span class="keyword">null</span>?<span class="string">""</span>:slice.toStringUtf8();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="注册函数-1"><a href="#注册函数-1" class="headerlink" title="注册函数"></a>注册函数</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProstoUDFPlugin</span> <span class="keyword">implements</span> <span class="title">Plugin</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Set&lt;Class&lt;?&gt;&gt; getFunctions() &#123;</span><br><span class="line">        <span class="keyword">return</span> ImmutableSet.&lt;Class&lt;?&gt;&gt;builder()</span><br><span class="line">                .add(PrefixUDF<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">                .<span class="title">add</span>(<span class="title">TunanAggregationUDF</span>.<span class="title">class</span>) // 新加的</span></span><br><span class="line"><span class="class">                .<span class="title">build</span>()</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://yerias.github.io/presto/udf.jpg" alt=""></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/05/03/presto/2/">Presto部署</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-03</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Presto/">Presto</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Presto/">Presto</a></span><div class="content"><h2 id="安装Presto"><a href="#安装Presto" class="headerlink" title="安装Presto"></a>安装Presto</h2><p>1.下载</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">wget</span> <span class="string">https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.200/presto-server-0.200.tar.gz</span></span><br></pre></td></tr></table></figure>

<p>官网下载最新版本: <a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-server" target="_blank" rel="noopener">点我进入官网下载</a> ，注意选择presto-server</p>
<p>2.解压</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">tar</span> <span class="string">-zxvf presto-server-0.200.tar.gz -C /usr/local/</span></span><br></pre></td></tr></table></figure>

<p>/usr/local/presto-server-0.200则为安装目录，另外Presto还需要数据目录，数据目录最好不要在安装目录里面，方便后面Presto的版本升级。</p>
<h2 id="配置Presto"><a href="#配置Presto" class="headerlink" title="配置Presto"></a>配置Presto</h2><p>在安装目录里创建etc目录。这目录会有以下配置：</p>
<ul>
<li>结点属性（Node Properties）：每个结点的环境配置</li>
<li>JVM配置（JVM Config）：Java虚拟机的命令行选项</li>
<li>配置属性（Config Properties）：Persto server的配置</li>
<li>Catelog属性（Catalog Properties）：配置Connector（数据源）</li>
</ul>
<h3 id="结点属性（Node-Properties）"><a href="#结点属性（Node-Properties）" class="headerlink" title="结点属性（Node Properties）"></a>结点属性（Node Properties）</h3><p>结点属性文件etc/node.properties，包含每个结点的配置。一个结点是一个Presto实例。这文件一般是在Presto第一次安装时创建的。以下是最小配置：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">node.environment</span>=<span class="string">production</span></span><br><span class="line"><span class="meta">node.id</span>=<span class="string">ffffffff-ffff-ffff-ffff-ffffffffffff</span></span><br><span class="line"><span class="meta">node.data-dir</span>=<span class="string">/var/presto/data</span></span><br></pre></td></tr></table></figure>

<p><code>node.environment</code>: 环境名字，Presto集群中的结点的环境名字都必须是一样的。<br><code>node.id</code>: 唯一标识，每个结点的标识都必须是为一的。就算重启或升级Presto都必须还保持原来的标识。<br><code>node.data-dir</code>: 数据目录，Presto用它来保存log和其他数据，<strong>建议放在自己定义的目录下</strong>。</p>
<h3 id="JVM配置（JVM-Config）"><a href="#JVM配置（JVM-Config）" class="headerlink" title="JVM配置（JVM Config）"></a>JVM配置（JVM Config）</h3><p>JVM配置文件etc/jvm.config，包含启动Java虚拟机时的命令行选项。格式是每一行是一个命令行选项。此文件数据是由shell解析，所以选项中包含空格或特殊字符会被忽略。</p>
<p>以下是参考配置：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">-server</span></span><br><span class="line"><span class="attr">-Xmx16G</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+UseG1GC</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">G1HeapRegionSize=32M</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+UseGCOverheadLimit</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+ExplicitGCInvokesConcurrent</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+HeapDumpOnOutOfMemoryError</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+ExitOnOutOfMemoryError</span></span><br></pre></td></tr></table></figure>

<p>注意：如果<code>ExitOnOutOfMemoryError</code>报错，注释即可</p>
<p>因为<code>OutOfMemoryError</code>会导致JVM存在不一致状态，所以用heap dump来debug，来找出进程为什么崩溃的原因。</p>
<h3 id="配置属性（Config-Properties）"><a href="#配置属性（Config-Properties）" class="headerlink" title="配置属性（Config Properties）"></a>配置属性（Config Properties）</h3><p>配置属性文件etc/config.properties，包含Presto server的配置。Presto server可以同时为coordinator和worker，但一个大集群里最好就是只指定一台机器为coordinator。<br>以下是coordinator的最小配置：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">coordinator</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">node-scheduler.include-coordinator</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">http-server.http.port</span>=<span class="string">8080</span></span><br><span class="line"><span class="meta">query.max-memory</span>=<span class="string">50GB</span></span><br><span class="line"><span class="meta">query.max-memory-per-node</span>=<span class="string">1GB</span></span><br><span class="line"><span class="meta">discovery-server.enabled</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">discovery.uri</span>=<span class="string">http://example.net:8080</span></span><br></pre></td></tr></table></figure>

<p>以下是worker的最小配置：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">coordinator</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">http-server.http.port</span>=<span class="string">8080</span></span><br><span class="line"><span class="meta">query.max-memory</span>=<span class="string">50GB</span></span><br><span class="line"><span class="meta">query.max-memory-per-node</span>=<span class="string">1GB</span></span><br><span class="line"><span class="meta">discovery.uri</span>=<span class="string">http://example.net:8080</span></span><br></pre></td></tr></table></figure>

<p>如果适用于测试目的，需要将一台机器同时配置为coordinator和worker，则使用以下配置：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">coordinator</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">node-scheduler.include-coordinator</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">http-server.http.port</span>=<span class="string">8080</span></span><br><span class="line"><span class="meta">query.max-memory</span>=<span class="string">2GB</span></span><br><span class="line"><span class="meta">query.max-memory-per-node</span>=<span class="string">512MB</span></span><br><span class="line"><span class="meta">discovery-server.enabled</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">discovery.uri</span>=<span class="string">http://example.net:8080</span></span><br></pre></td></tr></table></figure>

<p><code>coordinator</code>： 是否运行该实例为coordinator（接受client的查询和管理查询执行）。<br><code>node-scheduler.include-coordinator:coordinator</code>: 是否也作为work。对于大型集群来说，在coordinator里做worker的工作会影响查询性能。<br><code>http-server.http.port</code>:指定HTTP端口。Presto使用HTTP来与外部和内部进行交流。<br><code>query.max-memory</code>: 查询能用到的最大总内存<br><code>query.max-memory-per-node</code>: 查询能用到的最大单结点内存<br><code>discovery-server.enabled</code>: Presto使用Discovery服务去找到集群中的所有结点。每个Presto实例在启动时都会在Discovery服务里注册。这样可以简化部署，不需要额外的服务，Presto的coordinator内置一个Discovery服务。也是使用HTTP端口。<br><code>discovery.uri</code>: Discovery服务的URI。将example.net:8080替换为coordinator的host和端口。<strong>这个URI不能以斜杠结尾，这个错误需特别注意，不然会报404错误</strong>。</p>
<p>另外还有以下属性：<br><code>jmx.rmiregistry.port</code>: 指定JMX RMI的注册。JMX client可以连接此端口<br><code>jmx.rmiserver.port</code>: 指定JXM RMI的服务器。可通过JMX监听。</p>
<p>详情请查看<a href="https://prestodb.io/docs/current/admin/resource-groups.html" target="_blank" rel="noopener">Resource Groups</a></p>
<h3 id="Catelog属性（Catalog-Properties）"><a href="#Catelog属性（Catalog-Properties）" class="headerlink" title="Catelog属性（Catalog Properties）"></a>Catelog属性（Catalog Properties）</h3><p>Presto通过connector访问数据。而connector是挂载（mount）在catelog中。connector支持catelog里所有的schema和table。举个例子，Hive connector映射每个Hive数据库到schema，因此Hive connector挂载在hive catelog（所以可以把catelog理解为目录，挂载），而且Hive包含table clicks在数据库web，所以这个table在Presto是hive.web.clicks。<br>Catalog的注册是通过etc/catalog目录下的catalog属性文件。例如，创建etc/catalog/jmx.properties，将jmxconnector挂载在jmx catelog：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">connector.name</span>=<span class="string">jmx</span></span><br></pre></td></tr></table></figure>

<p>查看<a href="https://prestodb.io/docs/current/connector.html" target="_blank" rel="noopener">Connectors</a>查看更多信息。</p>
<p>启动命令：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">bin/launcher</span> <span class="string">start</span></span><br></pre></td></tr></table></figure>

<p>日志在val/log目录下：<br><code>launcher.log</code>: 记录服务初始化情况和一些JVM的诊断。<br><code>server.log: Presto</code>: 的主要日志文件。会自动被压缩。<br><code>http-request.log</code>: 记录HTTP请求。会自动被压缩。</p>
<h2 id="配置MySQL-Connector"><a href="#配置MySQL-Connector" class="headerlink" title="配置MySQL Connector"></a>配置<a href="https://prestodb.io/docs/current/connector/mysql.html" target="_blank" rel="noopener">MySQL Connector</a></h2><p>创建<code>etc/catalog/mysql.properties</code></p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">connector.name</span>=<span class="string">mysql</span></span><br><span class="line"><span class="meta">connection-url</span>=<span class="string">jdbc:mysql://example.net:3306</span></span><br><span class="line"><span class="meta">connection-user</span>=<span class="string">root</span></span><br><span class="line"><span class="meta">connection-password</span>=<span class="string">secret</span></span><br></pre></td></tr></table></figure>

<h2 id="配置Hive-Connector"><a href="#配置Hive-Connector" class="headerlink" title="配置Hive Connector"></a>配置<a href="https://prestodb.io/docs/current/connector/hive.html" target="_blank" rel="noopener">Hive Connector</a></h2><p>创建<code>etc/catalog/hive.properties</code></p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">connector.name</span>=<span class="string">hive-hadoop2</span></span><br><span class="line"><span class="meta">hive.metastore.uri</span>=<span class="string">thrift://example.net:9083</span></span><br><span class="line"><span class="meta">hive.config.resources</span>=<span class="string">/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml</span></span><br></pre></td></tr></table></figure>

<p>还可以在<code>jvm.config</code>中Hadoop的代理用户</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">-DHADOOP_USER_NAME</span>=<span class="string">hdfs_user</span></span><br></pre></td></tr></table></figure>

<h2 id="运行Presto命令行界面"><a href="#运行Presto命令行界面" class="headerlink" title="运行Presto命令行界面"></a>运行Presto命令行界面</h2><p>1.下载 presto-cli-0.200-executable.jar(<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/" target="_blank" rel="noopener">下载最新版</a>),<br>2.修改名字 presto-cli-0.200-executable.jar为 presto<br>3.修改执行权限chmod +x<br>4.运行</p>
<p>指定catelog</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">./presto</span> <span class="string">--server localhost:8080 --catalog hive --schema default</span></span><br></pre></td></tr></table></figure>

<p>不指定catelog，可在命令行使用多个catalog</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">./presto</span> <span class="string">--server localhost:8080</span></span><br></pre></td></tr></table></figure>

<p>注意: JDK必须大于<code>jdk-8u151</code></p>
<p><strong>hive join mysql</strong></p>
<p><code>select * from hive.default.emp a join mysql.tunan.dept b on a.deptno = b.deptno;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"> empno | ename  |    job    | jno  |    date    |  sal   | prize  | deptno | deptno |   dname    | level </span><br><span class="line"><span class="comment">-------+--------+-----------+------+------------+--------+--------+--------+--------+------------+-------</span></span><br><span class="line"> 7369  | SMITH  | CLERK     | 7902 | 1980-12-17 |  800.0 | NULL   | 20     | 20     | RESEARCH   |  1800 </span><br><span class="line"> 7499  | ALLEN  | SALESMAN  | 7698 | 1981-2-20  | 1600.0 |  300.0 | 30     | 30     | SALES      |  1900 </span><br><span class="line"> 7521  | WARD   | SALESMAN  | 7698 | 1981-2-22  | 1250.0 |  500.0 | 10     | 10     | ACCOUNTING |  1700 </span><br><span class="line"> 7566  | JONES  | MANAGER   | 7839 | 1981-4-2   | 2975.0 | NULL   | 10     | 10     | ACCOUNTING |  1700 </span><br><span class="line"> 7654  | MARTIN | SALESMAN  | 7698 | 1981-9-28  | 1250.0 | 1400.0 | 30     | 30     | SALES      |  1900</span><br></pre></td></tr></table></figure>

<p>命令帮助: help</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/05/02/spark/24/">SS+Kafka提交服务器&amp;窗口函数&amp;SS调优</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-02</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>SS+Kafka提交服务器</li>
<li>窗口函数</li>
<li>SS调优</li>
</ol>
<h2 id="SS-Kafka提交服务器"><a href="#SS-Kafka提交服务器" class="headerlink" title="SS+Kafka提交服务器"></a>SS+Kafka提交服务器</h2><p>由于Spark自身没有spark-streaming-kafka的依赖，所以Spark Streaming+Kafka的Application跑在服务器上需要添加spark-streaming-kafka的依赖，共有三种添加依赖的方式</p>
<ol>
<li>直接在IDEA中打胖包，但是服务器上有的东西需要标识为privated，不然依赖重复了，这种方式不推介</li>
<li>提交Application的时候使用–packages参数，格式为: <code>groupId:artifactId:version</code>，这种方式需要在有网络的情况下才能使用</li>
<li>使用–jars 传入依赖，推介，这里有个技巧，可以将需要的 jar 包放在固定目录下，需要传入依赖的时候只需要使用 $(echo /home/hadoop/lib/*.jar | tr ‘ ‘ ‘,’)即可以将目录下的 jar 包全都拼接上去</li>
</ol>
<p>简单的WC案例</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">wc_ss_kafka</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> groupId:<span class="type">String</span> = _</span><br><span class="line">  <span class="keyword">var</span> topic:<span class="type">String</span> = _</span><br><span class="line">  <span class="keyword">var</span> brokers:<span class="type">String</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (args.length&lt;<span class="number">3</span>)&#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Usage: com.tunan.spark.streming.kafka.wc.wc_ss_kafka &lt;groupId&gt; &lt;topic&gt; &lt;brokers&gt;"</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    groupId = args(<span class="number">0</span>)</span><br><span class="line">    topic = args(<span class="number">1</span>)</span><br><span class="line">    brokers = args(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="string">"bootstrap.servers"</span> -&gt; brokers,</span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">"group.id"</span> -&gt; groupId,</span><br><span class="line">      <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"earliest"</span>, <span class="comment">//latest</span></span><br><span class="line">      <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(topic)</span><br><span class="line">    <span class="keyword">val</span> stream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    stream.map(x=&gt;(x.value(),<span class="number">1</span>)).reduceByKey(_+_).foreachRDD(rdd =&gt;&#123;</span><br><span class="line">      rdd.foreach(println)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>作业提交</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">  --class com.tunan.spark.streming.kafka.wc.wc_ss_kafka \</span><br><span class="line">  --master yarn \</span><br><span class="line">  --deploy-mode client \</span><br><span class="line">  --jars $(echo /home/hadoop/lib/*.jar | tr ' ' ',') \</span><br><span class="line">  /home/hadoop/jar/tunan-spark-streaming-kafka-1.0.jar \</span><br><span class="line">  wc_group_id_for_each_stream test hadoop:9090,hadoop:9091,hadoop:9092</span><br></pre></td></tr></table></figure>

<p>查看结果<img src="https://yerias.github.io/spark_img/ss_kafka_submit.png" alt=""></p>
<h2 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h2><p>前面有介绍窗口函数，现在我们来看一下如何使用</p>
<p>在工作中常常有这样的需求:</p>
<ol>
<li>每隔5秒钟统计前10秒钟的数据</li>
<li>每隔1分钟统计前05分钟的数据</li>
</ol>
<p>这类每隔多少统计前多少时间的操作就时窗口操作</p>
<p>我们以一个例子来说明窗口操作。 对之前的单词计数的示例进行扩展，每10秒钟对过去30秒的数据进行wordcount。为此，我们必须在最近30秒的DStream数据中对键值对应用<code>reduceByKey</code>操作。这是通过使用<code>reduceByKeyAndWindow</code>操作完成的。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 每隔10秒统计前30秒的数据</span></span><br><span class="line">stream.map(x=&gt;(x.value(),<span class="number">1</span>)).reduceByKeyAndWindow((a:<span class="type">Int</span>,b:<span class="type">Int</span>)=&gt;a+b,<span class="type">Seconds</span>(<span class="number">30</span>),<span class="type">Seconds</span>(<span class="number">10</span>))</span><br><span class="line">.foreachRDD(rdd =&gt;&#123;</span><br><span class="line">    rdd.foreach(println)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p>一些常见的窗口操作如下表所示。所有这些操作都用到了上述两个参数：<code>windowLength</code>和<code>slideInterval</code>。</p>
<ul>
<li><p><code>window(windowLength, slideInterval)</code></p>
<p>基于源DStream产生的窗口化的批数据计算一个新的DStream</p>
</li>
<li><p><code>countByWindow(windowLength, slideInterval)</code></p>
<p>返回流中元素的一个滑动窗口数</p>
</li>
<li><p><code>reduceByWindow(func, windowLength, slideInterval)</code></p>
<p>返回一个单元素流。利用函数func聚集滑动时间间隔的流的元素创建这个单元素流。函数必须是相关联的以使计算能够正确的并行计算。</p>
</li>
<li><p><code>reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])</code></p>
<p>应用到一个(K,V)对组成的DStream上，返回一个由(K,V)对组成的新的DStream。每一个key的值均由给定的reduce函数聚集起来。注意：在默认情况下，这个算子利用了Spark默认的并发任务数去分组。你可以用numTasks参数设置不同的任务数</p>
</li>
<li><p><code>reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])</code></p>
<p>上述<code>reduceByKeyAndWindow()</code>的更高效的版本，其中使用前一窗口的reduce计算结果递增地计算每个窗口的reduce值。这是通过对进入滑动窗口的新数据进行reduce操作，以及“逆减（inverse reducing）”离开窗口的旧数据来完成的。一个例子是当窗口滑动时对键对应的值进行“一加一减”操作。但是，它仅适用于“可逆减函数（invertible reduce functions）”，即具有相应“反减”功能的减函数（作为参数invFunc）。 像<code>reduceByKeyAndWindow</code>一样，通过可选参数可以配置reduce任务的数量。 请注意，使用此操作必须启用检查点。</p>
</li>
<li><p><code>countByValueAndWindow(windowLength, slideInterval, [numTasks])</code></p>
<p>应用到一个(K,V)对组成的DStream上，返回一个由(K,V)对组成的新的DStream。每个key的值都是它们在滑动窗口中出现的频率。</p>
</li>
</ul>
<h2 id="SS调优"><a href="#SS调优" class="headerlink" title="SS调优"></a>SS调优</h2><p>通过spark-shell运行Spark Streaming + Kafka程序，查看Web UI界面</p>
<p><img src="https://yerias.github.io/spark_img/Web_UI_2.jpg" alt=""></p>
<p>我们需要作业的性能最高，那么需要一个最佳时间</p>
<ol>
<li>在下一个批次启动作业之前一定要运行完前一个批次数据的处理</li>
<li>batch time: 根据需求来定的</li>
</ol>
<p>影响任务运行时长的要素：</p>
<ol>
<li>数据规模(增加kafka分区数==&gt;增加Spark分区==&gt;增加task)</li>
<li>batch time</li>
<li>业务复杂度</li>
</ol>
<p>kafka限速</p>
<p><img src="https://yerias.github.io/spark_img/Web_UI_1.jpg" alt=""></p>
<p>我们看到在设置auto.offset.reset = earliest后，即从头消费，如果累积的数据量特别大，那么在第一次消费的就会撑爆Kafka，必须限制每秒多少条数据</p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Default</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td><code>spark.streaming.kafka.maxRatePerPartition</code></td>
<td>not set</td>
<td>每个Kafka分区读取数据的最大速率</td>
</tr>
</tbody></table>
<p>在设置maxRatePerPartition的值时，数据量=设置的值*分区数*读取时间，加入设置的值为10,分区为3，读取时间为10s，那么每次出来的数据量: 10*3*10=300</p>
<p><strong>优点</strong></p>
<ol>
<li>如果有很多数据量没有处理，并且从头开始，为了防止过载</li>
<li>高峰期限速，防止Kafka处理能力不够挂掉</li>
</ol>
<p><strong>缺点</strong></p>
<ol>
<li>是个固定值 ==&gt; 背压(backpressure 1.5版本引入)</li>
</ol>
<p>背压(backpressure )，在Spark1.5引入，它可以在运行时根据前一个批次数据的运行情况动态调整后续批次读入的数据量</p>
<p>打开参数：spark.streaming.backpressure.enabled </p>
<p>上限参数：spark.streaming.kafka.maxRatePerPartition</p>
<p>初始参数：spark.streaming.backpressure.initialRate</p>
<table>
<thead>
<tr>
<th align="left">Property Name</th>
<th align="left">Default</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>spark.streaming.backpressure.enabled</code></td>
<td align="left">false</td>
<td align="left">使Spark流能够根据当前的批调度延迟和处理时间来控制接收速率，从而使系统接收的速度只取决于系统能够处理的速度。</td>
</tr>
<tr>
<td align="left"><code>spark.streaming.kafka.maxRatePerPartition</code></td>
<td align="left">not set</td>
<td align="left">每个Kafka分区读取数据的最大速率</td>
</tr>
<tr>
<td align="left"><code>spark.streaming.backpressure.initialRate</code></td>
<td align="left">not set</td>
<td align="left">启用背压机制时每个接收器接收第一批数据的初始最大接收速率</td>
</tr>
</tbody></table>
<p>到此，Kafka数据量过载的问题完全解决</p>
<p>最后引入一个关于StreamingContext关闭时的参数</p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Default</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td><code>spark.streaming.stopGracefullyOnShutdown</code></td>
<td>false</td>
<td>如果是“true”，Spark会在JVM关闭时优雅地关闭“StreamingContext”，而不是立即关闭。</td>
</tr>
</tbody></table>
<p>其他调优可参考官网或者<a href="https://blog.csdn.net/Johnson8702/article/details/88944368" target="_blank" rel="noopener">博客</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/05/02/presto/1/">Presto扫盲</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-02</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Presto/">Presto</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Presto/">Presto</a></span><div class="content"><h2 id="Presto简介"><a href="#Presto简介" class="headerlink" title="Presto简介"></a>Presto简介</h2><h3 id="不是什么"><a href="#不是什么" class="headerlink" title="不是什么"></a>不是什么</h3><p>虽然Presto可以解析SQL，但它不是一个标准的数据库。不是MySQL、PostgreSQL或者Oracle的代替品，也不能用来处理在线事务（OLTP）</p>
<h3 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h3><p>Presto通过使用分布式查询，可以快速高效的完成海量数据的查询。作为Hive和Pig的补充，Presto不仅能访问HDFS，也能访问不同的数据源，包括：RDBMS和其他数据源（如Cassandra）。</p>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><img src="https://yerias.github.io/presto/20200502161939.jpg" alt=""></p>
<p>图中各个组件的概念及作用会在下文讲述。</p>
<h3 id="Presto中SQL运行过程：MapReduce-vs-Presto"><a href="#Presto中SQL运行过程：MapReduce-vs-Presto" class="headerlink" title="Presto中SQL运行过程：MapReduce vs Presto"></a>Presto中SQL运行过程：MapReduce vs Presto</h3><p><img src="https://yerias.github.io/presto/3280890894-5af69697e1249_articlex.png" alt=""></p>
<p>使用内存计算，减少与硬盘交互。</p>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol>
<li>Presto与hive对比，都能够处理PB级别的海量数据分析，但Presto是基于内存运算，减少没必要的硬盘IO，所以更快。</li>
<li>能够连接多个数据源，跨数据源连表查，如从hive查询大量网站访问记录，然后从mysql中匹配出设备信息。</li>
<li>部署也比hive简单，因为hive是基于HDFS的，需要先部署HDFS。</li>
</ol>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol>
<li>虽然能够处理PB级别的海量数据分析，但不是代表Presto把PB级别都放在内存中计算的。而是根据场景，如count，avg等聚合运算，是边读数据边计算，再清内存，再读数据再计算，这种耗的内存并不高。但是连表查，就可能产生大量的临时数据，因此速度会变慢，反而hive此时会更擅长。</li>
<li>为了达到实时查询，可能会想到用它直连MySQL来操作查询，这效率并不会提升，瓶颈依然在MySQL，此时还引入网络瓶颈，所以会比原本直接操作数据库要慢。</li>
</ol>
<h2 id="Presto概念"><a href="#Presto概念" class="headerlink" title="Presto概念"></a>Presto概念</h2><h3 id="服务器类型（Server-Types）"><a href="#服务器类型（Server-Types）" class="headerlink" title="服务器类型（Server Types）"></a>服务器类型（Server Types）</h3><p>Presto有两类服务器：coordinator和worker。</p>
<h4 id="Coordinator"><a href="#Coordinator" class="headerlink" title="Coordinator"></a>Coordinator</h4><p>Coordinator服务器是用来解析语句，执行计划分析和管理Presto的worker结点。Presto安装必须有一个Coordinator和多个worker。如果用于开发环境和测试，则一个Presto实例可以同时担任这两个角色。</p>
<p>Coordinator跟踪每个work的活动情况并协调查询语句的执行。 Coordinator为每个查询建立模型，模型包含多个stage，每个stage再转为task分发到不同的worker上执行。</p>
<p>Coordinator与Worker、client通信是通过REST API。</p>
<h4 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h4><p>Worker是负责执行任务和处理数据。Worker从connector获取数据。Worker之间会交换中间数据。Coordinator是负责从Worker获取结果并返回最终结果给client。</p>
<p>当Worker启动时，会广播自己去发现 Coordinator，并告知 Coordinator它是可用，随时可以接受task。</p>
<p>Worker与Coordinator、Worker通信是通过REST API。</p>
<h3 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h3><p>贯穿全文，你会看到一些术语：connector、catelog、schema和table。这些是Presto特定的数据源</p>
<h4 id="Connector"><a href="#Connector" class="headerlink" title="Connector"></a>Connector</h4><p>Connector是适配器，用于Presto和数据源（如Hive、RDBMS）的连接。你可以认为类似JDBC那样，但却是Presto的SPI的实现，使用标准的API来与不同的数据源交互。</p>
<p>Presto有几个内建Connector：JMX的Connector、System Connector（用于访问内建的System table）、Hive的Connector、TPCH（用于TPC-H基准数据）。还有很多第三方的Connector，所以Presto可以访问不同数据源的数据。</p>
<p>每个catalog都有一个特定的Connector。如果你使用catelog配置文件，你会发现每个文件都必须包含connector.name属性，用于指定catelog管理器（创建特定的Connector使用）。一个或多个catelog用同样的connector是访问同样的数据库。例如，你有两个Hive集群。你可以在一个Presto集群上配置两个catelog，两个catelog都是用Hive Connector，从而达到可以查询两个Hive集群。</p>
<h4 id="Catelog"><a href="#Catelog" class="headerlink" title="Catelog"></a>Catelog</h4><p>一个Catelog包含Schema和Connector。例如，你配置JMX的catelog，通过JXM Connector访问JXM信息。当你执行一条SQL语句时，可以同时运行在多个catelog。</p>
<p>Presto处理table时，是通过表的完全限定（fully-qualified）名来找到catelog。例如，一个表的权限定名是hive.test_data.test，则test是表名，test_data是schema，hive是catelog。</p>
<p>Catelog的定义文件是在Presto的配置目录中。</p>
<h4 id="Schema"><a href="#Schema" class="headerlink" title="Schema"></a>Schema</h4><p>Schema是用于组织table。把catelog好schema结合在一起来包含一组的表。当通过Presto访问hive或Mysq时，一个schema会同时转为hive和mysql的同等概念。</p>
<h4 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h4><p>Table跟关系型的表定义一样，但数据和表的映射是交给Connector。</p>
<h3 id="执行查询的模型（Query-Execution-Model）"><a href="#执行查询的模型（Query-Execution-Model）" class="headerlink" title="执行查询的模型（Query Execution Model）"></a>执行查询的模型（Query Execution Model）</h3><h4 id="语句（Statement）"><a href="#语句（Statement）" class="headerlink" title="语句（Statement）"></a>语句（Statement）</h4><p>Presto执行ANSI兼容的SQL语句。当Presto提起语句时，指的就是ANSI标准的SQL语句，包含着列名、表达式和谓词。</p>
<p>之所以要把语句和查询分开说，是因为Presto里，语句只是简单的文本SQL语句。而当语句执行时，Presto则会创建查询和分布式查询计划并在Worker上运行。</p>
<h4 id="查询（Query）"><a href="#查询（Query）" class="headerlink" title="查询（Query）"></a>查询（Query）</h4><p>当Presto解析一个语句时，它将其转换为一个查询，并创建一个分布式查询计划（多个互信连接的stage，运行在Worker上）。如果想获取Presto的查询情况，则获取每个组件（正在执行这语句的结点）的快照。</p>
<p>查询和语句的区别是，语句是存SQL文本，而查询是配置和实例化的组件。一个查询包含：stage、task、split、connector、其他组件和数据源。</p>
<h4 id="Stage"><a href="#Stage" class="headerlink" title="Stage"></a>Stage</h4><p>当Presto执行查询时，会将执行拆分为有层次结构的stage。例如，从hive中的10亿行数据中聚合数据，此时会创建一个用于聚合的根stage，用于聚合其他stage的数据。</p>
<p>层次结构的stage类似一棵树。每个查询都由一个根stage，用于聚合其他stage的数据。stage是Coordinator的分布式查询计划（distributed query plan）的模型，stage不是在worker上运行。</p>
<h4 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h4><p>由于stage不是在worker上运行。stage又会被分为多个task，在不同的work上执行。<br>Task是Presto结构里是“work horse”。一个分布式查询计划会被拆分为多个stage，并再转为task，然后task就运行或处理split。Task有输入和输出，一个stage可以分为多个并行执行的task，一个task可以分为多个并行执行的driver。</p>
<h4 id="Split"><a href="#Split" class="headerlink" title="Split"></a>Split</h4><p>Task运行在split上。split是一个大数据集合中的一块。分布式查询计划最底层的stage是通过split从connector上获取数据，分布式查询计划中间层或顶层则是从它们下层的stage获取数据。</p>
<p>Presto调度查询，coordinator跟踪每个机器运行什么任务，那些split正在被处理。</p>
<h4 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h4><p>Task包含一个或多个并行的driver。Driver在数据上处理，并生成输出，然后由Task聚合，最后传送给stage的其他task。一个driver是Operator的序列。driver是Presto最最低层的并行机制。一个driver有一个输出和一个输入。</p>
<h4 id="Operator"><a href="#Operator" class="headerlink" title="Operator"></a>Operator</h4><p>Operator用来消费，传送和生产数据。如一个Operator从connector中扫表获取数据，然后生产数据给其他Operator消费。一个过滤Operator消费数据，并应用谓词，最后生产出子集数据。</p>
<h4 id="Exchange"><a href="#Exchange" class="headerlink" title="Exchange"></a>Exchange</h4><p>Exchange在Presto结点的不同stage之间传送数据。Task生产和消费数据是通过Exchange客户端。</p>
<hr>
<p>参考：<a href="https://prestodb.io/docs/current/overview/concepts.html" target="_blank" rel="noopener">https://prestodb.io/docs/current/overview/concepts.html</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/04/30/spark/29/">Spark性能优化指南之Shuffle调优</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-30</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><h2 id="调优概述"><a href="#调优概述" class="headerlink" title="调优概述"></a>调优概述</h2><p>大多数Spark作业的性能主要就是消耗在了shuffle环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对shuffle过程进行调优。但是也必须提醒大家的是，影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解shuffle的原理，以及相关参数的说明，同时给出各个参数的调优建议。</p>
<h2 id="ShuffleManager发展概述"><a href="#ShuffleManager发展概述" class="headerlink" title="ShuffleManager发展概述"></a>ShuffleManager发展概述</h2><p>在Spark的源码中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。</p>
<p>在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。</p>
<p>因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。</p>
<p>下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。</p>
<h2 id="HashShuffleManager运行原理"><a href="#HashShuffleManager运行原理" class="headerlink" title="HashShuffleManager运行原理"></a>HashShuffleManager运行原理</h2><h3 id="未经优化的HashShuffleManager"><a href="#未经优化的HashShuffleManager" class="headerlink" title="未经优化的HashShuffleManager"></a>未经优化的HashShuffleManager</h3><p>下图说明了未经优化的HashShuffleManager的原理。这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。</p>
<p>我们先从shuffle write开始说起。shuffle write阶段，主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子（比如reduceByKey），而将每个task处理的数据按key进行“分类”。所谓“分类”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。</p>
<p>那么每个执行shuffle write的task，要为下一个stage创建多少个磁盘文件呢？很简单，下一个stage的task有多少个，当前stage的每个task就要创建多少份磁盘文件。比如下一个stage总共有100个task，那么当前stage的每个task都要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，每个Executor执行5个Task，那么每个Executor上总共就要创建500个磁盘文件，所有Executor上会创建5000个磁盘文件。由此可见，未经优化的shuffle write操作所产生的磁盘文件的数量是极其惊人的。</p>
<p>接着我们来说说shuffle read。shuffle read，通常就是一个stage刚开始时要做的事情。此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task给下游stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。</p>
<p>shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。</p>
<p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%988.png" alt=""></p>
<h3 id="优化后的HashShuffleManager"><a href="#优化后的HashShuffleManager" class="headerlink" title="优化后的HashShuffleManager"></a>优化后的HashShuffleManager</h3><p>下图说明了优化后的HashShuffleManager的原理。这里说的优化，是指我们可以设置一个参数，spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。</p>
<p>开启consolidate机制之后，在shuffle write过程中，task就不是为下游stage的每个task创建一个磁盘文件了。此时会出现shuffleFileGroup的概念，每个shuffleFileGroup会对应一批磁盘文件，磁盘文件的数量与下游stage的task数量是相同的。一个Executor上有多少个CPU core，就可以并行执行多少个task。而第一批并行执行的每个task都会创建一个shuffleFileGroup，并将数据写入对应的磁盘文件内。</p>
<p>当Executor的CPU core执行完一批task，接着执行下一批task时，下一批task就会复用之前已有的shuffleFileGroup，包括其中的磁盘文件。也就是说，此时task会将数据写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，consolidate机制允许不同的task复用同一批磁盘文件，这样就可以有效将多个task的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升shuffle write的性能。</p>
<p>假设第二个stage有100个task，第一个stage有50个task，总共还是有10个Executor，每个Executor执行5个task。那么原本使用未经优化的HashShuffleManager时，每个Executor会产生500个磁盘文件，所有Executor会产生5000个磁盘文件的。但是此时经过优化之后，每个Executor创建的磁盘文件的数量的计算公式为：CPU core的数量 * 下一个stage的task数量。也就是说，每个Executor此时只会创建100个磁盘文件，所有Executor只会创建1000个磁盘文件。</p>
<p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%989.png" alt=""></p>
<h2 id="SortShuffleManager运行原理"><a href="#SortShuffleManager运行原理" class="headerlink" title="SortShuffleManager运行原理"></a>SortShuffleManager运行原理</h2><p>SortShuffleManager的运行机制主要分成两种，一种是普通运行机制，另一种是bypass运行机制。当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为200），就会启用bypass机制。</p>
<h3 id="普通运行机制"><a href="#普通运行机制" class="headerlink" title="普通运行机制"></a>普通运行机制</h3><p>下图说明了普通的SortShuffleManager的原理。在该模式下，数据会先写入一个内存数据结构中，此时根据不同的shuffle算子，可能选用不同的数据结构。如果是reduceByKey这种聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存；如果是join这种普通的shuffle算子，那么会选用Array数据结构，直接写入内存。接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。</p>
<p>在溢写到磁盘文件之前，会先根据key对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。默认的batch数量是10000条，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过Java的BufferedOutputStream实现的。BufferedOutputStream是Java的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。</p>
<p>一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，由于一个task就只对应一个磁盘文件，也就意味着该task为下游stage的task准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。</p>
<p>SortShuffleManager由于有一个磁盘文件merge的过程，因此大大减少了文件数量。比如第一个stage有50个task，总共有10个Executor，每个Executor执行5个task，而第二个stage有100个task。由于每个task最终只有一个磁盘文件，因此此时每个Executor上只有5个磁盘文件，所有Executor只有50个磁盘文件。</p>
<p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%9810.png" alt=""></p>
<h3 id="bypass运行机制"><a href="#bypass运行机制" class="headerlink" title="bypass运行机制"></a>bypass运行机制</h3><p>下图说明了bypass SortShuffleManager的原理。bypass运行机制的触发条件如下： * shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。 * 不是聚合类的shuffle算子（比如reduceByKey）。</p>
<p>此时task会为每个下游task都创建一个临时磁盘文件，并将数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。</p>
<p>该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。</p>
<p>而该机制与普通SortShuffleManager运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。</p>
<p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%9811.png" alt=""></p>
<h2 id="shuffle相关参数调优"><a href="#shuffle相关参数调优" class="headerlink" title="shuffle相关参数调优"></a>shuffle相关参数调优</h2><p>以下是Shffule过程中的一些主要参数，这里详细讲解了各个参数的功能、默认值以及基于实践经验给出的调优建议。</p>
<h3 id="spark-shuffle-file-buffer"><a href="#spark-shuffle-file-buffer" class="headerlink" title="spark.shuffle.file.buffer"></a>spark.shuffle.file.buffer</h3><ul>
<li>默认值：32k</li>
<li>参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。</li>
<li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li>
</ul>
<h3 id="spark-reducer-maxSizeInFlight"><a href="#spark-reducer-maxSizeInFlight" class="headerlink" title="spark.reducer.maxSizeInFlight"></a>spark.reducer.maxSizeInFlight</h3><ul>
<li>默认值：48m</li>
<li>参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。</li>
<li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li>
</ul>
<h3 id="spark-shuffle-io-maxRetries"><a href="#spark-shuffle-io-maxRetries" class="headerlink" title="spark.shuffle.io.maxRetries"></a>spark.shuffle.io.maxRetries</h3><ul>
<li>默认值：3</li>
<li>参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。</li>
<li>调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。</li>
</ul>
<h3 id="spark-shuffle-io-retryWait"><a href="#spark-shuffle-io-retryWait" class="headerlink" title="spark.shuffle.io.retryWait"></a>spark.shuffle.io.retryWait</h3><ul>
<li>默认值：5s</li>
<li>参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。</li>
<li>调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。</li>
</ul>
<h3 id="spark-shuffle-memoryFraction"><a href="#spark-shuffle-memoryFraction" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.shuffle.memoryFraction</h3><ul>
<li>默认值：0.2</li>
<li>参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。</li>
<li>调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。</li>
</ul>
<h3 id="spark-shuffle-manager"><a href="#spark-shuffle-manager" class="headerlink" title="spark.shuffle.manager"></a>spark.shuffle.manager</h3><ul>
<li>默认值：sort</li>
<li>参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。</li>
<li>调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。</li>
</ul>
<h3 id="spark-shuffle-sort-bypassMergeThreshold"><a href="#spark-shuffle-sort-bypassMergeThreshold" class="headerlink" title="spark.shuffle.sort.bypassMergeThreshold"></a>spark.shuffle.sort.bypassMergeThreshold</h3><ul>
<li>默认值：200</li>
<li>参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。</li>
<li>调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。</li>
</ul>
<h3 id="spark-shuffle-consolidateFiles"><a href="#spark-shuffle-consolidateFiles" class="headerlink" title="spark.shuffle.consolidateFiles"></a>spark.shuffle.consolidateFiles</h3><ul>
<li>默认值：false</li>
<li>参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。</li>
<li>调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。</li>
</ul>
<p>本文分别讲解了开发过程中的优化原则、运行前的资源参数设置调优、运行中的数据倾斜的解决方案、为了精益求精的shuffle调优。希望大家能够在阅读本文之后，记住这些性能调优的原则以及方案，在Spark作业开发、测试以及运行的过程中多尝试，只有这样，我们才能开发出更优的Spark作业，不断提升其性能。</p>
<hr>
<p>参考博客: <a href="https://tech.meituan.com/2016/05/12/spark-tuning-pro.html" target="_blank" rel="noopener">https://tech.meituan.com/2016/05/12/spark-tuning-pro.html</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/04/29/spark/23/">Spark 读写压缩文件的一次简单尝试</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-29</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>我认为以节省存储空间为角度出发，Spark作业中的读写压缩文件是必不可少的话题，当然这在MR作业中也有体现和实际解决这种问题，现在我们就要在Spark中解决这种问题。</p>
<p>如果需要安装Lzo可以看我的其他<a href="https://yerias.github.io/2018/10/15/hadoop/13/">文章</a></p>
<p>源文件是一份access的原始数据</p>
<p><img src="https://yerias.github.io/spark_img/access.log.png" alt=""></p>
<p>我们在上传到服务器上的时候，使用lzop命令压缩该文件，得到压缩后的文件，上传HDFSF并对该文件创建索引，得到我们代码中需要处理的源文件</p>
<p><img src="https://yerias.github.io/spark_img/compress-1.png" alt=""></p>
<p>需要注意的是未压缩的文件是1G，压缩后为314M</p>
<p>下图是我们想要做的事情</p>
<p><img src="https://yerias.github.io/spark_img/format%E8%AF%BB%E5%86%99.jpg" alt=""></p>
<p><strong>各种压缩格式的性能比较</strong></p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>工具</th>
<th>算法</th>
<th>扩展名</th>
<th>是否支持分割</th>
<th>Hadoop编码/解码器</th>
<th>hadoop自带</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>N/A</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>No</td>
<td>org.apache.hadoop.io.compress.DefalutCodec</td>
<td>是</td>
</tr>
<tr>
<td>gzip</td>
<td>gzip</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>No</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
<td>是</td>
</tr>
<tr>
<td>bzip2</td>
<td>bzip2</td>
<td>bzip2</td>
<td>.bz2</td>
<td>yes</td>
<td>org.apache.hadoop.io.compress.Bzip2Codec</td>
<td>是</td>
</tr>
<tr>
<td>LZO</td>
<td>Lzop</td>
<td>LZO</td>
<td>.lzo</td>
<td>yes(建索引)</td>
<td>com.hadoop.compression.lzo.LzoCodec</td>
<td>是</td>
</tr>
<tr>
<td>LZ4</td>
<td>N/A</td>
<td>LZ4</td>
<td>.lz4</td>
<td>No</td>
<td>org.apache.hadoop.io.compress.Lz4Codec</td>
<td>否</td>
</tr>
<tr>
<td>Snappy</td>
<td>N/A</td>
<td>Snappy</td>
<td>.snappy</td>
<td>No</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
<td>否</td>
</tr>
</tbody></table>
<p>压缩比：Snappy&lt;LZ4&lt;LZO&lt;GZIP&lt;BZIP2</p>
<p><strong>各种压缩格式的性能优缺点</strong></p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td>lzo</td>
<td>优点：压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便<br/>缺点：不支持split</td>
</tr>
<tr>
<td>snappy</td>
<td>优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便<br/>缺点：压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）</td>
</tr>
<tr>
<td>gzip</td>
<td>优点：压缩速度快；支持hadoop native库<br/>缺点：不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令</td>
</tr>
<tr>
<td>bzip2</td>
<td>优点：支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便<br/>缺点：压缩/解压速度慢；不支持native</td>
</tr>
</tbody></table>
<h3 id="读LZO文件写HDFS"><a href="#读LZO文件写HDFS" class="headerlink" title="读LZO文件写HDFS"></a>读LZO文件写HDFS</h3><p>需要注意的是在Spark中读取HDFS上的压缩文件，需要使用newAPIHadoopFile接口，并且传入LzoTextInputFormat，这个依赖不好解决，仓库是twitter的，下载不了，最好的办法是去github下载源码，install到本地</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.hadoop.gplcompression<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-lzo<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.4.20<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>读取文件的具体代码</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> linesRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.newAPIHadoopFile(in, classOf[<span class="type">LzoTextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>])  .map(x =&gt; x._2.toString)</span><br></pre></td></tr></table></figure>

<p>现在我们已经读进来了，可以实现我们的业务逻辑，那么最后在Spark Core中如何写出去呢? 只需要使用saveAsTextFile</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">etl(linesRDD,hashMap)  .filter(x =&gt; x.responseSize != <span class="number">0</span>).saveAsTextFile(out)</span><br></pre></td></tr></table></figure>

<h3 id="读LZO文件写HDFS也为LZO文件"><a href="#读LZO文件写HDFS也为LZO文件" class="headerlink" title="读LZO文件写HDFS也为LZO文件"></a>读LZO文件写HDFS也为LZO文件</h3><p>读LZO文件上面的方法同样适用，而写为LZO文件只需要在saveAsTextFile中指定LzopCodec即可，适用于所有的格式压缩</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">etl(linesRDD,hashMap)  .filter(x =&gt; x.responseSize != <span class="number">0</span>).saveAsTextFile(out,classOf[<span class="type">LzopCodec</span>])</span><br></pre></td></tr></table></figure>

<h3 id="读LZO文件写MYSQL"><a href="#读LZO文件写MYSQL" class="headerlink" title="读LZO文件写MYSQL"></a>读LZO文件写MYSQL</h3><p>读写表和读写文件不同，读写表更适合用Spark SQL来实现，现在我们的表的字段非常多，不适用于使用tuple的实现方式，从而自定义外部数据源，在TableScan中也是实现与上面相同代码，就能将lzo文件分片读写</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">    logError(<span class="string">"进入buildScan方法"</span>)</span><br><span class="line">    <span class="comment">// 使用RDD拿到Lzo本文内容</span></span><br><span class="line">    <span class="keyword">val</span> lines = sqlContext.sparkContext.newAPIHadoopFile(path, classOf[<span class="type">LzoTextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>])</span><br><span class="line">    .map(_._2.toString)</span><br><span class="line">    <span class="comment">// 拿到响应的schema信息</span></span><br><span class="line">    <span class="keyword">val</span> fields = schema.fields</span><br><span class="line">    <span class="comment">// 拿到每行数据，做简单处理，返回RDD[Row]</span></span><br><span class="line">    lines.map(_.split(<span class="string">","</span>).map(_.trim)).map(_.zipWithIndex.map &#123;</span><br><span class="line">        <span class="keyword">case</span> (value, index) =&gt;</span><br><span class="line">        <span class="type">Utils</span>.caseTo(value, fields(index).dataType)</span><br><span class="line">    &#125;).map(x =&gt; <span class="type">Row</span>.fromSeq(x))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我在这里尝试了压缩写和普通写，都可以查询到数据</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">        properties.put(<span class="string">"user"</span>,<span class="string">"root"</span>)</span><br><span class="line">        properties.put(<span class="string">"password"</span>,<span class="string">"root"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> url = <span class="string">"jdbc:mysql://hadoop:3306/tunan?useUnicode=true&amp;characterEncoding=utf-8"</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!flat)&#123;</span><br><span class="line">            <span class="comment">// 保存到MySQL</span></span><br><span class="line">            result.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">              .jdbc(url,<span class="string">"domain_group"</span>,properties)</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="comment">// 保存到MySQL,压缩格式为lzo</span></span><br><span class="line">            result.write.format(<span class="string">"orc"</span>).option(<span class="string">"compression"</span>,<span class="string">"lzo"</span>).mode(<span class="string">"overwrite"</span>)</span><br><span class="line">              .jdbc(url,<span class="string">"domain_group"</span>,properties)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>

<p>查询：</p>
<p><img src="https://yerias.github.io/spark_img/compress-2.png" alt=""></p>
<h3 id="读LZO文件写HIVE"><a href="#读LZO文件写HIVE" class="headerlink" title="读LZO文件写HIVE"></a>读LZO文件写HIVE</h3><p>读的方式也是一样的，使用外部数据源的方式直接得到DataFrame，在写的时候指定了存储格式，而不指定压缩格式的话，会默认指定压缩格式为Snappy</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">textDf.write.format(<span class="string">"parquet"</span>).mode(<span class="string">"overwrite"</span>).saveAsTable(<span class="string">"store_format.parquet_tb"</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://yerias.github.io/spark_img/compress-3.png" alt=""></p>
<h3 id="读LZO文件压缩写HIVE"><a href="#读LZO文件压缩写HIVE" class="headerlink" title="读LZO文件压缩写HIVE"></a>读LZO文件压缩写HIVE</h3><p>读Lzo的方式如上，在这里我们尝试了几种格式的压缩写</p>
<ol>
<li><p>指定了存储格式为parquet，压缩格式为lzo</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">textDf.write.format(<span class="string">"parquet"</span>).option(<span class="string">"compression"</span>,<span class="string">"lzo"</span>).mode(<span class="string">"overwrite"</span>).saveAsTable(<span class="string">"store_format.parquet_tb"</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://yerias.github.io/spark_img/compress-4.png" alt=""></p>
<p>可以看得到压缩格式和存储格式发生了变化，我们继续查表</p>
<p><img src="https://yerias.github.io/spark_img/compress-5.png" alt=""></p>
</li>
<li><p>现在我们将压缩格式改为orc，存储格式还是使用lzo</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">textDf.write.format(<span class="string">"orc"</span>).option(<span class="string">"compression"</span>,<span class="string">"lzo"</span>).mode(<span class="string">"overwrite"</span>).saveAsTable(<span class="string">"store_format.parquet_tb"</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://yerias.github.io/spark_img/compress-6.png" alt=""></p>
<p>我们发现orc和lzo格式并不能一起使用，查出来的是无效的结果</p>
</li>
<li><p>继续使用压缩格式为orc，存储格式改为snappy</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">textDf.write.format(<span class="string">"orc"</span>).option(<span class="string">"compression"</span>,<span class="string">"snappy"</span>).mode(<span class="string">"overwrite"</span>).saveAsTable(<span class="string">"store_format.parquet_tb"</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://yerias.github.io/spark_img/compress-7.png" alt=""></p>
<p>查询结果：</p>
<p><img src="https://yerias.github.io/spark_img/compress-5.png" alt=""></p>
</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol>
<li>使用Lzo压缩的压缩比相比于Snappy较高</li>
<li>无论哪种压缩格式在MySQL中都是无效的</li>
<li>Parquet可以和Snappy或者Lzo搭配使用，ORC可以和Snappy或者Bzip搭配使用</li>
<li>ORC不能和Lzo搭配使用</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/04/29/kafka/3/">kafka eagle安装部署</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-29</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Kakfa/">Kakfa</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Kakfa/">Kakfa</a></span><div class="content"><h5 id="1-kafka-zookeeper准备"><a href="#1-kafka-zookeeper准备" class="headerlink" title="1.kafka+zookeeper准备"></a>1.kafka+zookeeper准备</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">这里假设你已经把kafka+zookeeper安装完成，但是需要注意的几点是：</span><br><span class="line">1.kafka需要开启JMX端口</span><br><span class="line">    找到kafka安装路径，进入到bin文件夹，修改下面的地方。</span><br><span class="line">    vi kafka-server-start.sh</span><br><span class="line">...</span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$KAFKA_HEAP_OPTS</span>"</span> = <span class="string">"x"</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">"-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70"</span></span><br><span class="line">    <span class="built_in">export</span> JMX_PORT=<span class="string">"9999"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">...</span><br><span class="line">    参考链接：[kafka添加jmx端口]:https://ke.smartloli.org/3.Manuals/11.Metrics.html</span><br><span class="line"></span><br><span class="line">2.了解kafka在zookeeper配置</span><br><span class="line">    需要查看kafka的server.properties配置</span><br><span class="line">    找到zookeeper.connect此项配置，这个是要配置到eagle里面的</span><br><span class="line">    此处假设zookeeper.connect=192.168.18.11:2181,192.168.18.12:2181,192.168.18.13:2181</span><br><span class="line">    ！！！PS:此处踩了坑，如果说这里的zookeeper地址后面加了其他路径，在kafka-eagle里面也要配置，</span><br><span class="line">    否则在kafka-eagle的Dashboard中无法读取到kafka的信息。比如我们有人安装的kafka集群里面就有    </span><br><span class="line">    192.168.18.11:2181/kafka1或者192.168.18.11:2181/kafka2这种地址。</span><br><span class="line">    如果你在安装kafka的时候没有配置多余路径，这样是最好的，如果有一定要加上。</span><br><span class="line">3.连通性测试</span><br><span class="line">  安装kafka-eagle的服务器，一定要提前测试是否能连接kafka注册的zookeeper端口</span><br><span class="line">  telnet 端口进行测试</span><br></pre></td></tr></table></figure>

<h5 id="2-JDK环境准备"><a href="#2-JDK环境准备" class="headerlink" title="2.JDK环境准备"></a>2.JDK环境准备</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">此处就忽略不说了，kafka既然会安装，也是依赖JDK环境的。版本没要求，但是最好是1.7以上。</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$JAVA_HOME</span></span><br><span class="line">测试一下JDK环境是否安装成功</span><br></pre></td></tr></table></figure>

<h5 id="3-开始安装kafka-eagle"><a href="#3-开始安装kafka-eagle" class="headerlink" title="3.开始安装kafka-eagle"></a>3.开始安装kafka-eagle</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">1.下载安装包</span><br><span class="line">软件安装目录建议按照自己的规范来，以后好找</span><br><span class="line"><span class="built_in">cd</span> /tmp</span><br><span class="line">wget https://github.com/smartloli/kafka-eagle-bin/archive/v1.2.2.tar.gz</span><br><span class="line">tar zxf v1.2.2.tar.gz</span><br><span class="line"><span class="built_in">cd</span> kafka-eagle-bin-1.2.2</span><br><span class="line">tar zxf kafka-eagle-web-1.2.2-bin.tar.gz -C /data/app/</span><br><span class="line"><span class="built_in">cd</span> /data/app</span><br><span class="line">mv kafka-eagle-web-1.2.2 kafka-eagle</span><br><span class="line"></span><br><span class="line">2.环境配置</span><br><span class="line">vi /etc/profile</span><br><span class="line"><span class="built_in">export</span> KE_HOME=/data/app/kafka-eagle</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$KE_HOME</span>/bin</span><br><span class="line">ps:此处的KE_HOME按照自己实际安装的目录来，我安装在/data/app/kafka-eagle下面</span><br><span class="line">如果你是安装的其他目录，别忘了修改。</span><br><span class="line"></span><br><span class="line">3.配置修改</span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;KE_HOME&#125;</span>/conf</span><br><span class="line">vi system-config.properties</span><br><span class="line"></span><br><span class="line"><span class="comment"># multi zookeeper&amp;kafka cluster list -- The client connection address of the Zookeeper cluster is set here</span></span><br><span class="line"><span class="comment">#如果只有一个集群的话，就写一个cluster1就行了</span></span><br><span class="line">kafka.eagle.zk.cluster.alias=cluster1,cluster2   </span><br><span class="line"><span class="comment">#这里填上刚才上准备工作中的zookeeper.connect地址</span></span><br><span class="line">cluster1.zk.list=192.168.18.11:2181,192.168.18.12:2181,192.168.18.13:2181</span><br><span class="line"><span class="comment">#如果多个集群，继续写，如果没有注释掉</span></span><br><span class="line">cluster2.zk.list=192.168.18.21:2181,192.168.18.22:2181,192.168.18.23:2181/kafka </span><br><span class="line"></span><br><span class="line"><span class="comment"># zk limit -- Zookeeper cluster allows the number of clients to connect to</span></span><br><span class="line">kafka.zk.limit.size=25</span><br><span class="line"></span><br><span class="line"><span class="comment"># kafka eagel webui port -- WebConsole port access address</span></span><br><span class="line">kafka.eagle.webui.port=8048     <span class="comment">###web界面地址端口</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kafka offset storage -- Offset stored in a Kafka cluster, if stored in the zookeeper, you can not use this option</span></span><br><span class="line">kafka.eagle.offset.storage=kafka</span><br><span class="line"></span><br><span class="line"><span class="comment"># delete kafka topic token -- Set to delete the topic token, so that administrators can have the right to delete</span></span><br><span class="line">kafka.eagle.topic.token=keadmin</span><br><span class="line"></span><br><span class="line"><span class="comment"># kafka sasl authenticate, current support SASL_PLAINTEXT</span></span><br><span class="line"><span class="comment">#如果kafka开启了sasl认证，需要在这个地方配置sasl认证文件</span></span><br><span class="line">kafka.eagle.sasl.enable=<span class="literal">false</span></span><br><span class="line">kafka.eagle.sasl.protocol=SASL_PLAINTEXT</span><br><span class="line">kafka.eagle.sasl.mechanism=PLAIN</span><br><span class="line">kafka.eagle.sasl.client=/data/kafka-eagle/conf/kafka_client_jaas.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">#下面两项是配置数据库的，默认使用sqlite，如果量大，建议使用mysql，这里我使用的是sqlit</span></span><br><span class="line"><span class="comment">#如果想使用mysql建议在文末查看官方文档</span></span><br><span class="line"><span class="comment"># Default use sqlite to store data</span></span><br><span class="line">kafka.eagle.driver=org.sqlite.JDBC</span><br><span class="line"><span class="comment"># It is important to note that the '/hadoop/kafka-eagle/db' path must exist.</span></span><br><span class="line">kafka.eagle.url=jdbc:sqlite:/data/app/kafka-eagle/db/ke.db   <span class="comment">#这个地址，按照安装目录进行配置</span></span><br><span class="line">kafka.eagle.username=root</span><br><span class="line">kafka.eagle.password=smartloli</span><br><span class="line"></span><br><span class="line"><span class="comment"># &lt;Optional&gt; set mysql address</span></span><br><span class="line"><span class="comment">#kafka.eagle.driver=com.mysql.jdbc.Driver</span></span><br><span class="line"><span class="comment">#kafka.eagle.url=jdbc:mysql://127.0.0.1:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span></span><br><span class="line"><span class="comment">#kafka.eagle.username=root</span></span><br><span class="line"><span class="comment">#kafka.eagle.password=smartloli</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">如果开启了sasl认证，需要自己去修改kafka-eagle目录下的conf/kafka_client_jaas.conf</span><br><span class="line">此处不多说</span><br></pre></td></tr></table></figure>

<h5 id="4-启动kafka-eagle"><a href="#4-启动kafka-eagle" class="headerlink" title="4.启动kafka-eagle"></a>4.启动kafka-eagle</h5><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">cd $&#123;KE_HOME&#125;/bin</span><br><span class="line">chmod +x ke.sh</span><br><span class="line">./ke.sh start</span><br><span class="line">查看日志是否出问题</span><br><span class="line">tailf ../<span class="built_in">log</span>/<span class="built_in">log</span>.<span class="built_in">log</span></span><br><span class="line">如果没问题，则直接登录</span><br><span class="line">http:<span class="comment">//host:8048/ke</span></span><br><span class="line">默认用户名:admin</span><br><span class="line">默认密码:<span class="number">12345</span></span><br><span class="line">如果进入到一下界面，就说明你安装成功了！</span><br></pre></td></tr></table></figure>

<p><img src="https://yerias.github.io/kafka/eagle.jpg" alt="eagle"></p>
<h5 id="5-问题汇总"><a href="#5-问题汇总" class="headerlink" title="5.问题汇总"></a>5.问题汇总</h5><ol>
<li><p>ZKPoolUtils.localhost-startStop-1 - ERROR - Unable to connect to zookeeper server within timeout: 100000</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">这个是网络问题，在kafka-eagle服务器上自己测试一下能否能telnet通配置的zk地址。</span><br><span class="line">cat system-config.properties|grep cluster1.zk.<span class="built_in">list</span></span><br><span class="line">测试这个配置的端口</span><br></pre></td></tr></table></figure>
</li>
<li><p>ERROR - Get kafka consumer has error,msg is No resolvable bootstrap urls given in bootstrap.servers</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">这个问题是配置的zk地址有问题，看看kafka配置的zk地址</span><br><span class="line">跟自己在eagle上配置的地址是否相同，有没有少目录或者端口配置。</span><br><span class="line">可以看看文章开头，kafka+zookeeper准备-了解kafka在zookeeper配置</span><br></pre></td></tr></table></figure>
</li>
<li><p>zookeeper state changed (AuthFailed)</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">这个问题有两种情况</span><br><span class="line">1.认证的问题，确认你配置的认证文件是否正确</span><br><span class="line">2<span class="selector-class">.zk</span>地址问题，看看<span class="selector-tag">kafka</span>配置的<span class="selector-tag">zk</span>地址，跟自己在<span class="selector-tag">eagle</span>上配置的地址是否相同</span><br><span class="line">可以看看文章开头，<span class="selector-tag">kafka</span>+<span class="selector-tag">zookeeper</span>准备<span class="selector-tag">-</span>了解<span class="selector-tag">kafka</span>在<span class="selector-tag">zookeeper</span>配置</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h5 id="6-kafka-eagle使用"><a href="#6-kafka-eagle使用" class="headerlink" title="6.kafka-eagle使用"></a>6.kafka-eagle使用</h5><p>kafka-eagle官方文档: <a href="https://ke.smartloli.org/2.Install/2.Installing.html" target="_blank" rel="noopener">https://ke.smartloli.org/2.Install/2.Installing.html</a></p>
<p>kafka-eagle下载地址: <a href="http://download.smartloli.org/" target="_blank" rel="noopener">http://download.smartloli.org/</a></p>
<p>kafka-eagle git地址: <a href="https://github.com/smartloli/kafka-eagle" target="_blank" rel="noopener">https://github.com/smartloli/kafka-eagle</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/04/29/spark/28/">Spark性能优化指南之数据倾斜调优</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-29</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><h2 id="调优概述"><a href="#调优概述" class="headerlink" title="调优概述"></a>调优概述</h2><p>有的时候，我们可能会遇到大数据计算中一个最棘手的问题——数据倾斜，此时Spark作业的性能会比期望差很多。数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。</p>
<h2 id="数据倾斜发生时的现象"><a href="#数据倾斜发生时的现象" class="headerlink" title="数据倾斜发生时的现象"></a>数据倾斜发生时的现象</h2><ul>
<li>绝大多数task执行得都非常快，但个别task执行极慢。比如，总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时。这种情况很常见。</li>
<li>原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。</li>
</ul>
<h2 id="数据倾斜发生的原理"><a href="#数据倾斜发生的原理" class="headerlink" title="数据倾斜发生的原理"></a>数据倾斜发生的原理</h2><p>数据倾斜的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。</p>
<p>因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。</p>
<p>下图就是一个很清晰的例子：hello这个key，在三个节点上对应了总共7条数据，这些数据都会被拉取到同一个task中进行处理；而world和you这两个key分别才对应1条数据，所以另外两个task只要分别处理1条数据即可。此时第一个task的运行时间可能是另外两个task的7倍，而整个stage的运行速度也由运行最慢的那个task所决定。</p>
<p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%982.png" alt=""></p>
<h2 id="如何定位导致数据倾斜的代码"><a href="#如何定位导致数据倾斜的代码" class="headerlink" title="如何定位导致数据倾斜的代码"></a>如何定位导致数据倾斜的代码</h2><p>数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。</p>
<h3 id="某个task执行特别慢的情况"><a href="#某个task执行特别慢的情况" class="headerlink" title="某个task执行特别慢的情况"></a>某个task执行特别慢的情况</h3><p>首先要看的，就是数据倾斜发生在第几个stage中。</p>
<p>如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以在Spark Web UI上深入看一下当前这个stage各个task分配的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。</p>
<p>比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完；而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而运行时间特别长的task需要处理几千KB的数据，处理的数据量差了10倍。此时更加能够确定是发生了数据倾斜。</p>
<p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%983.png" alt=""></p>
<p>知道数据倾斜发生在哪一个stage之后，接着我们就需要根据stage划分原理，推算出来发生倾斜的那个stage对应代码中的哪一部分，这部分代码中肯定会有一个shuffle类算子。精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。</p>
<p>这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。 * stage0，主要是执行从textFile到map操作，以及执行shuffle write操作。shuffle write操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。 * stage1，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行shuffle read操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val conf = new SparkConf()</span><br><span class="line">val sc = new SparkContext(conf)</span><br><span class="line"> </span><br><span class="line">val lines = sc.textFile(&quot;hdfs://...&quot;)</span><br><span class="line">val words = lines.flatMap(_.split(&quot; &quot;))</span><br><span class="line">val pairs = words.map((_, 1))</span><br><span class="line">val wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line"> </span><br><span class="line">wordCounts.collect().foreach(println(_))</span><br></pre></td></tr></table></figure>

<p>通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。比如我们在Spark Web UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是由educeByKey算子导致的数据倾斜问题。比如某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。</p>
<h3 id="某个task莫名其妙内存溢出的情况"><a href="#某个task莫名其妙内存溢出的情况" class="headerlink" title="某个task莫名其妙内存溢出的情况"></a>某个task莫名其妙内存溢出的情况</h3><p>这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。</p>
<p>但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。</p>
<h2 id="查看导致数据倾斜的key的数据分布情况"><a href="#查看导致数据倾斜的key的数据分布情况" class="headerlink" title="查看导致数据倾斜的key的数据分布情况"></a>查看导致数据倾斜的key的数据分布情况</h2><p>知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD/Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。</p>
<p>此时根据你执行操作的情况不同，可以有很多种查看key分布的方式： 1. 如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。 2. 如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。</p>
<p>举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val sampledPairs = pairs.sample(false, 0.1)</span><br><span class="line">val sampledWordCounts = sampledPairs.countByKey()</span><br><span class="line">sampledWordCounts.foreach(println(_))</span><br></pre></td></tr></table></figure>

<h2 id="数据倾斜的解决方案"><a href="#数据倾斜的解决方案" class="headerlink" title="数据倾斜的解决方案"></a>数据倾斜的解决方案</h2><h3 id="解决方案一：使用Hive-ETL预处理数据"><a href="#解决方案一：使用Hive-ETL预处理数据" class="headerlink" title="解决方案一：使用Hive ETL预处理数据"></a>解决方案一：使用Hive ETL预处理数据</h3><p><strong>方案适用场景：</strong>导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。</p>
<p><strong>方案实现思路：</strong>此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。</p>
<p><strong>方案实现原理：</strong>这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。</p>
<p><strong>方案优点：</strong>实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p>
<p><strong>方案缺点：</strong>治标不治本，Hive ETL中还是会发生数据倾斜。</p>
<p><strong>方案实践经验：</strong>在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p>
<p><strong>项目实践经验：</strong>在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。</p>
<h3 id="解决方案二：过滤少数导致倾斜的key"><a href="#解决方案二：过滤少数导致倾斜的key" class="headerlink" title="解决方案二：过滤少数导致倾斜的key"></a>解决方案二：过滤少数导致倾斜的key</h3><p><strong>方案适用场景：</strong>如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p>
<p><strong>方案实现思路：</strong>如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。</p>
<p><strong>方案实现原理：</strong>将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。</p>
<p><strong>方案优点：</strong>实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p>
<p><strong>方案缺点：</strong>适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p>
<p><strong>方案实践经验：</strong>在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</p>
<h3 id="解决方案三：提高shuffle操作的并行度"><a href="#解决方案三：提高shuffle操作的并行度" class="headerlink" title="解决方案三：提高shuffle操作的并行度"></a>解决方案三：提高shuffle操作的并行度</h3><p><strong>方案适用场景：</strong>如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。</p>
<p><strong>方案实现思路：</strong>在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。</p>
<p><strong>方案实现原理：</strong>增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。</p>
<p><strong>方案优点：</strong>实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p>
<p><strong>方案缺点：</strong>只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p>
<p><strong>方案实践经验：</strong>该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</p>
<p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%984.png" alt=""></p>
<h3 id="解决方案四：两阶段聚合（局部聚合-全局聚合）"><a href="#解决方案四：两阶段聚合（局部聚合-全局聚合）" class="headerlink" title="解决方案四：两阶段聚合（局部聚合+全局聚合）"></a>解决方案四：两阶段聚合（局部聚合+全局聚合）</h3><p><strong>方案适用场景：</strong>对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。</p>
<p><strong>方案实现思路：</strong>这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。</p>
<p><strong>方案实现原理：</strong>将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p>
<p><strong>方案优点：</strong>对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p>
<p><strong>方案缺点：</strong>仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。</p>
<p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%985.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">// 第一步，给RDD中的每个key都打上一个随机前缀。</span><br><span class="line">JavaPairRDD&lt;String, Long&gt; randomPrefixRdd = rdd.mapToPair(</span><br><span class="line">        new PairFunction&lt;Tuple2&lt;Long,Long&gt;, String, Long&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID = 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple2&lt;String, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple)</span><br><span class="line">                    throws Exception &#123;</span><br><span class="line">                Random random = new Random();</span><br><span class="line">                int prefix = random.nextInt(10);</span><br><span class="line">                return new Tuple2&lt;String, Long&gt;(prefix + &quot;_&quot; + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line">// 第二步，对打上随机前缀的key进行局部聚合。</span><br><span class="line">JavaPairRDD&lt;String, Long&gt; localAggrRdd = randomPrefixRdd.reduceByKey(</span><br><span class="line">        new Function2&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID = 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Long call(Long v1, Long v2) throws Exception &#123;</span><br><span class="line">                return v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line">// 第三步，去除RDD中每个key的随机前缀。</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; removedRandomPrefixRdd = localAggrRdd.mapToPair(</span><br><span class="line">        new PairFunction&lt;Tuple2&lt;String,Long&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID = 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;String, Long&gt; tuple)</span><br><span class="line">                    throws Exception &#123;</span><br><span class="line">                long originalKey = Long.valueOf(tuple._1.split(&quot;_&quot;)[1]);</span><br><span class="line">                return new Tuple2&lt;Long, Long&gt;(originalKey, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line">// 第四步，对去除了随机前缀的RDD进行全局聚合。</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; globalAggrRdd = removedRandomPrefixRdd.reduceByKey(</span><br><span class="line">        new Function2&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID = 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Long call(Long v1, Long v2) throws Exception &#123;</span><br><span class="line">                return v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure>

<h3 id="解决方案五：将reduce-join转为map-join"><a href="#解决方案五：将reduce-join转为map-join" class="headerlink" title="解决方案五：将reduce join转为map join"></a>解决方案五：将reduce join转为map join</h3><p><strong>方案适用场景：</strong>在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p>
<p><strong>方案实现思路：</strong>不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</p>
<p><strong>方案实现原理：</strong>普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。</p>
<p><strong>方案优点：</strong>对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p>
<p><strong>方案缺点：</strong>适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</p>
<p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%986.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">// 首先将数据量比较小的RDD的数据，collect到Driver中来。</span><br><span class="line">List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1.collect()</span><br><span class="line">// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。</span><br><span class="line">// 可以尽可能节省内存空间，并且减少网络传输性能开销。</span><br><span class="line">final Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data);</span><br><span class="line">  </span><br><span class="line">// 对另外一个RDD执行map类操作，而不再是join类操作。</span><br><span class="line">JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRdd = rdd2.mapToPair(</span><br><span class="line">        new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, Tuple2&lt;String, Row&gt;&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID = 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, String&gt; tuple)</span><br><span class="line">                    throws Exception &#123;</span><br><span class="line">                // 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。</span><br><span class="line">                List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1DataBroadcast.value();</span><br><span class="line">                // 可以将rdd1的数据转换为一个Map，便于后面进行join操作。</span><br><span class="line">                Map&lt;Long, Row&gt; rdd1DataMap = new HashMap&lt;Long, Row&gt;();</span><br><span class="line">                for(Tuple2&lt;Long, Row&gt; data : rdd1Data) &#123;</span><br><span class="line">                    rdd1DataMap.put(data._1, data._2);</span><br><span class="line">                &#125;</span><br><span class="line">                // 获取当前RDD数据的key以及value。</span><br><span class="line">                String key = tuple._1;</span><br><span class="line">                String value = tuple._2;</span><br><span class="line">                // 从rdd1数据Map中，根据key获取到可以join到的数据。</span><br><span class="line">                Row rdd1Value = rdd1DataMap.get(key);</span><br><span class="line">                return new Tuple2&lt;String, String&gt;(key, new Tuple2&lt;String, Row&gt;(value, rdd1Value));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line">// 这里得提示一下。</span><br><span class="line">// 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。</span><br><span class="line">// 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。</span><br><span class="line">// rdd2中每条数据都可能会返回多条join后的数据。</span><br></pre></td></tr></table></figure>

<h3 id="解决方案六：采样倾斜key并分拆join操作"><a href="#解决方案六：采样倾斜key并分拆join操作" class="headerlink" title="解决方案六：采样倾斜key并分拆join操作"></a>解决方案六：采样倾斜key并分拆join操作</h3><p><strong>方案适用场景：</strong>两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。</p>
<p><strong>方案实现思路：</strong> * 对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。 * 然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。 * 接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。 * 再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。 * 而另外两个普通的RDD就照常join即可。 * 最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。</p>
<p><strong>方案实现原理：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。</p>
<p><strong>方案优点：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。</p>
<p><strong>方案缺点：</strong>如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。</p>
<p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%987.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">// 首先从包含了少数几个导致数据倾斜key的rdd1中，采样10%的样本数据。</span><br><span class="line">JavaPairRDD&lt;Long, String&gt; sampledRDD = rdd1.sample(false, 0.1);</span><br><span class="line">  </span><br><span class="line">// 对样本数据RDD统计出每个key的出现次数，并按出现次数降序排序。</span><br><span class="line">// 对降序排序后的数据，取出top 1或者top 100的数据，也就是key最多的前n个数据。</span><br><span class="line">// 具体取出多少个数据量最多的key，由大家自己决定，我们这里就取1个作为示范。</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; mappedSampledRDD = sampledRDD.mapToPair(</span><br><span class="line">        new PairFunction&lt;Tuple2&lt;Long,String&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID = 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;Long, String&gt; tuple)</span><br><span class="line">                    throws Exception &#123;</span><br><span class="line">                return new Tuple2&lt;Long, Long&gt;(tuple._1, 1L);</span><br><span class="line">            &#125;     </span><br><span class="line">        &#125;);</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; countedSampledRDD = mappedSampledRDD.reduceByKey(</span><br><span class="line">        new Function2&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID = 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Long call(Long v1, Long v2) throws Exception &#123;</span><br><span class="line">                return v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; reversedSampledRDD = countedSampledRDD.mapToPair( </span><br><span class="line">        new PairFunction&lt;Tuple2&lt;Long,Long&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID = 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple)</span><br><span class="line">                    throws Exception &#123;</span><br><span class="line">                return new Tuple2&lt;Long, Long&gt;(tuple._2, tuple._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">final Long skewedUserid = reversedSampledRDD.sortByKey(false).take(1).get(0)._2;</span><br><span class="line">  </span><br><span class="line">// 从rdd1中分拆出导致数据倾斜的key，形成独立的RDD。</span><br><span class="line">JavaPairRDD&lt;Long, String&gt; skewedRDD = rdd1.filter(</span><br><span class="line">        new Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID = 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Boolean call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123;</span><br><span class="line">                return tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">// 从rdd1中分拆出不导致数据倾斜的普通key，形成独立的RDD。</span><br><span class="line">JavaPairRDD&lt;Long, String&gt; commonRDD = rdd1.filter(</span><br><span class="line">        new Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID = 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Boolean call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123;</span><br><span class="line">                return !tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125; </span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line">// rdd2，就是那个所有key的分布相对较为均匀的rdd。</span><br><span class="line">// 这里将rdd2中，前面获取到的key对应的数据，过滤出来，分拆成单独的rdd，并对rdd中的数据使用flatMap算子都扩容100倍。</span><br><span class="line">// 对扩容的每条数据，都打上0～100的前缀。</span><br><span class="line">JavaPairRDD&lt;String, Row&gt; skewedRdd2 = rdd2.filter(</span><br><span class="line">         new Function&lt;Tuple2&lt;Long,Row&gt;, Boolean&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID = 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Boolean call(Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123;</span><br><span class="line">                return tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).flatMapToPair(new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID = 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call(</span><br><span class="line">                    Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123;</span><br><span class="line">                Random random = new Random();</span><br><span class="line">                List&lt;Tuple2&lt;String, Row&gt;&gt; list = new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;();</span><br><span class="line">                for(int i = 0; i &lt; 100; i++) &#123;</span><br><span class="line">                    list.add(new Tuple2&lt;String, Row&gt;(i + &quot;_&quot; + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                return list;</span><br><span class="line">            &#125;</span><br><span class="line">              </span><br><span class="line">        &#125;);</span><br><span class="line"> </span><br><span class="line">// 将rdd1中分拆出来的导致倾斜的key的独立rdd，每条数据都打上100以内的随机前缀。</span><br><span class="line">// 然后将这个rdd1中分拆出来的独立rdd，与上面rdd2中分拆出来的独立rdd，进行join。</span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD1 = skewedRDD.mapToPair(</span><br><span class="line">        new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID = 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple)</span><br><span class="line">                    throws Exception &#123;</span><br><span class="line">                Random random = new Random();</span><br><span class="line">                int prefix = random.nextInt(100);</span><br><span class="line">                return new Tuple2&lt;String, String&gt;(prefix + &quot;_&quot; + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .join(skewedUserid2infoRDD)</span><br><span class="line">        .mapToPair(new PairFunction&lt;Tuple2&lt;String,Tuple2&lt;String,Row&gt;&gt;, Long, Tuple2&lt;String, Row&gt;&gt;() &#123;</span><br><span class="line">                        private static final long serialVersionUID = 1L;</span><br><span class="line">                        @Override</span><br><span class="line">                        public Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt; call(</span><br><span class="line">                            Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; tuple)</span><br><span class="line">                            throws Exception &#123;</span><br><span class="line">                            long key = Long.valueOf(tuple._1.split(&quot;_&quot;)[1]);</span><br><span class="line">                            return new Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt;(key, tuple._2);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line"> </span><br><span class="line">// 将rdd1中分拆出来的包含普通key的独立rdd，直接与rdd2进行join。</span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD2 = commonRDD.join(rdd2);</span><br><span class="line"> </span><br><span class="line">// 将倾斜key join后的结果与普通key join后的结果，uinon起来。</span><br><span class="line">// 就是最终的join结果。</span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD = joinedRDD1.union(joinedRDD2);</span><br></pre></td></tr></table></figure>

<h3 id="解决方案七：使用随机前缀和扩容RDD进行join"><a href="#解决方案七：使用随机前缀和扩容RDD进行join" class="headerlink" title="解决方案七：使用随机前缀和扩容RDD进行join"></a>解决方案七：使用随机前缀和扩容RDD进行join</h3><p><strong>方案适用场景：</strong>如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。</p>
<p><strong>方案实现思路：</strong> * 该方案的实现思路基本和“解决方案六”类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。 * 然后将该RDD的每条数据都打上一个n以内的随机前缀。 * 同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。 * 最后将两个处理后的RDD进行join即可。</p>
<p><strong>方案实现原理：</strong>将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。</p>
<p><strong>方案优点：</strong>对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</p>
<p><strong>方案缺点：</strong>该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</p>
<p><strong>方案实践经验：</strong>曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">// 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。</span><br><span class="line">JavaPairRDD&lt;String, Row&gt; expandedRDD = rdd1.flatMapToPair(</span><br><span class="line">        new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID = 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, Row&gt; tuple)</span><br><span class="line">                    throws Exception &#123;</span><br><span class="line">                List&lt;Tuple2&lt;String, Row&gt;&gt; list = new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;();</span><br><span class="line">                for(int i = 0; i &lt; 100; i++) &#123;</span><br><span class="line">                    list.add(new Tuple2&lt;String, Row&gt;(0 + &quot;_&quot; + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                return list;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line">// 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。</span><br><span class="line">JavaPairRDD&lt;String, String&gt; mappedRDD = rdd2.mapToPair(</span><br><span class="line">        new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID = 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple)</span><br><span class="line">                    throws Exception &#123;</span><br><span class="line">                Random random = new Random();</span><br><span class="line">                int prefix = random.nextInt(100);</span><br><span class="line">                return new Tuple2&lt;String, String&gt;(prefix + &quot;_&quot; + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line">// 将两个处理后的RDD进行join即可。</span><br><span class="line">JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRDD = mappedRDD.join(expandedRDD);</span><br></pre></td></tr></table></figure>

<h3 id="解决方案八：多种方案组合使用"><a href="#解决方案八：多种方案组合使用" class="headerlink" title="解决方案八：多种方案组合使用"></a>解决方案八：多种方案组合使用</h3><p>在实践中发现，很多情况下，如果只是处理较为简单的数据倾斜场景，那么使用上述方案中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方案组合起来使用。比如说，我们针对出现了多个数据倾斜环节的Spark作业，可以先运用解决方案一和二，预处理一部分数据，并过滤一部分数据来缓解；其次可以对某些shuffle操作提升并行度，优化其性能；最后还可以针对不同的聚合或join操作，选择一种方案来优化其性能。大家需要对这些方案的思路和原理都透彻理解之后，在实践中根据各种不同的情况，灵活运用多种方案，来解决自己的数据倾斜问题。</p>
<hr>
<p>参考博客 : <a href="https://tech.meituan.com/2016/05/12/spark-tuning-pro.html" target="_blank" rel="noopener">https://tech.meituan.com/2016/05/12/spark-tuning-pro.html</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/04/29/spark/27/">Spark性能优化指南之资源参数调优</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-29</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><h2 id="调优概述"><a href="#调优概述" class="headerlink" title="调优概述"></a>调优概述</h2><p>在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。</p>
<h2 id="Spark作业基本运行原理"><a href="#Spark作业基本运行原理" class="headerlink" title="Spark作业基本运行原理"></a>Spark作业基本运行原理</h2><p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%981.png" alt=""></p>
<p>详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。</p>
<p>在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。</p>
<p>Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。</p>
<p>当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。</p>
<p>因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。</p>
<p>task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。</p>
<p>以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。</p>
<h2 id="资源参数调优"><a href="#资源参数调优" class="headerlink" title="资源参数调优"></a>资源参数调优</h2><p>了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。</p>
<h3 id="num-executors"><a href="#num-executors" class="headerlink" title="num-executors"></a>num-executors</h3><ul>
<li>参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</li>
<li>参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。</li>
</ul>
<h3 id="executor-memory"><a href="#executor-memory" class="headerlink" title="executor-memory"></a>executor-memory</h3><ul>
<li>参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</li>
<li>参数调优建议：每个Executor进程的内存设置4G<del>8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3</del>1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</li>
</ul>
<h3 id="executor-cores"><a href="#executor-cores" class="headerlink" title="executor-cores"></a>executor-cores</h3><ul>
<li>参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</li>
<li>参数调优建议：Executor的CPU core数量设置为2<del>4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3</del>1/2左右比较合适，也是避免影响其他同学的作业运行。</li>
</ul>
<h3 id="driver-memory"><a href="#driver-memory" class="headerlink" title="driver-memory"></a>driver-memory</h3><ul>
<li>参数说明：该参数用于设置Driver进程的内存。</li>
<li>参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。</li>
</ul>
<h3 id="spark-default-parallelism"><a href="#spark-default-parallelism" class="headerlink" title="spark.default.parallelism"></a>spark.default.parallelism</h3><ul>
<li>参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。</li>
<li>参数调优建议：Spark作业的默认task数量为500<del>1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2</del>3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。</li>
</ul>
<h3 id="spark-storage-memoryFraction"><a href="#spark-storage-memoryFraction" class="headerlink" title="spark.storage.memoryFraction"></a>spark.storage.memoryFraction</h3><ul>
<li>参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</li>
<li>参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li>
</ul>
<h3 id="spark-shuffle-memoryFraction"><a href="#spark-shuffle-memoryFraction" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.shuffle.memoryFraction</h3><ul>
<li>参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</li>
<li>参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li>
</ul>
<p>资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。</p>
<h2 id="资源参数参考示例"><a href="#资源参数参考示例" class="headerlink" title="资源参数参考示例"></a>资源参数参考示例</h2><p>以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --master yarn-cluster \</span><br><span class="line">  --num-executors 100 \</span><br><span class="line">  --executor-memory 6G \</span><br><span class="line">  --executor-cores 4 \</span><br><span class="line">  --driver-memory 1G \</span><br><span class="line">  --conf spark.default.parallelism=1000 \</span><br><span class="line">  --conf spark.storage.memoryFraction=0.5 \</span><br><span class="line">  --conf spark.shuffle.memoryFraction=0.3 \</span><br></pre></td></tr></table></figure>

<p>根据实践经验来看，大部分Spark作业经过本次基础篇所讲解的开发调优与资源调优之后，一般都能以较高的性能运行了，足以满足我们的需求。但是在不同的生产环境和项目背景下，可能会遇到其他更加棘手的问题（比如各种数据倾斜），也可能会遇到更高的性能要求。为了应对这些挑战，需要使用更高级的技巧来处理这类问题。在后续的《Spark性能优化指南——高级篇》中，我们会详细讲解数据倾斜调优以及Shuffle调优。</p>
<hr>
<p>参考博客: <a href="https://tech.meituan.com/2016/04/29/spark-tuning-basic.html" target="_blank" rel="noopener">https://tech.meituan.com/2016/04/29/spark-tuning-basic.html</a>    </p>
</div><hr></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/2/">&gt;&gt;</a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>