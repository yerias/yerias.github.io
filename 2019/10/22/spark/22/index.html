<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Kafka Offset管理"><meta name="keywords" content="Spark"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>Kafka Offset管理 | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Current-Offset"><span class="toc-number">1.</span> <span class="toc-text">Current Offset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Committed-Offset"><span class="toc-number">2.</span> <span class="toc-text">Committed Offset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Offset存储模型"><span class="toc-number">3.</span> <span class="toc-text">Offset存储模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka-Offset-的几种管理方式"><span class="toc-number">4.</span> <span class="toc-text">Kafka Offset 的几种管理方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Redis实现Offset幂等性消费"><span class="toc-number">5.</span> <span class="toc-text">Redis实现Offset幂等性消费</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#使用场景"><span class="toc-number">5.1.</span> <span class="toc-text">使用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#原理阐述"><span class="toc-number">5.2.</span> <span class="toc-text">原理阐述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#代码实现"><span class="toc-number">5.3.</span> <span class="toc-text">代码实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#exactly-once方案"><span class="toc-number">5.4.</span> <span class="toc-text">exactly once方案</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#语义"><span class="toc-number">6.</span> <span class="toc-text">语义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#auto-offset-reset参数"><span class="toc-number">7.</span> <span class="toc-text">auto.offset.reset参数</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/xiaoqi.jpg"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><div class="follow-button"><a href="#">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">171</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">35</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">31</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">HADOOP</a><a class="author-info-links__name text-center" href="https://cwiki.apache.org/confluence/display/Hive/" target="_blank" rel="noopener">HIVE</a><a class="author-info-links__name text-center" href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">CDH</a><a class="author-info-links__name text-center" href="http://flume.apache.org/" target="_blank" rel="noopener">FLUME</a><a class="author-info-links__name text-center" href="https://azkaban.github.io/" target="_blank" rel="noopener">AZKABAN</a><a class="author-info-links__name text-center" href="https://www.maxinhong.com/" target="_blank" rel="noopener">叶梨子</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/qingyunzong" target="_blank" rel="noopener">老铁</a><a class="author-info-links__name text-center" href="https://blog.csdn.net/jim8973/" target="_blank" rel="noopener">飞哥</a><a class="author-info-links__name text-center" href="https://vinxikk.github.io/" target="_blank" rel="noopener">vinx</a><a class="author-info-links__name text-center" href="http://dongxicheng.org/" target="_blank" rel="noopener">懂西成(Hadoop技术内幕作者)</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/xing901022/" target="_blank" rel="noopener">xingoo</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/itboys/tag/" target="_blank" rel="noopener">大葱拌豆腐</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/shishanyuan" target="_blank" rel="noopener">郭景瞻(图解Spark作者)</a><a class="author-info-links__name text-center" href="https://segmentfault.com/u/wishdaren_5c243b920a3eb" target="_blank" rel="noopener">Wish大人</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/huxi2b/" target="_blank" rel="noopener">huxi_2b(kafka)</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">Kafka Offset管理</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-10-22</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Spark/">Spark</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">2.7k</span><span class="post-meta__separator">|</span><span>Reading time: 10 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>Kafka中的每个partition都由一系列有序的、不可变的消息组成，这些消息被连续的追加到partition中。partition中的每个消息都有一个连续的序号，用于partition唯一标识一条消息。</p>
<p><strong>Offset记录着下一条将要发送给Consumer的消息的序号。</strong></p>
<p>Offset从语义上来看拥有两种：Current Offset和Committed Offset。</p>
<h2 id="Current-Offset"><a href="#Current-Offset" class="headerlink" title="Current Offset"></a>Current Offset</h2><p>Current Offset保存在Consumer客户端中，它表示Consumer希望收到的下一条消息的序号。它仅仅在pull()方法中使用。例如，Consumer第一次调用pull()方法后收到了20条消息，那么Current Offset就被设置为20。这样Consumer下一次调用pull()方法时，Kafka就知道应该从序号为21的消息开始读取。这样就能够保证每次Consumer pull消息时，都能够收到不重复的消息。</p>
<h2 id="Committed-Offset"><a href="#Committed-Offset" class="headerlink" title="Committed Offset"></a>Committed Offset</h2><p>Committed Offset保存在Broker上，它表示Consumer已经确认消费过的消息的序号。主要通过<code>commitSync</code>和<code>commitAsync</code> API来操作。举个例子，Consumer通过pull() 方法收到20条消息后，此时Current Offset就是20，经过一系列的逻辑处理后，并没有调用<code>consumer.commitAsync()</code>或<code>consumer.commitSync()</code>来提交Committed Offset，那么此时Committed Offset依旧是0。</p>
<p>Committed Offset主要用于Consumer Rebalance。在Consumer Rebalance的过程中，一个partition被分配给了一个Consumer，那么这个Consumer该从什么位置开始消费消息呢？答案就是Committed Offset。另外，如果一个Consumer消费了5条消息（pull并且成功commitSync）之后宕机了，重新启动之后它仍然能够从第6条消息开始消费，因为Committed Offset已经被Kafka记录为5。</p>
<p>总结一下，Current Offset是针对Consumer的pull过程的，它可以保证每次pull都返回不重复的消息；而Committed Offset是用于Consumer Rebalance过程的，它能够保证新的Consumer能够从正确的位置开始消费一个partition，从而避免重复消费。</p>
<h2 id="Offset存储模型"><a href="#Offset存储模型" class="headerlink" title="Offset存储模型"></a>Offset存储模型</h2><p>由于一个partition只能固定的交给一个消费者组中的一个消费者消费，因此Kafka保存offset时并不直接为每个消费者保存，而是以groupid-topic-partition -&gt; offset的方式保存。如图所示：</p>
<p><img src="https://yerias.github.io/spark_img/group-topic-partition-offset.jpg" alt="group-topic-partition-offset"></p>
<p><strong>Kafka在保存Offset的时候，实际上是将Consumer Group和partition对应的offset以消息的方式保存在__consumers_offsets这个topic中</strong>。</p>
<p>下图展示了__consumers_offsets中保存的offset消息的格式：</p>
<p><img src="https://yerias.github.io/spark_img/consumers_offsets.jpg" alt=""></p>
<p><img src="https://yerias.github.io/spark_img/consumers_offsets_data.jpg" alt=""></p>
<p>如图所示，一条offset消息的格式为groupid-topic-partition -&gt; offset。因此consumer pull消息时，已知groupid和topic，又通过Coordinator分配partition的方式获得了对应的partition，自然能够通过Coordinator查找__consumers_offsets的方式获得最新的offset了。</p>
<h2 id="Kafka-Offset-的几种管理方式"><a href="#Kafka-Offset-的几种管理方式" class="headerlink" title="Kafka Offset 的几种管理方式"></a>Kafka Offset 的几种管理方式</h2><ul>
<li><code>Spark Checkpoint：</code>在 Spark Streaming 执行Checkpoint 操作时，将 Kafka Offset 一并保存到 HDFS 中。这种方式的问题在于：当 Spark Streaming 应用升级或更新时，以及当Spark 本身更新时，Checkpoint 可能无法恢复。因而，不推荐采用这种方式。</li>
<li><code>HBASE、Redis 等外部 NOSQL 数据库：</code>这一方式可以支持大吞吐量的 Offset 更新，但它最大的问题在于：用户需要自行编写 HBASE 或 Redis 的读写程序，并且需要维护一个额外的组件。</li>
<li><code>ZOOKEEPER：</code>老版本的位移offset是提交到zookeeper中的，目录结构是 ：/consumers/&lt;group.id&gt;/offsets/ &lt;topic&gt;/&lt;partitionId&gt; ，但是由于 ZOOKEEPER 的写入能力并不会随着 ZOOKEEPER 节点数量的增加而扩大，因而，当存在频繁的 Offset 更新时，ZOOKEEPER 集群本身可能成为瓶颈。因而，不推荐采用这种方式。</li>
<li><code>KAFKA 自身的一个特殊 Topic（__consumer_offsets）中</code>：这种方式支持大吞吐量的Offset 更新，又不需要手动编写 Offset 管理程序或者维护一套额外的集群，但是Kafka不支持事物，不能保证输出的幂等性(Exactly once)。</li>
</ul>
<p><img src="https://yerias.github.io/spark_img/offsets-manager.png" alt="offsets-manager"></p>
<h2 id="Redis实现Offset幂等性消费"><a href="#Redis实现Offset幂等性消费" class="headerlink" title="Redis实现Offset幂等性消费"></a>Redis实现Offset幂等性消费</h2><h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><p>Spark Streaming实时消费kafka数据的时候，程序停止或者Kafka节点挂掉会导致数据丢失，Spark Streaming也没有设置CheckPoint(节点挂了，数据容易丢失)，所以每次出现问题的时候，重启程序，而程序的消费方式是Direct，所以在程序down掉的这段时间Kafka上的数据是消费不到的，虽然可以设置offset为smallest，但是会导致重复消费，重新overwrite hive上的数据，但是不允许重复消费的场景就不能这样做。</p>
<h3 id="原理阐述"><a href="#原理阐述" class="headerlink" title="原理阐述"></a>原理阐述</h3><p>在Spark Streaming中消费 Kafka 数据的时候，有两种方式分别是 ：</p>
<ol>
<li><p>基于 Receiver-based 的 createStream 方法。receiver从Kafka中获取的数据都是存储在Spark Executor的内存中的，然后Spark Streaming启动的job会去处理那些数据。然而，在默认的配置下，这种方式可能会因为底层的失败而丢失数据。如果要启用高可靠机制，让数据零丢失，就必须启用Spark Streaming的<strong>预写日志机制</strong>（Write Ahead Log，WAL）。该机制会同步地将接收到的Kafka数据写入分布式文件系统（比如HDFS）上的预写日志中。所以，即使底层节点出现了失败，也可以使用预写日志中的数据进行恢复。本文对此方式不研究，有兴趣的可以自己实现，个人不喜欢这个方式。</p>
</li>
<li><p>Direct Approach (No Receivers) 方式的 createDirectStream 方法，但是第二种使用方式中  kafka 的 offset 是保存在 checkpoint 中的，如果程序重启的话，会丢失一部分数据，我使用的是这种方式。KafkaUtils.createDirectStream。本文将用代码说明如何将 kafka 中的 offset 保存到 Redis 中，以及如何从 Redis 中读取已存在的 offset。参数auto.offset.reset为latest的时候程序才会读取redis的offset。</p>
</li>
</ol>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>OffsetsManager trait</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">OffsetsManager</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">obtainOffsets</span></span>(topics: <span class="type">Array</span>[<span class="type">String</span>], groupId: <span class="type">String</span>): <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">storeOffsets</span></span>(offsetRanges: <span class="type">Array</span>[<span class="type">OffsetRange</span>], groupId: <span class="type">String</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>RedisOffsetsManager object</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RedisOffsetsManager</span> <span class="keyword">extends</span> <span class="title">OffsetsManager</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 拿到offset</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">obtainOffsets</span></span>(topics: <span class="type">Array</span>[<span class="type">String</span>], groupId: <span class="type">String</span>): <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>] = &#123;</span><br><span class="line">        <span class="comment">// 定义fromOffsets、jedis、offsetMap</span></span><br><span class="line">        <span class="keyword">var</span> fromOffsets: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>] = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">var</span> jedis: <span class="type">Jedis</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">var</span> offsetMap: util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 初始化jedis</span></span><br><span class="line">            jedis = <span class="type">RedisUtils</span>.getJedis</span><br><span class="line">            <span class="comment">// 拿到offsetMap</span></span><br><span class="line">            offsetMap = jedis.hgetAll(topics(<span class="number">0</span>) + <span class="string">"_"</span> + groupId)</span><br><span class="line">            <span class="comment">// 导入java集合转scala的依赖</span></span><br><span class="line">            <span class="keyword">import</span> scala.collection.<span class="type">JavaConverters</span>._</span><br><span class="line">            offsetMap.asScala.foreach(row =&gt; &#123;</span><br><span class="line">                <span class="comment">// 拿到topicPartition</span></span><br><span class="line">                <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(topics(<span class="number">0</span>), row._1.toInt)</span><br><span class="line">                <span class="comment">// 将topicPartition和offset放入fromOffsets</span></span><br><span class="line">                fromOffsets += topicPartition -&gt; row._2.toLong</span><br><span class="line">            &#125;)</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e:<span class="type">Exception</span> =&gt; e.printStackTrace()</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">// 关闭jedis</span></span><br><span class="line">            jedis.close()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 返回fromOffsets</span></span><br><span class="line">        fromOffsets</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// TODO 存储offset</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">storeOffsets</span></span>(offsetRanges: <span class="type">Array</span>[<span class="type">OffsetRange</span>], groupId: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 定义jedis</span></span><br><span class="line">        <span class="keyword">var</span> jedis:<span class="type">Jedis</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 初始化jedis</span></span><br><span class="line">            jedis = <span class="type">RedisUtils</span>.getJedis</span><br><span class="line">            offsetRanges.foreach(o  =&gt; &#123;</span><br><span class="line">                <span class="comment">// 存储offset</span></span><br><span class="line">                jedis.hset(o.topic + <span class="string">"_"</span> + groupId, o.partition + <span class="string">""</span>, o.untilOffset + <span class="string">""</span>)</span><br><span class="line">            &#125;)</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e:<span class="type">Exception</span> =&gt; e.printStackTrace()</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">// 关闭jedis0</span></span><br><span class="line"></span><br><span class="line">            jedis.close()</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里offsets保存在Redis，如果是MySQL同理，只需要实现obtainOffsets和storeOffsets方法</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MySQLOffsetsManager</span> <span class="keyword">extends</span> <span class="title">OffsetsManager</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">obtainOffsets</span></span>(topics: <span class="type">Array</span>[<span class="type">String</span>], groupId: <span class="type">String</span>): <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>] = ???</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">storeOffsets</span></span>(offsetRanges: <span class="type">Array</span>[<span class="type">OffsetRange</span>], groupId: <span class="type">String</span>): <span class="type">Unit</span> = ???</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>OffsetsRedis</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">OffsetsRedis</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 消费者组</span></span><br><span class="line">        <span class="keyword">val</span> groupId = <span class="string">"use_a_separate_group_id_for_each_stream_3"</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">            <span class="string">"bootstrap.servers"</span> -&gt; <span class="string">"hadoop:9090,hadoop:9091,hadoop:9092"</span>,</span><br><span class="line">            <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">            <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">            <span class="string">"group.id"</span> -&gt; groupId,</span><br><span class="line">            <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"earliest"</span>, <span class="comment">//latest</span></span><br><span class="line">            <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 拿到topic</span></span><br><span class="line">        <span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">"test"</span>) <span class="comment">// topic</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 拿到偏移量</span></span><br><span class="line">        <span class="keyword">val</span> fromOffsets = <span class="type">RedisOffsetsManager</span>.obtainOffsets(topics,groupId) <span class="comment">//Map[TopicPartition, Long]()</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> stream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">            ssc,</span><br><span class="line">            <span class="type">PreferConsistent</span>,</span><br><span class="line">            <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams, fromOffsets)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        stream.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (!rdd.isEmpty()) &#123;</span><br><span class="line">                <span class="comment">// 拿到offsetRanges 包括topic、partition、fromOffset、untilOffset</span></span><br><span class="line">                <span class="keyword">val</span> offsetRanges: <span class="type">Array</span>[<span class="type">OffsetRange</span>] = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 已经拿到了offset，可以开始业务逻辑处理</span></span><br><span class="line">                <span class="keyword">val</span> result = rdd.map(row =&gt; &#123;</span><br><span class="line">                    (row.value(), <span class="number">1</span>)</span><br><span class="line">                &#125;).reduceByKey(_ + _)</span><br><span class="line">                .collect()</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 事物写入redis</span></span><br><span class="line">                <span class="keyword">var</span> jedis: <span class="type">Jedis</span> = <span class="literal">null</span></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    jedis = <span class="type">RedisUtils</span>.getJedis</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 提交业务逻辑</span></span><br><span class="line">                    <span class="keyword">for</span> (pair &lt;- result) &#123;</span><br><span class="line">                        jedis.hincrBy(<span class="string">"wc_redis_ss"</span>, pair._1, pair._2)</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 写offset</span></span><br><span class="line">                    <span class="type">RedisOffsetsManager</span>.storeOffsets(offsetRanges,groupId)</span><br><span class="line"></span><br><span class="line">                &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                    <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">                    e.printStackTrace()</span><br><span class="line">                &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                    jedis.close()</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                println(<span class="string">s"拉取数据中..."</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">// 开始作业</span></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="exactly-once方案"><a href="#exactly-once方案" class="headerlink" title="exactly once方案"></a>exactly once方案</h3><p>准确的说也不是严格的方案，要根据实际的业务场景来配合。</p>
<p>现在的方案是保存rdd的最后一个offset，我们可以考虑在处理完一个消息之后就更新offset，保存offset和业务处理做成一个事务，当出现Exception的时候，都进行回退，或者将出现问题的offset和消息发送到另一个kafka或者保存到数据库，另行处理错误的消息。代码demo如下</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">stream.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">    <span class="keyword">if</span> (!rdd.isEmpty()) &#123;</span><br><span class="line">        <span class="comment">// 拿到offsetRanges 包括topic、partition、fromOffset、untilOffset</span></span><br><span class="line">        <span class="keyword">val</span> offsetRanges: <span class="type">Array</span>[<span class="type">OffsetRange</span>] = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 已经拿到了offset，可以开始业务逻辑处理</span></span><br><span class="line">        <span class="keyword">val</span> result = rdd.map(row =&gt; &#123;</span><br><span class="line">            (row.value(), <span class="number">1</span>)</span><br><span class="line">        &#125;).reduceByKey(_ + _)</span><br><span class="line">        .collect()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 事物写入redis</span></span><br><span class="line">        <span class="keyword">var</span> pipeline: <span class="type">Pipeline</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">var</span> jedis: <span class="type">Jedis</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            jedis = <span class="type">RedisUtils</span>.getJedis</span><br><span class="line">            pipeline = jedis.pipelined()</span><br><span class="line">            pipeline.multi()</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 提交业务逻辑</span></span><br><span class="line">            <span class="keyword">for</span> (pair &lt;- result) &#123;</span><br><span class="line">                pipeline.hincrBy(<span class="string">"wc_redis_ss"</span>, pair._1, pair._2)</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 写offset</span></span><br><span class="line">            offsetRanges.foreach(o =&gt; &#123;</span><br><span class="line">                pipeline.hset(topics(<span class="number">0</span>) + <span class="string">"_"</span> + groupId, o.partition + <span class="string">""</span>, o.untilOffset + <span class="string">""</span>)</span><br><span class="line">            &#125;)</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 提交&amp;同步</span></span><br><span class="line">            pipeline.exec()</span><br><span class="line">            pipeline.sync()</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">            <span class="comment">// 失败回滚</span></span><br><span class="line">            pipeline.discard()</span><br><span class="line">            e.printStackTrace()</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">// 关闭连接</span></span><br><span class="line">            pipeline.close()</span><br><span class="line">            jedis.close()</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        println(<span class="string">s"拉取数据中..."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p><code>注意：</code>pipeline不能在Redis集群中使用，但是适用于主从架构</p>
<p><em>20200701更新:</em> 在消费的时候使用事物保证精准一次消费语义效率太低，解决办法是类似于HBase的upsert语法，有则更新，没有则追加。使用try catch套住。</p>
<h2 id="语义"><a href="#语义" class="headerlink" title="语义"></a>语义</h2><p>数据流系统的语义通常根据系统可以处理每个记录的次数来捕获。在所有可能的操作条件下(除了故障等)，系统可以提供三种类型的保证。</p>
<ul>
<li>At most once: 每个记录要么处理一次，要么根本不处理。</li>
<li>At least once:  每个记录将被处理一次或多次。这比最多一次强，因为它确保不会丢失任何数据。但是可能有重复的。</li>
<li>Exactly once: 每条记录将被精确处理一次——没有数据会丢失，也没有数据会被多次处理。这显然是三者中最有力的保证。</li>
</ul>
<h2 id="auto-offset-reset参数"><a href="#auto-offset-reset参数" class="headerlink" title="auto.offset.reset参数"></a>auto.offset.reset参数</h2><p>Kafka 默认是定期帮你自动提交位移的（enable.auto.commit=true）。有时候，我们需要采用自己来管理位移提交，这时候需要设置 enable.auto.commit=false。</p>
<ul>
<li><p>earliest: 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费</p>
</li>
<li><p>latest: 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据</p>
</li>
<li><p>none: topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常</p>
</li>
</ul>
<p>默认建议用earliest。设置该参数后 kafka出错后重启，找到未消费的offset可以继续消费。</p>
<p>而latest 这个设置容易丢失消息，假如kafka出现问题，还有数据往topic中写，这个时候重启kafka，这个设置会从最新的offset开始消费,中间出问题的哪些就不管了。 </p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Tunan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yerias.github.io/2019/10/22/spark/22/">http://yerias.github.io/2019/10/22/spark/22/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="social-share" data-disabled="facebook,twitter,douban,linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/10/23/spark/23/"><i class="fa fa-chevron-left">  </i><span>Spark 读写压缩文件的一次简单尝试</span></a></div><div class="next-post pull-right"><a href="/2019/10/21/spark/21/"><span>SS黑名单管理&amp;窗口&amp;闭包&amp;SS对接Kafka&amp;KafkaRDD</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2020 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="footer_custom_text">大家好，我是图南，很高兴认识你们！</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>