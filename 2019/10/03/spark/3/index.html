<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Spark之WC产生多少个RDD"><meta name="keywords" content="Spark"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>Spark之WC产生多少个RDD | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#目录"><span class="toc-number">1.</span> <span class="toc-text">目录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#WC产生多少个RDD"><span class="toc-number">2.</span> <span class="toc-text">WC产生多少个RDD</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/xiaoqi.jpg"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><div class="follow-button"><a href="#">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">196</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">38</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">34</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">HADOOP</a><a class="author-info-links__name text-center" href="https://cwiki.apache.org/confluence/display/Hive/" target="_blank" rel="noopener">HIVE</a><a class="author-info-links__name text-center" href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">CDH</a><a class="author-info-links__name text-center" href="http://flume.apache.org/" target="_blank" rel="noopener">FLUME</a><a class="author-info-links__name text-center" href="https://azkaban.github.io/" target="_blank" rel="noopener">AZKABAN</a><a class="author-info-links__name text-center" href="https://www.maxinhong.com/" target="_blank" rel="noopener">叶梨子</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/qingyunzong" target="_blank" rel="noopener">老铁</a><a class="author-info-links__name text-center" href="https://blog.csdn.net/jim8973/" target="_blank" rel="noopener">飞哥</a><a class="author-info-links__name text-center" href="https://vinxikk.github.io/" target="_blank" rel="noopener">vinx</a><a class="author-info-links__name text-center" href="http://dongxicheng.org/" target="_blank" rel="noopener">懂西成(Hadoop技术内幕作者)</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/xing901022/" target="_blank" rel="noopener">xingoo</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/itboys/tag/" target="_blank" rel="noopener">大葱拌豆腐</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/shishanyuan" target="_blank" rel="noopener">郭景瞻(图解Spark作者)</a><a class="author-info-links__name text-center" href="https://segmentfault.com/u/wishdaren_5c243b920a3eb" target="_blank" rel="noopener">Wish大人</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/huxi2b/" target="_blank" rel="noopener">huxi_2b(kafka)</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">Spark之WC产生多少个RDD</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-10-03</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Spark/">Spark</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1.8k</span><span class="post-meta__separator">|</span><span>Reading time: 7 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>WC产生多少个RDD</li>
</ol>
<h2 id="WC产生多少个RDD"><a href="#WC产生多少个RDD" class="headerlink" title="WC产生多少个RDD"></a>WC产生多少个RDD</h2><p>一句标准的WC产生了多少个RDD？</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> result = sc.textFile(<span class="string">"E:\\Java\\spark\\tunan-spark\\tunan-spark-core\\data\\wc.txt"</span>).flatMap(_.split(<span class="string">"\t"</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">result.saveAsTextFile(<span class="string">"out"</span>)</span><br></pre></td></tr></table></figure>

<ol>
<li><p>使用<code>toDebugString</code>方法查看RDD的数量</p>
<p>result.toDebugString(不包括<code>saveAsTextFile</code>方法)</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">(<span class="number">2</span>) <span class="type">ShuffledRDD</span>[<span class="number">4</span>] at reduceByKey at wordcount.scala:<span class="number">11</span> []</span><br><span class="line"> +-(<span class="number">2</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at wordcount.scala:<span class="number">11</span> []</span><br><span class="line">    |  <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at flatMap at wordcount.scala:<span class="number">11</span> []</span><br><span class="line">    |  <span class="type">E</span>:\<span class="type">Java</span>\spark\tunan-spark\tunan-spark-core\data\wc.txt <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at wordcount.scala:<span class="number">11</span> []</span><br><span class="line">    |  <span class="type">E</span>:\<span class="type">Java</span>\spark\tunan-spark\tunan-spark-core\data\wc.txt <span class="type">HadoopRDD</span>[<span class="number">0</span>] at textFile at wordcount.scala:<span class="number">11</span> []</span><br></pre></td></tr></table></figure>

<p>上面的方法中：textFile算子中有HadoopRDD和MapPartitionsRDD；flatMap方法有MapPartitionsRDD；map方法有MapPartitionsRDD；reduceByKey方法有ShuffledRDD。</p>
<p>这里一共是5个RDD，如果加上saveAsTextFile方法中的一个MapPartitionsRDD，则一共是6个RDD，如果加上sort方法也是一样的算法。</p>
</li>
<li><p>查看源码的方式计算RDD的数量</p>
<p><code>sc.textFile(&quot;...&quot;)</code></p>
<p>textFile的作用是从HDFS、本地文件系统(在所有节点上可用)或任何hadoop支持的文件系统URI读取文本文件，并将其作为字符串的RDD返回。</p>
<p>path：受支持的文件系统上的文本文件的路径</p>
<p>minPartitions：建议的结果RDD的最小分区数，默认值是2</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(</span><br><span class="line">    path: <span class="type">String</span>,</span><br><span class="line">    minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[<span class="type">String</span>] = withScope &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    hadoopFile(path, classOf[<span class="type">TextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>],</span><br><span class="line">               minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在textFile值调用了hadoopFile方法，该方法传入了<code>path</code>，<code>TextInputFormat</code>(也就是mapreduce中的FileInputFormat方法，特点是按行读取)，<code>LongWritable</code>(mapreduce计算中的key，记录的是offset)，<code>Text</code>(mapreduce计算中的key，记录的是每行的内容)，<code>minPartitions</code>(分区数)，然后返回一个tuple，tuple记录的是key和value，我们这里做了一个处理，<code>.map(pair =&gt; pair._2.toString)</code>方法让结果只有内容，而忽略掉了offset。</p>
<p>继续看hadoopFile的源码</p>
<p>使用任意的InputFormat获取Hadoop文件的RDD，返回一个RDD类型的包含(offset，value)的元组</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hadoopFile</span></span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">      path: <span class="type">String</span>,		<span class="comment">//目录下的输入数据文件，路径可以用逗号分隔路径作为输入列表</span></span><br><span class="line">      inputFormatClass: <span class="type">Class</span>[_ &lt;: <span class="type">InputFormat</span>[<span class="type">K</span>, <span class="type">V</span>]],	<span class="comment">//要读取的数据的存储格式</span></span><br><span class="line">      keyClass: <span class="type">Class</span>[<span class="type">K</span>],	<span class="comment">//key的类型</span></span><br><span class="line">      valueClass: <span class="type">Class</span>[<span class="type">V</span>],	<span class="comment">//value的类型</span></span><br><span class="line">      minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]	<span class="comment">//分区数，默认值是2</span></span><br><span class="line">		= withScope &#123;assertNotStopped()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这是一种强制加载hdfs-site.xml的方法</span></span><br><span class="line">    <span class="type">FileSystem</span>.getLocal(hadoopConfiguration)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//一个Hadoop的配置文件大概10 KB,这是相当大的,所以广播它</span></span><br><span class="line">    <span class="keyword">val</span> confBroadcast = broadcast(<span class="keyword">new</span> <span class="type">SerializableConfiguration</span>(hadoopConfiguration))</span><br><span class="line">    <span class="keyword">val</span> setInputPathsFunc = (jobConf: <span class="type">JobConf</span>) =&gt; 						<span class="type">FileInputFormat</span>.setInputPaths(jobConf, path)	<span class="comment">//设置作业环境</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">HadoopRDD</span>(	<span class="comment">//创建一个HadoopRDD</span></span><br><span class="line">      <span class="keyword">this</span>,</span><br><span class="line">      confBroadcast,	<span class="comment">//广播配置</span></span><br><span class="line">      <span class="type">Some</span>(setInputPathsFunc),	<span class="comment">//作业环境也许可能出错，所以使用Some()</span></span><br><span class="line">      inputFormatClass,	<span class="comment">//读取文件的格式化类</span></span><br><span class="line">      keyClass,	<span class="comment">//key类型</span></span><br><span class="line">      valueClass,	<span class="comment">//value类型</span></span><br><span class="line">      minPartitions).setName(path)	<span class="comment">//分片数</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>因为Hadoop的RecordReader类为每条记录使用相同的Writable对象，直接保存或者直接使用aggregation或者shuffle将会产生很多对同一个对象的引用，所以我们保存、排序或者聚合操作writable对象前，要使用map方法做一个映射。</p>
<p>回到上一步的textFile方法中，Hadoop的返回值是一个包含offset和value的元组，我们只需要内容，所以使用map方法做一个映射，只拿元祖中的value即可</p>
<p><code>.map(pair =&gt; pair._2.toString)</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)	<span class="comment">//初始化检查</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.map(cleanF))	==&gt;(context, pid, iter) ==&gt; hadoopRDD, 分区<span class="type">ID</span>, 迭代器</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该map方法又创建一个MapPartitionsRDD，将map算子应用于每个分区的子RDD。这应用到了RDD五大特性之一的，对每个RDD做计算，实际上是对每个RDD的partition或者split做计算。由于MapPartitionsRDD较为复杂，暂不解析。</p>
<p><strong><em>到此，textFile产生了两个RDD，分别是HadoopRDD和MapPartitionsRDD。共两个RDD</em></strong></p>
</li>
</ol>
<p>   <code>.flatMap(_.split(&quot;\t&quot;))</code></p>
<p>   flatMap首先作用在每一个元素上，然后将结果扁平化，最后返回一个新的RDD</p>
   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.flatMap(cleanF))	<span class="comment">//对RDD的所有分区做计算</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>   <strong><em>到此，flatMap产生了一个RDD，是MapPartitionsRDD。共三个RDD</em></strong></p>
<p>   <code>.map((_, 1))</code></p>
<p>   将map作用在每一个元素上，然后返回一个新的RDD</p>
   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.map(cleanF))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>   <strong><em>到此，map产生了一个RDD，是MapPartitionsRDD。共四个RDD</em></strong></p>
<p>   <code>.reduceByKey(_ + _)</code></p>
<p>   使用联合和交换reduce函数合并每个键的值。<strong>在将结果发送到reduce之前，这也将在每个mapper上本地执行合并，类似于MapReduce中的“combiner”。</strong>输出将使用现有分区器/并行度级别进行哈希分区。</p>
   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">    reduceByKey(defaultPartitioner(self), func)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>   继续查看<code>reduceByKey(defaultPartitioner(self), func)</code>方法</p>
   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(partitioner: <span class="type">Partitioner</span>, func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">    combineByKeyWithClassTag[<span class="type">V</span>]((v: <span class="type">V</span>) =&gt; v, func, func, partitioner)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>   继续查看<code>combineByKeyWithClassTag[V]((v: V) =&gt; v, func, func, partitioner)</code>方法</p>
<p>   这是一个具体干活的方法，它使用一组自定义聚合函数组合每个键的元素。将RDD[(K, V)]转换为RDD[(K, C)]类型的结果，用于“组合类型”C。这是一个复杂的方法，暂不做解析。</p>
   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKeyWithClassTag</span></span>[<span class="type">C</span>](</span><br><span class="line">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    partitioner: <span class="type">Partitioner</span>,</span><br><span class="line">    mapSideCombine: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">    serializer: <span class="type">Serializer</span> = <span class="literal">null</span>)(<span class="keyword">implicit</span> ct: <span class="type">ClassTag</span>[<span class="type">C</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = self.withScope &#123;</span><br><span class="line">    require(mergeCombiners != <span class="literal">null</span>, <span class="string">"mergeCombiners must be defined"</span>) <span class="comment">// required as of Spark 0.9.0</span></span><br><span class="line">    <span class="keyword">if</span> (keyClass.isArray) &#123;</span><br><span class="line">        <span class="keyword">if</span> (mapSideCombine) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Cannot use map-side combining with array keys."</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (partitioner.isInstanceOf[<span class="type">HashPartitioner</span>]) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"HashPartitioner cannot partition array keys."</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> aggregator = <span class="keyword">new</span> <span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">        self.context.clean(createCombiner),</span><br><span class="line">        self.context.clean(mergeValue),</span><br><span class="line">        self.context.clean(mergeCombiners))</span><br><span class="line">    <span class="keyword">if</span> (self.partitioner == <span class="type">Some</span>(partitioner)) &#123;</span><br><span class="line">        self.mapPartitions(iter =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> context = <span class="type">TaskContext</span>.get()</span><br><span class="line">            <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context, aggregator.combineValuesByKey(iter, context))</span><br><span class="line">        &#125;, preservesPartitioning = <span class="literal">true</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](self, partitioner)	<span class="comment">//创建ShuffledRDD</span></span><br><span class="line">        .setSerializer(serializer)</span><br><span class="line">        .setAggregator(aggregator)</span><br><span class="line">        .setMapSideCombine(mapSideCombine)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>   <strong><em>到此，reduceByKey产生了一个RDD，是ShuffledRDD。共五个RDD</em></strong></p>
<p>   <code>.saveAsTextFile(&quot;...&quot;)</code></p>
<p>   将每个元素使用字符串表示形式，将此RDD保存为文本文件。</p>
   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsTextFile</span></span>(path: <span class="type">String</span>): <span class="type">Unit</span> = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> nullWritableClassTag = implicitly[<span class="type">ClassTag</span>[<span class="type">NullWritable</span>]]	</span><br><span class="line">    <span class="keyword">val</span> textClassTag = implicitly[<span class="type">ClassTag</span>[<span class="type">Text</span>]]</span><br><span class="line">    <span class="keyword">val</span> r = <span class="keyword">this</span>.mapPartitions &#123; iter =&gt;	<span class="comment">//注意这里</span></span><br><span class="line">        <span class="keyword">val</span> text = <span class="keyword">new</span> <span class="type">Text</span>()</span><br><span class="line">        iter.map &#123; x =&gt;</span><br><span class="line">            text.set(x.toString)</span><br><span class="line">            (<span class="type">NullWritable</span>.get(), text)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">RDD</span>.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, <span class="literal">null</span>)</span><br><span class="line">    .saveAsHadoopFile[<span class="type">TextOutputFormat</span>[<span class="type">NullWritable</span>, <span class="type">Text</span>]](path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>   通过每次将一个分区的数据以流的方式传入到HDFS中再关闭流</p>
   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanedF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>(	<span class="comment">//创建MapPartitionsRDD</span></span><br><span class="line">    <span class="keyword">this</span>,</span><br><span class="line">    (context: <span class="type">TaskContext</span>, index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedF(iter),</span><br><span class="line">    preservesPartitioning)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>   <del><strong><em>到此，saveAsTextFile产生了一个RDD，是MapPartitionsRDD。共六个RDD</em></strong></del></p>
<p>   <em>20200325更新：</em></p>
<p>   在最后的saveAsTextFile()算子中，我们忽略了一个RDD，它就是是<code>PairRDDFunctions</code>，这个RDD是通过RDD隐式转换过来的</p>
   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">rddToPairRDDFunctions</span></span>[<span class="type">K</span>, <span class="type">V</span>](rdd: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)])</span><br><span class="line">(<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>], vt: <span class="type">ClassTag</span>[<span class="type">V</span>], ord: <span class="type">Ordering</span>[<span class="type">K</span>] = <span class="literal">null</span>): <span class="type">PairRDDFunctions</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">PairRDDFunctions</span>(rdd)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>   <strong><em>修正后的 RDD数量是 7个</em></strong></p>
<ol start="3">
<li>总结：使用toDebugString方法，简单的看到了生成了多少个RDD，通过阅读源码的方式，详细了解到了生成了多少个RDD，他们分别做了什么事情。我们这个流程生成了<del>6</del>7个RDD，如果对结果进行排序，也是相同的方法可以看到答案。</li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Tunan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yerias.github.io/2019/10/03/spark/3/">http://yerias.github.io/2019/10/03/spark/3/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="social-share" data-disabled="facebook,twitter,douban,linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/10/04/spark/4/"><i class="fa fa-chevron-left">  </i><span>Spark之排序模块</span></a></div><div class="next-post pull-right"><a href="/2019/10/02/spark/2/"><span>Spark之Transformations&amp;Action</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2020 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="footer_custom_text">大家好，我是图南，很高兴认识你们！</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>