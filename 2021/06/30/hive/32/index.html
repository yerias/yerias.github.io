<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Hive运行SQL重置进度，如何控制map个数与参数调优"><meta name="keywords" content="Hive"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>Hive运行SQL重置进度，如何控制map个数与参数调优 | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#背景"><span class="toc-number">1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#思考"><span class="toc-number">2.</span> <span class="toc-text">思考</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive有哪些参数，如何查看这些参数"><span class="toc-number">2.1.</span> <span class="toc-text">Hive有哪些参数，如何查看这些参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#map个数的控制参数与性能调优"><span class="toc-number">2.2.</span> <span class="toc-text">map个数的控制参数与性能调优</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#数据准备，两张表"><span class="toc-number">2.2.1.</span> <span class="toc-text">数据准备，两张表</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#两个表进行关联，其中小表在前，大表在后"><span class="toc-number">2.2.2.</span> <span class="toc-text">两个表进行关联，其中小表在前，大表在后</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#案例演示决定map个数的因素"><span class="toc-number">2.3.</span> <span class="toc-text">案例演示决定map个数的因素</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/xiaoqi.jpg"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><div class="follow-button"><a href="#">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">265</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">39</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">35</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">HADOOP</a><a class="author-info-links__name text-center" href="https://cwiki.apache.org/confluence/display/Hive/" target="_blank" rel="noopener">HIVE</a><a class="author-info-links__name text-center" href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">CDH</a><a class="author-info-links__name text-center" href="http://flume.apache.org/" target="_blank" rel="noopener">FLUME</a><a class="author-info-links__name text-center" href="https://azkaban.github.io/" target="_blank" rel="noopener">AZKABAN</a><a class="author-info-links__name text-center" href="https://www.maxinhong.com/" target="_blank" rel="noopener">叶梨子</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/qingyunzong" target="_blank" rel="noopener">老铁</a><a class="author-info-links__name text-center" href="https://blog.csdn.net/jim8973/" target="_blank" rel="noopener">飞哥</a><a class="author-info-links__name text-center" href="https://vinxikk.github.io/" target="_blank" rel="noopener">vinx</a><a class="author-info-links__name text-center" href="http://dongxicheng.org/" target="_blank" rel="noopener">懂西成(Hadoop技术内幕作者)</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/xing901022/" target="_blank" rel="noopener">xingoo</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/itboys/tag/" target="_blank" rel="noopener">大葱拌豆腐</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/shishanyuan" target="_blank" rel="noopener">郭景瞻(图解Spark作者)</a><a class="author-info-links__name text-center" href="https://segmentfault.com/u/wishdaren_5c243b920a3eb" target="_blank" rel="noopener">Wish大人</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/huxi2b/" target="_blank" rel="noopener">huxi_2b(kafka)</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">Hive运行SQL重置进度，如何控制map个数与参数调优</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-06-30</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Hive/">Hive</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">4.3k</span><span class="post-meta__separator">|</span><span>Reading time: 16 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>生产上执行一条简单的sql却一直失败，猜测是Container的Memory或者Core超出了阈值，自动被杀了。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> t <span class="keyword">select</span> * <span class="keyword">from</span> t2</span><br></pre></td></tr></table></figure>

<p><img src="https://yerias.github.io/hadoop_img/hive_reset.png" alt=""></p>
<p>目前已知的是文件大小200M，压缩格式是ORC，从日志中发现了一个可疑的地方，mapper的个数是1，怀疑是mapper的个数太少了，遂增加mapper的个数后，执行成功。</p>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><h3 id="Hive有哪些参数，如何查看这些参数"><a href="#Hive有哪些参数，如何查看这些参数" class="headerlink" title="Hive有哪些参数，如何查看这些参数"></a>Hive有哪些参数，如何查看这些参数</h3><ol>
<li>Hive自带的配置属性列表封装在HiveConf类中，因此请参阅该HiveConf.java文件以获取Hive版本中可用的配置属性的完整列表。具体可以下载hive.src通过eclipse查看。全部属性有上千个吧，一般Hive的自带属性都是以hive.开头的，每个属性且自带详细的描述信息，其次Hive官网也有，但是属性不是特别全。<a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties" target="_blank" rel="noopener">Hive官方参数网址</a></li>
<li>Hive除了自身带了一些配置属性，因为其底层使用的是hadoop(HDFS,MR,YARN),所以有些HADOOP的配置属性Hive也可以使用，进行配置，但是有些则使用不了。比如mapred.max.split.size 就属于MR的参数，但是hive可以使用。</li>
</ol>
<h3 id="map个数的控制参数与性能调优"><a href="#map个数的控制参数与性能调优" class="headerlink" title="map个数的控制参数与性能调优"></a>map个数的控制参数与性能调优</h3><p>很显然，对于这个控制每个map的split输入大小的参数，不是hive自带的参数，而是MR提供的参数，但是Hive可以通过set的形式配置使用，而且对于调优有很大的作用。但是这个参数实际上要配合HDFS的blocksize一起使用。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 每个Map最大输入大小，</span></span><br><span class="line">hive&gt; set mapred.max.split.size;</span><br><span class="line">mapred.max.split.size=256000000</span><br><span class="line"><span class="comment">-- 每个Map最小输入大小</span></span><br><span class="line">hive&gt; set mapred.min.split.size;</span><br><span class="line">mapred.min.split.size=10000000</span><br><span class="line">hive&gt; set dfs.block.size;</span><br><span class="line">dfs.block.size=134217728   我们集群默认hdfs的block块大小是128Mb，但注意这个参数通过hive设置更改实际没有用的，只能hdfs设置。</span><br></pre></td></tr></table></figure>

<h4 id="数据准备，两张表"><a href="#数据准备，两张表" class="headerlink" title="数据准备，两张表"></a>数据准备，两张表</h4><p>如下进行两张表join，其中每张表的大小，hdfs上存储的文件总个数，以及每个文件的大小。</p>
<ol>
<li>大表总共158749566行，文件总大小4.4G,存储个数22个文件，每个大小200Mb左右。</li>
<li>小表总共1979375 行，文件大小50.7Mb，存储个数2个文件，大小50Mb以内。</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[finance<span class="meta">@hadoop</span>-client13-prd ~]$ hadoop fs -du  -h  hdfs:<span class="comment">//suninghadoop2/user/finance/hive/warehouse/fdm_tmp.db/company_liquidation_fgeics_company_ar_d</span></span><br><span class="line"><span class="number">206.7</span> M  hdfs:<span class="comment">//suninghadoop2/user/finance/hive/warehouse/fdm_tmp.db/company_liquidation_fgeics_company_ar_d/000000_0.deflate</span></span><br><span class="line">.....省略.................</span><br><span class="line">hdfs:<span class="comment">//suninghadoop2/user/finance/hive/warehouse/fdm_tmp.db/company_liquidation_fgeics_company_ar_d/000021_0.deflate</span></span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">[finance<span class="meta">@hadoop</span>-client13-prd ~]$ hadoop fs -du  -h  hdfs:<span class="comment">//suninghadoop2/user/finance/hive/warehouse/fdm_tmp.db/t_fgeics_company_liquidation_d_tmp</span></span><br><span class="line"><span class="number">36.4</span> M  hdfs:<span class="comment">//suninghadoop2/user/finance/hive/warehouse/fdm_tmp.db/t_fgeics_company_liquidation_d_tmp/000000_0.deflate</span></span><br><span class="line"><span class="number">14.3</span> M  hdfs:<span class="comment">//suninghadoop2/user/finance/hive/warehouse/fdm_tmp.db/t_fgeics_company_liquidation_d_tmp/000001_0.deflate</span></span><br></pre></td></tr></table></figure>

<h4 id="两个表进行关联，其中小表在前，大表在后"><a href="#两个表进行关联，其中小表在前，大表在后" class="headerlink" title="两个表进行关联，其中小表在前，大表在后"></a>两个表进行关联，其中小表在前，大表在后</h4><p>如下，运行如下代码，实现两个表进行关联。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.max.split.size=<span class="number">134217728</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size=<span class="number">134217728</span>;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> fdm_tmp.company_liquidation_fgeics_company_ar_d_tmp;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> fdm_tmp.company_liquidation_fgeics_company_ar_d_tmp</span><br><span class="line"><span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span></span><br><span class="line"> a.id</span><br><span class="line">,a.entid</span><br><span class="line">,a.ancheyear</span><br><span class="line">,b.liqmen</span><br><span class="line">,b.ligprincipal</span><br><span class="line">,a.regno</span><br><span class="line">,a.tel</span><br><span class="line">,a.postalcode</span><br><span class="line">,a.dom</span><br><span class="line">,a.email</span><br><span class="line">,a.busst</span><br><span class="line">,a.empnum</span><br><span class="line">,a.name</span><br><span class="line">,a.updated</span><br><span class="line">,b.etl_time</span><br><span class="line"><span class="keyword">from</span> fdm_tmp.t_fgeics_company_liquidation_d_tmp  b</span><br><span class="line"><span class="keyword">right</span> <span class="keyword">join</span> fdm_tmp.company_liquidation_fgeics_company_ar_d  a</span><br><span class="line"><span class="keyword">on</span> b.entid = a.entid;</span><br><span class="line"></span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 24; number of reducers: 17</span><br></pre></td></tr></table></figure>

<p>结果分析：</p>
<p>hive启动了24个map函数，17个reduce函数。在hadoop中，一般默认的split切片小于等于blocksize(128Mb),如果是小文件的话(未进行小文件的合并）则每个小文件启动一个map函数。而实际在hive中，并不是split的大小要小于等于blocksize，而是可以远大于blocksize。比如这里，4.4G文件表，按128Mb切片算的话，至少实际需要35个map，而实际只需要24个,平均每个map处理了187Mb的文件。这是为什么呢？</p>
<p>此外这里明明设置了set mapred.max.split.size=134217728，每个map最大split块是 128Mb，而实际为什么参数没有用呢？网上有很多关于这方面的文章，但是几乎都是转载抄袭，没有任何深入理解，或者深入剖析决定map个数的原因。</p>
<h3 id="案例演示决定map个数的因素"><a href="#案例演示决定map个数的因素" class="headerlink" title="案例演示决定map个数的因素"></a>案例演示决定map个数的因素</h3><p>其实决定map个数的因素有很多，比如文件是否压缩，压缩的后的文件是否支持切分，比如文件默认的inputfort格式，不同实现类的split算法也不同，那么map的个数调优方式也不同，下面按分类详细说明hive中决定map个数的因素与常见map调优的使用。</p>
<p>首先分类: 处理的文件是否压缩，且压缩算法是否支持文件的切分</p>
<ol>
<li><p>文件使用了压缩，且压缩算法不支持文件切分</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt;  set io.compression.codecs; --配置了哪些压缩算法</span><br><span class="line">io.compression.codecs=org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.BZip2Codec</span><br><span class="line">hive&gt; set hive.exec.compress.output;</span><br><span class="line">hive.exec.compress.output=true  <span class="comment">--是否开启压缩</span></span><br><span class="line">hive&gt; set  mapreduce.output.fileoutputformat.compress.codec;  --使用的压缩算法</span><br><span class="line">mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodec</span><br></pre></td></tr></table></figure>

<p>我们知道hive中有些压缩算法是不支持文件切分的，如下我们使用的默认的deflate算法，就不支持文件切分。</p>
<p><img src="https://yerias.github.io/hadoop_img/20190817104035345.png" alt=""></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">------------------------使用不同参数执行上面代码产生的map个数-----------------------------</span></span><br><span class="line"><span class="comment">--1.使用系统配置的默认值</span></span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size  = <span class="number">256000000</span>; <span class="comment">--</span></span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size = <span class="number">256000000</span>;</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 24; number of reducers: 17</span><br><span class="line"> </span><br><span class="line"><span class="comment">--2.降低系统默认值</span></span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size=<span class="number">134217728</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size=<span class="number">134217728</span>;</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 24; number of reducers: 17</span><br><span class="line"> </span><br><span class="line"><span class="comment">--3.调高系统默认值</span></span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size=<span class="number">500000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size=<span class="number">256000000</span>;</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 9; number of reducers: 17</span><br><span class="line"> </span><br><span class="line"><span class="comment">--4.调高系统默认值</span></span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size=<span class="number">1024000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size=<span class="number">1024000000</span>;</span><br><span class="line">Hadoop job information for Stage-1: number of mappers:6 ; number of reducers: 17</span><br></pre></td></tr></table></figure>

<p>如上我们使用不同的参数配置，来运行上面同一段代码，看系统产生的map个数，细心的人会发现，当我们使用默认值是产生了24个map,平均每个map处理了187Mb文件，但当我们调低set mapred.max.split.size=134217728时(每个map最多处理128Mb)，相应的map个数并没有增加，这是为什么呢？</p>
<p>关于这个问题就要说到决定map个数的首要因素：<strong>文件是否启动压缩，且压缩算法是否支持文件切分了。</strong>因为这里文件存储使用默认的deflate算法，不支持文件切分，所以设置的参数split.size=134217728没有生效。因为每个map处理的splitsize实际上要大于等于每个文件存储的大小。这里每个文件存储的大小200Mb左右，所以每个map处理的最小尺寸要大于200Mb。</p>
<p>而当我们将set mapred.max.split.size=102400000设置的很大时，为什么又可以控制map个数了呢？<strong>因为deflate压缩算法虽然不支持文件切分，但是可以进行文件合并哇。</strong>从hive0.5开始就默认map前进行小文件合并了。如下，我们使用的也是默认的开启map前文件合并。但是注意即使这里支持文件合并，也是基于文件块的整个文件块合并，而不是基于blocksize的block合并。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; set hive.input.format; --hive0.5开始的默认值，这个值会影响map个数的控制</span><br><span class="line">hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</span><br></pre></td></tr></table></figure>

<p>提示： </p>
<p>通过上面分析总结，<strong>当hive需要处理的文件是压缩，且压缩算法不支持文件切分的时候，决定map个数的因素主要是文件块实际存储的大小</strong>，如果文件块本身很大，比如500Mb左右，那么每个map处理的splitsize至少要是500Mb左右。这个时候我们不能人为通过参数降低每个map的splitsize来增加map个数，只能通过增加splitsize，减少map个数。</p>
<p>但是一般经验来说，每个map处理的splitsize最好是128Mb(等于blocksize）,这样效率最高。所以这个时候如果我们想增加map个数，只能通过临时表或者insert …select的形式，通过参数配置将文件块重新存储成较小的文件块，然后再进行处理。反之，如果文件块本身很小，那么我们可以通过增加splitsize来减少map，进而调优提高程序的运行效率。</p>
<p>总结：</p>
<p>如果hive处理的文件是压缩模式，且压缩模式不支持文件切分，那么这个时候我们只能通过控制参数来减少map个数，而不能通过配置参数来增加map个数，所以Hive对于压缩不可切分文件的调优有限。可以首先通过hadoop fs -du -s -h命令查看文件的存储大小结果，然后根据实际进行调优。常用的配置参数如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; <span class="comment">--hive0.5开始就是默认值，执行map前进行小文件合并</span></span><br><span class="line"><span class="comment">----------------------------------------------------------------------</span></span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size=<span class="number">256000000</span>  </span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size=<span class="number">10000000</span></span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node=<span class="number">8000000</span> <span class="comment">--每个节点处理的最小split</span></span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack=<span class="number">8000000</span> <span class="comment">--每个机架处理的最小slit.</span></span><br><span class="line"><span class="comment">------------------------------------------------------------------------</span></span><br><span class="line"><span class="number">1.</span>注意一般来说这四个参数的配置结果大小要满足如下关系。</span><br><span class="line">max.split.size &gt;= min.split.size &gt;= min.size.per.node &gt;= min.size.per.node</span><br><span class="line"> </span><br><span class="line"><span class="number">2.</span>这四个参数的作用优先级分别如下</span><br><span class="line">max.split.size &lt;= min.split.size &lt;= min.size.per.node &lt;= min.size.per.node</span><br><span class="line"> </span><br><span class="line">比如如下，同样上面的代码，我们将其参数设置如下，发现只启动了<span class="number">12</span>个<span class="keyword">map</span>，故max.split.size没有起作用。</span><br><span class="line">当四个参数设置矛盾时，系统会自动以优先级最高的参数为准，进行计算</span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size=<span class="number">300000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node=<span class="number">300000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack=<span class="number">300000000</span>;</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 12; number of reducers: 17</span><br><span class="line"> </span><br><span class="line">3.注意这四个参数可以选择性设置，可以选择性设置大小或者使用默认值，但仍遵循前两条规则。</span><br></pre></td></tr></table></figure>
</li>
<li><p>文件未使用压缩，或压缩算法支持文件切分</p>
<p>同样是上面那个4.4g文件，我们这个时候让其为非压缩模式，发现这个时候文件总大小为23.4G,存储为22个文件，平均每个文件大小都在1.1G左右。所以压缩有时候是个好东西。如下我们所有非压缩的性能测试基于此文件。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[finance]$ hadoop fs -count hdfs:<span class="comment">//suninghadoop2/user/finance/hive/warehouse/fdm_tmp.db/company_liquidation_fgeics_company_ar_d</span></span><br><span class="line"><span class="number">1</span>           <span class="number">22</span>        <span class="number">25154158871</span> hdfs:<span class="comment">//suninghadoop2/user/finance/hive/warehouse/fdm_tmp.db/company_liquidation_fgeics_company_ar_d</span></span><br><span class="line">-----------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">[finance]$ hadoop fs -du -s -h hdfs:<span class="comment">//suninghadoop2/user/finance/hive/warehouse/fdm_tmp.db/company_liquidation_fgeics_company_ar_d</span></span><br><span class="line"><span class="number">23.4</span> G  hdfs:<span class="comment">//suninghadoop2/user/finance/hive/warehouse/fdm_tmp.db/company_liquidation_fgeics_company_ar_d</span></span><br><span class="line">-----------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">[finance]$ hadoop fs -du -h hdfs:<span class="comment">//suninghadoop2/user/finance/hive/warehouse/fdm_tmp.db/company_liquidation_fgeics_company_ar_d</span></span><br><span class="line"><span class="number">1.1</span> G  hdfs:<span class="comment">//suninghadoop2/user/finance/hive/warehouse/fdm_tmp.db/company_liquidation_fgeics_company_ar_d/000000_0</span></span><br><span class="line"><span class="number">1.1</span> G  hdfs:<span class="comment">//suninghadoop2/user/finance/hive/warehouse/fdm_tmp.db/company_liquidation_fgeics_company_ar_d/000001_0</span></span><br><span class="line">...............................</span><br></pre></td></tr></table></figure>

<p><strong>若这时set hive.input.format为HiveInputFormat</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; set hive.input.format; --从hive0.5就默认是CombineHiveInputFormat，所以这个用的不多</span><br><span class="line">hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br></pre></td></tr></table></figure>

<p>如果这时set hive.input.format为HiveInputFormat，这个时候有以下三个属性值来确定InputSplit的个数：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.map.tasks=<span class="number">2</span></span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size=<span class="number">256000000</span></span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size=<span class="number">10000000</span></span><br><span class="line"><span class="keyword">set</span>  dfs.block.size=<span class="number">134217728</span>   <span class="comment">--128Mb</span></span><br><span class="line"><span class="comment">--------------------------------------------------------------------------------------</span></span><br><span class="line"><span class="number">1.</span> goalSize：该值由 totalSize/numSplits  totlasize文件块大小，numslplits=mapred.map.tasks=<span class="number">2</span> </span><br><span class="line"><span class="number">2.</span> minSize：由配置参数 mapred.min.split.size（或者新版本的mapreduce.input.fileinputformat.split.minsize）mapred.min.split.size=<span class="number">10000000</span> </span><br><span class="line">   决定的 InputFormat的最小长度，默认为<span class="number">1</span></span><br><span class="line"><span class="number">3.</span><span class="keyword">blockSize</span>：HDFS 中的文件存储块<span class="keyword">block</span>的大小，默认为<span class="number">128</span>MB。</span><br><span class="line"> </span><br><span class="line">这三个参数决定一个 InputFormat 分片的最终的长度，计算方法如下：</span><br><span class="line">splitSize = <span class="keyword">max</span>&#123;minSize,<span class="keyword">min</span>&#123;goalSize,<span class="keyword">blockSize</span>&#125;&#125;</span><br></pre></td></tr></table></figure>

<p>总结:</p>
<p>当hive处理的文件是非压缩或者压缩可切分，且hive.input.format为HiveInputFormat时，这个时候决定map个数的参数主要是splitSize = max{minSize,min{goalSize,blockSize}} ，只有这个时候一般map的splitsize小于等于blocksize（128Mb）。但其实这种方式现在企业实际开发中已经使用的很少了。</p>
<p><strong>若hive.input.format为默认CombineHiveInputFormat</strong>    </p>
<p>如下，使用默认的map函数split参数，发现未压缩的23.4g的文件(22个）这里共使用了112个map函数，符合参数的设置结果set mapred.max.split.size=256000000  ; 23.4*1024/112=187Mb&lt;256000000。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--使用默认配置参数，实现对非压缩文件的操作</span></span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size=<span class="number">256000000</span>  ;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size=<span class="number">10000000</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> fdm_tmp.company_liquidation_fgeics_company_ar_d_tmp;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> fdm_tmp.company_liquidation_fgeics_company_ar_d_tmp</span><br><span class="line"><span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span></span><br><span class="line"> a.id</span><br><span class="line">,a.entid</span><br><span class="line">,a.ancheyear</span><br><span class="line">,b.liqmen</span><br><span class="line">,b.ligprincipal</span><br><span class="line">,a.regno</span><br><span class="line">,a.tel</span><br><span class="line">,a.postalcode</span><br><span class="line">,a.dom</span><br><span class="line">,a.email</span><br><span class="line">,a.busst</span><br><span class="line">,a.empnum</span><br><span class="line">,a.name</span><br><span class="line">,a.updated</span><br><span class="line">,b.etl_time</span><br><span class="line"><span class="keyword">from</span> fdm_tmp.t_fgeics_company_liquidation_d_tmp  b</span><br><span class="line"><span class="keyword">right</span> <span class="keyword">join</span> fdm_tmp.company_liquidation_fgeics_company_ar_d  a</span><br><span class="line"><span class="keyword">on</span> b.entid = a.entid;</span><br><span class="line"> </span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 112; number of reducers: 85</span><br><span class="line">Time taken: 152.679 seconds</span><br></pre></td></tr></table></figure>

<p>如下，将上面默认的参数对应调小，运行同样上面的代码，看map数是否有增加？ 很显然，我们通过设置max.split.size的值实现了增加map个数的功能。这里map的个数由122个数变成了199个，平均每个map处理数据120Mb左右。但是运行时间却变慢了很多，时间由153s变成了243s。所以map的split大小并不是要接近blocksize才高效，这主要跟集群的性能配置有关。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--将map的split参数最大值设置为128Mb.</span></span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size=<span class="number">134217728</span> ;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size=<span class="number">10000000</span>;</span><br><span class="line"> </span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 199; number of reducers: 85</span><br><span class="line">Time taken: 243.038 seconds</span><br></pre></td></tr></table></figure>

<p>如下,将上面的默认参数增加，运行同样的代码，结果虽然我们将maxsplitsize设置的特别大，但是对应map的个数并没有对应的成倍减少，如果按最大值算应该在20多个map，而实际不是这样。这说明，光设置最大值是没有用的，这只是一个峰值，还有对应的设置最小值。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--只将max.split.size设置的特别大，min.split.size还是10Mb左右。</span></span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size=<span class="number">1024000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size= <span class="number">10000000</span>;</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 106; number of reducers: 85</span><br></pre></td></tr></table></figure>

<p>map的多个参数配合使用，精确控制map的个数 </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"> </span><br><span class="line"><span class="comment">--1.只将max.split.size设置的特别大，且将 min.split.size使用默认值，发现map个数没有成倍减少。</span></span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size=<span class="number">1024000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size=<span class="number">10000000</span>;</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 106; number of reducers: 85</span><br><span class="line"><span class="comment">------------------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment">--2.同时将max.split.size设置的特别大，且将 min.split.size同时设置很大为256Mb左右</span></span><br><span class="line">    但是发现map的个数并没有减少，还是和上面一样，这说明控制map的个数还有别的因素</span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size=<span class="number">1024000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size=<span class="number">256000000</span>;</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 106; number of reducers: 85</span><br><span class="line"><span class="comment">------------------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment">---3.配合min.split.size.per.node使用，发现map个数仍然没有减少</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size=<span class="number">1024000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size= <span class="number">256000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node=<span class="number">256000000</span>;<span class="comment">--默认值是800000</span></span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack=<span class="number">800000</span>;<span class="comment">---默认值</span></span><br><span class="line"><span class="comment">--------------------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment">---4.配合min.split.size.per.rack使用，map个数精准减少了，每个map处理的数据在256和1024之间</span></span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 106; number of reducers: 85</span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size=<span class="number">1024000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size= <span class="number">256000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node=<span class="number">256000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack=<span class="number">256000000</span>;</span><br><span class="line"> </span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 88; number of reducers: 85</span><br></pre></td></tr></table></figure>

<p>总结:</p>
<p>如果Hive处理的的文件为非压缩格式或者压缩可切分，且inputFormat为CombineHiveInputFormat时，则控制map个数是由以下四个参数起作用，关于这四个参数作用优先级与使用注意事项请参考如下。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mapred.min.split.size 或者 mapreduce.input.fileinputformat.split.minsize。</span><br><span class="line">mapred.max.split.size 或者 mapreduce.input.fileinputformat.split.maxsize。</span><br><span class="line">mapred.min.split.size.per.rack 或者 mapreduce.input.fileinputformat.split.minsize.per.rack。</span><br><span class="line">mapred.min.split.size.per.node 或者 mapreduce.input.fileinputformat.split.minsize.per.node。</span><br><span class="line"> </span><br><span class="line"><span class="keyword">set</span> hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; <span class="comment">--hive0.5开始就是默认值，执行map前进行小文件合并</span></span><br><span class="line"><span class="comment">----------------------------------------------------------------------</span></span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size=<span class="number">256000000</span>   <span class="comment">--集群默认值</span></span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size=<span class="number">10000000</span>     <span class="comment">--集群默认值</span></span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node=<span class="number">8000000</span> <span class="comment">--每个节点处理的最小split</span></span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack=<span class="number">8000000</span> <span class="comment">--每个机架处理的最小slit.</span></span><br><span class="line"><span class="comment">------------------------------------------------------------------------</span></span><br><span class="line"><span class="number">1.</span>注意一般来说这四个参数的配置结果大小要满足如下关系。</span><br><span class="line">max.split.size &gt;= min.split.size &gt;= min.size.per.node &gt;= min.size.per.node</span><br><span class="line"> </span><br><span class="line"><span class="number">2.</span>这四个参数的作用优先级分别如下</span><br><span class="line">max.split.size &lt;= min.split.size &lt;= min.size.per.node &lt;= min.size.per.node</span><br><span class="line"> </span><br><span class="line">比如如下，同样上面的代码，我们将其参数设置如下，发现只启动了<span class="number">12</span>个<span class="keyword">map</span>，故max.split.size没有起作用。</span><br><span class="line">当四个参数设置矛盾时，系统会自动以优先级最高的参数为准，进行计算</span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size=<span class="number">300000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node=<span class="number">300000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack=<span class="number">300000000</span>;</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 12; number of reducers: 17</span><br><span class="line"> </span><br><span class="line">3.注意这四个参数可以选择性设置，可以选择性设置大小或者使用默认值，但仍遵循前两条规则。</span><br></pre></td></tr></table></figure>

<p>所以如果对于Hive调优，想通过控制map个数进行调优，首先确定集群是否启动了压缩，且压缩的算法是否直接文件切分，然后再确定集群配置的默认的hive.input.format是什么实现类，不同实现类对于split的算法不同，当然控制map的参数也不同。所以对于控制map个数调优远远不是网上很多人说的那么简单。</p>
</li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Tunan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yerias.github.io/2021/06/30/hive/32/">http://yerias.github.io/2021/06/30/hive/32/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Hive/">Hive</a></div><div class="social-share" data-disabled="facebook,twitter,douban,linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/07/04/hive/33/"><i class="fa fa-chevron-left">  </i><span>如何控制reduce个数与参数调优</span></a></div><div class="next-post pull-right"><a href="/2021/06/26/hive/31/"><span>in/exists和not in/not exists的一些性能思考</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2021 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="footer_custom_text">大家好，我是图南，很高兴认识你们！</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>