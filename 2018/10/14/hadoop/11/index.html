<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="数据倾斜&amp;MRchain解决数据倾斜&amp;大小表Reduce Join&amp;大小表Map Join&amp;SQL的执行计划"><meta name="keywords" content="Hadoop,MapReduce"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>数据倾斜&amp;MRchain解决数据倾斜&amp;大小表Reduce Join&amp;大小表Map Join&amp;SQL的执行计划 | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#目标"><span class="toc-number">1.</span> <span class="toc-text">目标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据倾斜"><span class="toc-number">2.</span> <span class="toc-text">数据倾斜</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MRchain解决数据倾斜"><span class="toc-number">3.</span> <span class="toc-text">MRchain解决数据倾斜</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SQL的执行计划"><span class="toc-number">4.</span> <span class="toc-text">SQL的执行计划</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#大小表Reduce-Join-emp、dept"><span class="toc-number">5.</span> <span class="toc-text">大小表Reduce Join(emp、dept)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#大小表Map-Join-emp、dept"><span class="toc-number">6.</span> <span class="toc-text">大小表Map Join(emp、dept)</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/xiaoqi.jpg"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><div class="follow-button"><a href="#">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">100</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">25</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">21</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">HADOOP</a><a class="author-info-links__name text-center" href="https://cwiki.apache.org/confluence/display/Hive/" target="_blank" rel="noopener">HIVE</a><a class="author-info-links__name text-center" href="https://cwiki.apache.org/" target="_blank" rel="noopener">WIKI</a><a class="author-info-links__name text-center" href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">CDH</a><a class="author-info-links__name text-center" href="http://flume.apache.org/" target="_blank" rel="noopener">FLUME</a><a class="author-info-links__name text-center" href="https://azkaban.github.io/" target="_blank" rel="noopener">AZKABAN</a><a class="author-info-links__name text-center" href="https://www.maxinhong.com/" target="_blank" rel="noopener">叶梨子</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/qingyunzong" target="_blank" rel="noopener">老铁</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">主页</a><a class="site-page" href="/archives">归档</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a></span></div><div id="post-info"><div id="post-title">数据倾斜&amp;MRchain解决数据倾斜&amp;大小表Reduce Join&amp;大小表Map Join&amp;SQL的执行计划</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-14</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Hadoop/MapReduce/">MapReduce</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">3.9k</span><span class="post-meta__separator">|</span><span>Reading time: 18 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>数据倾斜</li>
<li>MRchain解决数据倾斜</li>
<li>大小表Reduce Join</li>
<li>大小表Map Join</li>
<li>SQL的执行计划</li>
</ol>
<h2 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h2><ol>
<li><p>数据倾斜怎么造成的</p>
<p>mapreduce计算是将map相同的key丢到reduce，在reduce中进行聚合操作,在map和reduce中间有个shuffle操作，shuffle会将map阶段相同的key划分到reduce阶段中的一个reduce中去，数据倾斜就是数据的key 的分化严重不均，造成一部分数据很多，一部分数据很少的局面。</p>
</li>
<li><p>数据倾斜产生的问题</p>
<ul>
<li><p>有一个或多个reduce卡住</p>
</li>
<li><p>各种container报错OOM</p>
</li>
<li><p>读写的数据量极大，至少远远超过其它正常的reduce</p>
</li>
<li><p>伴随着数据倾斜，会出现任务被kill等各种诡异的表现。</p>
</li>
</ul>
</li>
<li><p>原因和解决方法</p>
<p>原因:</p>
<ul>
<li>单个值有大量记录(1.内存的限制存在，2.可能会对集群其他任务的运行产生不稳定的影响)</li>
<li>唯一值较多(单个唯一值的记录数占用内存不会超过分配给reduce的内存)</li>
</ul>
<p>解决办法:</p>
<ul>
<li><p>增加reduce个数</p>
</li>
<li><p>使用自定义partitioner</p>
</li>
<li><p>增加reduce 的jvm内存（效果不好）</p>
</li>
<li><p>map 阶段将造成倾斜的key 先分成多组加随机数并且在reduce阶段去除随机数</p>
</li>
<li><p>从业务和数据上解决数据倾斜</p>
<p>我们通过设计的角度尝试解决它</p>
<ul>
<li>数据预处理，过滤掉异常值</li>
<li>将数据打散让它的并行度变大，再汇集</li>
</ul>
</li>
<li><p>平台的优化方法</p>
<ul>
<li>join 操作中，使用 map join 在 map 端就先进行 join ，免得到reduce 时卡住</li>
<li>能先进行 group 操作的时候先进行 group 操作，把 key 先进行一次 reduce,之后再进行 count 或者 distinct count 操作</li>
<li>设置map端输出、中间结果压缩</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="MRchain解决数据倾斜"><a href="#MRchain解决数据倾斜" class="headerlink" title="MRchain解决数据倾斜"></a>MRchain解决数据倾斜</h2><p>核心思想: 第一个mapredue把具有数据倾斜特性的数据加盐(随机数)，进行聚合；第二个mapreduce把第一个mapreduce的加盐结果进行去盐，再聚合，问题是两个MR IO高。</p>
<p>参考代码:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.item;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.utils.FileUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.chain.ChainMapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.chain.ChainReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.Random;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-02-04 14:50</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ChainMRDriver</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Random r;</span><br><span class="line">    String in = <span class="string">"data/skew/access.txt"</span>;</span><br><span class="line">    String out1 = <span class="string">"out/mr1"</span>;</span><br><span class="line">    String out2 = <span class="string">"out/mr2"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(conf, <span class="keyword">new</span> ChainMRDriver(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">super</span>.getConf();</span><br><span class="line">        FileUtil.checkFileIsExists(conf, out1);</span><br><span class="line">        FileUtil.checkFileIsExists(conf, out2);</span><br><span class="line"></span><br><span class="line">        Job job1 = Job.getInstance(conf);</span><br><span class="line">        Job job2 = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        job1.setJarByClass(ChainMRDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job2.setJarByClass(ChainMRDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        ChainMapper.addMapper(job1, ChainMRDriver.incRanDomMapper<span class="class">.<span class="keyword">class</span>, <span class="title">LongWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line">        ChainReducer.setReducer(job1, ChainMRDriver.incRanDomReduver<span class="class">.<span class="keyword">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line"></span><br><span class="line">        ChainMapper.addMapper(job2, ChainMRDriver.decRanDomMapper<span class="class">.<span class="keyword">class</span>, <span class="title">LongWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line">        ChainReducer.setReducer(job2, ChainMRDriver.decRanDomReduver<span class="class">.<span class="keyword">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job1, <span class="keyword">new</span> Path(in));</span><br><span class="line">        FileOutputFormat.setOutputPath(job1, <span class="keyword">new</span> Path(out1));</span><br><span class="line">        FileInputFormat.setInputPaths(job2, <span class="keyword">new</span> Path(out1));</span><br><span class="line">        FileOutputFormat.setOutputPath(job2, <span class="keyword">new</span> Path(out2));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//提交job1和job2 job1--&gt;job2 必须按照顺序提交</span></span><br><span class="line">        System.out.println(<span class="string">"=============第一阶段=============="</span>);</span><br><span class="line">        <span class="keyword">boolean</span> b = job1.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">if</span> (b) &#123;</span><br><span class="line">            System.out.println(<span class="string">"=============第二阶段=============="</span>);</span><br><span class="line">            <span class="keyword">boolean</span> b1 = job2.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">            <span class="keyword">return</span> b1 ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">incRanDomMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//创建对象</span></span><br><span class="line">            r = <span class="keyword">new</span> Random();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//把数据读出来，加盐  www.baidu.com   2</span></span><br><span class="line">            String[] line = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">            String incR = r.nextInt(<span class="number">10</span>) +<span class="string">"_"</span>+line[<span class="number">0</span>];</span><br><span class="line">            <span class="keyword">int</span> number = Integer.parseInt(line[<span class="number">1</span>]);</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(incR), <span class="keyword">new</span> IntWritable(number));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (r != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">//回收对象</span></span><br><span class="line">                r = <span class="keyword">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">incRanDomReduver</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">int</span> sums = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (IntWritable sum : values) &#123;</span><br><span class="line">                sums += sum.get();</span><br><span class="line">            &#125;</span><br><span class="line">            context.write(key, <span class="keyword">new</span> IntWritable(sums));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">decRanDomMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="comment">//去盐 聚合</span></span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            String[] line = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">            String decWord = line[<span class="number">0</span>].split(<span class="string">"_"</span>)[<span class="number">1</span>];</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(decWord), <span class="keyword">new</span> IntWritable(Integer.parseInt(line[<span class="number">1</span>])));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">decRanDomReduver</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">int</span> sums = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (IntWritable sum : values) &#123;</span><br><span class="line">                sums += sum.get();</span><br><span class="line">            &#125;</span><br><span class="line">            context.write(key, <span class="keyword">new</span> IntWritable(sums));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="SQL的执行计划"><a href="#SQL的执行计划" class="headerlink" title="SQL的执行计划"></a>SQL的执行计划</h2><p>如何运行SQL的执行计划</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">EXPLAIN</span> [<span class="keyword">EXTENDED</span>] Syntax</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">EXPLAIN</span> <span class="keyword">select</span> e.empno,e.ename,d.deptno,d.dname <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure>

<p>解析这句SQL的执行计划</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">+-----------------------------------------------------------------+--+</span><br><span class="line">                      		Explain                       </span><br><span class="line">+-----------------------------------------------------------------+--+</span><br><span class="line"> STAGE DEPENDENCIES:     <span class="comment">//阶段性依赖                           </span></span><br><span class="line">   Stage-<span class="number">4</span> is a root stage   <span class="comment">//这是一个根依赖                       </span></span><br><span class="line">   Stage-<span class="number">3</span> depends on stages: Stage-<span class="number">4</span>    <span class="comment">//Stage-3依赖Stage-4           </span></span><br><span class="line">   Stage-<span class="number">0</span> depends on stages: Stage-<span class="number">3</span>    <span class="comment">//Stage-0依赖Stage-3           </span></span><br><span class="line">                                                    </span><br><span class="line"> STAGE PLANS:   <span class="comment">// 阶段性计划                                    </span></span><br><span class="line">   Stage: Stage-<span class="number">4</span>     <span class="comment">//阶段4                              </span></span><br><span class="line">     Map Reduce Local Work   <span class="comment">//这是一个本地作业                    </span></span><br><span class="line">       Alias -&gt; Map Local Tables:    <span class="comment">// Map本地表的别名为d 即表dept              </span></span><br><span class="line">         d                                          </span><br><span class="line">           Fetch Operator   <span class="comment">//抓取                        </span></span><br><span class="line">             limit: -<span class="number">1</span>    <span class="comment">//limit为-1，即把数据全部读出来了                   </span></span><br><span class="line">       Alias -&gt; Map Local Operator Tree:  <span class="comment">//Map本地操作树          </span></span><br><span class="line">         d                                          </span><br><span class="line">           TableScan     <span class="comment">//表扫描                           </span></span><br><span class="line">             alias: d    <span class="comment">//别名d                           </span></span><br><span class="line">             Statistics: Num rows: <span class="number">2</span> Data size: <span class="number">284</span> Basic stats: PARTIAL Column stats: NONE 	<span class="comment">//统计 </span></span><br><span class="line">             Filter Operator  <span class="comment">//过滤                      </span></span><br><span class="line">               predicate: <span class="function">deptno is not <span class="title">null</span> <span class="params">(type: <span class="keyword">boolean</span>)</span>  <span class="comment">//断定：deptno字段 不为空</span></span></span><br><span class="line"><span class="function">               Statistics: Num rows: 1 Data size: 142 Basic stats: COMPLETE Column stats: NONE 	   <span class="comment">//统计</span></span></span><br><span class="line"><span class="function">               HashTable Sink Operator  <span class="comment">//输出类型为HashTable           </span></span></span><br><span class="line"><span class="function">                 keys:                              </span></span><br><span class="line"><span class="function">                   0 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                   1 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                                                    </span></span><br><span class="line"><span class="function">   Stage: Stage-3   <span class="comment">//阶段3                                </span></span></span><br><span class="line"><span class="function">     Map Reduce                                     </span></span><br><span class="line"><span class="function">       Map Operator Tree:    <span class="comment">//Map操作树                       </span></span></span><br><span class="line"><span class="function">           TableScan   <span class="comment">//表扫描                             </span></span></span><br><span class="line"><span class="function">             alias: e  <span class="comment">//e表 即emp表                             </span></span></span><br><span class="line"><span class="function">             Statistics: Num rows: 6 Data size: 657 Basic stats: COMPLETE Column stats: NONE  	<span class="comment">//统计		</span></span></span><br><span class="line"><span class="function">             Filter Operator    <span class="comment">//过滤                    </span></span></span><br><span class="line"><span class="function">               predicate: deptno is not <span class="title">null</span> <span class="params">(type: <span class="keyword">boolean</span>)</span> 	<span class="comment">//断定：deptno字段 不为空</span></span></span><br><span class="line"><span class="function">               Statistics: Num rows: 3 Data size: 328 Basic stats: COMPLETE Column stats: NONE 	  <span class="comment">//统计</span></span></span><br><span class="line"><span class="function">               Map Join Operator    <span class="comment">// Map Join  操作               </span></span></span><br><span class="line"><span class="function">                 condition map:   <span class="comment">//Map条件                 </span></span></span><br><span class="line"><span class="function">                      Inner Join 0 to 1             </span></span><br><span class="line"><span class="function">                 keys:                              </span></span><br><span class="line"><span class="function">                   0 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                   1 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                 outputColumnNames: _col0, _col1, _col11, _col12  <span class="comment">//数值字段在表中的位置</span></span></span><br><span class="line"><span class="function">                 Statistics: Num rows: 3 Data size: 360 Basic stats: COMPLETE Column stats: NONE	<span class="comment">//统计 </span></span></span><br><span class="line"><span class="function">                 Select Operator   <span class="comment">//Select操作                 </span></span></span><br><span class="line"><span class="function">                   expressions: <span class="title">_col0</span> <span class="params">(type: <span class="keyword">int</span>)</span>, <span class="title">_col1</span> <span class="params">(type: string)</span>, <span class="title">_col11</span> <span class="params">(type: <span class="keyword">int</span>)</span>, <span class="title">_col12</span> <span class="params">(type: string)</span> 	<span class="comment">//表达式</span></span></span><br><span class="line"><span class="function">                   outputColumnNames: _col0, _col1, _col2, _col3 	<span class="comment">//数值字段在表中的位置</span></span></span><br><span class="line"><span class="function">                   Statistics: Num rows: 3 Data size: 360 Basic stats: COMPLETE Column stats: NONE 	  <span class="comment">//统计</span></span></span><br><span class="line"><span class="function">                   File Output Operator   <span class="comment">//文件输出操作          </span></span></span><br><span class="line"><span class="function">                     compressed: <span class="keyword">false</span>    <span class="comment">//是否压缩：否          </span></span></span><br><span class="line"><span class="function">                     Statistics: Num rows: 3 Data size: 360 Basic stats: COMPLETE Column stats: NONE	<span class="comment">//统计 </span></span></span><br><span class="line"><span class="function">                     table:   <span class="comment">//表文件的输入、输出、序列化类型                      </span></span></span><br><span class="line"><span class="function">                         input format: org.apache.hadoop.mapred.TextInputFormat <span class="comment">//文件输入格式</span></span></span><br><span class="line"><span class="function">                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat <span class="comment">//文件输出格式</span></span></span><br><span class="line"><span class="function">                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe <span class="comment">//反序列化</span></span></span><br><span class="line"><span class="function">       Local Work:                                  </span></span><br><span class="line"><span class="function">         Map Reduce Local Work                      </span></span><br><span class="line"><span class="function">                                                    </span></span><br><span class="line"><span class="function">   Stage: Stage-0   <span class="comment">//阶段0                                </span></span></span><br><span class="line"><span class="function">     Fetch Operator                                 </span></span><br><span class="line"><span class="function">       limit: -1     <span class="comment">// limit设置的值                               </span></span></span><br><span class="line"><span class="function">       Processor Tree:                              </span></span><br><span class="line"><span class="function">         ListSink                                   </span></span><br><span class="line"><span class="function">                                                    </span></span><br><span class="line"><span class="function">+-----------------------------------------------------------------+--+</span></span><br></pre></td></tr></table></figure>

<p>从执行计划得知，hive中执行SQL语句底层执行的是MapReduce。</p>
<p>我们在SQL中关联了两张表分别是emp dept，并从两张表中取出某些字段，在SQL执行计划中共分为三个阶段，分别是stage4、stage3、stage0。</p>
<p>stage4是根stage，stage3依赖stage4，同时stage0依赖stage3。</p>
<p>stage4是一个本地作业，读取的是dept表，输出一个Map类型的hashTable，关联的key是两张表的deptno，在执行计划中表现为0 deptno和 1 deptno，即执行的MapReduce中的Key是deptno字段。</p>
<p>stage3是MapReduce中的Map阶段，扫描emp表，执行一个Map Join操作，条件是两张表的dept字段相等(内连接)，现在我们得到的是一张包含所有字段的大表，得到需要的字段的对应位置，并且匹配字段的类型，在输出的时候检查是否需要压缩，以及输入、输出、和序列化类型</p>
<p>Stage-0阶段取出limit中指定的记录数</p>
<p>总结: 我们发现执行该SQL没有Reduce阶段，在现有的版本中默认设置<code>hive.auto.convert.join</code>(是否自动转换为mapjoin)为true，该参数配合<code>hive.mapjoin.smalltable.filesize</code>参数(小表的最大文件大小)默认为25M。即小于25M的表为小表，自动转为mapjoin，小表上传到hadoop缓存，提供给各个大表join使用。大表和小表根据关联的key形成一张大表，取出select需要的字段，最后根据limit设置的值取出对应的记录数。</p>
<p>参考参数：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--是否自动转换为mapjoin</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">true</span>;</span><br><span class="line"><span class="comment">--小表的最大文件大小，默认为25000000，即25M</span></span><br><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize = <span class="number">25000000</span>;</span><br><span class="line"><span class="comment">--是否将多个mapjoin合并为一个</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask = <span class="literal">true</span>;</span><br><span class="line"><span class="comment">--多个mapjoin转换为1个时，所有小表的文件大小总和的最大值。</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask.size = <span class="number">10000000</span>;</span><br></pre></td></tr></table></figure>

<h2 id="大小表Reduce-Join-emp、dept"><a href="#大小表Reduce-Join-emp、dept" class="headerlink" title="大小表Reduce Join(emp、dept)"></a>大小表Reduce Join(emp、dept)</h2><p>Reduce Join的核心思路是定义输出字段作为一个实体类，用来作为输出，实体类中定义一个标志用来区分表的来源</p>
<ol>
<li><p>将大小两个表在SQL中join的字段作为MapReduce中的key，原因是MapReduce中的key具有排序和分区的作用</p>
</li>
<li><p>Map中获取context中切片所在的文件名，按行获取文件中的数据并且根据获取的文件名分别将数据set到对象中，并写出Map。</p>
</li>
<li><p>Reduce中每次获取key相同的一组value值数据，这组value值既有dept中的数据，也</p>
</li>
</ol>
<p>   有emp中的数据，只要他们有相同的key，就会在shuffle中丢到一个reduce，这时候获取这组数据的值，根据flag来判断来自哪个表，如果是dept表则将数据设置到新new出来的对象中，添加到List列表中，同时创建一个保存emp表中数据的变量，由于emp表是小表，emp表中需要的数据对应dept/emp中的key的字段是唯一的，所以只需要把value中所有的对象都遍历循环出来，dept表数据添加到了List列表，emp表的数据添加到了变量中，最后循环List列表把变量set到每一个对象中，即完成了全部对象的全部成员属性。最后输出即可。</p>
<p>   参考代码:</p>
   <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.hadoop.join;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.pojo.JoinMain;</span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.utils.FileUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-01-29 16:39</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReduceJoinDriver</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    String in = <span class="string">"data/join/"</span>;</span><br><span class="line">    String out = <span class="string">"out/"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(conf, <span class="keyword">new</span> ReduceJoinDriver(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//获得configuration</span></span><br><span class="line">        Configuration conf = <span class="keyword">super</span>.getConf();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//检查文件夹</span></span><br><span class="line">        FileUtil.checkFileIsExists(conf, out);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用新方法这里怎么操作?</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置驱动类</span></span><br><span class="line">        job.setJarByClass(ReduceJoinDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置Map/Reducer类</span></span><br><span class="line">        job.setMapperClass(JoinMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(JoinReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置Map参数类型</span></span><br><span class="line">        job.setMapOutputKeyClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(JoinMain<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//job.setNumReduceTasks(3);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置Reducer参数类型</span></span><br><span class="line">        job.setOutputKeyClass(JoinMain<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置文件的输入输出</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(in));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(out));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//提交任务</span></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> b ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">JoinMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">JoinMain</span>&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            FileSplit fileSplit = (FileSplit) context.getInputSplit();</span><br><span class="line">            name = fileSplit.getPath().getName();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//做一个传入的表的判断</span></span><br><span class="line">            <span class="keyword">if</span> (name.contains(<span class="string">"emp"</span>))&#123;  <span class="comment">//emp</span></span><br><span class="line">                <span class="comment">//读取数据</span></span><br><span class="line">                String[] lines = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">                <span class="keyword">if</span> (lines.length ==<span class="number">8</span>)&#123;</span><br><span class="line">                    <span class="comment">//细粒度划分</span></span><br><span class="line">                    Integer empno = Integer.parseInt(lines[<span class="number">0</span>].trim());</span><br><span class="line">                    String ename = lines[<span class="number">1</span>];</span><br><span class="line">                    Integer deptno = Integer.parseInt(lines[lines.length-<span class="number">1</span>].trim());</span><br><span class="line">                    <span class="comment">//写入数据</span></span><br><span class="line">                    context.write(<span class="keyword">new</span> IntWritable(deptno),<span class="keyword">new</span> JoinMain(empno,ename,deptno,<span class="string">""</span>,<span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;      <span class="comment">//dept</span></span><br><span class="line">                <span class="comment">//读取数据</span></span><br><span class="line">                String[] lines = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">                <span class="keyword">if</span> (lines.length ==<span class="number">3</span>)&#123;</span><br><span class="line">                    <span class="keyword">int</span> deptno = Integer.parseInt(lines[<span class="number">0</span>].trim());</span><br><span class="line">                    String dname = lines[<span class="number">1</span>];</span><br><span class="line">                    context.write(<span class="keyword">new</span> IntWritable(deptno),<span class="keyword">new</span> JoinMain(<span class="number">0</span>, <span class="string">""</span>, deptno, dname, <span class="number">2</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">super</span>.cleanup(context);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">JoinReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">IntWritable</span>, <span class="title">JoinMain</span>,<span class="title">JoinMain</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="comment">//核心思路在 每个deptno组 进一次reduce ，前提是map中的key是deptno</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(IntWritable key, Iterable&lt;JoinMain&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            List&lt;JoinMain&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            String dname=<span class="string">""</span>;</span><br><span class="line">            <span class="comment">// 1.取出map中每行数据，判断flag值</span></span><br><span class="line">            <span class="comment">// 2.取出所有的emp中数据放入list中</span></span><br><span class="line">            <span class="comment">// 3.取出dept中的dname赋值给变量</span></span><br><span class="line">            <span class="comment">// 4.取出属于这个deptno中的所有数据，并给dname赋值</span></span><br><span class="line">            <span class="comment">// 5.每条赋值dname的数据写入reduce</span></span><br><span class="line">            <span class="keyword">for</span> (JoinMain main : values)&#123;</span><br><span class="line">                <span class="comment">// emp表</span></span><br><span class="line">                <span class="keyword">if</span> (main.getFlag() == <span class="number">1</span>)&#123;</span><br><span class="line">                    <span class="comment">//给emp表全部行重新赋值</span></span><br><span class="line">                    JoinMain m = <span class="keyword">new</span> JoinMain();</span><br><span class="line">                    m.setDeptno(main.getDeptno());</span><br><span class="line">                    m.setEmpno(main.getEmpno());</span><br><span class="line">                    m.setEname(main.getEname());</span><br><span class="line">                    <span class="comment">//写出到list</span></span><br><span class="line">                    list.add(m);</span><br><span class="line">                &#125;<span class="keyword">else</span> <span class="keyword">if</span> (main.getFlag() ==<span class="number">2</span> )&#123; <span class="comment">//dept</span></span><br><span class="line">                    <span class="comment">//拿到dept表中的dname</span></span><br><span class="line">                dname = main.getDname();</span><br><span class="line">            &#125;&#125;</span><br><span class="line">            <span class="comment">//循环赋值</span></span><br><span class="line">            <span class="keyword">for</span> (JoinMain bean : list) &#123;</span><br><span class="line">                bean.setDname(dname);</span><br><span class="line">                context.write(bean,NullWritable.get());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="大小表Map-Join-emp、dept"><a href="#大小表Map-Join-emp、dept" class="headerlink" title="大小表Map Join(emp、dept)"></a>大小表Map Join(emp、dept)</h2><p>Map Join的核心思想是把小表添加到缓存中(Map中)，在map中读取大表每行数据时set到对象值时取出小表(Map)对应key的值即可</p>
<ol>
<li><p>setup中，通过context获取小表文件切片的路径，然后通过读取流的方式读取为字符，按行获取到字符后切分，使用HashMap结构设置key和value分别为map方法中大表join时需要的键和值。</p>
</li>
<li><p>在map方法中读取文件数据，并且根据key取出HashMap(小表)中的value，一起set到对象中即可，最后写出，写出时，可以把value设置为NullWritable。</p>
<p>参考代码:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.hadoop.join;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.pojo.JoinMain;</span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.utils.FileUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"><span class="keyword">import</span> org.codehaus.groovy.runtime.wrappers.LongWrapper;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-02-01 23:10</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapJoinDriver</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String in = <span class="string">"data/join/emp.txt"</span>;</span><br><span class="line">    <span class="keyword">private</span> String out = <span class="string">"out"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(conf, <span class="keyword">new</span> MapJoinDriver(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> strings</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> : int</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@describe</span> : 设置配置文件，不用设置reduce</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@author</span> : tunan</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@date</span> : 2020/2/1 23:14</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">super</span>.getConf();</span><br><span class="line">        FileUtil.checkFileIsExists(conf,out);</span><br><span class="line"></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(MapJoinDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"data/join/dept.txt"</span>));</span><br><span class="line"></span><br><span class="line">        job.setMapperClass(MapperJoin<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(JoinMain<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(in));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(out));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> b ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MapperJoin</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">JoinMain</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> HashMap&lt;Integer, String&gt; hashMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//得到缓存文件路径</span></span><br><span class="line">            <span class="comment">//String path = context.getCacheFiles()[0].getPath().toString();</span></span><br><span class="line">            String path = context.getCacheFiles()[<span class="number">0</span>].getPath();</span><br><span class="line">            <span class="comment">/*URI[] files = context.getCacheFiles();   //URI 通过getPath()解码 没有toString()方法</span></span><br><span class="line"><span class="comment">            String s = files[0].getPath();*/</span></span><br><span class="line">            <span class="comment">//得到文件</span></span><br><span class="line">            <span class="comment">//File file = new File(cacheFiles[0]);</span></span><br><span class="line">            <span class="comment">//String path = file.getPath();</span></span><br><span class="line">            <span class="comment">//得到文件的流        InputStreamReader将字节转换为字符</span></span><br><span class="line">            BufferedReader br = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(<span class="keyword">new</span> FileInputStream(path)));</span><br><span class="line">            <span class="comment">//读取文件为字符串</span></span><br><span class="line">            String line ;</span><br><span class="line">            <span class="keyword">while</span>(StringUtils.isNotEmpty(line=br.readLine()))&#123;</span><br><span class="line">                <span class="comment">//切分字符串得到字符串数组</span></span><br><span class="line">                String[] split = line.split(<span class="string">"\t"</span>);</span><br><span class="line">                hashMap.put(Integer.parseInt(split[<span class="number">0</span>]),split[<span class="number">1</span>]);</span><br><span class="line">            &#125;</span><br><span class="line">            IOUtils.closeStream(br);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@describe</span> :</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@author</span> : tunan</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> key</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@return</span> : void</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@date</span> : 2020/2/1 23:38</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            String[] line = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">            <span class="keyword">if</span> (line.length &gt;= <span class="number">8</span>)&#123;</span><br><span class="line">                Integer empno = Integer.parseInt(line[<span class="number">0</span>].trim());</span><br><span class="line">                String ename = line[<span class="number">1</span>];</span><br><span class="line">                Integer deptno = Integer.parseInt(line[line.length-<span class="number">1</span>].trim());</span><br><span class="line">                String dname = hashMap.get(deptno);</span><br><span class="line">                context.write(<span class="keyword">new</span> JoinMain(empno,ename,deptno,dname),NullWritable.get());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">super</span>.cleanup(context);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Tunan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yerias.github.io/2018/10/14/hadoop/11/">http://yerias.github.io/2018/10/14/hadoop/11/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a><a class="post-meta__tags" href="/tags/MapReduce/">MapReduce</a></div><div class="social-share" data-disabled="facebook,twitter,douban,linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/10/15/hadoop/13/"><i class="fa fa-chevron-left">  </i><span>HADOOP安装LZO压缩</span></a></div><div class="next-post pull-right"><a href="/2018/10/13/hadoop/10/"><span>InputFormat&amp;Partitioner&amp;Conbiner&amp;Sort&amp;OutputFormat</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2020 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="footer_custom_text">大家好，我是图南，很高兴认识你们！</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>