<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>单节点部署三台kafka</title>
      <link href="/2020/05/22/kafka/2/"/>
      <url>/2020/05/22/kafka/2/</url>
      
        <content type="html"><![CDATA[<ol><li><p>下载地址：<a href="http://archive.cloudera.com/kafka/kafka/4/kafka-2.2.1-kafka4.1.0.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/kafka/kafka/4/kafka-2.2.1-kafka4.1.0.tar.gz</a></p></li><li><p>解压</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf kafka-2.2.1-kafka4.1.0.tar.gz -C ../app</span><br></pre></td></tr></table></figure></li><li><p>在部署kafka之前 ，检测zookeeper是ok的</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] ls /</span><br><span class="line">[zktest, tunan, zookeeper, kafka]</span><br></pre></td></tr></table></figure></li><li><p>编辑config/server.properties文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">broker.id=0</span></span><br><span class="line"><span class="meta">#</span><span class="bash">log.dirs=/tmp/kafka-logs</span></span><br><span class="line"></span><br><span class="line">broker.id=0</span><br><span class="line">host.name=hadoop</span><br><span class="line">port=9090</span><br><span class="line">log.dirs=/home/hadoop/tmp/kafka-logs00</span><br><span class="line">zookeeper.connect=hadoop:2181/kafka</span><br></pre></td></tr></table></figure></li><li><p>复制Kafka文件夹为三份</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp -R kafka_2.11-2.2.1-kafka-4.1.0/ kafka01</span><br><span class="line">cp -R kafka_2.11-2.2.1-kafka-4.1.0/ kafka02</span><br><span class="line">mv  kafka_2.11-2.2.1-kafka-4.1.0/ kafka03</span><br></pre></td></tr></table></figure></li><li><p>修改kafka02的config/server.properties文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">broker.id=1</span><br><span class="line">host.name=hadoop</span><br><span class="line">port=9091</span><br><span class="line">log.dirs=/home/hadoop/tmp/kafka-logs01</span><br><span class="line">zookeeper.connect=hadoop:2181/kafka</span><br></pre></td></tr></table></figure></li><li><p>修改kafka03的config/server.properties文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">broker.id=2</span><br><span class="line">host.name=hadoop</span><br><span class="line">port=9092</span><br><span class="line">log.dirs=/home/hadoop/tmp/kafka-logs02</span><br><span class="line">zookeeper.connect=hadoop:2181/kafka</span><br></pre></td></tr></table></figure></li><li><p>启动(三台都需要输入命令)</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin/kafka-server-start.sh -daemon config/server.properties</span><br></pre></td></tr></table></figure></li><li><p>创建topic</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./kafka-topics.sh \</span><br><span class="line">--create \</span><br><span class="line">--zookeeper hadoop:2181/kafka \</span><br><span class="line">--partitions 3 \</span><br><span class="line">--replication-factor 2 \</span><br><span class="line">--topic test</span><br></pre></td></tr></table></figure></li><li><p>查看topic</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./kafka-topics.sh \</span><br><span class="line">--list \</span><br><span class="line">--zookeeper hadoop:2181/kafka</span><br></pre></td></tr></table></figure></li><li><p>查看指定topic的状况</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./kafka-topics.sh \</span><br><span class="line">--describe \</span><br><span class="line">--zookeeper hadoop:2181/kafka \</span><br><span class="line">--topic test</span><br></pre></td></tr></table></figure></li><li><p>测试</p><p>启动生产者</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./kafka-console-producer.sh \</span><br><span class="line">--broker-list hadoop:9090,hadoop:9091,hadoop:9092 \</span><br><span class="line">--topic test</span><br></pre></td></tr></table></figure><p>启动消费者</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh \</span><br><span class="line">--bootstrap-server hadoop:9090,hadoop:9091,hadoop:9092 \</span><br><span class="line">--from-beginning \</span><br><span class="line">--topic test</span><br></pre></td></tr></table></figure><p>发送数据</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">a</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">a</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>接收数据</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a</span><br><span class="line">a</span><br><span class="line">...</span><br></pre></td></tr></table></figure></li><li><p>关闭kafka集群(每台都要执行)</p><p>修改kafka-server-stop.sh</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">PIDS=$(ps ax | grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '&#123;print $1&#125;')</span><br></pre></td></tr></table></figure><p>修改为</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">PIDS=$(jps -lm | grep -i 'kafka'| awk '&#123;print $1&#125;')</span><br></pre></td></tr></table></figure><p><strong>命令详解：</strong>使用jps -lm命令列出所有的java进程，然后通过管道，利用grep -i ‘kafka.Kafka’命令将kafka进程筛出来，最后再接一管道命令，利用awk将进程号取出来。</p><p>分别执行kafka-server-stop.sh</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Kakfa </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kakfa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>修改Flume源码使taildir source支持递归（可配置）</title>
      <link href="/2020/05/22/flume/6/"/>
      <url>/2020/05/22/flume/6/</url>
      
        <content type="html"><![CDATA[<p>Flume的source选哪个？<br>taildir source首选！</p><ol><li>断点还原 <code>positionFile</code>可以记录偏移量</li><li>可配置文件组，里面使用正则表达式配置多个要监控的文件</li></ol><p>这么好的taildir source有一点不完美，不能支持递归监控文件夹。</p><p>所以就只能修改源代码了，需要注意的是无论是Apache版本的还是CDH的都能够兼容使用，我这里使用的版本是flume-ng-1.6.0-cdh5.16.2，但是即使你使用Apache的版本编译源码，也是没问题的。</p><h2 id="改源码，先读源码"><a href="#改源码，先读源码" class="headerlink" title="改源码，先读源码"></a>改源码，先读源码</h2><p>Flume的taildir source启动会调用<code>start()</code>方法作初始化，里面创建一个<code>ReliableTaildirEventReader</code>,这里用到了建造者模式</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    logger.info(<span class="string">"&#123;&#125; TaildirSource source starting with directory: &#123;&#125;"</span>, getName(), filePaths);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        reader = <span class="keyword">new</span> ReliableTaildirEventReader.Builder()</span><br><span class="line">                .filePaths(filePaths)</span><br><span class="line">                .headerTable(headerTable)</span><br><span class="line">                .positionFilePath(positionFilePath)</span><br><span class="line">                .skipToEnd(skipToEnd)</span><br><span class="line">                .addByteOffset(byteOffsetHeader)</span><br><span class="line">                .cachePatternMatching(cachePatternMatching)</span><br><span class="line">                .recursive(isRecursive)</span><br><span class="line">                .annotateFileName(fileHeader)</span><br><span class="line">                .fileNameHeader(fileHeaderKey)</span><br><span class="line">                .build();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> FlumeException(<span class="string">"Error instantiating ReliableTaildirEventReader"</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line">    idleFileChecker = Executors.newSingleThreadScheduledExecutor(</span><br><span class="line">            <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"idleFileChecker"</span>).build());</span><br><span class="line">    idleFileChecker.scheduleWithFixedDelay(<span class="keyword">new</span> idleFileCheckerRunnable(),</span><br><span class="line">            idleTimeout, checkIdleInterval, TimeUnit.MILLISECONDS);</span><br><span class="line"></span><br><span class="line">    positionWriter = Executors.newSingleThreadScheduledExecutor(</span><br><span class="line">            <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"positionWriter"</span>).build());</span><br><span class="line">    positionWriter.scheduleWithFixedDelay(<span class="keyword">new</span> PositionWriterRunnable(),</span><br><span class="line">            writePosInitDelay, writePosInterval, TimeUnit.MILLISECONDS);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">super</span>.start();</span><br><span class="line">    logger.debug(<span class="string">"TaildirSource started"</span>);</span><br><span class="line">    sourceCounter.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>taildir source属于<code>PollableSource</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A &#123;<span class="doctag">@link</span> Source&#125; that requires an external driver to poll to determine</span></span><br><span class="line"><span class="comment"> * whether there are &#123;<span class="doctag">@linkplain</span> Event events&#125; that are available to ingest</span></span><br><span class="line"><span class="comment"> * from the source.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@see</span> org.apache.flume.source.EventDrivenSourceRunner</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">PollableSource</span> <span class="keyword">extends</span> <span class="title">Source</span> </span>&#123;</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>这段注释的意思是<code>PollableSource</code>是需要一个外部驱动去查看有没有需要消费的事件，从而拉取事件，讲白了就是<strong>定时拉取</strong>。所以flume也不一定是真正实时的，只是隔一会儿不停地来查看事件而已。(与之相应的是另一种<code>EventDrivenSourceRunner</code>)<br>那么taildir source在定时拉取事件的时候是调用的<code>process</code>方法</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Status status = Status.READY;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        existingInodes.clear();</span><br><span class="line">        existingInodes.addAll(reader.updateTailFiles());</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">long</span> inode : existingInodes) &#123;</span><br><span class="line">            TailFile tf = reader.getTailFiles().get(inode);</span><br><span class="line">            <span class="keyword">if</span> (tf.needTail()) &#123;</span><br><span class="line">                tailFileProcess(tf, <span class="keyword">true</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        closeTailFiles();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            TimeUnit.MILLISECONDS.sleep(retryInterval);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            logger.info(<span class="string">"Interrupted while sleeping"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">        logger.error(<span class="string">"Unable to tail files"</span>, t);</span><br><span class="line">        status = Status.BACKOFF;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> status;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重点就是下面这几行</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">existingInodes.addAll(reader.updateTailFiles());</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">long</span> inode : existingInodes) &#123;</span><br><span class="line">    TailFile tf = reader.getTailFiles().get(inode);</span><br><span class="line">    <span class="keyword">if</span> (tf.needTail()) &#123;</span><br><span class="line">        tailFileProcess(tf, <span class="keyword">true</span>);</span><br><span class="line">&#125; &#125;</span><br></pre></td></tr></table></figure><p>从<code>reader.updateTailFiles()</code>获取需要监控的文件，然后对每一个进行处理，查看最后修改时间，判定是否需要<code>tail</code>，需要<code>tail</code>就<code>tail</code><br>那么进入<code>reader.updateTailFiles()</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (TaildirMatcher taildir : taildirCache) &#123;</span><br><span class="line">    Map&lt;String, String&gt; headers = headerTable.row(taildir.getFileGroup());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (File f : taildir.getMatchingFiles()) &#123;</span><br><span class="line">        <span class="keyword">long</span> inode = getInode(f);</span><br><span class="line">        TailFile tf = tailFiles.get(inode);</span><br><span class="line">        <span class="keyword">if</span> (tf == <span class="keyword">null</span> || !tf.getPath().equals(f.getAbsolutePath())) &#123;</span><br><span class="line">            <span class="keyword">long</span> startPos = skipToEnd ? f.length() : <span class="number">0</span>;</span><br><span class="line">            tf = openFile(f, headers, inode, startPos);</span><br></pre></td></tr></table></figure><p>遍历每一个正则表达式匹配对应的匹配器，每个匹配器去获取匹配的文件！<code>taildir.getMatchingFiles()</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">List&lt;File&gt; <span class="title">getMatchingFiles</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> now = TimeUnit.SECONDS.toMillis(</span><br><span class="line">            TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()));</span><br><span class="line">    <span class="keyword">long</span> currentParentDirMTime = parentDir.lastModified();</span><br><span class="line">    List&lt;File&gt; result;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// calculate matched files if</span></span><br><span class="line">    <span class="comment">// - we don't want to use cache (recalculate every time) OR</span></span><br><span class="line">    <span class="comment">// - directory was clearly updated after the last check OR</span></span><br><span class="line">    <span class="comment">// - last mtime change wasn't already checked for sure</span></span><br><span class="line">    <span class="comment">//   (system clock hasn't passed that second yet)</span></span><br><span class="line">    <span class="keyword">if</span> (!cachePatternMatching ||</span><br><span class="line">            lastSeenParentDirMTime &lt; currentParentDirMTime ||</span><br><span class="line">            !(currentParentDirMTime &lt; lastCheckedTime)) &#123;</span><br><span class="line">        lastMatchedFiles = sortByLastModifiedTime(getMatchingFilesNoCache(isRecursive));</span><br><span class="line">        lastSeenParentDirMTime = currentParentDirMTime;</span><br><span class="line">        lastCheckedTime = now;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> lastMatchedFiles;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到<code>getMatchingFilesNoCache(isRecursive)</code>就是获取匹配的文件的方法，也就是需要修改的方法了！<br>ps：这里的<code>isRecursive</code>是我加的~<br>点进去：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> List&lt;File&gt; <span class="title">getMatchingFilesNoCache</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    List&lt;File&gt; result = Lists.newArrayList();</span><br><span class="line">    <span class="keyword">try</span> (DirectoryStream&lt;Path&gt; stream = Files.newDirectoryStream(parentDir.toPath(), fileFilter)) &#123;</span><br><span class="line">        <span class="keyword">for</span> (Path entry : stream) &#123;</span><br><span class="line">            result.add(entry.toFile());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        logger.error(<span class="string">"I/O exception occurred while listing parent directory. "</span> +</span><br><span class="line">                <span class="string">"Files already matched will be returned. "</span> + parentDir.toPath(), e);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>源码是用了<code>Files.newDirectoryStream(parentDir.toPath(), fileFilter))</code>，将父目录下符合正则表达式的文件都添加到一个迭代器里。（这里还用了<code>try (...)</code>的语法糖）</p><h2 id="找到地方了，开始改！"><a href="#找到地方了，开始改！" class="headerlink" title="找到地方了，开始改！"></a>找到地方了，开始改！</h2><p>我在这个<code>getMatchingFilesNoCache()</code>方法下面下了一个重载的方法, 可增加扩展性：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> List&lt;File&gt; <span class="title">getMatchingFilesNoCache</span><span class="params">(<span class="keyword">boolean</span> recursion)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!recursion) &#123;</span><br><span class="line">        <span class="keyword">return</span> getMatchingFilesNoCache();</span><br><span class="line">    &#125;</span><br><span class="line">    List&lt;File&gt; result = Lists.newArrayList();</span><br><span class="line">    <span class="comment">// 使用非递归的方式遍历文件夹</span></span><br><span class="line">    Queue&lt;File&gt; dirs = <span class="keyword">new</span> ArrayBlockingQueue&lt;&gt;(<span class="number">10</span>);</span><br><span class="line">    dirs.offer(parentDir);</span><br><span class="line">    <span class="keyword">while</span> (dirs.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        File dir = dirs.poll();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            DirectoryStream&lt;Path&gt; stream = Files.newDirectoryStream(dir.toPath(), fileFilter);</span><br><span class="line">            stream.forEach(path -&gt; result.add(path.toFile()));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            logger.error(<span class="string">"I/O exception occurred while listing parent directory. "</span> +</span><br><span class="line">                    <span class="string">"Files already matched will be returned. (recursion)"</span> + parentDir.toPath(), e);</span><br><span class="line">        &#125;</span><br><span class="line">        File[] dirList = dir.listFiles();</span><br><span class="line">        <span class="keyword">assert</span> dirList != <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">for</span> (File f : dirList) &#123;</span><br><span class="line">            <span class="keyword">if</span> (f.isDirectory()) &#123;</span><br><span class="line">                dirs.add(f);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我使用了非递归的方式遍历文件夹，就是树到队列的转换。<br>到这里，核心部分就改完了。接下来要处理这个<code>recursion</code>的参数</p><h2 id="华丽的分割线后，顺腾摸瓜！"><a href="#华丽的分割线后，顺腾摸瓜！" class="headerlink" title="华丽的分割线后，顺腾摸瓜！"></a>华丽的分割线后，顺腾摸瓜！</h2><p>一路改构造方法，添加这个参数，最终参数从哪来呢？<br>flume的source启动时会调用<code>configure</code>方法，将<code>Context</code>中的内容配置进<code>reader</code>等对象中。<br><code>isRecursive = context.getBoolean(RECURSIVE, DEFAULT_RECURSIVE);</code><br><code>context</code>从<code>TaildirSourceConfigurationConstants</code>中获取配置名和默认值</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Whether to support recursion. */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String RECURSIVE = <span class="string">"recursive"</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">boolean</span> DEFAULT_RECURSIVE = <span class="keyword">false</span>;</span><br></pre></td></tr></table></figure><p>这里的<code>recursive</code>也就是flume配置文件里配置项了</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Whether to support recusion</span><br><span class="line">a1.sources.r1.recursive = <span class="keyword">true</span></span><br></pre></td></tr></table></figure><h2 id="大功告成，打包试试！"><a href="#大功告成，打包试试！" class="headerlink" title="大功告成，打包试试！"></a>大功告成，打包试试！</h2><p>执行package将其放在flume的lib下，替换原来的<code>flume-taildir-source***.jar</code><br>启动，测试，成功！</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">a1.sources.r1.type</span> = <span class="string">TAILDIR</span></span><br><span class="line"><span class="meta">a1.sources.r1.cachePatternMatching</span> = <span class="string">false</span></span><br><span class="line"><span class="meta">a1.sources.r1.positionFile</span> = <span class="string">/home/hadoop/app/flume/position/taildir_position.json</span></span><br><span class="line"><span class="meta">a1.sources.r1.filegroups</span> = <span class="string">f1</span></span><br><span class="line"><span class="meta">a1.sources.r1.filegroups.f1</span> = <span class="string">/home/hadoop/data/taildir/.*txt</span></span><br><span class="line"><span class="meta">a1.sources.r1.recursive</span> = <span class="string">false</span></span><br></pre></td></tr></table></figure><p>具体代码见GitHub地址：<a href="https://github.com/yerias/recursion-flume-taildir" target="_blank" rel="noopener">https://github.com/yerias/recursion-flume-taildir</a></p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink整合CDH Hadoop编译&amp;简单测试</title>
      <link href="/2020/05/10/flink/1/"/>
      <url>/2020/05/10/flink/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li><p>准备工作</p></li><li><p>下载源码包</p></li><li><p>准备操作</p><ol><li>配置支持CDH依赖</li><li>编译Flink-shaded</li><li>flink测试模块删减</li><li>配置支持maven-assembly-plugin插件</li><li>node、npm等依赖添加国内仓库</li><li>Kafka Schema Registry相关maven库配置</li></ol></li><li><p>执行Flink编译</p></li><li><p>提取编译后的安装包</p></li><li><p>单节点部署测试</p></li></ol><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><ul><li>maven3</li><li>jdk1.8</li></ul><h2 id="下载源码包"><a href="#下载源码包" class="headerlink" title="下载源码包"></a>下载源码包</h2><p>Flink官网: <a href="https://flink.apache.org/" target="_blank" rel="noopener">https://flink.apache.org/</a></p><p>下载地址: <a href="https://www.apache.org/dyn/closer.lua/flink/flink-1.10.0/flink-1.10.0-src.tgz" target="_blank" rel="noopener">Apache Flink 1.10.0 Source Release</a></p><h2 id="准备操作"><a href="#准备操作" class="headerlink" title="准备操作"></a>准备操作</h2><h3 id="配置支持CDH依赖"><a href="#配置支持CDH依赖" class="headerlink" title="配置支持CDH依赖"></a>配置支持CDH依赖</h3><p>maven默认不支持cdh的依赖下载，修改maven目录下conf中的settings.xml如下：（这里的cloudera-releases是flink源码中配置的id）</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mirrors</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>nexus-aliyun<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>*,!cloudera-releases,!cloudera<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>Nexus aliyun<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- hortonworks maven --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>nexus-hortonworks<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>*,!central<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>Nexus hortonworks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repo.hortonworks.com/content/groups/public/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>confluent<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>confluent<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>Nexus public mirror<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://packages.confluent.io/maven/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirrors</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>注意:</strong> 有时候hortonworks的仓库并不好使，如果发现不好使，注释掉即可</p><p>修改<code>flink-1.10.0/pom.xml</code>，添加：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--添加CDH的仓库--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="编译Flink-shaded"><a href="#编译Flink-shaded" class="headerlink" title="编译Flink-shaded"></a>编译Flink-shaded</h3><p>不同的 Flink 版本使用的 Flink-shaded不同，Flink 10.0 版本使用Flink-shaded 9.0<br>如果不编译的话会报错找不到：flink-shaded-hadoop-2:jar:2.6.0-cdh5.16.2-9.0</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">[ERROR]</span> <span class="string">Failed to execute goal on project flink-hadoop-fs: Could not resolve dependencies for project org.apache.flink:flink-hadoop-fs:jar:1.9.1: The following artifacts could not be resolved: org.apache.flink:flink-shaded-hadoop-2:jar:2.6.0-cdh5.14.2-7.0, org.apache.hadoop:hadoop-hdfs:jar:tests:2.6.0-cdh5.14.2, org.apache.hadoop:hadoop-common:jar:tests:2.6.0-cdh5.14.2: Could not find artifact org.apache.flink:flink-shaded-hadoop-2:jar:2.6.0-cdh5.14.2-7.0 in nexus-hortonworks (https://repo.hortonworks.com/content/groups/public/) -&gt; [Help 1]</span></span><br></pre></td></tr></table></figure><p>因此，这一步需要手动编译flink-shaded-hadoop-2，并将其打入到maven库。</p><ol><li><p>下载<a href="https://github.com/apache/flink-shaded/tree/release-10.0" target="_blank" rel="noopener">flink-shaded-10.0-src.tgz</a></p></li><li><p>修改项目pom.xml</p><p>在 <code>flink-shaded-7.0/pom.xml</code>文件中添加 cloudera 的maven库：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--添加CDH的仓库--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在<code>flink-shaded-7.0/flink-shaded-hadoop-2/pom.xml</code>文件中也添加：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--添加CDH的仓库--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在 <code>flink-shaded-7.0/flink-shaded-hadoop-2-uber/pom.xml</code> 中的 dependencyManagement 标签中添加如下依赖：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>commons-cli<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-cli<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>注意：这一步一定要添加，不然编译成功后，启动不了，并 .out 文件中抛出如下错误：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">Exception</span> <span class="string">in thread "main" java.lang.NoSuchMethodError: org.apache.commons.cli.Option.builder(Ljava/lang/String;)Lorg/apache/commons/cli/Option$Builder;</span></span><br></pre></td></tr></table></figure><p>原因是项目打包后，依赖的 commons-cli 是1.2版本的，build 方法在该版本中不存在。</p></li><li><p>开始编译：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">mvn</span> <span class="string">clean install -DskipTests -Dhadoop.version=2.6.0-cdh5.16.2</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="flink测试模块删减"><a href="#flink测试模块删减" class="headerlink" title="flink测试模块删减"></a>flink测试模块删减</h3><p>删除flink中的以下test模块，防止编译出错：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">module</span>&gt;</span>flink-tests<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">module</span>&gt;</span>flink-end-to-end-tests<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">module</span>&gt;</span>flink-yarn-tests<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">module</span>&gt;</span>flink-fs-tests<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置支持maven-assembly-plugin插件"><a href="#配置支持maven-assembly-plugin插件" class="headerlink" title="配置支持maven-assembly-plugin插件"></a>配置支持maven-assembly-plugin插件</h3><p>编辑<code>flink-1.10.0/flink-libraries/pom.xml</code>，新增maven-assembly-plugin插件，否则会报错。</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span><span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="node、npm等依赖添加国内仓库"><a href="#node、npm等依赖添加国内仓库" class="headerlink" title="node、npm等依赖添加国内仓库"></a>node、npm等依赖添加国内仓库</h3><p>为flink-runtime-web/添加国内仓库，编辑flink-1.10.0/flink-runtime-web/pom.xml。<br>Flink1.9.x的flink-runtime-web模块引入了frontend-maven-plugin依赖，并安装了node和部分依赖组件，添加国内仓库，否则会访问不到：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">nodeDownloadRoot</span>&gt;</span>https://registry.npm.taobao.org/dist/<span class="tag">&lt;/<span class="name">nodeDownloadRoot</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">npmDownloadRoot</span>&gt;</span>https://registry.npmjs.org/npm/-/<span class="tag">&lt;/<span class="name">npmDownloadRoot</span>&gt;</span></span><br></pre></td></tr></table></figure><p>注意frontend-maven-plugin原本就有，我们只是进行修改，修改后的完整配置是:</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.github.eirslett<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>frontend-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.6<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">id</span>&gt;</span>install node and npm<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goal</span>&gt;</span>install-node-and-npm<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">nodeDownloadRoot</span>&gt;</span>https://registry.npm.taobao.org/dist/<span class="tag">&lt;/<span class="name">nodeDownloadRoot</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">npmDownloadRoot</span>&gt;</span>https://registry.npmjs.org/npm/-/<span class="tag">&lt;/<span class="name">npmDownloadRoot</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">nodeVersion</span>&gt;</span>v10.9.0<span class="tag">&lt;/<span class="name">nodeVersion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">id</span>&gt;</span>npm install<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goal</span>&gt;</span>npm<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">arguments</span>&gt;</span>ci --cache-max=0 --no-save  --no-bin-links<span class="tag">&lt;/<span class="name">arguments</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">environmentVariables</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">HUSKY_SKIP_INSTALL</span>&gt;</span>true<span class="tag">&lt;/<span class="name">HUSKY_SKIP_INSTALL</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">environmentVariables</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">id</span>&gt;</span>npm run build<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goal</span>&gt;</span>npm<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">arguments</span>&gt;</span>run build<span class="tag">&lt;/<span class="name">arguments</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">workingDirectory</span>&gt;</span>web-dashboard<span class="tag">&lt;/<span class="name">workingDirectory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure><p>否则报错：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">[ERROR]</span> <span class="string">Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.6:npm (npm install) on project flink-runtime-web_2.11: Failed to run task: 'npm ci --cache-max=0 --no-save' failed. org.apache.commons.exec.ExecuteException: Process exited with an error: 1 (Exit value: 1) -&gt; [Help 1]</span></span><br></pre></td></tr></table></figure><h3 id="Kafka-Schema-Registry相关maven库配置"><a href="#Kafka-Schema-Registry相关maven库配置" class="headerlink" title="Kafka Schema Registry相关maven库配置"></a>Kafka Schema Registry相关maven库配置</h3><p>相关jar包在 Maven仓库中下载不到，所以需要在maven的settings文件中添加如下信息(在文章开头已经配置了)：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mirrors</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>confluent<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>confluent<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Nexus public mirror<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://packages.confluent.io/maven/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirrors</span>&gt;</span></span><br></pre></td></tr></table></figure><p>同时在flink项目的主pom里添加如下仓库配置</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>confluent<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://packages.confluent.io/maven/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">releases</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">releases</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>ICM<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.icm.edu.pl/artifactory/repo/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br></pre></td></tr></table></figure><p>否则会找不到io/confluent/kafka-schema-registry-client、kafka-schema-registry-parent、rest-utils-parent等依赖包。</p><h2 id="执行Flink编译"><a href="#执行Flink编译" class="headerlink" title="执行Flink编译"></a>执行Flink编译</h2><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">mvn</span> <span class="string">-T2C clean package -DskipTests -Pvendor-repos -Dhadoop.version=2.6.0-cdh5.16.2  -Dscala-2.12 -Drat.skip=true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数说明：</span></span><br><span class="line"><span class="comment"># -Pinclude-hadoop  # 将 hadoop的 jar包，打入到lib/中</span></span><br><span class="line"><span class="comment"># -Pvendor-repos# 激活 Maven 构建配置文件，其中包括 Cloudera，Hortonworks 或 MapR 等流行的 Hadoop 供应商的存储库。</span></span><br><span class="line"><span class="comment"># -Dhadoop.version=2.6.0-cdh5.14.2# 指定 hadoop 的版本</span></span><br><span class="line"><span class="comment"># -Dscala-2.11# Scala版本，默认是2.11，如果开发使用的Scala版本不同需要指定</span></span><br></pre></td></tr></table></figure><p>编译过程中的遇到比较多的问题都是跟依赖下载有关，一些问题的解决都在上面提到了，还有一些下不下来的包直接通过<a href="https://mvnrepository.com/" target="_blank" rel="noopener">maven库</a>或官方库下载到本地库</p><p>如果编译所需依赖都已下载，时间大概在半小时左右，视情况而定。<br>编译成功效果：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Reactor Summary:</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] force-shading ...................................... SUCCESS [  <span class="number">2.031</span> s]</span><br><span class="line">[INFO] flink .............................................. SUCCESS [  <span class="number">1.561</span> s]</span><br><span class="line">[INFO] flink-annotations .................................. SUCCESS [  <span class="number">4.092</span> s]</span><br><span class="line">[INFO] flink-shaded-curator ............................... SUCCESS [  <span class="number">5.122</span> s]</span><br><span class="line">[INFO] flink-metrics ...................................... SUCCESS [  <span class="number">0.663</span> s]</span><br><span class="line">[INFO] flink-metrics-core ................................. SUCCESS [  <span class="number">4.475</span> s]</span><br><span class="line">[INFO] flink-test-utils-parent ............................ SUCCESS [  <span class="number">0.676</span> s]</span><br><span class="line">[INFO] flink-test-utils-junit ............................. SUCCESS [  <span class="number">3.405</span> s]</span><br><span class="line">[INFO] flink-core ......................................... SUCCESS [ <span class="number">19.344</span> s]</span><br><span class="line">[INFO] flink-java ......................................... SUCCESS [  <span class="number">7.728</span> s]</span><br><span class="line">[INFO] flink-queryable-state .............................. SUCCESS [  <span class="number">0.193</span> s]</span><br><span class="line">[INFO] flink-queryable-state-client-java .................. SUCCESS [  <span class="number">1.361</span> s]</span><br><span class="line">[INFO] flink-filesystems .................................. SUCCESS [  <span class="number">1.046</span> s]</span><br><span class="line">[INFO] flink-hadoop-fs .................................... SUCCESS [  <span class="number">3.766</span> s]</span><br><span class="line">[INFO] flink-runtime ...................................... SUCCESS [<span class="number">01</span>:<span class="number">07</span> min]</span><br><span class="line">[INFO] flink-scala ........................................ SUCCESS [<span class="number">01</span>:<span class="number">35</span> min]</span><br><span class="line">[INFO] flink-mapr-fs ...................................... SUCCESS [  <span class="number">3.541</span> s]</span><br><span class="line">[INFO] flink-filesystems :: flink-fs-hadoop-shaded ........ SUCCESS [  <span class="number">8.875</span> s]</span><br><span class="line">[INFO] flink-s3-fs-base ................................... SUCCESS [ <span class="number">28.538</span> s]</span><br><span class="line">[INFO] flink-s3-fs-hadoop ................................. SUCCESS [ <span class="number">22.938</span> s]</span><br><span class="line">[INFO] flink-s3-fs-presto ................................. SUCCESS [ <span class="number">30.871</span> s]</span><br><span class="line">[INFO] flink-swift-fs-hadoop .............................. SUCCESS [ <span class="number">49.025</span> s]</span><br><span class="line">[INFO] flink-oss-fs-hadoop ................................ SUCCESS [ <span class="number">25.729</span> s]</span><br><span class="line">[INFO] flink-azure-fs-hadoop .............................. SUCCESS [ <span class="number">29.693</span> s]</span><br><span class="line">[INFO] flink-optimizer .................................... SUCCESS [  <span class="number">3.194</span> s]</span><br><span class="line">[INFO] flink-clients ...................................... SUCCESS [  <span class="number">1.632</span> s]</span><br><span class="line">[INFO] flink-streaming-java ............................... SUCCESS [ <span class="number">10.892</span> s]</span><br><span class="line">[INFO] flink-test-utils ................................... SUCCESS [  <span class="number">8.855</span> s]</span><br><span class="line">[INFO] flink-runtime-web .................................. SUCCESS [<span class="number">05</span>:<span class="number">56</span> min]</span><br><span class="line">[INFO] flink-streaming-scala .............................. SUCCESS [<span class="number">01</span>:<span class="number">37</span> min]</span><br><span class="line">[INFO] flink-table ........................................ SUCCESS [  <span class="number">0.818</span> s]</span><br><span class="line">[INFO] flink-table-common ................................. SUCCESS [  <span class="number">4.308</span> s]</span><br><span class="line">[INFO] flink-table-api-java ............................... SUCCESS [  <span class="number">5.168</span> s]</span><br><span class="line">[INFO] flink-table-api-java-bridge ........................ SUCCESS [  <span class="number">3.254</span> s]</span><br><span class="line">[INFO] flink-table-api-scala .............................. SUCCESS [ <span class="number">19.390</span> s]</span><br><span class="line">[INFO] flink-table-api-scala-bridge ....................... SUCCESS [ <span class="number">22.887</span> s]</span><br><span class="line">[INFO] flink-sql-parser ................................... SUCCESS [ <span class="number">13.172</span> s]</span><br><span class="line">[INFO] flink-state-backends ............................... SUCCESS [  <span class="number">0.712</span> s]</span><br><span class="line">[INFO] flink-statebackend-rocksdb ......................... SUCCESS [  <span class="number">4.609</span> s]</span><br><span class="line">[INFO] flink-libraries .................................... SUCCESS [  <span class="number">1.007</span> s]</span><br><span class="line">[INFO] flink-cep .......................................... SUCCESS [ <span class="number">11.703</span> s]</span><br><span class="line">[INFO] flink-table-planner ................................ SUCCESS [<span class="number">04</span>:<span class="number">36</span> min]</span><br><span class="line">[INFO] flink-connectors ................................... SUCCESS [  <span class="number">1.105</span> s]</span><br><span class="line">[INFO] flink-orc .......................................... SUCCESS [ <span class="number">10.208</span> s]</span><br><span class="line">[INFO] flink-jdbc ......................................... SUCCESS [ <span class="number">10.759</span> s]</span><br><span class="line">[INFO] flink-hadoop-compatibility ......................... SUCCESS [ <span class="number">18.036</span> s]</span><br><span class="line">[INFO] flink-table-runtime-blink .......................... SUCCESS [ <span class="number">12.010</span> s]</span><br><span class="line">[INFO] flink-table-planner-blink .......................... SUCCESS [<span class="number">05</span>:<span class="number">52</span> min]</span><br><span class="line">[INFO] flink-hbase ........................................ SUCCESS [<span class="number">07</span>:<span class="number">06</span> min]</span><br><span class="line">[INFO] flink-hcatalog ..................................... SUCCESS [ <span class="number">22.569</span> s]</span><br><span class="line">[INFO] flink-metrics-jmx .................................. SUCCESS [  <span class="number">2.176</span> s]</span><br><span class="line">[INFO] flink-connector-kafka-base ......................... SUCCESS [ <span class="number">19.286</span> s]</span><br><span class="line">[INFO] flink-connector-kafka-<span class="number">0.9</span> .......................... SUCCESS [  <span class="number">7.931</span> s]</span><br><span class="line">[INFO] flink-connector-kafka-<span class="number">0.10</span> ......................... SUCCESS [  <span class="number">3.107</span> s]</span><br><span class="line">[INFO] flink-connector-kafka-<span class="number">0.11</span> ......................... SUCCESS [  <span class="number">7.556</span> s]</span><br><span class="line">[INFO] flink-formats ...................................... SUCCESS [  <span class="number">0.361</span> s]</span><br><span class="line">[INFO] flink-json ......................................... SUCCESS [  <span class="number">8.512</span> s]</span><br><span class="line">[INFO] flink-connector-elasticsearch-base ................. SUCCESS [ <span class="number">15.884</span> s]</span><br><span class="line">[INFO] flink-connector-elasticsearch2 ..................... SUCCESS [ <span class="number">38.263</span> s]</span><br><span class="line">[INFO] flink-connector-elasticsearch5 ..................... SUCCESS [ <span class="number">40.943</span> s]</span><br><span class="line">[INFO] flink-connector-elasticsearch6 ..................... SUCCESS [ <span class="number">15.306</span> s]</span><br><span class="line">[INFO] flink-csv .......................................... SUCCESS [  <span class="number">2.397</span> s]</span><br><span class="line">[INFO] flink-connector-hive ............................... SUCCESS [<span class="number">01</span>:<span class="number">39</span> min]</span><br><span class="line">[INFO] flink-connector-rabbitmq ........................... SUCCESS [  <span class="number">1.635</span> s]</span><br><span class="line">[INFO] flink-connector-twitter ............................ SUCCESS [  <span class="number">6.631</span> s]</span><br><span class="line">[INFO] flink-connector-nifi ............................... SUCCESS [  <span class="number">3.832</span> s]</span><br><span class="line">[INFO] flink-connector-cassandra .......................... SUCCESS [ <span class="number">31.134</span> s]</span><br><span class="line">[INFO] flink-avro ......................................... SUCCESS [ <span class="number">15.516</span> s]</span><br><span class="line">[INFO] flink-connector-filesystem ......................... SUCCESS [  <span class="number">6.685</span> s]</span><br><span class="line">[INFO] flink-connector-kafka .............................. SUCCESS [  <span class="number">7.954</span> s]</span><br><span class="line">[INFO] flink-connector-gcp-pubsub ......................... SUCCESS [  <span class="number">4.723</span> s]</span><br><span class="line">[INFO] flink-sql-connector-elasticsearch6 ................. SUCCESS [ <span class="number">16.608</span> s]</span><br><span class="line">[INFO] flink-sql-connector-kafka-<span class="number">0.9</span> ...................... SUCCESS [  <span class="number">2.115</span> s]</span><br><span class="line">[INFO] flink-sql-connector-kafka-<span class="number">0.10</span> ..................... SUCCESS [  <span class="number">4.332</span> s]</span><br><span class="line">[INFO] flink-sql-connector-kafka-<span class="number">0.11</span> ..................... SUCCESS [  <span class="number">1.628</span> s]</span><br><span class="line">[INFO] flink-sql-connector-kafka .......................... SUCCESS [  <span class="number">7.450</span> s]</span><br><span class="line">[INFO] flink-connector-kafka-<span class="number">0.8</span> .......................... SUCCESS [  <span class="number">4.267</span> s]</span><br><span class="line">[INFO] flink-avro-confluent-registry ...................... SUCCESS [ <span class="number">11.695</span> s]</span><br><span class="line">[INFO] flink-parquet ...................................... SUCCESS [  <span class="number">8.175</span> s]</span><br><span class="line">[INFO] flink-sequence-file ................................ SUCCESS [  <span class="number">1.294</span> s]</span><br><span class="line">[INFO] flink-examples ..................................... SUCCESS [  <span class="number">0.457</span> s]</span><br><span class="line">[INFO] flink-examples-batch ............................... SUCCESS [ <span class="number">48.271</span> s]</span><br><span class="line">[INFO] flink-examples-streaming ........................... SUCCESS [ <span class="number">32.402</span> s]</span><br><span class="line">[INFO] flink-examples-table ............................... SUCCESS [ <span class="number">31.118</span> s]</span><br><span class="line">[INFO] flink-examples-build-helper ........................ SUCCESS [  <span class="number">0.274</span> s]</span><br><span class="line">[INFO] flink-examples-streaming-twitter ................... SUCCESS [  <span class="number">1.147</span> s]</span><br><span class="line">[INFO] flink-examples-streaming-state-machine ............. SUCCESS [  <span class="number">0.807</span> s]</span><br><span class="line">[INFO] flink-examples-streaming-gcp-pubsub ................ SUCCESS [  <span class="number">6.252</span> s]</span><br><span class="line">[INFO] flink-container .................................... SUCCESS [  <span class="number">1.893</span> s]</span><br><span class="line">[INFO] flink-queryable-state-runtime ...................... SUCCESS [  <span class="number">3.343</span> s]</span><br><span class="line">[INFO] flink-gelly ........................................ SUCCESS [ <span class="number">10.752</span> s]</span><br><span class="line">[INFO] flink-gelly-scala .................................. SUCCESS [<span class="number">01</span>:<span class="number">01</span> min]</span><br><span class="line">[INFO] flink-gelly-examples ............................... SUCCESS [ <span class="number">49.843</span> s]</span><br><span class="line">[INFO] flink-cep-scala .................................... SUCCESS [ <span class="number">28.524</span> s]</span><br><span class="line">[INFO] flink-state-processor-api .......................... SUCCESS [  <span class="number">5.763</span> s]</span><br><span class="line">[INFO] flink-table-uber ................................... SUCCESS [ <span class="number">15.688</span> s]</span><br><span class="line">[INFO] flink-table-uber-blink ............................. SUCCESS [  <span class="number">3.355</span> s]</span><br><span class="line">[INFO] flink-sql-client ................................... SUCCESS [ <span class="number">23.316</span> s]</span><br><span class="line">[INFO] flink-quickstart ................................... SUCCESS [  <span class="number">2.348</span> s]</span><br><span class="line">[INFO] flink-quickstart-java .............................. SUCCESS [  <span class="number">2.894</span> s]</span><br><span class="line">[INFO] flink-quickstart-scala ............................. SUCCESS [  <span class="number">3.156</span> s]</span><br><span class="line">[INFO] flink-contrib ...................................... SUCCESS [  <span class="number">0.203</span> s]</span><br><span class="line">[INFO] flink-connector-wikiedits .......................... SUCCESS [  <span class="number">2.453</span> s]</span><br><span class="line">[INFO] flink-mesos ........................................ SUCCESS [<span class="number">01</span>:<span class="number">05</span> min]</span><br><span class="line">[INFO] flink-yarn ......................................... SUCCESS [  <span class="number">6.167</span> s]</span><br><span class="line">[INFO] flink-metrics-dropwizard ........................... SUCCESS [  <span class="number">0.927</span> s]</span><br><span class="line">[INFO] flink-metrics-graphite ............................. SUCCESS [  <span class="number">0.302</span> s]</span><br><span class="line">[INFO] flink-metrics-influxdb ............................. SUCCESS [  <span class="number">1.825</span> s]</span><br><span class="line">[INFO] flink-metrics-prometheus ........................... SUCCESS [  <span class="number">1.091</span> s]</span><br><span class="line">[INFO] flink-metrics-statsd ............................... SUCCESS [  <span class="number">0.853</span> s]</span><br><span class="line">[INFO] flink-metrics-datadog .............................. SUCCESS [  <span class="number">1.633</span> s]</span><br><span class="line">[INFO] flink-metrics-slf4j ................................ SUCCESS [  <span class="number">0.880</span> s]</span><br><span class="line">[INFO] flink-python ....................................... SUCCESS [  <span class="number">4.793</span> s]</span><br><span class="line">[INFO] flink-scala-shell .................................. SUCCESS [ <span class="number">43.526</span> s]</span><br><span class="line">[INFO] flink-dist ......................................... SUCCESS [ <span class="number">18.899</span> s]</span><br><span class="line">[INFO] flink-docs ......................................... SUCCESS [  <span class="number">5.133</span> s]</span><br><span class="line">[INFO] flink-ml-parent .................................... SUCCESS [  <span class="number">0.399</span> s]</span><br><span class="line">[INFO] flink-ml-api ....................................... SUCCESS [  <span class="number">4.031</span> s]</span><br><span class="line">[INFO] flink-ml-lib ....................................... SUCCESS [  <span class="number">1.550</span> s]</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: <span class="number">18</span>:<span class="number">19</span> min (Wall Clock)</span><br><span class="line">[INFO] Finished at: <span class="number">2020</span>-<span class="number">01</span>-<span class="number">02</span>T23:<span class="number">01</span>:<span class="number">28</span>+<span class="number">08</span>:<span class="number">00</span></span><br><span class="line">[INFO] Final Memory: <span class="number">334</span>M/<span class="number">847</span>M</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure><h2 id="提取编译后的安装包"><a href="#提取编译后的安装包" class="headerlink" title="提取编译后的安装包"></a>提取编译后的安装包</h2><p>编译成功之后的位置在flink-1.10.0/flink-dist/target/flink-1.10.0-bin中，将文件目录压缩，上传到服务器上就可以配置使用了。</p><h2 id="单节点部署测试"><a href="#单节点部署测试" class="headerlink" title="单节点部署测试"></a>单节点部署测试</h2><ol><li><p>修改flink-conf.yaml</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">jobmanager.rpc.address</span>: <span class="string">hadoop</span></span><br><span class="line"><span class="meta">taskmanager.numberOfTaskSlots</span>: <span class="string">4</span></span><br><span class="line"><span class="meta">rest.port</span>: <span class="string">18081</span></span><br></pre></td></tr></table></figure></li><li><p>修改slaves</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">hadoop</span></span><br></pre></td></tr></table></figure></li><li><p>启动</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">start-cluster.sh</span></span><br></pre></td></tr></table></figure></li><li><p>UI启动</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">hadoop</span>:<span class="string">18081</span></span><br></pre></td></tr></table></figure></li><li><p>在windows快速生成Flink项目</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">mvn</span> <span class="string">archetype:generate \</span></span><br><span class="line">  <span class="meta">-DarchetypeGroupId</span>=<span class="string">org.apache.flink \</span></span><br><span class="line">  <span class="meta">-DarchetypeArtifactId</span>=<span class="string">flink-quickstart-scala \</span></span><br><span class="line">  <span class="meta">-DarchetypeVersion</span>=<span class="string">$&#123;1:-1.10.0&#125; \</span></span><br><span class="line">  <span class="meta">-DgroupId</span>=<span class="string">com.tunan.flink \</span></span><br><span class="line">  <span class="meta">-DartifactId</span>=<span class="string">tunan-flink \</span></span><br><span class="line">  <span class="meta">-Dversion</span>=<span class="string">1.0.0 \</span></span><br><span class="line">  <span class="meta">-Dpackage</span>=<span class="string">com.tunan.quickstart \</span></span><br><span class="line">  <span class="meta">-DinteractiveMode</span>=<span class="string">false</span></span><br></pre></td></tr></table></figure></li></ol><hr><p>参考:</p><p><a href="https://ci.apache.org/projects/flink/flink-docs-stable/flinkDev/building.html" target="_blank" rel="noopener">Apache Flink官方编译手册</a></p><p><a href="https://blog.csdn.net/qq_38976805/article/details/103067833" target="_blank" rel="noopener">Flink1.9.1源码编译支持hadoop-2.6.0-cdh5.16.2</a></p><p><a href="https://blog.csdn.net/qq_38976805/article/details/103067833" target="_blank" rel="noopener">Flink1.9.1源码编译支持hadoop-2.6.0-cdh5.16.2</a></p><p><a href="https://www.jianshu.com/p/d5ed58d7aa65" target="_blank" rel="noopener">Maven添加Kafka Schema Registry的pom依赖</a></p><p><a href="https://blog.csdn.net/u010886217/article/details/84350465" target="_blank" rel="noopener">Maven-CDH版本hadoop添加pom的依赖</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PrestoUDF开发</title>
      <link href="/2020/05/04/presto/3/"/>
      <url>/2020/05/04/presto/3/</url>
      
        <content type="html"><![CDATA[<h2 id="Presto函数"><a href="#Presto函数" class="headerlink" title="Presto函数"></a>Presto函数</h2><p>在 Presto 中，函数大体分为三种：scalar，aggregation 和 window 类型。分别如下：</p><p>1）scalar标量函数，简单来说就是 Java 中的一个静态方法，本身没有任何状态。</p><p>2）aggregation累积状态的函数，或聚集函数，如count，avg。如果只是单节点，单机状态可以直接用一个变量存储即可，但是presto是分布式计算引擎，状态数据会在多个节点之间传输，因此状态数据需要被序列化成 Presto 的内部格式才可以被传输。</p><p>3）window 窗口函数，如同sparkSQL中的窗口函数类似</p><p>官网地址：<a href="https://prestodb.github.io/docs/current/develop/functions.html" target="_blank" rel="noopener">https://prestodb.github.io/docs/current/develop/functions.html</a></p><h2 id="自定义Scalar函数的实现"><a href="#自定义Scalar函数的实现" class="headerlink" title="自定义Scalar函数的实现"></a>自定义Scalar函数的实现</h2><h3 id="定义一个java类"><a href="#定义一个java类" class="headerlink" title="定义一个java类"></a>定义一个java类</h3><ol><li><p>用 @ScalarFunction 的 Annotation 标记实现业务逻辑的静态方法。</p></li><li><p>用 @Description 描述函数的作用，这里的内容会在 SHOW FUNCTIONS 中显示。</p></li><li><p>用@SqlType 标记函数的返回值类型，如返回字符串，因此是 StandardTypes.VARCHAR。</p></li><li><p>Java 方法的返回值必须使用 Presto 内部的序列化方式，因此字符串类型必须返回 Slice， 使用 Slices.utf8Slice 方法可以方便的将 String 类型转换成 Slice 类型</p></li></ol><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunna.spark.presto.udf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.function.Description;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.function.ScalarFunction;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.function.SqlType;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.type.StandardTypes;</span><br><span class="line"><span class="keyword">import</span> io.airlift.slice.Slice;</span><br><span class="line"><span class="keyword">import</span> io.airlift.slice.Slices;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PrefixUDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Description</span>(<span class="string">"输入的数据加上前缀"</span>)      <span class="comment">//描述</span></span><br><span class="line">    <span class="meta">@ScalarFunction</span>(<span class="string">"tunan_prefix"</span>)       <span class="comment">//方法名称</span></span><br><span class="line">    <span class="meta">@SqlType</span>(StandardTypes.VARCHAR)       <span class="comment">//返回类型</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Slice <span class="title">prefix</span><span class="params">(@SqlType(StandardTypes.VARCHAR)</span>Slice input)</span>&#123;</span><br><span class="line">        <span class="comment">// Slices.utf8Slice 方法可以方便的将 String 类型转换成 Slice 类型</span></span><br><span class="line">        <span class="keyword">return</span> Slices.utf8Slice(<span class="string">"tunan_prefix_"</span>+input.toStringUtf8());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Presto插件机制"><a href="#Presto插件机制" class="headerlink" title="Presto插件机制"></a>Presto插件机制</h3><p>presto不能像hive那样配置自定义的udf，要采用这种插件机制实现。Presto 的插件(Plugin)机制，是 Presto 能够整合多种数据源的核心。通过实现不同的 Plugin，Presto 允许用户在不同类型的数据源之间进行 JOIN 等计算。Presto 内部的所有数据源都是通过插件机制实现， 例如 MySQL、Hive、HBase等。Presto 插件机制不仅通过加载 Connector 来实现不同数据源的访问，还通过加载 FunctionFactory 来实现 UDF 的加载。 Presto 的 Plugin 遵循 Java 中的 ServiceLoader 规范， 实现非常简单。</p><p>实现一个plugin接口如：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunna.spark.presto.udf;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.Plugin;</span><br><span class="line"><span class="keyword">import</span> com.google.common.collect.ImmutableSet;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Set;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProstoUDFPlugin</span> <span class="keyword">implements</span> <span class="title">Plugin</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Set&lt;Class&lt;?&gt;&gt; getFunctions() &#123;</span><br><span class="line">        <span class="keyword">return</span> ImmutableSet.&lt;Class&lt;?&gt;&gt;builder()</span><br><span class="line">                .add(PrefixUDF<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">                .<span class="title">build</span>()</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="注册函数"><a href="#注册函数" class="headerlink" title="注册函数"></a>注册函数</h3><p>在resources下创建META-INF/services目录，创建文件com.facebook.presto.spi.Plugin，拷贝Plugin的全限定名</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">com.tunna.spark.presto.udf.ProstoUDFPlugin</span><br></pre></td></tr></table></figure><p>最后在presto的plugin目录下创建我们自己的目录，并且打包上传到我们自己的目录下，需要重启presto才能将jar中的自定义函数加载进去，如果有多个依赖，都需要放在我们创建的目录下</p><h2 id="自定义Aggregation函数的实现"><a href="#自定义Aggregation函数的实现" class="headerlink" title="自定义Aggregation函数的实现"></a>自定义Aggregation函数的实现</h2><h3 id="实现原理步骤"><a href="#实现原理步骤" class="headerlink" title="实现原理步骤"></a>实现原理步骤</h3><p>Presto 把 Aggregation 函数分解成三个步骤执行：</p><ol><li><p>input(state, data): 针对每条数据，执行 input 函数。这个过程是并行执行的，因此在每个有数据的节点都会执行，最终得到多个累积的状态数据。</p></li><li><p>combine(state1, state2)：将所有节点的状态数据聚合起来，多次执行，直至所有状态数据被聚合成一个最终状态，也就是 Aggregation 函数的输出结果。</p></li><li><p>output(final_state, out)：最终输出结果到一个 BlockBuilder。</p></li></ol><h3 id="具体代码实现过程"><a href="#具体代码实现过程" class="headerlink" title="具体代码实现过程"></a>具体代码实现过程</h3><ol><li>一个继承AccumulatorState的State接口，自定义get和set方法</li><li>定义一个 Java 类，使用 @AggregationFunction 标记为 Aggregation 函数</li><li>使用 @InputFunction、 @CombineFunction、@OutputFunction 分别标记计算函数、合并结果函数和最终输出函数在 Plugin 处注册 Aggregation 函数</li></ol><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunna.spark.presto.aggudf;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.block.BlockBuilder;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.function.*;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.type.StandardTypes;</span><br><span class="line"><span class="keyword">import</span> io.airlift.slice.Slice;</span><br><span class="line"><span class="keyword">import</span> io.airlift.slice.Slices;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> com.facebook.presto.spi.type.VarcharType.VARCHAR;</span><br><span class="line"></span><br><span class="line"><span class="meta">@AggregationFunction</span>(<span class="string">"tunan_concat"</span>)    <span class="comment">// Agg方法名</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TunanAggregationUDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@InputFunction</span>  <span class="comment">//输入函数</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">input</span><span class="params">(StringValueState state,@SqlType(StandardTypes.VARCHAR)</span> Slice value)</span>&#123;</span><br><span class="line"></span><br><span class="line">        state.setStringValue(Slices.utf8Slice(isNull(state.getStringValue())+<span class="string">"|"</span>+value.toStringUtf8()));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@CombineFunction</span>    <span class="comment">//合并函数</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">combine</span><span class="params">(StringValueState state1,StringValueState state2)</span></span>&#123;</span><br><span class="line">        state1.setStringValue(Slices.utf8Slice(isNull(state1.getStringValue())+<span class="string">"|"</span>+isNull(state2.getStringValue())));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@OutputFunction</span>(StandardTypes.VARCHAR)  <span class="comment">//输出函数</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">output</span><span class="params">(StringValueState state, BlockBuilder builder)</span></span>&#123;</span><br><span class="line">        VARCHAR.writeSlice(builder,state.getStringValue());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断null值</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">isNull</span><span class="params">(Slice slice)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> slice ==<span class="keyword">null</span>?<span class="string">""</span>:slice.toStringUtf8();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="注册函数-1"><a href="#注册函数-1" class="headerlink" title="注册函数"></a>注册函数</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProstoUDFPlugin</span> <span class="keyword">implements</span> <span class="title">Plugin</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Set&lt;Class&lt;?&gt;&gt; getFunctions() &#123;</span><br><span class="line">        <span class="keyword">return</span> ImmutableSet.&lt;Class&lt;?&gt;&gt;builder()</span><br><span class="line">                .add(PrefixUDF<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">                .<span class="title">add</span>(<span class="title">TunanAggregationUDF</span>.<span class="title">class</span>) // 新加的</span></span><br><span class="line"><span class="class">                .<span class="title">build</span>()</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://yerias.github.io/presto/udf.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Presto </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Presto </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Presto部署</title>
      <link href="/2020/05/03/presto/2/"/>
      <url>/2020/05/03/presto/2/</url>
      
        <content type="html"><![CDATA[<h2 id="安装Presto"><a href="#安装Presto" class="headerlink" title="安装Presto"></a>安装Presto</h2><p>1.下载</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">wget</span> <span class="string">https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.200/presto-server-0.200.tar.gz</span></span><br></pre></td></tr></table></figure><p>官网下载最新版本: <a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-server" target="_blank" rel="noopener">点我进入官网下载</a> ，注意选择presto-server</p><p>2.解压</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">tar</span> <span class="string">-zxvf presto-server-0.200.tar.gz -C /usr/local/</span></span><br></pre></td></tr></table></figure><p>/usr/local/presto-server-0.200则为安装目录，另外Presto还需要数据目录，数据目录最好不要在安装目录里面，方便后面Presto的版本升级。</p><h2 id="配置Presto"><a href="#配置Presto" class="headerlink" title="配置Presto"></a>配置Presto</h2><p>在安装目录里创建etc目录。这目录会有以下配置：</p><ul><li>结点属性（Node Properties）：每个结点的环境配置</li><li>JVM配置（JVM Config）：Java虚拟机的命令行选项</li><li>配置属性（Config Properties）：Persto server的配置</li><li>Catelog属性（Catalog Properties）：配置Connector（数据源）</li></ul><h3 id="结点属性（Node-Properties）"><a href="#结点属性（Node-Properties）" class="headerlink" title="结点属性（Node Properties）"></a>结点属性（Node Properties）</h3><p>结点属性文件etc/node.properties，包含每个结点的配置。一个结点是一个Presto实例。这文件一般是在Presto第一次安装时创建的。以下是最小配置：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">node.environment</span>=<span class="string">production</span></span><br><span class="line"><span class="meta">node.id</span>=<span class="string">ffffffff-ffff-ffff-ffff-ffffffffffff</span></span><br><span class="line"><span class="meta">node.data-dir</span>=<span class="string">/var/presto/data</span></span><br></pre></td></tr></table></figure><p><code>node.environment</code>: 环境名字，Presto集群中的结点的环境名字都必须是一样的。<br><code>node.id</code>: 唯一标识，每个结点的标识都必须是为一的。就算重启或升级Presto都必须还保持原来的标识。<br><code>node.data-dir</code>: 数据目录，Presto用它来保存log和其他数据，<strong>建议放在自己定义的目录下</strong>。</p><h3 id="JVM配置（JVM-Config）"><a href="#JVM配置（JVM-Config）" class="headerlink" title="JVM配置（JVM Config）"></a>JVM配置（JVM Config）</h3><p>JVM配置文件etc/jvm.config，包含启动Java虚拟机时的命令行选项。格式是每一行是一个命令行选项。此文件数据是由shell解析，所以选项中包含空格或特殊字符会被忽略。</p><p>以下是参考配置：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">-server</span></span><br><span class="line"><span class="attr">-Xmx16G</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+UseG1GC</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">G1HeapRegionSize=32M</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+UseGCOverheadLimit</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+ExplicitGCInvokesConcurrent</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+HeapDumpOnOutOfMemoryError</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+ExitOnOutOfMemoryError</span></span><br></pre></td></tr></table></figure><p>注意：如果<code>ExitOnOutOfMemoryError</code>报错，注释即可</p><p>因为<code>OutOfMemoryError</code>会导致JVM存在不一致状态，所以用heap dump来debug，来找出进程为什么崩溃的原因。</p><h3 id="配置属性（Config-Properties）"><a href="#配置属性（Config-Properties）" class="headerlink" title="配置属性（Config Properties）"></a>配置属性（Config Properties）</h3><p>配置属性文件etc/config.properties，包含Presto server的配置。Presto server可以同时为coordinator和worker，但一个大集群里最好就是只指定一台机器为coordinator。<br>以下是coordinator的最小配置：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">coordinator</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">node-scheduler.include-coordinator</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">http-server.http.port</span>=<span class="string">8080</span></span><br><span class="line"><span class="meta">query.max-memory</span>=<span class="string">50GB</span></span><br><span class="line"><span class="meta">query.max-memory-per-node</span>=<span class="string">1GB</span></span><br><span class="line"><span class="meta">discovery-server.enabled</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">discovery.uri</span>=<span class="string">http://example.net:8080</span></span><br></pre></td></tr></table></figure><p>以下是worker的最小配置：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">coordinator</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">http-server.http.port</span>=<span class="string">8080</span></span><br><span class="line"><span class="meta">query.max-memory</span>=<span class="string">50GB</span></span><br><span class="line"><span class="meta">query.max-memory-per-node</span>=<span class="string">1GB</span></span><br><span class="line"><span class="meta">discovery.uri</span>=<span class="string">http://example.net:8080</span></span><br></pre></td></tr></table></figure><p>如果适用于测试目的，需要将一台机器同时配置为coordinator和worker，则使用以下配置：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">coordinator</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">node-scheduler.include-coordinator</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">http-server.http.port</span>=<span class="string">8080</span></span><br><span class="line"><span class="meta">query.max-memory</span>=<span class="string">2GB</span></span><br><span class="line"><span class="meta">query.max-memory-per-node</span>=<span class="string">512MB</span></span><br><span class="line"><span class="meta">discovery-server.enabled</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">discovery.uri</span>=<span class="string">http://example.net:8080</span></span><br></pre></td></tr></table></figure><p><code>coordinator</code>： 是否运行该实例为coordinator（接受client的查询和管理查询执行）。<br><code>node-scheduler.include-coordinator:coordinator</code>: 是否也作为work。对于大型集群来说，在coordinator里做worker的工作会影响查询性能。<br><code>http-server.http.port</code>:指定HTTP端口。Presto使用HTTP来与外部和内部进行交流。<br><code>query.max-memory</code>: 查询能用到的最大总内存<br><code>query.max-memory-per-node</code>: 查询能用到的最大单结点内存<br><code>discovery-server.enabled</code>: Presto使用Discovery服务去找到集群中的所有结点。每个Presto实例在启动时都会在Discovery服务里注册。这样可以简化部署，不需要额外的服务，Presto的coordinator内置一个Discovery服务。也是使用HTTP端口。<br><code>discovery.uri</code>: Discovery服务的URI。将example.net:8080替换为coordinator的host和端口。<strong>这个URI不能以斜杠结尾，这个错误需特别注意，不然会报404错误</strong>。</p><p>另外还有以下属性：<br><code>jmx.rmiregistry.port</code>: 指定JMX RMI的注册。JMX client可以连接此端口<br><code>jmx.rmiserver.port</code>: 指定JXM RMI的服务器。可通过JMX监听。</p><p>详情请查看<a href="https://prestodb.io/docs/current/admin/resource-groups.html" target="_blank" rel="noopener">Resource Groups</a></p><h3 id="Catelog属性（Catalog-Properties）"><a href="#Catelog属性（Catalog-Properties）" class="headerlink" title="Catelog属性（Catalog Properties）"></a>Catelog属性（Catalog Properties）</h3><p>Presto通过connector访问数据。而connector是挂载（mount）在catelog中。connector支持catelog里所有的schema和table。举个例子，Hive connector映射每个Hive数据库到schema，因此Hive connector挂载在hive catelog（所以可以把catelog理解为目录，挂载），而且Hive包含table clicks在数据库web，所以这个table在Presto是hive.web.clicks。<br>Catalog的注册是通过etc/catalog目录下的catalog属性文件。例如，创建etc/catalog/jmx.properties，将jmxconnector挂载在jmx catelog：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">connector.name</span>=<span class="string">jmx</span></span><br></pre></td></tr></table></figure><p>查看<a href="https://prestodb.io/docs/current/connector.html" target="_blank" rel="noopener">Connectors</a>查看更多信息。</p><p>启动命令：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">bin/launcher</span> <span class="string">start</span></span><br></pre></td></tr></table></figure><p>日志在val/log目录下：<br><code>launcher.log</code>: 记录服务初始化情况和一些JVM的诊断。<br><code>server.log: Presto</code>: 的主要日志文件。会自动被压缩。<br><code>http-request.log</code>: 记录HTTP请求。会自动被压缩。</p><h2 id="配置MySQL-Connector"><a href="#配置MySQL-Connector" class="headerlink" title="配置MySQL Connector"></a>配置<a href="https://prestodb.io/docs/current/connector/mysql.html" target="_blank" rel="noopener">MySQL Connector</a></h2><p>创建<code>etc/catalog/mysql.properties</code></p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">connector.name</span>=<span class="string">mysql</span></span><br><span class="line"><span class="meta">connection-url</span>=<span class="string">jdbc:mysql://example.net:3306</span></span><br><span class="line"><span class="meta">connection-user</span>=<span class="string">root</span></span><br><span class="line"><span class="meta">connection-password</span>=<span class="string">secret</span></span><br></pre></td></tr></table></figure><h2 id="配置Hive-Connector"><a href="#配置Hive-Connector" class="headerlink" title="配置Hive Connector"></a>配置<a href="https://prestodb.io/docs/current/connector/hive.html" target="_blank" rel="noopener">Hive Connector</a></h2><p>创建<code>etc/catalog/hive.properties</code></p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">connector.name</span>=<span class="string">hive-hadoop2</span></span><br><span class="line"><span class="meta">hive.metastore.uri</span>=<span class="string">thrift://example.net:9083</span></span><br><span class="line"><span class="meta">hive.config.resources</span>=<span class="string">/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml</span></span><br></pre></td></tr></table></figure><p>还可以在<code>jvm.config</code>中Hadoop的代理用户</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">-DHADOOP_USER_NAME</span>=<span class="string">hdfs_user</span></span><br></pre></td></tr></table></figure><h2 id="运行Presto命令行界面"><a href="#运行Presto命令行界面" class="headerlink" title="运行Presto命令行界面"></a>运行Presto命令行界面</h2><p>1.下载 presto-cli-0.200-executable.jar(<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/" target="_blank" rel="noopener">下载最新版</a>),<br>2.修改名字 presto-cli-0.200-executable.jar为 presto<br>3.修改执行权限chmod +x<br>4.运行</p><p>指定catelog</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">./presto</span> <span class="string">--server localhost:8080 --catalog hive --schema default</span></span><br></pre></td></tr></table></figure><p>不指定catelog，可在命令行使用多个catalog</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">./presto</span> <span class="string">--server localhost:8080</span></span><br></pre></td></tr></table></figure><p>注意: JDK必须大于<code>jdk-8u151</code></p><p><strong>hive join mysql</strong></p><p><code>select * from hive.default.emp a join mysql.tunan.dept b on a.deptno = b.deptno;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"> empno | ename  |    job    | jno  |    date    |  sal   | prize  | deptno | deptno |   dname    | level </span><br><span class="line"><span class="comment">-------+--------+-----------+------+------------+--------+--------+--------+--------+------------+-------</span></span><br><span class="line"> 7369  | SMITH  | CLERK     | 7902 | 1980-12-17 |  800.0 | NULL   | 20     | 20     | RESEARCH   |  1800 </span><br><span class="line"> 7499  | ALLEN  | SALESMAN  | 7698 | 1981-2-20  | 1600.0 |  300.0 | 30     | 30     | SALES      |  1900 </span><br><span class="line"> 7521  | WARD   | SALESMAN  | 7698 | 1981-2-22  | 1250.0 |  500.0 | 10     | 10     | ACCOUNTING |  1700 </span><br><span class="line"> 7566  | JONES  | MANAGER   | 7839 | 1981-4-2   | 2975.0 | NULL   | 10     | 10     | ACCOUNTING |  1700 </span><br><span class="line"> 7654  | MARTIN | SALESMAN  | 7698 | 1981-9-28  | 1250.0 | 1400.0 | 30     | 30     | SALES      |  1900</span><br></pre></td></tr></table></figure><p>命令帮助: help</p>]]></content>
      
      
      <categories>
          
          <category> Presto </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Presto </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Presto扫盲</title>
      <link href="/2020/05/02/presto/1/"/>
      <url>/2020/05/02/presto/1/</url>
      
        <content type="html"><![CDATA[<h2 id="Presto简介"><a href="#Presto简介" class="headerlink" title="Presto简介"></a>Presto简介</h2><h3 id="不是什么"><a href="#不是什么" class="headerlink" title="不是什么"></a>不是什么</h3><p>虽然Presto可以解析SQL，但它不是一个标准的数据库。不是MySQL、PostgreSQL或者Oracle的代替品，也不能用来处理在线事务（OLTP）</p><h3 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h3><p>Presto通过使用分布式查询，可以快速高效的完成海量数据的查询。作为Hive和Pig的补充，Presto不仅能访问HDFS，也能访问不同的数据源，包括：RDBMS和其他数据源（如Cassandra）。</p><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><img src="https://yerias.github.io/presto/20200502161939.jpg" alt=""></p><p>图中各个组件的概念及作用会在下文讲述。</p><h3 id="Presto中SQL运行过程：MapReduce-vs-Presto"><a href="#Presto中SQL运行过程：MapReduce-vs-Presto" class="headerlink" title="Presto中SQL运行过程：MapReduce vs Presto"></a>Presto中SQL运行过程：MapReduce vs Presto</h3><p><img src="https://yerias.github.io/presto/3280890894-5af69697e1249_articlex.png" alt=""></p><p>使用内存计算，减少与硬盘交互。</p><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol><li>Presto与hive对比，都能够处理PB级别的海量数据分析，但Presto是基于内存运算，减少没必要的硬盘IO，所以更快。</li><li>能够连接多个数据源，跨数据源连表查，如从hive查询大量网站访问记录，然后从mysql中匹配出设备信息。</li><li>部署也比hive简单，因为hive是基于HDFS的，需要先部署HDFS。</li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li>虽然能够处理PB级别的海量数据分析，但不是代表Presto把PB级别都放在内存中计算的。而是根据场景，如count，avg等聚合运算，是边读数据边计算，再清内存，再读数据再计算，这种耗的内存并不高。但是连表查，就可能产生大量的临时数据，因此速度会变慢，反而hive此时会更擅长。</li><li>为了达到实时查询，可能会想到用它直连MySQL来操作查询，这效率并不会提升，瓶颈依然在MySQL，此时还引入网络瓶颈，所以会比原本直接操作数据库要慢。</li></ol><h2 id="Presto概念"><a href="#Presto概念" class="headerlink" title="Presto概念"></a>Presto概念</h2><h3 id="服务器类型（Server-Types）"><a href="#服务器类型（Server-Types）" class="headerlink" title="服务器类型（Server Types）"></a>服务器类型（Server Types）</h3><p>Presto有两类服务器：coordinator和worker。</p><h4 id="Coordinator"><a href="#Coordinator" class="headerlink" title="Coordinator"></a>Coordinator</h4><p>Coordinator服务器是用来解析语句，执行计划分析和管理Presto的worker结点。Presto安装必须有一个Coordinator和多个worker。如果用于开发环境和测试，则一个Presto实例可以同时担任这两个角色。</p><p>Coordinator跟踪每个work的活动情况并协调查询语句的执行。 Coordinator为每个查询建立模型，模型包含多个stage，每个stage再转为task分发到不同的worker上执行。</p><p>Coordinator与Worker、client通信是通过REST API。</p><h4 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h4><p>Worker是负责执行任务和处理数据。Worker从connector获取数据。Worker之间会交换中间数据。Coordinator是负责从Worker获取结果并返回最终结果给client。</p><p>当Worker启动时，会广播自己去发现 Coordinator，并告知 Coordinator它是可用，随时可以接受task。</p><p>Worker与Coordinator、Worker通信是通过REST API。</p><h3 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h3><p>贯穿全文，你会看到一些术语：connector、catelog、schema和table。这些是Presto特定的数据源</p><h4 id="Connector"><a href="#Connector" class="headerlink" title="Connector"></a>Connector</h4><p>Connector是适配器，用于Presto和数据源（如Hive、RDBMS）的连接。你可以认为类似JDBC那样，但却是Presto的SPI的实现，使用标准的API来与不同的数据源交互。</p><p>Presto有几个内建Connector：JMX的Connector、System Connector（用于访问内建的System table）、Hive的Connector、TPCH（用于TPC-H基准数据）。还有很多第三方的Connector，所以Presto可以访问不同数据源的数据。</p><p>每个catalog都有一个特定的Connector。如果你使用catelog配置文件，你会发现每个文件都必须包含connector.name属性，用于指定catelog管理器（创建特定的Connector使用）。一个或多个catelog用同样的connector是访问同样的数据库。例如，你有两个Hive集群。你可以在一个Presto集群上配置两个catelog，两个catelog都是用Hive Connector，从而达到可以查询两个Hive集群。</p><h4 id="Catelog"><a href="#Catelog" class="headerlink" title="Catelog"></a>Catelog</h4><p>一个Catelog包含Schema和Connector。例如，你配置JMX的catelog，通过JXM Connector访问JXM信息。当你执行一条SQL语句时，可以同时运行在多个catelog。</p><p>Presto处理table时，是通过表的完全限定（fully-qualified）名来找到catelog。例如，一个表的权限定名是hive.test_data.test，则test是表名，test_data是schema，hive是catelog。</p><p>Catelog的定义文件是在Presto的配置目录中。</p><h4 id="Schema"><a href="#Schema" class="headerlink" title="Schema"></a>Schema</h4><p>Schema是用于组织table。把catelog好schema结合在一起来包含一组的表。当通过Presto访问hive或Mysq时，一个schema会同时转为hive和mysql的同等概念。</p><h4 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h4><p>Table跟关系型的表定义一样，但数据和表的映射是交给Connector。</p><h3 id="执行查询的模型（Query-Execution-Model）"><a href="#执行查询的模型（Query-Execution-Model）" class="headerlink" title="执行查询的模型（Query Execution Model）"></a>执行查询的模型（Query Execution Model）</h3><h4 id="语句（Statement）"><a href="#语句（Statement）" class="headerlink" title="语句（Statement）"></a>语句（Statement）</h4><p>Presto执行ANSI兼容的SQL语句。当Presto提起语句时，指的就是ANSI标准的SQL语句，包含着列名、表达式和谓词。</p><p>之所以要把语句和查询分开说，是因为Presto里，语句只是简单的文本SQL语句。而当语句执行时，Presto则会创建查询和分布式查询计划并在Worker上运行。</p><h4 id="查询（Query）"><a href="#查询（Query）" class="headerlink" title="查询（Query）"></a>查询（Query）</h4><p>当Presto解析一个语句时，它将其转换为一个查询，并创建一个分布式查询计划（多个互信连接的stage，运行在Worker上）。如果想获取Presto的查询情况，则获取每个组件（正在执行这语句的结点）的快照。</p><p>查询和语句的区别是，语句是存SQL文本，而查询是配置和实例化的组件。一个查询包含：stage、task、split、connector、其他组件和数据源。</p><h4 id="Stage"><a href="#Stage" class="headerlink" title="Stage"></a>Stage</h4><p>当Presto执行查询时，会将执行拆分为有层次结构的stage。例如，从hive中的10亿行数据中聚合数据，此时会创建一个用于聚合的根stage，用于聚合其他stage的数据。</p><p>层次结构的stage类似一棵树。每个查询都由一个根stage，用于聚合其他stage的数据。stage是Coordinator的分布式查询计划（distributed query plan）的模型，stage不是在worker上运行。</p><h4 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h4><p>由于stage不是在worker上运行。stage又会被分为多个task，在不同的work上执行。<br>Task是Presto结构里是“work horse”。一个分布式查询计划会被拆分为多个stage，并再转为task，然后task就运行或处理split。Task有输入和输出，一个stage可以分为多个并行执行的task，一个task可以分为多个并行执行的driver。</p><h4 id="Split"><a href="#Split" class="headerlink" title="Split"></a>Split</h4><p>Task运行在split上。split是一个大数据集合中的一块。分布式查询计划最底层的stage是通过split从connector上获取数据，分布式查询计划中间层或顶层则是从它们下层的stage获取数据。</p><p>Presto调度查询，coordinator跟踪每个机器运行什么任务，那些split正在被处理。</p><h4 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h4><p>Task包含一个或多个并行的driver。Driver在数据上处理，并生成输出，然后由Task聚合，最后传送给stage的其他task。一个driver是Operator的序列。driver是Presto最最低层的并行机制。一个driver有一个输出和一个输入。</p><h4 id="Operator"><a href="#Operator" class="headerlink" title="Operator"></a>Operator</h4><p>Operator用来消费，传送和生产数据。如一个Operator从connector中扫表获取数据，然后生产数据给其他Operator消费。一个过滤Operator消费数据，并应用谓词，最后生产出子集数据。</p><h4 id="Exchange"><a href="#Exchange" class="headerlink" title="Exchange"></a>Exchange</h4><p>Exchange在Presto结点的不同stage之间传送数据。Task生产和消费数据是通过Exchange客户端。</p><hr><p>参考：<a href="https://prestodb.io/docs/current/overview/concepts.html" target="_blank" rel="noopener">https://prestodb.io/docs/current/overview/concepts.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Presto </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Presto </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FileNotFountException: file:/home/hadoop/lib/tunan-spark-core-1.0.jar!/ip2region.db</title>
      <link href="/2020/04/28/error/8/"/>
      <url>/2020/04/28/error/8/</url>
      
        <content type="html"><![CDATA[<p>以后看到标题这种Error，先别管其他的，首先看看代码中有没有把Master注释掉，不然jar包中的文件永远到不了服务器的环境中去，就算把文件在服务器上的路径写死都没用。</p><p>由于Spark不会自动清理–files和–jars传到服务器中的文件，因此只要我们传上去的jar包运行通一次，后面不管代码中有没有指定Master，都能找到服务器中的文件。</p><p>报错图示1：</p><p><img src="https://yerias.github.io/error/notfount1.png" alt="notfount1"></p><p>报错图示2：</p><p><img src="https://yerias.github.io/error/notfount2.png" alt="notfount2"></p>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>error: object hadoop is not a member of packee com</title>
      <link href="/2020/04/27/error/7/"/>
      <url>/2020/04/27/error/7/</url>
      
        <content type="html"><![CDATA[<p>这个问题是在Spark读取Lzo压缩文件的时候碰见的，Spark读取Lzo压缩文件的时候，就算文件添加了索引，也不能分片，原因是要在获取文件的时候使用newAPIHadoopFile算子读取文件获取rdd</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.newAPIHadoopFile(in, classOf[<span class="type">LzoTextInputFormat</span>], classOf[<span class="type">LongWritable</span>],</span><br><span class="line">                              classOf[<span class="type">Text</span>]).map(x =&gt; x._2.toString)</span><br></pre></td></tr></table></figure><p>而这里最重要的就是LzoTextInputFormat类，这个类是Twitter的，但是添加了Twitter<a href="https://maven.twttr.com/" target="_blank" rel="noopener">仓库</a>后，能进仓库，但是不能下载，最后向朋友要了jar包，关键在于只给了我hadoop-lzo-0.4.20.jar，我通过添加外部依赖的方式，加到了项目里，运行的时候就报了error: object hadoop is not a member of packee com的错误</p><p><img src="E:%5Chexo%5Cyeriasblog%5Cthemes%5Cmelody%5Csource%5Cerror%5Chadoop-lzo.png" alt="hadoop-lzo"></p><p>其原因是只有jar包，没有pom文件，最后将jar包和pom文件一起放入maven仓库中，解决问题</p><hr><p><strong>终极解决办法是在github上下载源码，通过编译maven install到本地仓库</strong></p>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introducing Window Functions in Spark SQL</title>
      <link href="/2020/04/26/%E7%BF%BB%E8%AF%91/2/"/>
      <url>/2020/04/26/%E7%BF%BB%E8%AF%91/2/</url>
      
        <content type="html"><![CDATA[<p>原文：<a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="noopener">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a></p><hr><p>在这篇博客文章中，我们将介绍Apache Spark 1.4中添加的新窗口函数特性。窗口函数允许Spark SQL的用户计算结果，比如给定行的秩或输入行的范围内的移动平均值。它们显著提高了Spark的SQL和DataFrame api的表达能力。本博客将首先介绍窗口函数的概念，然后讨论如何与Spark SQL和Spark的DataFrame API一起使用它们。</p><h2 id="什么是窗口函数"><a href="#什么是窗口函数" class="headerlink" title="什么是窗口函数?"></a>什么是窗口函数?</h2><p>在1.4之前，Spark SQL支持两种函数，可用于计算单个返回值。内置函数或udf(如substr或round)将单个行中的值作为输入，并为每个输入行生成单个返回值。聚合函数(如SUM或MAX)对一组行进行操作，并为每个组计算单个返回值。</p><p>虽然这两种方法在实践中都非常有用，但是仍然有大量的操作不能单独使用这些类型的函数来表示。具体来说，无法同时对一组行进行操作，同时仍然为每个输入行返回一个值。这种限制使得执行各种数据处理任务(如计算移动平均值、计算累计和或访问当前行之前的行值)变得非常困难。幸运的是，对于Spark SQL的用户来说，窗口函数填补了这一空白。</p><p>在其核心，一个窗口函数根据一组行(称为Frame)为表的每个输入行计算一个返回值。每个输入行都可以有一个与之关联的唯一frame。窗口函数的这种特性使它们比其他函数更强大，并允许用户以简洁的方式表达各种数据处理任务，而这些任务如果没有窗口函数是很难(如果不是不可能)表达的。现在，让我们看两个例子。</p><p>假设我们有一个如下所示的productRevenue表。</p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/1-1.png" alt="1-1"></p><p>我们想回答两个问题:</p><ol><li><p>每一类中最畅销和第二畅销的产品是什么(<strong>分组top n</strong>)?</p></li><li><p>每种产品的收入和同类产品中最畅销的产品的收入有什么不同(<strong>最大值 - 当前值</strong>)?</p></li></ol><p>回答第一个问题“在每个类别中，最畅销和第二畅销的产品是什么?”，我们需要根据产品的收入对其进行分类，并根据排名选择最畅销和第二畅销的产品。下面是通过使用窗口函数dense_rank来回答这个问题的SQL查询(我们将在下一节中解释使用窗口函数的语法)。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  product,</span><br><span class="line">  <span class="keyword">category</span>,</span><br><span class="line">  revenue</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">        product,</span><br><span class="line">        <span class="keyword">category</span>,</span><br><span class="line">        revenue,</span><br><span class="line">        <span class="keyword">dense_rank</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">category</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> revenue <span class="keyword">DESC</span>) <span class="keyword">as</span> <span class="keyword">rank</span></span><br><span class="line">    <span class="keyword">FROM</span> productRevenue) tmp</span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">  <span class="keyword">rank</span> &lt;= <span class="number">2</span></span><br></pre></td></tr></table></figure><p>这个查询的结果如下所示。如果不使用窗口函数，就很难用SQL表达查询，即使可以表达SQL查询，底层引擎也很难有效地评估查询。</p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/1-2.png" alt="1-2"></p><p>对于第二个问题，“每种产品的收入与同类产品中最畅销的产品的收入有什么不同?”，要计算一个产品的收入差异，我们需要找到每个产品在相同类别下的最高收入价值。下面是一个用于回答这个问题的Python DataFrame程序(<strong>python代码不重要，看思路</strong>)。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.window <span class="keyword">import</span> Window</span><br><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> func</span><br><span class="line">windowSpec = \</span><br><span class="line">  Window <span class="comment"># 定义开窗函数</span></span><br><span class="line">    .partitionBy(df[<span class="string">'category'</span>]) \   <span class="comment"># 分组</span></span><br><span class="line">    .orderBy(df[<span class="string">'revenue'</span>].desc()) \<span class="comment"># 排序</span></span><br><span class="line">    .rangeBetween(-sys.maxsize, sys.maxsize)  <span class="comment"># 拿到最大值</span></span><br><span class="line">dataFrame = sqlContext.table(<span class="string">"productRevenue"</span>) <span class="comment"># 拿到表</span></span><br><span class="line">revenue_difference = \</span><br><span class="line">  (func.max(dataFrame[<span class="string">'revenue'</span>]).over(windowSpec) - dataFrame[<span class="string">'revenue'</span>]) <span class="comment"># 使用开窗函数</span></span><br><span class="line">dataFrame.select(     <span class="comment"># 查询</span></span><br><span class="line">  dataFrame[<span class="string">'product'</span>],</span><br><span class="line">  dataFrame[<span class="string">'category'</span>],</span><br><span class="line">  dataFrame[<span class="string">'revenue'</span>],</span><br><span class="line">  revenue_difference.alias(<span class="string">"revenue_difference"</span>))   <span class="comment"># 给个别名</span></span><br></pre></td></tr></table></figure><p>这个程序的结果如下所示。在不使用窗口函数的情况下，用户必须找到所有类别的所有最高收入值，然后将这个派生的数据集与原始的productRevenue表连接起来，以计算收入差异。</p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/1-3.png" alt="1-3"></p><h2 id="使用窗口函数"><a href="#使用窗口函数" class="headerlink" title="使用窗口函数"></a>使用窗口函数</h2><p>Spark SQL支持三种窗口函数:排序函数、分析函数和聚合函数。可用的排序函数和分析函数总结如下表所示。对于聚合函数，用户可以使用任何现有的聚合函数作为窗口函数。</p><table><thead><tr><th></th><th><strong>SQL</strong></th><th><strong>DataFrame API</strong></th></tr></thead><tbody><tr><td><strong>Ranking functions</strong></td><td>rank</td><td>rank</td></tr><tr><td></td><td>dense_rank</td><td>denseRank</td></tr><tr><td></td><td>percent_rank</td><td>percentRank</td></tr><tr><td></td><td>ntile</td><td>ntile</td></tr><tr><td></td><td>row_number</td><td>rowNumber</td></tr><tr><td><strong>Analytic functions</strong></td><td>cume_dist</td><td>cumeDist</td></tr><tr><td></td><td>first_value</td><td>firstValue</td></tr><tr><td></td><td>last_value</td><td>lastValue</td></tr><tr><td></td><td>lag</td><td>lag</td></tr><tr><td></td><td>lead</td><td>lead</td></tr></tbody></table><p>要使用窗口函数，用户需要标记一个函数被任意一方用作窗口函数</p><ol><li>在SQL中支持的函数后添加OVER子句，例如<code>avg(revenue) OVER (...)</code>; </li><li>调用DataFrame API中支持的函数上的over方法，例如<code>rank().over(...)</code></li></ol><p>一旦一个函数被标记为一个窗口函数，下一个关键步骤就是定义与这个函数相关的窗口规范。窗口规范定义在与给定输入行关联的frame中包含哪些行。一个窗口规范包括三个部分:</p><ul><li><p>分区规范:控制哪些行将与给定行位于同一分区中。此外，在订购和计算frame之前，用户可能希望确保将category列具有相同值的所有行收集到相同的机器上。如果没有给出分区规范，那么所有数据必须收集到一台机器上。</p></li><li><p>排序规范:控制分区中的行排序的方式，确定给定行在其分区中的位置。</p></li><li><p>Frame规范:根据当前输入行的相对位置，声明当前输入行的frame中包含哪些行。例如，“当前行之前的三行到当前行”描述了一个frame，其中包括当前输入行和出现在当前行之前的三行。</p></li></ul><p>在SQL中， <code>PARTITION BY</code> 和 <code>ORDER BY</code> 关键字分别用于为分区规范指定分区表达式和为排序规范指定排序表达式。SQL语法如下所示。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">OVER (PARTITION BY ... ORDER BY ...)</span><br></pre></td></tr></table></figure><p>在DataFrame API中，我们提供了实用程序函数来定义窗口规范。以Python为例，用户可以按如下方式指定分区表达式和排序表达式。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.window <span class="keyword">import</span> Window</span><br><span class="line">windowSpec = \</span><br><span class="line">  Window \<span class="comment"># 定义窗口</span></span><br><span class="line">    .partitionBy(...) \  <span class="comment"># 分区</span></span><br><span class="line">    .orderBy(...)  <span class="comment"># 排序</span></span><br></pre></td></tr></table></figure><p>除了排序和分区之外，用户还需要定义frame的开始边界、frame的结束边界和frame的类型，这是frame规范的三个组成部分。</p><p>边界有<code>UNBOUNDED PRECEDING</code>, <code>UNBOUNDED FOLLOWING</code>, <code>CURRENT ROW</code>, <code>PRECEDING</code>,和<code>&lt;value&gt; FOLLOWING</code>五种类型。 <code>&lt;value&gt; FOLLOWING</code>. <code>UNBOUNDED PRECEDING</code> and <code>UNBOUNDED FOLLOWING</code>分别表示分区的第一行和最后一行。对于其他三种类型的边界，它们指定当前输入行的位置偏移量，并根据框架的类型定义它们的特定含义。有两种类型的frame，<em>ROW</em> frame 和<em>RANGE</em> frame.</p><h3 id="ROW-frame"><a href="#ROW-frame" class="headerlink" title="ROW frame"></a>ROW frame</h3><p>ROW frame是基于当前输入行位置的物理偏移量，也就是说<code>CURRENT ROW</code>, <code>&lt;value&gt; PRECEDING</code>, or <code>&lt;value&gt; FOLLOWING</code>指定物理偏移.如果使用<code>CURRENT ROW</code>作为边界，它表示当前输入行。 <code>PRECEDING</code> and <code>FOLLOWING</code>分别描述当前输入行之前和之后出现的行数。</p><p>下图演示了一个行frame， <code>1 PRECEDING</code>作为开始边界， <code>1 FOLLOWING</code> 作为结束边界(在SQL中表现为<code>ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING</code>)</p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/2-1-1024x338.png" alt="2-1-1024x338"></p><h3 id="RANGE-frame"><a href="#RANGE-frame" class="headerlink" title="RANGE frame"></a>RANGE frame</h3><p>RANGE frames基于当前输入行位置的逻辑偏移量，语法与ROW frame类似。逻辑偏移量是当前输入行的排序表达式的值与frame的边界行相同表达式的值之间的差。由于这个定义，当使用RANGE frame时，只允许一个排序表达式。此外，对于RANGE frame，对于边界计算而言，具有与当前输入行相同的排序表达式值的所有行都被认为是相同的行。</p><p>现在，让我们看一个例子。在本例中，排序表达式是revenue;开始边界是 <code>2000 PRECEDING</code>;结束边界是<code>1000 FOLLOWING</code>，(这个frame被在SQL中被定义为<code>RANGE BETWEEN 2000 PRECEDING AND 1000 FOLLOWING</code>)，下面的五幅图说明了如何使用当前输入行的更新来更新frame。基本上，对于每一个当前的输入行，基于收入值，我们计算收入范围<code>[current revenue value - 2000, current revenue value + 1000]</code>。收入值在此范围内的所有行都位于当前输入行的frame中。</p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/2-2-1024x369.png" alt="2-2-1024x369"></p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/2-3-1024x263.png" alt="2-3-1024x263"></p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/2-4-1024x263.png" alt="2-4-1024x263"></p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/2-5-1024x263.png" alt="2-5-1024x263"></p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/2-6-1024x263.png" alt="2-6-1024x263"></p><p>总之，要定义窗口规范，用户可以在SQL中使用以下语法。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">OVER (PARTITION BY ... ORDER BY ... frame_type BETWEEN <span class="keyword">start</span> <span class="keyword">AND</span> <span class="keyword">end</span>)</span><br></pre></td></tr></table></figure><p>在这里，<code>frame_type</code>可以是行(对于ROW frame)或范围(对于 RANGE frame);</p><p>都可以使用 <code>UNBOUNDED PRECEDING</code>, <code>CURRENT ROW</code>, <code>&lt;value&gt; PRECEDING</code>, and <code>&lt;value&gt; FOLLOWING</code>其中的任意一个作为开始;  <code>UNBOUNDED FOLLOWING</code>, <code>CURRENT ROW</code>, <code>&lt;value&gt; PRECEDING</code>, and<code>&lt;value&gt; FOLLOWING</code>其中的任意一个作为结束.</p><p>在Python DataFrame API中，用户可以定义如下的窗口规范。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.window <span class="keyword">import</span> Window</span><br><span class="line"><span class="comment"># Defines partitioning specification and ordering specification.</span></span><br><span class="line">windowSpec = \</span><br><span class="line">  Window \</span><br><span class="line">    .partitionBy(...) \</span><br><span class="line">    .orderBy(...)</span><br><span class="line"><span class="comment"># Defines a Window Specification with a ROW frame.</span></span><br><span class="line">windowSpec.rowsBetween(start, end)</span><br><span class="line"><span class="comment"># Defines a Window Specification with a RANGE frame.</span></span><br><span class="line">windowSpec.rangeBetween(start, end)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 原文翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 原文翻译 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ORCFile in HDP 2: Better Compression, Better Performance</title>
      <link href="/2020/04/26/%E7%BF%BB%E8%AF%91/1/"/>
      <url>/2020/04/26/%E7%BF%BB%E8%AF%91/1/</url>
      
        <content type="html"><![CDATA[<p>原文：<a href="https://blog.cloudera.com/orcfile-in-hdp-2-better-compression-better-performance/" target="_blank" rel="noopener">https://blog.cloudera.com/orcfile-in-hdp-2-better-compression-better-performance/</a></p><hr><p>即将发布的Hive 0.12将在存储层带来一些新的重大改进，包括更高的压缩和更好的查询性能。</p><h2 id="高压缩"><a href="#高压缩" class="headerlink" title="高压缩"></a>高压缩</h2><p>ORCFile是在Hive 0.11中引入的，提供了很好的压缩，通过一些技术来实现，包括运行长度编码、字符串字典编码和位图编码。</p><p>这种对效率的关注导致了一些令人印象深刻的压缩比。这张图片显示了TPC-DS数据集在不同编码下的规模为500。该数据集包含随机生成的数据，包括字符串、浮点数和整数数据。</p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/ORCFile.png" alt="ORCFile"></p><p>我们已经看到，有些客户的集群已经从存储的角度扩展到了ORCFile，这是一种释放空间的方法，同时与现有的作业100%兼容。</p><p>存储在ORCFile中的数据可以通过HCatalog读取或写入，因此任何Pig或Map/Reduce进程都可以无缝地运行。Hive 12 建立在这些令人印象深刻的压缩比上，并在Hive和执行层提供深度集成来加速查询，从处理更大的数据集和更低的延迟的角度来看都是如此。</p><h2 id="谓词下推"><a href="#谓词下推" class="headerlink" title="谓词下推"></a>谓词下推</h2><p>SQL查询通常会有一些WHERE条件，可以用来方便地排除需要考虑的行。在旧版本的Hive中，在稍后通过SQL处理消除之前，将行从存储层中读取出来。有很多浪费的开销，Hive 12通过允许将谓词下推并在存储层本身进行计算来优化这一点。它是由环境控制的<code>hive.optimize.ppd=true</code>.</p><p>这需要读者具有足够的智慧来理解谓词。幸运的是，ORC已经进行了相应的改进，允许将谓词推入其中，并利用其内联索引提供性能优势。</p><p>例如，如果你有一个SQL查询:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(*) <span class="keyword">FROM</span> CUSTOMER <span class="keyword">WHERE</span> CUSTOMER.state = ‘CA’;</span><br></pre></td></tr></table></figure><p>ORCFile只返回与WHERE谓词实际匹配的行，而跳过驻留在任何其他状态的客户。从表中读取的列越多，要避免的数据封送处理就越多，速度也就越快。</p><h2 id="ORCFile内联索引"><a href="#ORCFile内联索引" class="headerlink" title="ORCFile内联索引"></a>ORCFile内联索引</h2><p>在进入下一节之前，我们需要花一点时间讨论ORCFile如何将行分解为行组，并在这些行组中应用列式压缩和索引。</p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/orc2.png" alt="orc2"></p><p>ORC的谓词下推将参考内联索引，以尝试确定何时可以一次性跳过整个块。有时，您的数据集自然会促进这一点。例如，如果您的数据是一个时间序列，并且时间戳是单调递增的，那么当您在这个时间戳上加上where条件时，ORC将能够跳过许多行组。</p><p>在其他情况下，可能需要对数据进行排序。如果对一列进行排序，相关记录将被限制在磁盘上的一个区域，其他部分将很快被跳过。</p><p>对于数字类型和字符串类型，跳过可以工作。在这两个实例中，都是通过在内联索引中记录一个最小值和一个最大值并确定查找值是否落在这个范围之外来完成的。</p><p>排序可以导致非常好的加速。这里有一个权衡，您需要提前决定对哪些列排序。决策过程在某种程度上类似于决定在传统SQL系统中索引哪些列。最好的回报是您拥有一个经常使用的列，并且在非常特定的条件下访问它，并且在很多查询中使用它。请记住，您可以在创建表时设置hive.enforce.sorting=true并使用SORT BY关键字强制Hive对列进行排序。</p><p>ORCFile是我们的一个重要的刺激举措，以提高Hive性能100倍。为了显示影响，我们使用修改后的数据模式运行修改后的TPC-DS查询27。查询27在一个大型事实表上执行星型模式连接，访问4个独立的维度表。在修改后的模式中，销售的状态被反规范化为事实表，结果表按状态排序。通过这种方式，当查询扫描事实表时，它可以跳过整个行块，因为查询根据状态进行筛选。这导致了一些增量加速，正如您可以从下面的图表中看到的。</p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/orc1.png" alt="orc1"></p><p>这个功能给你最好的性价比时:</p><ol><li><p>在具有中等到较大基数的列上，您经常以一种精确的方式过滤大型事实表。</p></li><li><p>您可以选择大量的列，或者宽列。您保存的数据封送处理越多，您的加速就越大。</p></li></ol><h2 id="使用ORCFile"><a href="#使用ORCFile" class="headerlink" title="使用ORCFile"></a>使用ORCFile</h2><p>使用ORCFile或将现有数据转换为ORCFile非常简单。要使用它，只需添加存储为orc到您的create table语句的结尾，如下所示:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> mytable (</span><br><span class="line">...</span><br><span class="line">) <span class="keyword">STORED</span> <span class="keyword">AS</span> orc;</span><br></pre></td></tr></table></figure><p>若要将现有数据转换为ORCFile，请创建一个与源表具有相同模式并存储为orc的表，然后可以使用如下查询:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> orctable <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> oldtable;</span><br></pre></td></tr></table></figure><p>Hive将处理所有细节的ORCFile转换，你可以自由删除旧表，以释放大量的空间。</p><p>创建ORC表时，可以使用许多表属性进一步优化ORC的工作方式。</p><table><thead><tr><th><strong>Key</strong></th><th><strong>Default</strong></th><th><strong>Notes</strong></th></tr></thead><tbody><tr><td><code>orc.compress</code></td><td><code>ZLIB</code></td><td>压缩除使用列式压缩外，还可以使用NONE, ZLIB, SNAPPY</td></tr><tr><td><code>orc.compress.size</code></td><td><code>262,144 (= 256 KiB)</code></td><td>每个压缩块中的字节数</td></tr><tr><td>orc.stripe.size</td><td><code>268,435,456 (= 256 MiB)</code></td><td>每条数据中的字节数</td></tr><tr><td><code>orc.row.index.stride</code></td><td><code>10,000</code></td><td>索引项之间的行数 (必须 &gt;= 1,000)</td></tr><tr><td><code>orc.create.index</code></td><td><code>true</code></td><td>是否创建内联索引</td></tr></tbody></table><p>例如，假设您想使用snappy压缩而不是zlib压缩。方法如下:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> mytable (</span><br><span class="line">...</span><br><span class="line">) <span class="keyword">STORED</span> <span class="keyword">AS</span> orc tblproperties (<span class="string">"orc.compress"</span>=<span class="string">"SNAPPY"</span>);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 原文翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 原文翻译 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单节点部署redis</title>
      <link href="/2020/04/23/redis/1/"/>
      <url>/2020/04/23/redis/1/</url>
      
        <content type="html"><![CDATA[<h3 id="第一步：下载redis安装包-整个安装流程建议在root用户下完成"><a href="#第一步：下载redis安装包-整个安装流程建议在root用户下完成" class="headerlink" title="第一步：下载redis安装包(整个安装流程建议在root用户下完成)"></a>第一步：下载redis安装包(整个安装流程建议在root用户下完成)</h3><p>wget <a href="http://download.redis.io/releases/redis-5.0.5.tar.gz" target="_blank" rel="noopener">http://download.redis.io/releases/redis-5.0.5.tar.gz</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop local]# wget http://download.redis.io/releases/redis-5.0.5.tar.gz</span><br><span class="line">--2017-12-13 12:35:12--  http://download.redis.io/releases/redis-5.0.5.tar.gz</span><br><span class="line">Resolving download.redis.io (download.redis.io)... 109.74.203.151</span><br><span class="line">Connecting to download.redis.io (download.redis.io)|109.74.203.151|:80... connected.</span><br><span class="line">HTTP request sent, awaiting response... 200 OK</span><br><span class="line">Length: 1723533 (1.6M) [application/x-gzip]</span><br><span class="line">Saving to: ‘redis-5.0.5.tar.gz’</span><br><span class="line"></span><br><span class="line"><span class="meta">100%</span><span class="bash">[==========================================================================================================&gt;] 1,723,533    608KB/s   <span class="keyword">in</span> 2.8s   </span></span><br><span class="line"></span><br><span class="line">2017-12-13 12:35:15 (608 KB/s) - ‘redis-5.0.5.tar.gz’ saved [1723533/1723533]</span><br></pre></td></tr></table></figure><h3 id="第二步：解压压缩包"><a href="#第二步：解压压缩包" class="headerlink" title="第二步：解压压缩包"></a>第二步：解压压缩包</h3><p>tar -zxvf redis-5.0.5.tar.gz</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop local]# tar -zxvf redis-5.0.5.tar.gz</span><br></pre></td></tr></table></figure><h3 id="第三步：yum安装gcc依赖"><a href="#第三步：yum安装gcc依赖" class="headerlink" title="第三步：yum安装gcc依赖"></a>第三步：yum安装gcc依赖</h3><p>yum install gcc</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop local]# yum install gcc</span><br></pre></td></tr></table></figure><p>遇到选择,输入y即可</p><h3 id="第四步：跳转到redis解压目录下"><a href="#第四步：跳转到redis解压目录下" class="headerlink" title="第四步：跳转到redis解压目录下"></a>第四步：跳转到redis解压目录下</h3><p>cd redis-5.0.5</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop local]# cd redis-5.0.5</span><br></pre></td></tr></table></figure><h3 id="第五步：编译安装"><a href="#第五步：编译安装" class="headerlink" title="第五步：编译安装"></a>第五步：编译安装</h3><p>make MALLOC=libc　　</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop redis-5.0.5]# make MALLOC=libc</span><br></pre></td></tr></table></figure><p>cd src &amp;&amp; make install</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop redis-5.0.5]# cd src &amp;&amp; make install</span><br><span class="line">    CC Makefile.dep</span><br><span class="line"></span><br><span class="line">Hint: It's a good idea to run 'make test' ;)</span><br><span class="line"></span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br></pre></td></tr></table></figure><h3 id="第六步：修改配置文件"><a href="#第六步：修改配置文件" class="headerlink" title="第六步：修改配置文件"></a>第六步：修改配置文件</h3><p>修改redis.conf</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 关闭保护模式</span></span><br><span class="line">protected-mode no</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置端口号</span></span><br><span class="line">prot 16379</span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许后台运行</span></span><br><span class="line">daemonize yes</span><br><span class="line"><span class="meta">#</span><span class="bash"> 日志文件保存位置</span></span><br><span class="line">logfile /home/hadoop/tmp/redis.log</span><br></pre></td></tr></table></figure><h3 id="第七步：设置redis服务"><a href="#第七步：设置redis服务" class="headerlink" title="第七步：设置redis服务"></a>第七步：设置redis服务</h3><p>1、在/etc目录下新建redis目录</p><p>mkdir redis</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop etc]# mkdir redis</span><br></pre></td></tr></table></figure><p>2、将/home/hadoop/app/redis-5.0.5/redis.conf 文件复制一份到/etc/redis目录下，并命名为6379.conf　　</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop redis]# cp /home/hadoop/app/redis-5.0.5/redis.conf /etc/redis/6379.conf</span><br></pre></td></tr></table></figure><p>3、将redis的启动脚本复制一份放到/etc/init.d目录下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop init.d]# cp /home/hadoop/app/redis-5.0.5/utils/redis_init_script /etc/init.d/redisd</span><br></pre></td></tr></table></figure><p>4、设置redis开机自启动</p><p>先切换到/etc/init.d目录下</p><p>然后执行自启命令</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop init.d]# chkconfig redisd on</span><br><span class="line">service redisd does not support chkconfig</span><br></pre></td></tr></table></figure><p>看结果是redisd不支持chkconfig</p><p>解决方法：</p><p>使用vim编辑redisd文件，在第一行加入如下两行注释，保存退出</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> chkconfig:   2345 90 10</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> description:  Redis is a persistent key-value database</span></span><br></pre></td></tr></table></figure><p>注释的意思是，redis服务必须在运行级2，3，4，5下被启动或关闭，启动的优先级是90，关闭的优先级是10。</p><p>再次执行开机自启命令，成功</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop init.d]# chkconfig redisd on</span><br></pre></td></tr></table></figure><p>现在可以直接已服务的形式启动和关闭redis了</p><p>启动：</p><p>service redisd start　</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop ~]# service redisd start</span><br><span class="line">Starting Redis server...</span><br><span class="line">2288:C 13 Dec 13:51:38.087 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo</span><br><span class="line">2288:C 13 Dec 13:51:38.087 # Redis version=4.0.6, bits=64, commit=00000000, modified=0, pid=2288, just started</span><br><span class="line">2288:C 13 Dec 13:51:38.087 # Configuration loaded</span><br></pre></td></tr></table></figure><p>关闭：</p><p>方法1：service redisd stop</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop ~]# service redisd stop</span><br><span class="line">Stopping ...</span><br><span class="line">Redis stopped</span><br></pre></td></tr></table></figure><p>方法2：redis-cli SHUTDOWN</p><p>如果出现如下问题：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop ~]# service redisd start</span><br><span class="line">/var/run/redis_6379.pid exists, process is already running or crashed</span><br></pre></td></tr></table></figure><p>可参考资料：<a href="http://blog.csdn.net/luozhonghua2014/article/details/54649295" target="_blank" rel="noopener">http://blog.csdn.net/luozhonghua2014/article/details/54649295</a></p><h3 id="开启客户端"><a href="#开启客户端" class="headerlink" title="开启客户端"></a>开启客户端</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 本机启动</span></span><br><span class="line">./redis-cli</span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定端口和主机启动</span></span><br><span class="line">./redis-cli -p 16379 -h hadoop</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka eagle安装部署</title>
      <link href="/2020/04/22/kafka/3/"/>
      <url>/2020/04/22/kafka/3/</url>
      
        <content type="html"><![CDATA[<h5 id="1-kafka-zookeeper准备"><a href="#1-kafka-zookeeper准备" class="headerlink" title="1.kafka+zookeeper准备"></a>1.kafka+zookeeper准备</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">这里假设你已经把kafka+zookeeper安装完成，但是需要注意的几点是：</span><br><span class="line">1.kafka需要开启JMX端口</span><br><span class="line">    找到kafka安装路径，进入到bin文件夹，修改下面的地方。</span><br><span class="line">    vi kafka-server-start.sh</span><br><span class="line">...</span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$KAFKA_HEAP_OPTS</span>"</span> = <span class="string">"x"</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">"-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70"</span></span><br><span class="line">    <span class="built_in">export</span> JMX_PORT=<span class="string">"9999"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">...</span><br><span class="line">    参考链接：[kafka添加jmx端口]:https://ke.smartloli.org/3.Manuals/11.Metrics.html</span><br><span class="line"></span><br><span class="line">2.了解kafka在zookeeper配置</span><br><span class="line">    需要查看kafka的server.properties配置</span><br><span class="line">    找到zookeeper.connect此项配置，这个是要配置到eagle里面的</span><br><span class="line">    此处假设zookeeper.connect=192.168.18.11:2181,192.168.18.12:2181,192.168.18.13:2181</span><br><span class="line">    ！！！PS:此处踩了坑，如果说这里的zookeeper地址后面加了其他路径，在kafka-eagle里面也要配置，</span><br><span class="line">    否则在kafka-eagle的Dashboard中无法读取到kafka的信息。比如我们有人安装的kafka集群里面就有    </span><br><span class="line">    192.168.18.11:2181/kafka1或者192.168.18.11:2181/kafka2这种地址。</span><br><span class="line">    如果你在安装kafka的时候没有配置多余路径，这样是最好的，如果有一定要加上。</span><br><span class="line">3.连通性测试</span><br><span class="line">  安装kafka-eagle的服务器，一定要提前测试是否能连接kafka注册的zookeeper端口</span><br><span class="line">  telnet 端口进行测试</span><br></pre></td></tr></table></figure><h5 id="2-JDK环境准备"><a href="#2-JDK环境准备" class="headerlink" title="2.JDK环境准备"></a>2.JDK环境准备</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">此处就忽略不说了，kafka既然会安装，也是依赖JDK环境的。版本没要求，但是最好是1.7以上。</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$JAVA_HOME</span></span><br><span class="line">测试一下JDK环境是否安装成功</span><br></pre></td></tr></table></figure><h5 id="3-开始安装kafka-eagle"><a href="#3-开始安装kafka-eagle" class="headerlink" title="3.开始安装kafka-eagle"></a>3.开始安装kafka-eagle</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">1.下载安装包</span><br><span class="line">软件安装目录建议按照自己的规范来，以后好找</span><br><span class="line"><span class="built_in">cd</span> /tmp</span><br><span class="line">wget https://github.com/smartloli/kafka-eagle-bin/archive/v1.2.2.tar.gz</span><br><span class="line">tar zxf v1.2.2.tar.gz</span><br><span class="line"><span class="built_in">cd</span> kafka-eagle-bin-1.2.2</span><br><span class="line">tar zxf kafka-eagle-web-1.2.2-bin.tar.gz -C /data/app/</span><br><span class="line"><span class="built_in">cd</span> /data/app</span><br><span class="line">mv kafka-eagle-web-1.2.2 kafka-eagle</span><br><span class="line"></span><br><span class="line">2.环境配置</span><br><span class="line">vi /etc/profile</span><br><span class="line"><span class="built_in">export</span> KE_HOME=/data/app/kafka-eagle</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$KE_HOME</span>/bin</span><br><span class="line">ps:此处的KE_HOME按照自己实际安装的目录来，我安装在/data/app/kafka-eagle下面</span><br><span class="line">如果你是安装的其他目录，别忘了修改。</span><br><span class="line"></span><br><span class="line">3.配置修改</span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;KE_HOME&#125;</span>/conf</span><br><span class="line">vi system-config.properties</span><br><span class="line"></span><br><span class="line"><span class="comment"># multi zookeeper&amp;kafka cluster list -- The client connection address of the Zookeeper cluster is set here</span></span><br><span class="line"><span class="comment">#如果只有一个集群的话，就写一个cluster1就行了</span></span><br><span class="line">kafka.eagle.zk.cluster.alias=cluster1,cluster2   </span><br><span class="line"><span class="comment">#这里填上刚才上准备工作中的zookeeper.connect地址</span></span><br><span class="line">cluster1.zk.list=192.168.18.11:2181,192.168.18.12:2181,192.168.18.13:2181</span><br><span class="line"><span class="comment">#如果多个集群，继续写，如果没有注释掉</span></span><br><span class="line">cluster2.zk.list=192.168.18.21:2181,192.168.18.22:2181,192.168.18.23:2181/kafka </span><br><span class="line"></span><br><span class="line"><span class="comment"># zk limit -- Zookeeper cluster allows the number of clients to connect to</span></span><br><span class="line">kafka.zk.limit.size=25</span><br><span class="line"></span><br><span class="line"><span class="comment"># kafka eagel webui port -- WebConsole port access address</span></span><br><span class="line">kafka.eagle.webui.port=8048     <span class="comment">###web界面地址端口</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kafka offset storage -- Offset stored in a Kafka cluster, if stored in the zookeeper, you can not use this option</span></span><br><span class="line">kafka.eagle.offset.storage=kafka</span><br><span class="line"></span><br><span class="line"><span class="comment"># delete kafka topic token -- Set to delete the topic token, so that administrators can have the right to delete</span></span><br><span class="line">kafka.eagle.topic.token=keadmin</span><br><span class="line"></span><br><span class="line"><span class="comment"># kafka sasl authenticate, current support SASL_PLAINTEXT</span></span><br><span class="line"><span class="comment">#如果kafka开启了sasl认证，需要在这个地方配置sasl认证文件</span></span><br><span class="line">kafka.eagle.sasl.enable=<span class="literal">false</span></span><br><span class="line">kafka.eagle.sasl.protocol=SASL_PLAINTEXT</span><br><span class="line">kafka.eagle.sasl.mechanism=PLAIN</span><br><span class="line">kafka.eagle.sasl.client=/data/kafka-eagle/conf/kafka_client_jaas.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">#下面两项是配置数据库的，默认使用sqlite，如果量大，建议使用mysql，这里我使用的是sqlit</span></span><br><span class="line"><span class="comment">#如果想使用mysql建议在文末查看官方文档</span></span><br><span class="line"><span class="comment"># Default use sqlite to store data</span></span><br><span class="line">kafka.eagle.driver=org.sqlite.JDBC</span><br><span class="line"><span class="comment"># It is important to note that the '/hadoop/kafka-eagle/db' path must exist.</span></span><br><span class="line">kafka.eagle.url=jdbc:sqlite:/data/app/kafka-eagle/db/ke.db   <span class="comment">#这个地址，按照安装目录进行配置</span></span><br><span class="line">kafka.eagle.username=root</span><br><span class="line">kafka.eagle.password=smartloli</span><br><span class="line"></span><br><span class="line"><span class="comment"># &lt;Optional&gt; set mysql address</span></span><br><span class="line"><span class="comment">#kafka.eagle.driver=com.mysql.jdbc.Driver</span></span><br><span class="line"><span class="comment">#kafka.eagle.url=jdbc:mysql://127.0.0.1:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span></span><br><span class="line"><span class="comment">#kafka.eagle.username=root</span></span><br><span class="line"><span class="comment">#kafka.eagle.password=smartloli</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">如果开启了sasl认证，需要自己去修改kafka-eagle目录下的conf/kafka_client_jaas.conf</span><br><span class="line">此处不多说</span><br></pre></td></tr></table></figure><h5 id="4-启动kafka-eagle"><a href="#4-启动kafka-eagle" class="headerlink" title="4.启动kafka-eagle"></a>4.启动kafka-eagle</h5><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">cd $&#123;KE_HOME&#125;/bin</span><br><span class="line">chmod +x ke.sh</span><br><span class="line">./ke.sh start</span><br><span class="line">查看日志是否出问题</span><br><span class="line">tailf ../<span class="built_in">log</span>/<span class="built_in">log</span>.<span class="built_in">log</span></span><br><span class="line">如果没问题，则直接登录</span><br><span class="line">http:<span class="comment">//host:8048/ke</span></span><br><span class="line">默认用户名:admin</span><br><span class="line">默认密码:<span class="number">12345</span></span><br><span class="line">如果进入到一下界面，就说明你安装成功了！</span><br></pre></td></tr></table></figure><p><img src="https://yerias.github.io/kafka/eagle.jpg" alt="eagle"></p><h5 id="5-问题汇总"><a href="#5-问题汇总" class="headerlink" title="5.问题汇总"></a>5.问题汇总</h5><ol><li><p>ZKPoolUtils.localhost-startStop-1 - ERROR - Unable to connect to zookeeper server within timeout: 100000</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">这个是网络问题，在kafka-eagle服务器上自己测试一下能否能telnet通配置的zk地址。</span><br><span class="line">cat system-config.properties|grep cluster1.zk.<span class="built_in">list</span></span><br><span class="line">测试这个配置的端口</span><br></pre></td></tr></table></figure></li><li><p>ERROR - Get kafka consumer has error,msg is No resolvable bootstrap urls given in bootstrap.servers</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">这个问题是配置的zk地址有问题，看看kafka配置的zk地址</span><br><span class="line">跟自己在eagle上配置的地址是否相同，有没有少目录或者端口配置。</span><br><span class="line">可以看看文章开头，kafka+zookeeper准备-了解kafka在zookeeper配置</span><br></pre></td></tr></table></figure></li><li><p>zookeeper state changed (AuthFailed)</p><figure class="highlight css"><table><tr><td class="code"><pre><span class="line">这个问题有两种情况</span><br><span class="line">1.认证的问题，确认你配置的认证文件是否正确</span><br><span class="line">2<span class="selector-class">.zk</span>地址问题，看看<span class="selector-tag">kafka</span>配置的<span class="selector-tag">zk</span>地址，跟自己在<span class="selector-tag">eagle</span>上配置的地址是否相同</span><br><span class="line">可以看看文章开头，<span class="selector-tag">kafka</span>+<span class="selector-tag">zookeeper</span>准备<span class="selector-tag">-</span>了解<span class="selector-tag">kafka</span>在<span class="selector-tag">zookeeper</span>配置</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-kafka-eagle使用"><a href="#6-kafka-eagle使用" class="headerlink" title="6.kafka-eagle使用"></a>6.kafka-eagle使用</h5><p>kafka-eagle官方文档: <a href="https://ke.smartloli.org/2.Install/2.Installing.html" target="_blank" rel="noopener">https://ke.smartloli.org/2.Install/2.Installing.html</a></p><p>kafka-eagle下载地址: <a href="http://download.smartloli.org/" target="_blank" rel="noopener">http://download.smartloli.org/</a></p><p>kafka-eagle git地址: <a href="https://github.com/smartloli/kafka-eagle" target="_blank" rel="noopener">https://github.com/smartloli/kafka-eagle</a></p>]]></content>
      
      
      <categories>
          
          <category> Kakfa </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kakfa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Error: java.io.IOException: Invalid LZO header</title>
      <link href="/2020/04/18/error/6/"/>
      <url>/2020/04/18/error/6/</url>
      
        <content type="html"><![CDATA[<p>在使用Flume传输数据的时候，需要注意几个字段</p><p>我们这里使用的是flume传输到hdfs</p><p>参数：hdfs.fileType 指定的数据传输类型，默认SequenceFile，如果直接传输本文本数据，则会乱码。在传输文本数据的时候它的值要修改为DataStream</p><p>而现在根据我们的错误提示就知道我们使用了Lzo压缩，所以需要把它的值修改为CompressedStream，即可解决问题。</p><table><thead><tr><th>Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td>hdfs.fileType</td><td>SequenceFile</td><td>File format: currently <code>SequenceFile</code>, <code>DataStream</code> or <code>CompressedStream</code> (1)DataStream will not compress output file and please don’t set codeC (2)CompressedStream requires set hdfs.codeC with an available codeC</td></tr></tbody></table><hr><p>还需要注意的一个参数是：hdfs.codeC ，在使用flume时，可以将数据压缩输出，它的值可选为gzip, bzip2, lzo, lzop, snappy</p><p>lzop的后缀是lzo</p><p>lzo的后缀是lzp.default</p><hr><p>还是要熟悉一下flume的文档。。。<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#hdfs-sink" target="_blank" rel="noopener">hdfs</a></p>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM之运行时数据区</title>
      <link href="/2020/04/15/jvm/1/"/>
      <url>/2020/04/15/jvm/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>jvm命令</li><li>jvm的运行时数据区</li><li>jvm会发生哪些ERROR</li><li>从一个class出发理解数据区</li></ol><h2 id="jvm命令"><a href="#jvm命令" class="headerlink" title="jvm命令"></a>jvm命令</h2><h3 id="JVM参数类型"><a href="#JVM参数类型" class="headerlink" title="JVM参数类型"></a>JVM参数类型</h3><ol><li>标准: 稳定的，长期没有变化</li><li>X: 相对变化较少的</li><li>XX: 变化较大，JVM调优重点</li></ol><p>设置参数时，idea指定在VM options里面，命令行直接加在java命令后</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">java -Xss10m -XX:+PrintGCDetails JVMParams</span><br></pre></td></tr></table></figure><h3 id="常见的XX类型的参数"><a href="#常见的XX类型的参数" class="headerlink" title="常见的XX类型的参数"></a>常见的XX类型的参数</h3><ol><li><p>-XX:+PrintGCDetails: 打印GC日志</p></li><li><p>-XX:+PrintFlagsInitial: 打印所有初始的参数信息</p></li><li><p>-XX:+PrintFlagsFinal: 打印所有最终的参数信息</p></li><li><p>-Xms设置堆的最小空间大小。</p></li><li><p>-Xmx设置堆的最大空间大小。</p></li><li><p>-XX:NewSize设置新生代最小空间大小。</p></li><li><p>-XX:MaxNewSize设置新生代最大空间大小。</p></li><li><p>-XX:PermSize设置永久代最小空间大小。</p></li><li><p>-XX:MaxPermSize设置永久代最大空间大小。</p></li><li><p>-Xss设置每个线程的堆栈大小。</p></li><li><p>没有直接设置老年代的参数，但是可以设置堆空间大小和新生代空间大小两个参数来间接控制</p><p> <strong>老年代空间大小=堆空间大小-年轻代大空间大小</strong></p></li></ol><p>例如：</p><p>java -XX:+PrintFlagsFinal -version </p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">uintx MaxHeapSize             := <span class="number">2048917504</span>     &#123;product&#125;</span><br><span class="line">intx MaxInlineLevel            = <span class="number">9</span>              &#123;product&#125;</span><br><span class="line">intx MaxInlineSize       = <span class="number">35</span>             &#123;product&#125;</span><br><span class="line">bool ParGCTrimOverflow      = <span class="keyword">true</span>           &#123;product&#125;</span><br><span class="line">bool ParGCUseLocalOverflow     = <span class="keyword">false</span>          &#123;product&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>上面只显示部分参数，但是能够说明我们需要理解的内容，即 ‘=’ 表示默认值，‘:=’ 表示被修改过的值。同时还有数值类型和布尔类型。</p><h3 id="几个特殊的XX类型参数"><a href="#几个特殊的XX类型参数" class="headerlink" title="几个特殊的XX类型参数"></a>几个特殊的XX类型参数</h3><p>-Xms、-Xmx、-Xss 实际上是XX类型的缩写</p><p>-Xms ==&gt; -XX:InitialHeapSize: 表示为: -Xms10m<br>-Xmx ==&gt; -XX:MaxHeapSize: 表示为: -Xmx10m<br>-Xss ==&gt; -XX:ThreadStackSize</p><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><ol><li><p>查看java进程：jps</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[hadoop<span class="meta">@hadoop</span> ~]$ jps</span><br><span class="line"><span class="number">13612</span> JVMParams</span><br><span class="line"><span class="number">13644</span> Jps</span><br></pre></td></tr></table></figure></li><li><p>查看java进程的参数信息<br>jinfo -flag name pid </p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[hadoop<span class="meta">@hadoop</span> ~]$ jinfo -flag MaxHeapSize <span class="number">13612</span></span><br><span class="line">-XX:MaxHeapSize=<span class="number">2048917504</span></span><br><span class="line">[hadoop<span class="meta">@hadoop</span> ~]$ jinfo -flag InitialHeapSize <span class="number">13612</span></span><br><span class="line">-XX:InitialHeapSize=<span class="number">130023424</span></span><br><span class="line">[hadoop<span class="meta">@hadoop</span> ~]$ jinfo -flag ThreadStackSize <span class="number">13612</span></span><br><span class="line">-XX:ThreadStackSize=<span class="number">1024</span></span><br></pre></td></tr></table></figure><p>怎么理解-XX:MaxHeapSize=2048917504，-XX:InitialHeapSize=130023424 ?</p><p>分析：</p><p>我主机的物理内存为8G，2048917504k = 1.9G，130023424  = 124M</p><p>理论上heap的最大值为物理内存的1/4，最小值为物理内存的1/64</p><p>但是一般情况下，我们会把MaxHeapSize和InitialHeapSize设置相同的值，防止内存抖动</p></li><li><p>查看java进程的默认和设置的参数</p><p>jinfo -flags pid </p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[hadoop<span class="meta">@hadoop</span> ~]$ jinfo -flags  <span class="number">13612</span></span><br><span class="line">Non-<span class="keyword">default</span> VM flags: -XX:CICompilerCount=<span class="number">2</span> ...</span><br><span class="line">Command line:  -Xss10m -XX:+PrintGCDetails ...</span><br></pre></td></tr></table></figure></li></ol><h2 id="jvm的运行时数据区"><a href="#jvm的运行时数据区" class="headerlink" title="jvm的运行时数据区"></a>jvm的运行时数据区</h2><p><img src="https://yerias.github.io/java_img/%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA.jpg" alt="运行时数据区"></p><h3 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h3><p>程序计数器是每个线程私有的</p><p>程序计数器是一块较小的内存空间，它可以看做是当前线程的行号指示器，这在多线程环境下非常有用。使得线程切换后能够恢复到正确的执行位置。</p><p>在java虚拟机规范中，这是唯一一个没有规定任何OutOfMemoryError的地方</p><h3 id="Java虚拟机栈"><a href="#Java虚拟机栈" class="headerlink" title="Java虚拟机栈"></a>Java虚拟机栈</h3><p>java虚拟机栈也是每个线程私有的，它的生命周期和线程相同</p><p>虚拟机栈描述的是java方法执行的线程内存模型: 每个方法被执行的时候，java虚拟机栈都会同步创建一个Frame(栈帧)用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法被调用直到执行完毕的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。</p><p>最常用的就是局部变量表，局部变量表存放了编译期可知的各种java虚拟机的基本数据类型(boolean、byte、char、sort、int、long、float、double)、对象引用(reference类型)和returnAddress类型，这部分在后面讲有详细的解释。</p><p>在java虚拟机规范中，这个区域可能存在两种异常，如果线程请求的栈深度大于虚拟机所允许的深度，会抛出StackOverflowError异常，常见的有循环调用方法名；如果java虚拟机栈容量可以动态扩展，当栈无法申请到足够的内存时就会抛出OutOfMemoryError异常。</p><h3 id="本地方法栈"><a href="#本地方法栈" class="headerlink" title="本地方法栈"></a>本地方法栈</h3><p>本地方法栈是线程私有的</p><p>本地方法栈与虚拟机栈所发挥的作用是非常类似的，其区别只是虚拟机栈为虚拟机执行java方法服务，而本地方法栈则为本地(native)方法服务</p><p>常见的本地方法有getClass、hashCode、clone、notify、notifyAll、wait、sleep等</p><p>与java虚拟机栈一样，本地方法栈也会在栈深度溢出的时候或者栈扩展失败的时候抛出StackOutflowError和OutOfMemoryError异常。</p><h3 id="java堆"><a href="#java堆" class="headerlink" title="java堆"></a>java堆</h3><p>java堆是所有线程共享的，是虚拟机所管理的内存中最大的一块，在虚拟机启动时创建。</p><p>此内存区域的唯一目的是存放对象实例，对象实例包括对象和数组。</p><p>如果在java堆中没有内存完成实例分配，并且堆也无法扩展，java虚拟机将会抛出OutOfMemoryError异常。</p><h3 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h3><p>方法区是所有线程共享的</p><p>它用于存储已被虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据，简单点说就是Class。</p><p>方法区只是java虚拟机的规范，它属于堆的一个逻辑部分，为了和堆区分开，也叫非堆。在jdk8之前，方法区的具体实现叫做永久代，</p><ol><li>由于类及方法的信息大小很难确定，所以内存设置小了会发生OOM，设置大了又浪费</li><li>GC复杂度高，回收效率低</li><li>合并 HotSpot 与 JRockit </li></ol><p>所以在jdk8完全用元空间替换了永久代，元空间直接使用的系统内存。</p><p>在java虚拟机规范中，如果方法区无法满足内存分配需求时，会抛出OouOfMemoryError</p><h3 id="直接内存"><a href="#直接内存" class="headerlink" title="直接内存"></a>直接内存</h3><p>直接内存不是java虚拟机规范中定义的内存区域，但是这部分也会被频繁使用，所以也可能会抛出OutOfMemoryError。</p><h2 id="jvm会发生哪些ERROR"><a href="#jvm会发生哪些ERROR" class="headerlink" title="jvm会发生哪些ERROR"></a>jvm会发生哪些ERROR</h2><h3 id="java堆内存OOM异常测试"><a href="#java堆内存OOM异常测试" class="headerlink" title="java堆内存OOM异常测试"></a>java堆内存OOM异常测试</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">-Xmx10m -Xms10m -XX:+HeapDumpOnOutOfMemoryError</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HeapOOM</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">OOMObject</span></span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        ArrayList&lt;OOMObject&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">            list.add(<span class="keyword">new</span> OOMObject());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">Dumping heap to java_pid184.hprof ...</span><br><span class="line">Heap dump file created [<span class="number">10209413</span> bytes in <span class="number">0.080</span> secs]</span><br></pre></td></tr></table></figure><h3 id="java虚拟机栈和本地方法栈SOF测试"><a href="#java虚拟机栈和本地方法栈SOF测试" class="headerlink" title="java虚拟机栈和本地方法栈SOF测试"></a>java虚拟机栈和本地方法栈SOF测试</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">-Xss2M</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JavaVMStackSOF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> stackLength=<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">stackLeak</span><span class="params">()</span></span>&#123;</span><br><span class="line">        stackLength++;</span><br><span class="line">        stackLeak();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        JavaVMStackSOF stackSOF = <span class="keyword">new</span> JavaVMStackSOF();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            stackSOF.stackLeak();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">            System.out.println(<span class="string">"栈深度："</span>+stackSOF.stackLength);</span><br><span class="line">            <span class="keyword">throw</span> e;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">栈深度：<span class="number">41075</span></span><br><span class="line">Exception in thread <span class="string">"main"</span> java.lang.StackOverflowError</span><br></pre></td></tr></table></figure><h3 id="java虚拟机栈和本地方法栈OOM测试"><a href="#java虚拟机栈和本地方法栈OOM测试" class="headerlink" title="java虚拟机栈和本地方法栈OOM测试"></a>java虚拟机栈和本地方法栈OOM测试</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">-Xss4m</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JavaVMStackOOM</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">dontStop</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">stackleakByThread</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">            <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">                    dontStop();</span><br><span class="line">            &#125;).start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        JavaVMStackOOM oom = <span class="keyword">new</span> JavaVMStackOOM();</span><br><span class="line">        oom.stackleakByThread();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果:</p><p>死机</p><h3 id="方法区和运行时常量池OOM"><a href="#方法区和运行时常量池OOM" class="headerlink" title="方法区和运行时常量池OOM"></a>方法区和运行时常量池OOM</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">-XX:PerSize=<span class="number">6</span>m -XX:MaxPermSize=<span class="number">6</span>M</span><br><span class="line"><span class="keyword">import</span> java.util.HashSet;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RuntimeConstantPoolOOM</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        HashSet&lt;String&gt; set = <span class="keyword">new</span> HashSet&lt;String&gt;();</span><br><span class="line">        <span class="keyword">short</span> i = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">            set.add(String.valueOf(i++).intern());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果:</p><p>jdk8没有测试出来</p><h2 id="从一个class出发理解数据区"><a href="#从一个class出发理解数据区" class="headerlink" title="从一个class出发理解数据区"></a>从一个class出发理解数据区</h2><p><img src="https://yerias.github.io/java_img/class%E7%B1%BB%E5%AF%B9%E6%AF%94%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA.jpg" alt="class类对比运行时数据区"></p><p>如图所示，很容易的理解各区分别保存java代码中的哪些部分</p><p>堆区: 保存的People对象</p><p>栈区: 保存的栈帧，栈帧中保存了引用name和age和引用people</p><p>方法区: 保存的People.class相关的，包括类型信息、常量、静态变量sss</p><p>运行时常量池: 保存name和age字符串内容</p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban配置Plugin实现Spark作业提交(非Shell)</title>
      <link href="/2020/04/10/azkaban/2/"/>
      <url>/2020/04/10/azkaban/2/</url>
      
        <content type="html"><![CDATA[<p>第一步，我们要打开azkaban的<a href="https://github.com/azkaban/azkaban/tree/master/az-hadoop-jobtype-plugin/src/jobtypes" target="_blank" rel="noopener">官网</a>，配置一些文件和参数，如图所示</p><p><img src="https://yerias.github.io/azkaban_img/az%E9%85%8D%E7%BD%AESpark%E6%8F%90%E4%BA%A4.jpg" alt="az配置Spark提交"></p><p>将<code>spark</code>、<code>common.properties</code>、<code>commonprivate.properties</code>拷贝到服务器中对应的目录，最终的文件展示如下</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">[hadoop<span class="meta">@hadoop</span> jobtypes]$ tree</span><br><span class="line">.</span><br><span class="line">├── commonprivate.properties</span><br><span class="line">├── common.properties</span><br><span class="line">└── spark</span><br><span class="line">    ├── plugin.properties</span><br><span class="line">    └── <span class="keyword">private</span>.properties</span><br></pre></td></tr></table></figure><ol><li><p>配置commonprivate.properties中hadoop.home和spark.home指定的家目录</p></li><li><p>配置common.properties中hadoop.home和spark.home指定的家目录</p></li><li><p>修改private.properties文件中的参数(临时方案，可行)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">jobtype.classpath=$&#123;hadoop.classpath&#125;:$&#123;spark.home&#125;/conf:$&#123;spark.home&#125;/lib<span class="comment">/*</span></span><br><span class="line"><span class="comment">===&gt;</span></span><br><span class="line"><span class="comment">jobtype.classpath=hadoop.classpath:$&#123;spark.home&#125;/conf:$&#123;spark.home&#125;/lib/*</span></span><br></pre></td></tr></table></figure><p>这么做的原因是我们以上的文件中没有配置hadoop.classpath，官方也没有说明hadoop.classpath应该配置什么参数，目前修改掉引用不影响程序的使用。</p></li></ol><p>第二步，在conf/azkaban.properties文件下增加一个配置，主机名:端口号(随意修改)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">azkaban.webserver.url=https:<span class="comment">//hadoop:8666</span></span><br></pre></td></tr></table></figure><p>第三步，提交作业，所配的参数需要参考<a href="https://github.com/azkaban/azkaban/blob/master/az-hadoop-jobtype-plugin/src/main/java/azkaban/jobtype/SparkJobArg.java" target="_blank" rel="noopener">官网</a></p><p>测试案例:</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">config:</span><br><span class="line">    user.to.proxy: hadoop</span><br><span class="line">nodes:</span><br><span class="line">  - name: sparkwc</span><br><span class="line">    <span class="class"><span class="keyword">type</span></span>: spark</span><br><span class="line">    config:</span><br><span class="line">      <span class="class"><span class="keyword">class</span></span>: com.data.spark.wc.<span class="type">SparkWC</span></span><br><span class="line">      master: yarn</span><br><span class="line">      deploy-mode: client</span><br><span class="line">      executor-memory: <span class="number">512</span>M</span><br><span class="line">      driver-memory: <span class="number">512</span>M</span><br><span class="line">      conf.spark.testing.memory: <span class="number">471859200</span></span><br><span class="line">      execution-jar: tunan-spark-utils<span class="number">-1.0</span>.jar</span><br><span class="line">      jars: tunan-spark-core<span class="number">-1.0</span>.jar</span><br><span class="line">      params: hdfs:<span class="comment">//hadoop:9000/input/wc.txt hdfs://hadoop:9000/out</span></span><br></pre></td></tr></table></figure><p>注意配置文件中的 jar 没有写路径，这么提交<strong>需要把 jar 包和配置文件一起打成zip包</strong>，提交到AZ的Web界面</p><p>第四步，查看结果</p><p><em>20200413更新：</em>  数据下标越界问题：hadoop下的/share/hadoop/common/lib/paranamer-2.3.jar过时，使用–jars传spark下的/jars/paranamer-2.8.jar    </p><p><em>20200423更新：</em> 所有参数都可以使用conf传递，</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">execution_jar  对应的是appjar</span><br><span class="line">jars 对应的是 依赖jar</span><br><span class="line">params: $&#123;execution_jar&#125; 参数<span class="number">1</span> 参数<span class="number">2</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark疯狂踩坑系列</title>
      <link href="/2020/03/30/error/5/"/>
      <url>/2020/03/30/error/5/</url>
      
        <content type="html"><![CDATA[<p>如果WEB UI界面或者程序日志里面看不到错误，使用以下方式查看日志</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn logs -applicationId application_1585536649766_xxxx</span><br></pre></td></tr></table></figure><p>错误1</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Error: Could not find or load main class org.apache.spark.deploy.yarn.ApplicationMaster</span><br></pre></td></tr></table></figure><p>解决办法：</p><p>检查spark-defaults.conf中的配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark.yarn.jars=hdfs://hadoop:9000/spark-yarn/jars/*.jar</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark.yarn.archive  hdfs://hadoop000:8020/tmp/spark-archive/spark2.4.5.zip</span><br></pre></td></tr></table></figure><p>以上两种配置方式不可以错乱</p><p>错误2</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">java.lang.NoClassDefFoundError: org/lionsoul/ip2region/DbConfig</span><br></pre></td></tr></table></figure><p>解决办法</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--jars /home/hadoop/lib/ip2region-1.7.2.jar</span><br></pre></td></tr></table></figure><p>错误3</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">java.lang.NullPointerException</span><br><span class="line">at com.tunan.spark.utils.IpParseUtil.IpParse(IpParseUtil.java:19)</span><br></pre></td></tr></table></figure><p>解决办法</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--files /home/hadoop/lib/ip2region.db</span><br></pre></td></tr></table></figure><hr><p>代码中拿出文件有两种方式</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String dbPath = GetIPRegion.class.getResource("/ip2region.db").getPath();</span><br><span class="line">String dbPath = SparkFiles.get(<span class="string">"/ip2region.db"</span>);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MR编程时，Driver传递的参数Mapper显示为NULL</title>
      <link href="/2020/03/26/error/4/"/>
      <url>/2020/03/26/error/4/</url>
      
        <content type="html"><![CDATA[<p>在进行MR编程时，除了需要拿到HDFS上面的数据，有时候还需要Driver和Mapper或者Reducer之间进行参数传递</p><p>先看看我碰到的问题</p><p><img src="https://yerias.github.io/java_img/14.png" alt=""></p><p><img src="https://yerias.github.io/java_img/15.png" alt=""></p><p>在Driver中配置向Conf中配置了参数，在Mapper中从Context中拿出来的却是null值，<strong>问题出现在Job.getInstance() 中没有把Conf传递进去。</strong></p><p>所以以后要注意，在创建Job的时候要把Conf也放进去。</p><p>还需要注意的是Conf中的设值必须在Job.getInstance()上面完成</p>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM之内存模型</title>
      <link href="/2020/03/26/jvm/4/"/>
      <url>/2020/03/26/jvm/4/</url>
      
        <content type="html"><![CDATA[<p>Java内存模型其实就是围绕着在并发过程中如果解决原子性、有序性和可见性的通信规则</p><p><img src="https://yerias.github.io/java_img/1.jpg" alt="线程、主内存、工作内存三者之间的关系"></p><h2 id="主内存与工作内存"><a href="#主内存与工作内存" class="headerlink" title="主内存与工作内存"></a>主内存与工作内存</h2><p>Java内存模型的主要目的就是定义程序中各种变量的访问规则，即关注在虚拟机中把变量存储到内存和从内存中取出变量这样的底层细节。</p><p>此处的变量指的是包括了实例字段，静态字段和构成数组对象的元素。但不包括局部变量和方法参数，因为后者是线程私有的。</p><ol><li><p>主内存和工作内存的关系？</p><p>Java内存模型规定了所有的变量都存储在主内存中，每条线程还有自己的工作内存，线程的工作内存保存了被该线程使用的变量的主内存副本。线程对变量的所有操作(读取、赋值等)都必须在工作内存中进行，而不能直接读写主内存中的数据。不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递需要通过主内存来完成。</p></li><li><p>主内存和工作内存如何交互？</p><p>主内存和工作内存如何交互？即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步回主内存。Java内存模型定义了8种操作来完成。分别是</p><ul><li>lock(锁定)</li><li>unlock(解锁)</li><li>read(读取)</li><li>load(载入)</li><li>use(使用)</li><li>assign(赋值)</li><li>store(存储)</li><li>write(写入)</li></ul><p>他们每一种操作都是原子的、不可再分的(double和long类型，32位主机会拆分成两次执行)</p><p>他们规定了八种执行规则，但这不是我们关心的重点，作为开发者更需要了解的是先行发生原则–用来确定一个操作在并发环境下是否安全</p></li></ol><h2 id="Volatile变量的特殊规则"><a href="#Volatile变量的特殊规则" class="headerlink" title="Volatile变量的特殊规则"></a>Volatile变量的特殊规则</h2><p>Volatile是Java虚拟机提供的最轻量级的<strong>同步机制</strong></p><p>为什么说他是最轻量级的同步机制？因为它只能保证可见性、有序性，而不能保证原子性。虽然应用场景有限，但是快，能保证其他线程的立即可见性。</p><p>当一个变量被定义成volatile之后，它具备两项特性：</p><ol><li><p><strong>保证此变量对所有线程的可见性，</strong>这里的可见性是指一条线程修改了这个变量的值，新值对于其他线程来说是立即得知的。由于volatile不能保证原子性(比如运算是分两步来做的)，只能在以下两条规则的场景中进行运算。</p><ul><li>运算结果不依赖变量的当前值。</li><li>变量不需要与其他的状态变量共同参与不变约束。</li></ul><p>简单点说，就是自己玩自己的，典型的引用场景就是作位状态标志的修饰符。</p></li><li><p><strong>通过加入内存屏障禁止指令重排序</strong>，指令重排序优化是编辑器做的一种代码执行顺序的优化，只关注结果，不关注过程，但是这么做可能让程序逻辑混乱，比如本来应该在后面执行的代码，跑到前面来执行了，这时候就要使用volatile禁止指令重排序，从而保证代码的执行顺序和程序的顺序相同。</p></li></ol><h2 id="可见性、有序性、原子性"><a href="#可见性、有序性、原子性" class="headerlink" title="可见性、有序性、原子性"></a>可见性、有序性、原子性</h2><ol><li><p>原子性</p><p>Java内存模型直接保证的原子性变量操作包括read、load、assign、use、store、write这六个，我们大致可以认为，对基本数据类型的访问、读写都是具备原子性的。</p><p>对于其他场景的原子性保证，Java内存模型提供了lock和unlock操作满足这种需求，反应到代码中就是synchronized关键字，因此<strong>synchronized是具备原子性的</strong>。</p></li><li><p>可见性</p><p>可见性就是指当一个线程修改了共享变量的值时，其他线程能够立即得知这个修改。</p><p><strong>volatile</strong>的特殊规则保证了新值能够立即从工作内存同步到主内存，以及每次使用前从主内存刷新。</p><p><strong>synchronized</strong>是通过”对一个变量执行unlock操作之前，必须先把此变量同步回主内存中”实现的。</p><p><strong>final</strong>关键字修饰的字段在构造器中初始化完成，并且构造器没有把”this”的引用逃逸出去，那么在其他线程中就能看见final修饰字段的值。</p></li><li><p>有序性</p><p>java程序中天然的有序性可以总结为一句话: 如果在本线程内观察，所有的操作都是有序的；如果在一个线程中观察另一个线程，所有的操作都是无序的。前半句是指 “线程内变量为串行语义”，后半句是指 “指令重排序” 现象和 “工作内存和主内存同步延迟” 现象。</p><p><strong>volatile</strong>关键字本身就包含了禁止指令重排序的语义。</p><p><strong>synchronized</strong>是由 “一个变量在同一时刻只允许一条线程对其进行lock操作” 这条规则获得的。 </p></li></ol><h2 id="先行发生原则"><a href="#先行发生原则" class="headerlink" title="先行发生原则"></a>先行发生原则</h2><p>先行发生是Java内存模型定义的两项操作之间的顺序关系，如果说操作A先行发生于操作B，其实就是说在发生操作B之前，操作A产生的影响被操作B观察到，”影响” 包括修改了内存中共享变量的值、发生了消息、调用了方法等。</p><p>Java内存模型中 “天然的” 先行发生关系包括以下八种，如果两个操作之间的关系不在此列，并且无法从下列规则推导出来，则他们就没有<strong>顺序型保障</strong>，虚拟机可以对它们进行任意的<strong>重排序</strong>。 </p><ol><li>程序次序规则</li><li>管程锁定规则</li><li>volatile变量规则</li><li>线程启动规则</li><li>线程终止规则</li><li>线程中断规则</li><li>对象终结规则</li><li>传递性</li></ol><ul><li>“时间上的先后顺序” 与 “先行发生” 之间有什么不同?</li></ul><p>时间先后顺序与先行发生原则之间基本没有因果关系，所以我们衡量并发安全问题的时候不要受时间顺序的干扰，一切必须以先行发生原则为准。</p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>执行Hive SQL/MR 报错：Current usage: 77.8mb of 512.0mb physical memory used; 1.1gb of 1.0gb virtual memory used. Killing container.</title>
      <link href="/2020/03/25/error/3/"/>
      <url>/2020/03/25/error/3/</url>
      
        <content type="html"><![CDATA[<p>从错误消息中，可以看到使用的虚拟内存超过了当前1.0gb的限制。这可以通过两种方式解决：</p><p><strong>禁用虚拟内存限制检查</strong></p><p>YARN只会忽略该限制；为此，请将其添加到您的<code>yarn-site.xml</code>：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether virtual memory limits will be enforced for containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>此设置的默认值为<code>true</code>。</p><p><strong>增加虚拟内存与物理内存的比率</strong></p><p>在<code>yarn-site.xml</code>更改中，此值将高于当前设置</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Ratio between virtual memory to physical memory when setting memory limits for containers. Container allocations are expressed in terms of physical memory, and virtual memory usage is allowed to exceed this allocation by this ratio.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>默认是 <code>2.1</code></p><p>还可以增加分配给容器的物理内存量。</p><p>注意：更改配置后不要忘记重新启动yarn。</p><hr><p>如果不能修改集群配置，我们可以参考这么做：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">-Dmapreduce.map.memory.mb=<span class="number">4096</span></span><br></pre></td></tr></table></figure><hr><p>在运行MR的作业中我们需要关心一下几个参数：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">yarn.nodemanager.resource.memory-mb</span><br><span class="line">yarn.scheduler.minimum-allocation-mb</span><br><span class="line">yarn.scheduler.maximum-allocation-mb</span><br><span class="line">yarn.nodemanager.vmem-check-enabled</span><br><span class="line">yarn.nodemanager.vmem-pmem-ratio</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapJoin，文件在HDFS上Idea报错：File does not exist: /xxx/yyy.txt#yyy.txt</title>
      <link href="/2020/03/25/error/2/"/>
      <url>/2020/03/25/error/2/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.io.FileNotFoundException: File does not exist: /data/dept.txt#dept.txt</span><br></pre></td></tr></table></figure><p>先去HDFS上确定文件是否存在，文件不存在，put文件上去，再次运行</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://192.168.91.10:9000/data/emp.txt</span><br></pre></td></tr></table></figure><p>有是Path找不到，再去HDFS上检查这个文件是否存在，文件不存在，再次put上去，然后运行</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INFO DFSClient: Could not obtain BP-1292531802-192.168.181.10-1583457649867:blk_1073746058_5236 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.91.10:50010,DS-f8088840-5065-4320-8b51-4563ccec125a,DISK] Dead nodes:  DatanodeInfoWithStorage[192.168.91.10:50010,DS-f8088840-5065-4320-8b51-4563ccec125a,DISK]. Will get new block locations from namenode and retry...</span><br><span class="line">WARN DFSClient: DFS chooseDataNode: got # 2 IOException, will wait for 3263.5374223592803 msec.</span><br><span class="line">WARN BlockReaderFactory: I/O error constructing remote block reader.</span><br><span class="line">java.net.ConnectException: Connection timed out: no further information</span><br></pre></td></tr></table></figure><p>继续报错，连接不到DataNode的块，去命令行输入jps发现进程没挂，然后我怀疑是因为我的host没有配置hadoop的ip映射关系，于是配上后，继续运行</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java.lang.Exception: java.io.FileNotFoundException: dept.txt (系统找不到指定的文件。)</span><br><span class="line">Caused by: java.io.FileNotFoundException: dept.txt (系统找不到指定的文件。)</span><br></pre></td></tr></table></figure><p>不出所望的再次报错，这次的日志多，上下文分析一下</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">WARN FileUtil: Command &apos;G:\hadoop-2.7.2\bin\winutils.exe symlink E:\Java\hadoop\hadoop-client\dept.txt \tmp\hadoop-Tunan\mapred\local\1585106809398\dept.txt&apos; failed 1 with: CreateSymbolicLink error (1314): ???????????</span><br><span class="line">WARN LocalDistributedCacheManager: Failed to create symlink: \tmp\hadoop-Tunan\mapred\local\1585106809398\dept.txt &lt;- E:\Java\hadoop\hadoop-client/dept.txt</span><br></pre></td></tr></table></figure><p>由于我们给hdfs上面文件的路径创建了一个链接，也就是symlink，我们发现这个两个警告是symlink创建失败，于是使用管理员的身份启动Idea，解决</p><hr><p>需要注意是除了管理员身份启动Idea，还需要在在程序的main方法下开启symlink的功能，才能在<code>new FileInputStream(new File(&quot;dept.txt&quot;))</code>中使用</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">FileSystem.enableSymlinks();</span><br></pre></td></tr></table></figure><p>还需要注意的是本地操作HDFS的一些配置</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"hadoop"</span>);</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://192.168.91.10:9000"</span>);</span><br><span class="line">conf.set(<span class="string">"dfs.client.use.datanode.hostname"</span>, <span class="string">"true"</span>);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala之闭包&amp;柯里化</title>
      <link href="/2020/03/23/scala/4/"/>
      <url>/2020/03/23/scala/4/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>闭包</li><li>方法与函数的区别</li><li>柯里化</li></ol><h2 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h2><p>说到柯里化必先说起闭包，我们先不关心闭包和柯里化是什么，而是看一个transformation</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> init:<span class="type">Int</span> = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> i = list.foldLeft[<span class="type">Int</span>](init)((x,y) =&gt; &#123;</span><br><span class="line">    println(<span class="string">s"init = <span class="subst">$init</span> | x = <span class="subst">$x</span> | y = <span class="subst">$y</span>"</span> )</span><br><span class="line">    x+y</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">println(i)</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">init = <span class="number">10</span> | x = <span class="number">10</span> | y = <span class="number">1</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">11</span> | y = <span class="number">2</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">13</span> | y = <span class="number">3</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">16</span> | y = <span class="number">4</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">20</span> | y = <span class="number">5</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">25</span> | y = <span class="number">6</span></span><br><span class="line"><span class="number">31</span></span><br></pre></td></tr></table></figure><p>从结果来看 <code>foldLeft</code> 需要三个参数，<code>init</code>初始值，变量x，变量y，然后将他们累加(不考虑其他运算规则)</p><p>现在我们来看看闭包的解释：</p><ol><li><p>闭包是一个函数，返回值依赖于声明在函数外部的一个或多个变量。</p></li><li><p>闭包通常来讲可以简单的认为是可以访问一个函数里面局部变量的另外一个函数。</p></li></ol><p><code>val i = list.foldLeft[Int](init)((x,y))</code> 是一个闭包，返回值依赖声明在函数外的<code>init</code></p><p>如果我们把<code>foldLeft[Int](init)((x,y))</code> 拆分成两个函数的话，一个是有明确参数引用的 <code>foldLeft[Int](init)</code>， 另外一个是匿名函数<code>((x,y))</code>，如果我们观察仔细的话，会发现上面的代码中，第一次输出的结果 <code>init = x</code>  我们就可以简单的认为匿名函数<code>((x,y))</code>访问了<code>flodLeft</code>中的局部变量<code>init</code> ,事实上，<code>x</code>第一次的值就是拿的<code>init</code> 。</p><p>我们现在可以总结闭包就是在函数外面声明了一个变量，在函数中引用了这个变量，就称之为闭包，其他函数可以依赖闭包(函数)中的这个变量。由于闭包是把外部变量包了进来，所以这个变量的生命周期和闭包的生命周期一致。</p><p>最后在看一个案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> factor = <span class="number">3</span></span><br><span class="line"><span class="keyword">val</span> multiplier = (i:<span class="type">Int</span>) =&gt; i * factor</span><br><span class="line">println(multiplier(<span class="number">3</span>))</span><br></pre></td></tr></table></figure><p>没错，<strong>函数的返回值依赖于声明在函数外部的一个或多个变量就是闭包。</strong></p><h2 id="方法与函数的区别"><a href="#方法与函数的区别" class="headerlink" title="方法与函数的区别"></a>方法与函数的区别</h2><p>还需要讲一下方法与函数区别，因为柯里化指的是将原来接受两个参数的方法变成新的接受一个参数的函数的过程。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">method</span> </span>= (x:<span class="type">Int</span>,y:<span class="type">Int</span>) =&gt; x+y</span><br><span class="line">method: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span><span class="comment">//方法</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> func = (x:<span class="type">Int</span>,y:<span class="type">Int</span>) =&gt; x+y</span><br><span class="line">func: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1307</span>/<span class="number">1309775952</span>@<span class="number">32</span>a1aabf<span class="comment">//函数</span></span><br></pre></td></tr></table></figure><ol><li>首先应该要知道=号右边的内容 <code>(x: Int, y: Int) =&gt; x + y</code>是一个函数体。</li><li>方法只能用def修饰，函数可以用def修饰，也可以用val修饰。</li><li>当函数用def来接收之后，不再显示为function，转换为方法。</li><li>方法可以省略参数，函数不可以。</li><li>函数可以作为方法的参数。</li></ol><h2 id="柯里化"><a href="#柯里化" class="headerlink" title="柯里化"></a>柯里化</h2><p>理解闭包后，我们看一个复杂的闭包案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">addBase</span></span>(x:<span class="type">Int</span>) = (y:<span class="type">Int</span>) =&gt; x+y</span><br><span class="line">addBase: (x: <span class="type">Int</span>)<span class="type">Int</span> =&gt; <span class="type">Int</span></span><br></pre></td></tr></table></figure><p>首先我们看到<code>addBase</code>是一个方法，<code>(y:Int) =&gt; x+y</code>是一个函数体，下面我们试试执行<code>addBase(x:Int)</code></p><p>会返回什么?</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; addBase(<span class="number">3</span>)</span><br><span class="line">res27: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1308</span>/<span class="number">761477414</span>@<span class="number">553</span>f7b1e</span><br></pre></td></tr></table></figure><p>返回了一个函数，现在我们明白了，<code>addBase(x:Int)</code>是一个<strong>方法</strong>，在传入具体的值后，返回了一个具体的<strong>函数</strong></p><p>到现在 “=” 号后面的 <code>(y:Int) =&gt; x+y</code> 这段还没有使用到，但是我们知道 “=” 号后面是一个函数体，给函数中传入y 返回 x+y? 我们试试。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> addThree = addBase(<span class="number">3</span>)<span class="comment">//使用个变量接收函数</span></span><br><span class="line">addThree: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1308</span>/<span class="number">761477414</span>@<span class="number">65e1</span>c98c</span><br><span class="line">scala&gt; addThree(<span class="number">4</span>)<span class="comment">//传入4</span></span><br><span class="line">res29: <span class="type">Int</span> = <span class="number">7</span><span class="comment">//输出7</span></span><br></pre></td></tr></table></figure><p>等等。。。这就是一个闭包啊，x=3是外部传入的给函数<code>addBase</code>的，这时的<code>addBase</code>就是函数，它的内部维护了一个变量x=3，这时另外一个函数<code>addThree</code> 在输出x+y前，访问了<code>addBase</code>中的x=3。</p><p>完全符合闭包的定义规则，<strong>函数的返回值依赖于声明在函数外部的一个或多个变量就是闭包</strong>。</p><p>上面的<code>addBase</code>方法使我们一次传入一个3一次传入一个4，那么有没有办法一次传入呢？</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; addBase(<span class="number">3</span>)(<span class="number">4</span>)</span><br><span class="line">res34: <span class="type">Int</span> = <span class="number">7</span></span><br></pre></td></tr></table></figure><p>没错，这就是柯里化，<strong>柯里化指的是将原来接受两个参数的方法变成新的接受一个参数的函数的过程。并且新的函数返回一个以原有第二个参数作为参数的函数。</strong></p><p>我们发现了上面闭包的代码其实就是柯里化的过程</p><p>现在我们总结下柯里化是什么?</p><ol><li><p>柯里化指的是将原来接受两个参数的方法变成新的接受一个参数的函数的过程。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">sum1</span></span>(x:<span class="type">Int</span>): <span class="type">Int</span> =&gt; <span class="type">Int</span> = (y:<span class="type">Int</span>)=&gt;x+y</span><br><span class="line">sum1: (x: <span class="type">Int</span>)<span class="type">Int</span> =&gt; <span class="type">Int</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> res1 = sum1(<span class="number">3</span>)</span><br><span class="line">res1: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1321</span>/<span class="number">962341885</span>@<span class="number">41</span>d283de</span><br><span class="line"></span><br><span class="line">scala&gt; res1(<span class="number">4</span>)</span><br><span class="line">res38: <span class="type">Int</span> = <span class="number">7</span></span><br></pre></td></tr></table></figure><p>==&gt;上面是接受两个参数，下面是接受一个参数</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">sum2</span></span>(x:<span class="type">Int</span>)(y:<span class="type">Int</span>): <span class="type">Int</span> = x+y</span><br><span class="line">sum2: (x: <span class="type">Int</span>)(y: <span class="type">Int</span>)<span class="type">Int</span></span><br><span class="line"></span><br><span class="line">scala&gt; sum2(<span class="number">2</span>)(<span class="number">3</span>)</span><br><span class="line">res39: <span class="type">Int</span> = <span class="number">5</span></span><br></pre></td></tr></table></figure></li><li><p>新的函数返回一个以原有第二个参数作为参数的函数。</p><p>意思就是原来要分两步做的事情，现在分一步就做好了，底层自动调用和返回。在这个过程中，使用了闭包。</p></li></ol><p>懵逼之余。。。返回最开始的<code>foldLeft</code>案例，看<code>foldLeft</code> 方法源码：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foldLeft</span></span>[<span class="type">B</span>](z: <span class="type">B</span>)(<span class="meta">@deprecatedName</span>(<span class="symbol">'f</span>) op: (<span class="type">B</span>, <span class="type">A</span>) =&gt; <span class="type">B</span>): <span class="type">B</span></span><br></pre></td></tr></table></figure><p>这其实就是一个柯里化应用</p><ol><li><p>折叠操作是一个递归的过程，将上一次的计算结果代入到函数中 </p></li><li><p>作为结果的参数在<code>foldLeft</code>是第一个参数，下个参数在<code>foldRight</code>是第二个参数</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> i = list.foldLeft[<span class="type">Int</span>](init)(_+_)</span><br></pre></td></tr></table></figure><p>==&gt;<code>init</code> = 10 作为初始值只用一次，然后(_+_)累加，累加的结果放在第一个参数位置</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">x = <span class="number">10</span> | y = <span class="number">1</span></span><br><span class="line">x = <span class="number">11</span> | y = <span class="number">2</span></span><br><span class="line">x = <span class="number">13</span> | y = <span class="number">3</span></span><br><span class="line">x = <span class="number">16</span> | y = <span class="number">4</span></span><br><span class="line">x = <span class="number">20</span> | y = <span class="number">5</span></span><br><span class="line">x = <span class="number">25</span> | y = <span class="number">6</span></span><br></pre></td></tr></table></figure></li></ol><p><strong>总结</strong></p><ol><li><p>柯里化技术在提高适用性还是在延迟执行或者固定易变因素等方面有着重要重要的作用，加上scala语言本身就是推崇简洁编码，使得同样功能的函数在定义与转换的时候会更加灵活多样。另外在Spark的源码中有大量运用scala柯里化技术的情况，需要掌握好该技术才能看得懂相关的源代码。</p></li><li><p>在scala柯里化中，闭包也发挥着重要的作用。所谓的闭包就是变量出了函数的定义域外在其他代码块还能其作用，这样的情况称之为闭包。就上述讨论的案例而言，如果没有闭包作用，那么转换后函数其实返回的匿名函数是无法在与第一个参数x相关结合的，自然也就无法保证其所实现的功能是跟原来一致的。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala之var和val的比较&amp;lazy懒加载</title>
      <link href="/2020/03/19/scala/3/"/>
      <url>/2020/03/19/scala/3/</url>
      
        <content type="html"><![CDATA[<h3 id="1：内容是否可变：val修饰的是不可变的，var修饰是可变的"><a href="#1：内容是否可变：val修饰的是不可变的，var修饰是可变的" class="headerlink" title="1：内容是否可变：val修饰的是不可变的，var修饰是可变的"></a>1：内容是否可变：val修饰的是不可变的，var修饰是可变的</h3><p>下面看一段代码，你猜是否有错误</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ValAndVar</span> </span>&#123;</span><br><span class="line">    <span class="comment">//val 修饰由于不可变性必须初始化</span></span><br><span class="line">    <span class="keyword">val</span> <span class="type">LOVE</span>:<span class="type">String</span> = _</span><br><span class="line">    <span class="keyword">var</span> <span class="type">SEX</span>:<span class="type">String</span>  = _</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> name = <span class="string">"tunan"</span></span><br><span class="line">        <span class="keyword">var</span> age = <span class="number">18</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//val 修饰由于不可变性不能重新赋值</span></span><br><span class="line">        name = <span class="string">"zhangsan"</span></span><br><span class="line">        age = <span class="number">19</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>真实的结果:</p><p><img src="https://yerias.github.io/scala_img/val%E5%92%8Cvar%E7%9A%84%E5%8F%AF%E5%8F%98%E6%80%A7%E6%AF%94%E8%BE%83.jpg" alt="val和var的可变性比较"></p><ol><li>val是不可变的，所以修饰的变量必须初始化</li><li>val是不可变的，所以修饰的变量不能重新赋值</li><li>val是不可变的，所以是多线程安全的</li><li>val是不可变的，不用担心会改变它修饰的对象的状态</li><li>val是不可变的，增强了代码的可读性，不用担心它的内容发生变化</li><li>var是可变的，可以增强代码的灵活性，和val互补</li></ol><h3 id="2：val修饰的变量在编译后类似于java中的中的变量被final修饰"><a href="#2：val修饰的变量在编译后类似于java中的中的变量被final修饰" class="headerlink" title="2：val修饰的变量在编译后类似于java中的中的变量被final修饰"></a>2：val修饰的变量在编译后类似于java中的中的变量被final修饰</h3><ol><li><p>先看源代码</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ValAndVar</span> </span>&#123;</span><br><span class="line">    <span class="keyword">val</span> <span class="type">LOVE</span>:<span class="type">String</span> = <span class="string">"篮球"</span></span><br><span class="line">    <span class="keyword">var</span> <span class="type">SEX</span>:<span class="type">String</span>  = _</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> name:<span class="type">String</span> = <span class="string">"tunan"</span></span><br><span class="line">        <span class="keyword">var</span> age:<span class="type">Int</span> = <span class="number">18</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>再看反编译后的代码(只保留了我们想要的部分)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ValAndVar$</span> </span>&#123;</span><br><span class="line">  public static <span class="type">ValAndVar</span>$ <span class="type">MODULE</span>$;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">String</span> <span class="type">LOVE</span>;</span><br><span class="line">    </span><br><span class="line">  public void main(<span class="type">String</span>[] args) &#123;</span><br><span class="line">    <span class="type">String</span> name = <span class="string">"tunan"</span>;</span><br><span class="line">    int age = <span class="number">18</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们发现这段代码很诡异，scala中的类变量，在字节码层面转换成了 parivate final ，而main方法中的变量却没有添加final修饰，这是否证明编译器有问题？</p><p>答案是否定的，对于val或者final都只是给编译器用的，编译器如果发现你给此变量重新赋值会抛出错误。同时字节码(bytecode)不具备表达一个局部变量是不可变(immutable)的能力。</p><p>所以就有了现在结果。</p></li></ol><h3 id="3：lazy修饰符可以修饰变量，但是这个变量必须是val修饰的"><a href="#3：lazy修饰符可以修饰变量，但是这个变量必须是val修饰的" class="headerlink" title="3：lazy修饰符可以修饰变量，但是这个变量必须是val修饰的"></a>3：lazy修饰符可以修饰变量，但是这个变量必须是val修饰的</h3><ol><li><p>在证明lazy修饰的变量必须是val之前，我们先看看lazy是什么？</p><p>Scala中使用关键字lazy来定义惰性变量，实现延迟加载(懒加载)。<br>惰性变量只能是不可变变量，并且只有在调用惰性变量时，才会去实例化这个变量。</p><p>在Java中，一般使用get和set实现延迟加载(懒加载)，而在Scala中对延迟加载这一特性提供了语法级别的支持:</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> name = initName()</span><br></pre></td></tr></table></figure><p>使用lazy关键字修饰变量后，只有在使用该变量时，才会调用其实例化方法。也就是说在定义name=initName()时并不会调用initName()方法，只有在后面的代码中使用变量name时才会调用initName()方法。</p><p>如果<strong>不使用lazy关键字对变量修饰</strong>，那么变量name是立即实例化的，下面将通过一组案例对比认识：</p><p><code>不使用lazy修饰的方法：</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LazyDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initName</span></span>:<span class="type">String</span>=&#123;</span><br><span class="line">        println(<span class="string">"初始化initName"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"返回intName"</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="comment">//        lazy val name = initName</span></span><br><span class="line">        <span class="keyword">val</span> name = initName<span class="comment">//程序走到这里，就打印了initName的输出语句</span></span><br><span class="line">        println(<span class="string">"hello，欢迎来到图南之家"</span>)</span><br><span class="line">        println(name)<span class="comment">//程序走到这里，打印initName的返回值</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的name没有使用lazy关键字进行修饰，所以name是立即实例化的。</p><p>结果：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">初始化initName</span><br><span class="line">hello，欢迎来到图南之家</span><br><span class="line">返回intName</span><br></pre></td></tr></table></figure><p><code>使用lazy修饰后的方法：</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LazyDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initName</span></span>:<span class="type">String</span>=&#123;</span><br><span class="line">        println(<span class="string">"初始化initName"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"返回intName"</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">lazy</span> <span class="keyword">val</span> name = initName<span class="comment">//不调用initName方法，即不打印initName中的输出语句</span></span><br><span class="line"><span class="comment">//        val name = initName</span></span><br><span class="line">        println(<span class="string">"hello，欢迎来到图南之家"</span>)<span class="comment">//打印main方法中的输出语句</span></span><br><span class="line">        println(name)<span class="comment">//打印initName的输出语句，打印返回值</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在声明name时，并没有立即调用实例化方法initName(),而是在使用name时，才会调用实例化方法,并且无论调用多少次，实例化方法只会执行一次。</p><p>结果：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">hello，欢迎来到图南之家</span><br><span class="line">初始化initName</span><br><span class="line">返回intName</span><br></pre></td></tr></table></figure></li><li><p>证明lazy只能修饰的变量只能使用val</p><p>我们发现name都是使用val修饰的，如果我们使用var修饰会怎么样呢？</p><p><img src="https://yerias.github.io/scala_img/lazy%E6%87%92%E5%8A%A0%E8%BD%BD.jpg" alt="lazy懒加载"></p><p>我们发现报错：<code>&#39;lazy&#39; modifier allowed only with value definitions</code></p><p>实际上就是认为<code>lazy</code>修饰的变量只能<code>val</code>修饰</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>执行Saprk or Scala程序:找不到或者无法加载主类 xxx</title>
      <link href="/2020/03/19/error/1/"/>
      <url>/2020/03/19/error/1/</url>
      
        <content type="html"><![CDATA[<p>使用百度和谷歌，测试了广大程序员给出的各种解决办法，包括更换jdk版本(之前也是jdk8，小版本不同)，更换scala 的版本(2.12大版本内更换小版本，因为我的spark2.4.5需要的scala版本是2.12)，更换idea的输出路径，重构代码，清理idea的缓存，删除依赖重新下载，重建项目，导入其他同学的项目，皆出现<code>找不到或者无法加载主类 xxx</code>异常</p><p>最后重装idea，解决！</p>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA的包装类解析</title>
      <link href="/2020/03/17/java/11/"/>
      <url>/2020/03/17/java/11/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>什么是包装类</li><li>自动装箱和自动拆箱</li><li>包装类可以为null，而基本类型不可以</li><li>包装类型可用于泛型，而基本类型不可以</li><li>基本类型比包装类型更高效</li><li>两个包装类型的值可以相同，但却可以不相等</li></ol><h2 id="什么是包装类"><a href="#什么是包装类" class="headerlink" title="什么是包装类"></a>什么是包装类</h2><p>Java的每个基本类型都有对应的包装类型，比如说int的包装类型是Integer，double的包装类型是Double。</p><h2 id="自动装箱和自动拆箱"><a href="#自动装箱和自动拆箱" class="headerlink" title="自动装箱和自动拆箱"></a>自动装箱和自动拆箱</h2><p>既然有了基本类型和包装类型，肯定有些是要在他们之间进行转换。把基本类型转换成包装类型的过程叫做装箱。反之，把包装类型转换成基本类型的过程叫做拆箱。</p><p>在Java5之前，开发人员要进行手动拆装箱</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Integer integer2 = <span class="keyword">new</span> Integer(<span class="number">10</span>);</span><br><span class="line"><span class="keyword">int</span> j = integer2.intValue();</span><br></pre></td></tr></table></figure><p>java5引入了自动拆装箱的功能，减少了开发人员的工作</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Integer chenmo  = <span class="number">10</span>;  <span class="comment">// 自动装箱</span></span><br><span class="line"><span class="keyword">int</span> wanger = chenmo;     <span class="comment">// 自动拆箱</span></span><br></pre></td></tr></table></figure><p>使用反编译工作编译后的结果如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Integer integer3 = Integer.valueOf(<span class="number">10</span>);</span><br><span class="line"><span class="keyword">int</span> k = integer3.intValue();</span><br></pre></td></tr></table></figure><p>也就说自动装箱是调用了<code>Integer.valueOf()</code>完成的，自动拆箱是通过调用<code>integer.intValue()</code>完成的。</p><p>理解了自动拆装箱的原理后，我们来看一道面试题</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">int</span> b = <span class="number">100</span>;</span><br><span class="line">System.out.println(a ==b);</span><br><span class="line"></span><br><span class="line">Integer c = <span class="number">100</span>;</span><br><span class="line">Integer d = <span class="number">100</span>;</span><br><span class="line">System.out.println(c == d);</span><br><span class="line"></span><br><span class="line">Integer e = <span class="number">200</span>;</span><br><span class="line">Integer f = <span class="number">200</span>;</span><br><span class="line">System.out.println(e == f);</span><br><span class="line"></span><br><span class="line">System.out.println( a == c);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> h = <span class="number">200</span>;</span><br><span class="line">System.out.println( e == h);</span><br></pre></td></tr></table></figure><p>在看这段代码之前，我们要明白的是 == 号，基础类型比较的是值，引用类型比较的是内存地址</p><p>第一段代码，很好理解，基础类型比较的是值</p><p>第二段代码是包装类，这里需要引入一个缓冲池(IntegerCache )的概念，JVM把-128到127的数值存到了内存中，需要的时候直接从内存拿，而不是重新创建一个对象</p><p>第三段代码也很容易理解，如果-128到127是从缓冲池中拿，那么超过这个范围的，自然就是堆中创建了</p><p>第四段是基本类型和包装类型做比较，这时候包装类型会先转成基本类型，然后再比较</p><p>第五段代码同上，没有在堆中创建的对象这一步</p><p>结果即是：<code>true、true、false、true、true</code></p><p>之前我们就已经知道了自动装箱是Integer.valueOf()方法，我们现在看看它的源码，如果做到-128到127是缓冲池中拿，而超过了则需要在堆中创建。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Integer <span class="title">valueOf</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//如果需要包装的数值大于IntegerCache.low 并且小于IntegerCache.high，就在IntegerCache.cache中拿，否则就new一个Integer，从这里看cache应该是个数组，保存了-128到127的数值</span></span><br><span class="line">    <span class="keyword">if</span> (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high)</span><br><span class="line">       <span class="comment">// 数据中的位置是0 - 255 ==&gt; -127 - 128 ==&gt; -IntegerCache.low的位置的值是0</span></span><br><span class="line">        <span class="keyword">return</span> IntegerCache.cache[i + (-IntegerCache.low)];</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Integer(i);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">IntegerCache</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> low = -<span class="number">128</span>;<span class="comment">//最小值-128</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> high;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> Integer cache[];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//静态代码块，初始化时就加载好了</span></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="comment">// 最大值可以通过配置文件设置</span></span><br><span class="line">        <span class="keyword">int</span> h = <span class="number">127</span>;<span class="comment">//最大值127</span></span><br><span class="line">        <span class="comment">//从jvm的配置中拿到自定义缓冲池最大值的参数</span></span><br><span class="line">        String integerCacheHighPropValue =</span><br><span class="line">            sun.misc.VM.getSavedProperty(<span class="string">"java.lang.Integer.IntegerCache.high"</span>);</span><br><span class="line">        <span class="comment">//integerCacheHighPropValue的值默认是null</span></span><br><span class="line">        <span class="keyword">if</span> (integerCacheHighPropValue != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">//拿到传入的最大值</span></span><br><span class="line">                <span class="keyword">int</span> i = parseInt(integerCacheHighPropValue);</span><br><span class="line">                i = Math.max(i, <span class="number">127</span>);</span><br><span class="line">                <span class="comment">// 最大值不能超过Integer定义的最大值</span></span><br><span class="line">                h = Math.min(i, Integer.MAX_VALUE - (-low) -<span class="number">1</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span>( NumberFormatException nfe) &#123;</span><br><span class="line">                <span class="comment">// 如果属性不能被解析成int型，就忽略它。</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        high = h;<span class="comment">//赋值给high</span></span><br><span class="line"><span class="comment">//(high - low) + 1 = 256</span></span><br><span class="line">        cache = <span class="keyword">new</span> Integer[(high - low) + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span> j = low;<span class="comment">// 遍历 0 - 256</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; cache.length; k++)</span><br><span class="line">            <span class="comment">// -127开始累加，并且都放到cache中，注意k是从0开始的</span></span><br><span class="line">            cache[k] = <span class="keyword">new</span> Integer(j++);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// range [-128, 127] must be interned (JLS7 5.1.7)</span></span><br><span class="line">        <span class="keyword">assert</span> IntegerCache.high &gt;= <span class="number">127</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">IntegerCache</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>看到这里也就明白了，-128-127是如何实现的，超过这个范围为什么是在堆中创建了，在这个源码中，还有一个地方没有解释，那就是参数<code>java.lang.Integer.IntegerCache.high</code>，这个参数可以加载到手动传入的值，从而扩大或者缩小缓冲池的最大值</p><p>如果我们设置这个值的大小为200：<code>-Djava.lang.Integer.IntegerCache.high=200</code>，那么我们就能看到下面的代码的结果是true</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Integer e = <span class="number">200</span>;</span><br><span class="line">Integer f = <span class="number">200</span>;</span><br><span class="line">System.out.println(e == f);</span><br></pre></td></tr></table></figure><p>看完上面的分析之后，我希望大家记住一点：<strong>当需要进行自动装箱时，如果数字在 -128 至 127 之间时，会直接使用缓存中的对象，而不是重新创建一个对象</strong>。</p><p>注意：缓冲池只有Integer类型有</p><h2 id="包装类可以为null，而基本类型不可以"><a href="#包装类可以为null，而基本类型不可以" class="headerlink" title="包装类可以为null，而基本类型不可以"></a>包装类可以为null，而基本类型不可以</h2><p>别小看这一点区别，它使得包装类型可以应用于 POJO 中，而基本类型则不行。</p><p>那为什么 POJO 的属性必须要用包装类型呢？</p><p>对于基本数据类型，数据库的查询结果可能是 null，如果使用基本类型的话，因为要自动拆箱（将包装类型转为基本类型，比如说把 Integer 对象转换成 int 值），就会抛出 <code>NullPointerException</code> 的异常。因为基础类型的值只能是数值。</p><h2 id="包装类型可用于泛型，而基本类型不可以"><a href="#包装类型可用于泛型，而基本类型不可以" class="headerlink" title="包装类型可用于泛型，而基本类型不可以"></a>包装类型可用于泛型，而基本类型不可以</h2><p>我们先尝试定义一个List</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;<span class="keyword">int</span>&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br></pre></td></tr></table></figure><p>报错：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Syntax error, insert <span class="string">"Dimensions"</span> to complete ReferenceTypeList&lt;Integer&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br></pre></td></tr></table></figure><p>原因是泛型只能使用Object 类及其子类，所以包装类型可用于泛型，而基本类型不可以</p><h2 id="基本类型比包装类型更高效"><a href="#基本类型比包装类型更高效" class="headerlink" title="基本类型比包装类型更高效"></a>基本类型比包装类型更高效</h2><p>基本数据类型在栈中直接存储具体的数值，而包装类型则存储在堆中，栈中存放的是引用</p><p><img src="https://yerias.github.io/java_img/13.png" alt=""></p><p>很显然，相对于基本数据类型而言，包装类型需要占用更多的内存空间，假如没有基本数据类型的话，对于数值这类经常能用到的数据来说，每次都要通过new来创建包装类型就显得非常笨重。</p><h2 id="两个包装类型的值可以相同，但却可以不相等"><a href="#两个包装类型的值可以相同，但却可以不相等" class="headerlink" title="两个包装类型的值可以相同，但却可以不相等"></a>两个包装类型的值可以相同，但却可以不相等</h2><p>两个包装类型的值可以相同，但却不相等</p><p>不相等是因为两个包装类型在使用“==”进行判断的时候，判断的是其指向的内存地址是否相等。包装类的值如果是在堆中创建出的话，因为内存地址不同，所以返回的是false。</p><p>而值相同是因为包装类型在做<code>equals</code>比较的时候，都先拆箱成了基础类型，然后再做比较，即比较的是内容，所以为true。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (obj <span class="keyword">instanceof</span> Integer) &#123;</span><br><span class="line">        <span class="keyword">return</span> value == ((Integer)obj).intValue();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala之使用ScalikeJDBC操作MySQL</title>
      <link href="/2020/03/16/scala/2/"/>
      <url>/2020/03/16/scala/2/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>简介</li><li>配置</li><li>操作数据库</li></ol><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>ScalikeJDBC是一款给Scala开发者使用的简介访问类库，它是基于SQL的，使用者只需要关注SQL逻辑的编写，所有的数据库操作都交给ScalikeJDBC。这个类库内置包含了JDBCAPI，并且给用户提供了简单易用并且非常灵活的API。并且，QueryDSl（通用查询查询框架）使你的代码类型安全，半年过去可重复使用。我们可以在生产环境大胆地使用这款DB访问类库。</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><ol><li><p>解决依赖</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.11.8<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scalikejdbc.version</span>&gt;</span>3.3.2<span class="tag">&lt;/<span class="name">scalikejdbc.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mysql.jdbc.version</span>&gt;</span>5.1.38<span class="tag">&lt;/<span class="name">mysql.jdbc.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--Scala相关依赖--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--scalikejdbc相关依赖--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scalikejdbc<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scalikejdbc_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scalikejdbc.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scalikejdbc<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scalikejdbc-config_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scalikejdbc.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;mysql.jdbc.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>解决配置</p><p>在<code>src</code>的<code>main</code>目录下配置一个<code>resource</code>文件夹，文件夹下再创建一个<code>application.conf</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">db.default.driver="com.mysql.jdbc.Driver"</span><br><span class="line">db.default.url="jdbc:mysql://hadoop001/ruoze_d6?characterEncoding=utf-8"</span><br><span class="line">db.default.user="root"</span><br><span class="line">db.default.password="123456"</span><br><span class="line"></span><br><span class="line"><span class="comment"># Connection Pool settings</span></span><br><span class="line">db.default.poolInitialSize=10</span><br><span class="line">db.default.poolMaxSize=20</span><br><span class="line">db.default.connectionTimeoutMillis=1000</span><br></pre></td></tr></table></figure></li></ol><h2 id="操作数据库"><a href="#操作数据库" class="headerlink" title="操作数据库"></a>操作数据库</h2><ol><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> Employer(</span><br><span class="line">    <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">10</span>),</span><br><span class="line">    age <span class="built_in">varchar</span>(<span class="number">4</span>),</span><br><span class="line">    salary  <span class="built_in">varchar</span>(<span class="number">10</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure></li><li><p>scala编程实现增删改查操作</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.wsk.bigdata.scala.scalikejdbc</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scalikejdbc._</span><br><span class="line"><span class="keyword">import</span> scalikejdbc.config._</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义样例类获取数据</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Employer</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span>, salary: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">JdbcTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">DBs</span>.setupAll()<span class="comment">//初始化配置</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//数据</span></span><br><span class="line">    <span class="keyword">val</span> employers = <span class="type">List</span>(<span class="type">Employer</span>(<span class="string">"zhangsan"</span>, <span class="number">20</span>, <span class="number">18000</span>), <span class="type">Employer</span>(<span class="string">"zhangliu"</span>, <span class="number">50</span>, <span class="number">300000</span>), <span class="type">Employer</span>(<span class="string">"lisi"</span>, <span class="number">22</span>, <span class="number">22000</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//批量插入</span></span><br><span class="line">    insert(employers)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//查询出结果</span></span><br><span class="line">    <span class="keyword">val</span> results = select()</span><br><span class="line">    <span class="keyword">for</span> (employer &lt;- results) &#123;</span><br><span class="line">      println(employer.name, employer.age, employer.salary)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//修改</span></span><br><span class="line">    update(<span class="number">1000</span>, <span class="string">"zhangsan"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//根据条件删除</span></span><br><span class="line">    deleteByname(<span class="string">"zhangliu"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//删除所有</span></span><br><span class="line">    deleteAll()</span><br><span class="line"></span><br><span class="line">   <span class="comment">//关闭资源</span></span><br><span class="line">    <span class="type">DBs</span>.closeAll()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">//插入数据</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(employers: <span class="type">List</span>[<span class="type">Employer</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//事物插入</span></span><br><span class="line">    <span class="type">DB</span>.localTx &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      <span class="keyword">for</span> (employer &lt;- employers) &#123;</span><br><span class="line">        <span class="type">SQL</span>(<span class="string">"insert into wsktest(name,age,salary) values(?,?,?)"</span>)</span><br><span class="line">          .bind(employer.name, employer.age, employer.salary)</span><br><span class="line">          .update()<span class="comment">//更新操作</span></span><br><span class="line">                .apply()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  <span class="comment">//查询操作</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">select</span></span>(): <span class="type">List</span>[<span class="type">Employer</span>] = &#123;</span><br><span class="line">    <span class="type">DB</span>.readOnly &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      <span class="type">SQL</span>(<span class="string">"select * from wsktest"</span>)</span><br><span class="line">        .map(rs =&gt; <span class="type">Employer</span>(rs.string(<span class="string">"name"</span>), rs.int(<span class="string">"age"</span>), rs.long(<span class="string">"salary"</span>)))</span><br><span class="line">        .list()  <span class="comment">//结果转换成list</span></span><br><span class="line">        .apply() </span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//更新操作</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(age: <span class="type">Int</span>, name: <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="type">DB</span>.autoCommit &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      <span class="type">SQL</span>(<span class="string">"update wsktest set age = ? where name = ?"</span>)</span><br><span class="line">        .bind(age, name)</span><br><span class="line">        .update()<span class="comment">//更新操作</span></span><br><span class="line">        .apply()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//根据条件删除</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteByname</span></span>(name: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">DB</span>.autoCommit &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      <span class="type">SQL</span>(<span class="string">"delete from wsktest where name = ?"</span>)</span><br><span class="line">        .bind(name)<span class="comment">//更新操作</span></span><br><span class="line">        .update()</span><br><span class="line">        .apply()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//删除所有</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteAll</span></span>(): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="type">DB</span>.autoCommit &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      <span class="type">SQL</span>(<span class="string">"delete from wsktest "</span>)</span><br><span class="line">        .update()<span class="comment">//更新操作</span></span><br><span class="line">        .apply()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Curator的介绍&amp;使用</title>
      <link href="/2020/03/15/zookeeper/2/"/>
      <url>/2020/03/15/zookeeper/2/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>简介</li><li>基于Curator的Zookeeper基本用法</li><li>监听器</li><li>分布式锁</li><li>Leader选举</li></ol><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Apache Curator是一个比较完善的ZooKeeper客户端框架，通过封装的一套高级API 简化了ZooKeeper的操作。通过查看官方文档，可以发现Curator主要解决了三类问题：</p><ul><li>封装ZooKeeper client与ZooKeeper server之间的连接处理</li><li>提供了一套Fluent风格的操作API</li><li>提供ZooKeeper各种应用场景(recipe， 比如：分布式锁服务、集群领导选举、共享计数器、缓存机制、分布式队列等)的抽象封装</li></ul><h3 id="Curator主要从以下几个方面降低了zk使用的复杂性："><a href="#Curator主要从以下几个方面降低了zk使用的复杂性：" class="headerlink" title="Curator主要从以下几个方面降低了zk使用的复杂性："></a>Curator主要从以下几个方面降低了zk使用的复杂性：</h3><ul><li>重试机制:提供可插拔的重试机制, 它将给捕获所有可恢复的异常配置一个重试策略，并且内部也提供了几种标准的重试策略(比如指数补偿)</li><li>连接状态监控: Curator初始化之后会一直对zk连接进行监听，一旦发现连接状态发生变化将会作出相应的处理</li><li>zk客户端实例管理:Curator会对zk客户端到server集群的连接进行管理，并在需要的时候重建zk实例，保证与zk集群连接的可靠性</li><li>各种使用场景支持:Curator实现了zk支持的大部分使用场景（甚至包括zk自身不支持的场景），这些实现都遵循了zk的最佳实践，并考虑了各种极端情况</li></ul><h2 id="基于Curator的Zookeeper基本用法"><a href="#基于Curator的Zookeeper基本用法" class="headerlink" title="基于Curator的Zookeeper基本用法"></a>基于Curator的Zookeeper基本用法</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CuratorBase</span> </span>&#123;</span><br><span class="line">    <span class="comment">//会话超时时间</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> SESSION_TIMEOUT = <span class="number">30</span> * <span class="number">1000</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//连接超时时间</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> CONNECTION_TIMEOUT = <span class="number">3</span> * <span class="number">1000</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//ZooKeeper服务地址</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String CONNECT_ADDR = <span class="string">"192.168.1.1:2100,192.168.1.1:2101,192.168.1.:2102"</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//创建连接实例</span></span><br><span class="line">    <span class="keyword">private</span> CuratorFramework client = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;  </span><br><span class="line">        <span class="comment">//1 重试策略：初试时间为1s 重试10次</span></span><br><span class="line">        RetryPolicy retryPolicy = <span class="keyword">new</span> ExponentialBackoffRetry(<span class="number">1000</span>, <span class="number">10</span>);</span><br><span class="line">        <span class="comment">//2 通过工厂创建连接</span></span><br><span class="line">        CuratorFramework client = CuratorFrameworkFactory.builder()</span><br><span class="line">                    .connectString(CONNECT_ADDR).connectionTimeoutMs(CONNECTION_TIMEOUT)</span><br><span class="line">                    .sessionTimeoutMs(SESSION_TIMEOUT)</span><br><span class="line">                    .retryPolicy(retryPolicy)</span><br><span class="line"><span class="comment">//命名空间           .namespace("super")</span></span><br><span class="line">                    .build();</span><br><span class="line">        <span class="comment">//3 开启连接</span></span><br><span class="line">        cf.start();</span><br><span class="line">        </span><br><span class="line">        System.out.println(States.CONNECTED);</span><br><span class="line">        System.out.println(cf.getState());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建永久节点</span></span><br><span class="line">        client.create().forPath(<span class="string">"/curator"</span>,<span class="string">"/curator data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//创建永久有序节点</span></span><br><span class="line">        client.create().withMode(CreateMode.PERSISTENT_SEQUENTIAL).forPath(<span class="string">"/curator_sequential"</span>,<span class="string">"/curator_sequential data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//创建临时节点</span></span><br><span class="line">        client.create().withMode(CreateMode.EPHEMERAL)</span><br><span class="line">            .forPath(<span class="string">"/curator/ephemeral"</span>,<span class="string">"/curator/ephemeral data"</span>.getBytes());</span><br><span class="line"> </span><br><span class="line">        <span class="comment">//创建临时有序节点</span></span><br><span class="line">        client.create().withMode(CreateMode.EPHEMERAL_SEQUENTIAL) .forPath(<span class="string">"/curator/ephemeral_path1"</span>,<span class="string">"/curator/ephemeral_path1 data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        client.create().withProtection().withMode(CreateMode.EPHEMERAL_SEQUENTIAL).forPath(<span class="string">"/curator/ephemeral_path2"</span>,<span class="string">"/curator/ephemeral_path2 data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//测试检查某个节点是否存在</span></span><br><span class="line">        Stat stat1 = client.checkExists().forPath(<span class="string">"/curator"</span>);</span><br><span class="line">        Stat stat2 = client.checkExists().forPath(<span class="string">"/curator2"</span>);</span><br><span class="line">        </span><br><span class="line">        System.out.println(<span class="string">"'/curator'是否存在： "</span> + (stat1 != <span class="keyword">null</span> ? <span class="keyword">true</span> : <span class="keyword">false</span>));</span><br><span class="line">        System.out.println(<span class="string">"'/curator2'是否存在： "</span> + (stat2 != <span class="keyword">null</span> ? <span class="keyword">true</span> : <span class="keyword">false</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取某个节点的所有子节点</span></span><br><span class="line">        System.out.println(client.getChildren().forPath(<span class="string">"/"</span>));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//获取某个节点数据</span></span><br><span class="line">        System.out.println(<span class="keyword">new</span> String(client.getData().forPath(<span class="string">"/curator"</span>)));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//设置某个节点数据</span></span><br><span class="line">        client.setData().forPath(<span class="string">"/curator"</span>,<span class="string">"/curator modified data"</span>.getBytes());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建测试节点</span></span><br><span class="line">        client.create().orSetData().creatingParentContainersIfNeeded()</span><br><span class="line">            .forPath(<span class="string">"/curator/del_key1"</span>,<span class="string">"/curator/del_key1 data"</span>.getBytes());</span><br><span class="line"> </span><br><span class="line">        client.create().orSetData().creatingParentContainersIfNeeded()</span><br><span class="line">        .forPath(<span class="string">"/curator/del_key2"</span>,<span class="string">"/curator/del_key2 data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        client.create().forPath(<span class="string">"/curator/del_key2/test_key"</span>,<span class="string">"test_key data"</span>.getBytes());</span><br><span class="line">              </span><br><span class="line">        <span class="comment">//删除该节点</span></span><br><span class="line">        client.delete().forPath(<span class="string">"/curator/del_key1"</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//级联删除子节点</span></span><br><span class="line">        client.delete().guaranteed().deletingChildrenIfNeeded().forPath(<span class="string">"/curator/del_key2"</span>);</span><br><span class="line">    ｝ </span><br><span class="line">｝</span><br></pre></td></tr></table></figure><ul><li><code>orSetData()</code>方法：如果节点存在则Curator将会使用给出的数据设置这个节点的值，相当于 setData() 方法</li><li><code>creatingParentContainersIfNeeded()</code>方法：如果指定节点的父节点不存在，则Curator将会自动级联创建父节点</li><li><code>guaranteed()</code>方法：如果服务端可能删除成功，但是client没有接收到删除成功的提示，Curator将会在后台持续尝试删除该节点</li><li><code>deletingChildrenIfNeeded()</code>方法：如果待删除节点存在子节点，则Curator将会级联删除该节点的子节点</li></ul><p>事务管理：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">* 事务管理：碰到异常，事务会回滚</span><br><span class="line">     * <span class="meta">@throws</span> Exception</span><br><span class="line">     */</span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testTransaction</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">//定义几个基本操作</span></span><br><span class="line">        CuratorOp createOp = client.transactionOp().create()</span><br><span class="line">                .forPath(<span class="string">"/curator/one_path"</span>,<span class="string">"some data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        CuratorOp setDataOp = client.transactionOp().setData()</span><br><span class="line">                .forPath(<span class="string">"/curator"</span>,<span class="string">"other data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        CuratorOp deleteOp = client.transactionOp().delete()</span><br><span class="line">                .forPath(<span class="string">"/curator"</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//事务执行结果</span></span><br><span class="line">        List&lt;CuratorTransactionResult&gt; results = client.transaction()</span><br><span class="line">                .forOperations(createOp,setDataOp,deleteOp);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//遍历输出结果</span></span><br><span class="line">        <span class="keyword">for</span>(CuratorTransactionResult result : results)&#123;</span><br><span class="line">            System.out.println(<span class="string">"执行结果是： "</span> + result.getForPath() + <span class="string">"--"</span> + result.getType());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//因为节点“/curator”存在子节点，所以在删除的时候将会报错，事务回滚</span></span><br></pre></td></tr></table></figure><h2 id="监听器"><a href="#监听器" class="headerlink" title="监听器"></a>监听器</h2><p>Curator提供了三种Watcher(Cache)来监听结点的变化：</p><ul><li><strong>Path Cache</strong>：监视一个路径下1）孩子结点的创建、2）删除，3）以及结点数据的更新。产生的事件会传递给注册的PathChildrenCacheListener。</li><li><strong>Node Cache</strong>：监视一个结点的创建、更新、删除，并将结点的数据缓存在本地。</li><li><strong>Tree Cache</strong>：Path Cache和Node Cache的“合体”，监视路径下的创建、更新、删除事件，并缓存路径下所有孩子结点的数据。</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 在注册监听器的时候，如果传入此参数，当事件触发时，逻辑由线程池处理</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        ExecutorService pool = Executors.newFixedThreadPool(<span class="number">2</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 监听数据节点的变化情况</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">final</span> NodeCache nodeCache = <span class="keyword">new</span> NodeCache(client, <span class="string">"/zk-huey/cnode"</span>, <span class="keyword">false</span>);</span><br><span class="line">        nodeCache.start(<span class="keyword">true</span>);</span><br><span class="line">        nodeCache.getListenable().addListener(</span><br><span class="line">            <span class="keyword">new</span> NodeCacheListener() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">nodeChanged</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    System.out.println(<span class="string">"Node data is changed, new data: "</span> + </span><br><span class="line">                        <span class="keyword">new</span> String(nodeCache.getCurrentData().getData()));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;, </span><br><span class="line">            pool</span><br><span class="line">        );</span><br><span class="line">        </span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 监听子节点的变化情况</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">final</span> PathChildrenCache childrenCache = <span class="keyword">new</span> PathChildrenCache(client, <span class="string">"/zk-huey"</span>, <span class="keyword">true</span>);</span><br><span class="line">        childrenCache.start(StartMode.POST_INITIALIZED_EVENT);</span><br><span class="line">        childrenCache.getListenable().addListener(</span><br><span class="line">            <span class="keyword">new</span> PathChildrenCacheListener() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">childEvent</span><span class="params">(CuratorFramework client, PathChildrenCacheEvent event)</span></span></span><br><span class="line"><span class="function">                        <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">switch</span> (event.getType()) &#123;</span><br><span class="line">                        <span class="keyword">case</span> CHILD_ADDED:</span><br><span class="line">                            System.out.println(<span class="string">"CHILD_ADDED: "</span> + event.getData().getPath());</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        <span class="keyword">case</span> CHILD_REMOVED:</span><br><span class="line">                            System.out.println(<span class="string">"CHILD_REMOVED: "</span> + event.getData().getPath());</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        <span class="keyword">case</span> CHILD_UPDATED:</span><br><span class="line">                            System.out.println(<span class="string">"CHILD_UPDATED: "</span> + event.getData().getPath());</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        <span class="keyword">default</span>:</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            pool</span><br><span class="line">        );</span><br><span class="line">        </span><br><span class="line">        client.setData().forPath(<span class="string">"/zk-huey/cnode"</span>, <span class="string">"world"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        Thread.sleep(<span class="number">10</span> * <span class="number">1000</span>);</span><br><span class="line">        pool.shutdown();</span><br><span class="line">        client.close();</span><br></pre></td></tr></table></figure><h2 id="分布式锁"><a href="#分布式锁" class="headerlink" title="分布式锁"></a>分布式锁</h2><p>分布式编程时，比如最容易碰到的情况就是应用程序在线上多机部署，于是当多个应用同时访问某一资源时，就需要某种机制去协调它们。例如，现在一台应用正在rebuild缓存内容，要临时锁住某个区域暂时不让访问；又比如调度程序每次只想一个任务被一台应用执行等等。</p><p>下面的程序会启动两个线程t1和t2去争夺锁，拿到锁的线程会占用5秒。运行多次可以观察到，有时是t1先拿到锁而t2等待，有时又会反过来。Curator会用我们提供的lock路径的结点作为全局锁，这个结点的数据类似这种格式：[<em>c</em>64e0811f-9475-44ca-aa36-c1db65ae5350-lock-0000000005]，每次获得锁时会生成这种串，释放锁时清空数据。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.curator.framework.CuratorFramework;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.CuratorFrameworkFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.recipes.locks.InterProcessMutex;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.retry.RetryNTimes;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Curator framework's distributed lock test.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CuratorDistrLockTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Zookeeper info */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ZK_ADDRESS = <span class="string">"192.168.1.100:2181"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ZK_LOCK_PATH = <span class="string">"/zktest"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 1.Connect to zk</span></span><br><span class="line">        CuratorFramework client = CuratorFrameworkFactory.newClient(</span><br><span class="line">                ZK_ADDRESS,</span><br><span class="line">                <span class="keyword">new</span> RetryNTimes(<span class="number">10</span>, <span class="number">5000</span>)</span><br><span class="line">        );</span><br><span class="line">        client.start();</span><br><span class="line">        System.out.println(<span class="string">"zk client start successfully!"</span>);</span><br><span class="line"></span><br><span class="line">        Thread t1 = <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">            doWithLock(client);</span><br><span class="line">        &#125;, <span class="string">"t1"</span>);</span><br><span class="line">        Thread t2 = <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">            doWithLock(client);</span><br><span class="line">        &#125;, <span class="string">"t2"</span>);</span><br><span class="line"></span><br><span class="line">        t1.start();</span><br><span class="line">        t2.start();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">doWithLock</span><span class="params">(CuratorFramework client)</span> </span>&#123;</span><br><span class="line">        InterProcessMutex lock = <span class="keyword">new</span> InterProcessMutex(client, ZK_LOCK_PATH);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (lock.acquire(<span class="number">10</span> * <span class="number">1000</span>, TimeUnit.SECONDS)) &#123;</span><br><span class="line">                System.out.println(Thread.currentThread().getName() + <span class="string">" hold lock"</span>);</span><br><span class="line">                Thread.sleep(<span class="number">5000L</span>);</span><br><span class="line">                System.out.println(Thread.currentThread().getName() + <span class="string">" release lock"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                lock.release();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Leader选举"><a href="#Leader选举" class="headerlink" title="Leader选举"></a>Leader选举</h2><p>当集群里的某个服务down机时，我们可能要从slave结点里选出一个作为新的master，这时就需要一套能在分布式环境中自动协调的Leader选举方法。Curator提供了LeaderSelector监听器实现Leader选举功能。同一时刻，只有一个Listener会进入takeLeadership()方法，说明它是当前的Leader。注意：<strong>当Listener从takeLeadership()退出时就说明它放弃了“Leader身份”</strong>，这时Curator会利用Zookeeper再从剩余的Listener中选出一个新的Leader。autoRequeue()方法使放弃Leadership的Listener有机会重新获得Leadership，如果不设置的话放弃了的Listener是不会再变成Leader的。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.curator.framework.CuratorFramework;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.CuratorFrameworkFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.recipes.leader.LeaderSelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.recipes.leader.LeaderSelectorListener;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.state.ConnectionState;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.retry.RetryNTimes;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.utils.EnsurePath;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Curator framework's leader election test.</span></span><br><span class="line"><span class="comment"> * Output:</span></span><br><span class="line"><span class="comment"> *  LeaderSelector-2 take leadership!</span></span><br><span class="line"><span class="comment"> *  LeaderSelector-2 relinquish leadership!</span></span><br><span class="line"><span class="comment"> *  LeaderSelector-1 take leadership!</span></span><br><span class="line"><span class="comment"> *  LeaderSelector-1 relinquish leadership!</span></span><br><span class="line"><span class="comment"> *  LeaderSelector-0 take leadership!</span></span><br><span class="line"><span class="comment"> *  LeaderSelector-0 relinquish leadership! </span></span><br><span class="line"><span class="comment"> *      ...</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CuratorLeaderTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Zookeeper info */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ZK_ADDRESS = <span class="string">"192.168.1.100:2181"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ZK_PATH = <span class="string">"/zktest"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        LeaderSelectorListener listener = <span class="keyword">new</span> LeaderSelectorListener() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">takeLeadership</span><span class="params">(CuratorFramework client)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                System.out.println(Thread.currentThread().getName() + <span class="string">" take leadership!"</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// takeLeadership() method should only return when leadership is being relinquished.</span></span><br><span class="line">                Thread.sleep(<span class="number">5000L</span>);</span><br><span class="line"></span><br><span class="line">                System.out.println(Thread.currentThread().getName() + <span class="string">" relinquish leadership!"</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">stateChanged</span><span class="params">(CuratorFramework client, ConnectionState state)</span> </span>&#123;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">            registerListener(listener);</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">            registerListener(listener);</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">            registerListener(listener);</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        Thread.sleep(Integer.MAX_VALUE);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">registerListener</span><span class="params">(LeaderSelectorListener listener)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 1.Connect to zk</span></span><br><span class="line">        CuratorFramework client = CuratorFrameworkFactory.newClient(</span><br><span class="line">                ZK_ADDRESS,</span><br><span class="line">                <span class="keyword">new</span> RetryNTimes(<span class="number">10</span>, <span class="number">5000</span>)</span><br><span class="line">        );</span><br><span class="line">        client.start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.Ensure path</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">new</span> EnsurePath(ZK_PATH).ensure(client.getZookeeperClient());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.Register listener</span></span><br><span class="line">        LeaderSelector selector = <span class="keyword">new</span> LeaderSelector(client, ZK_PATH, listener);</span><br><span class="line">        selector.autoRequeue();</span><br><span class="line">        selector.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><p>转载：<a href="https://www.cnblogs.com/erbing/p/9799098.html" target="_blank" rel="noopener">https://www.cnblogs.com/erbing/p/9799098.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Zookeeper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Zookeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA的String类源码解析</title>
      <link href="/2020/03/11/java/10/"/>
      <url>/2020/03/11/java/10/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>源码的角度解析String不可变</li><li>String Pool 的角度解析String不可变</li><li>String对象不可变性的优缺点</li><li>String对象是否真的不可变</li><li>从源码的角度解析StringBuilder的可变</li><li>从源码的角度解析StringBuffer和StringBuilder的异同</li><li>编译器对String做出了哪些优化</li></ol><h2 id="从源码的角度解析String不可变"><a href="#从源码的角度解析String不可变" class="headerlink" title="从源码的角度解析String不可变"></a>从源码的角度解析String不可变</h2><p>所谓的不可变类是指这个类的实例一旦创建完成后，就不能改变其成员变量值。</p><p>String类中每一个看起来会修改String值的方法，实际上都是创建了一个全新的String对象</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">upcase</span><span class="params">(String s)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> s.toUpperCase();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    String name = <span class="string">"tunan"</span>;</span><br><span class="line">    System.out.println(name);<span class="comment">//tunan</span></span><br><span class="line"></span><br><span class="line">    String name2 = upcase(name);</span><br><span class="line">    System.out.println(name2);<span class="comment">//TUNAN</span></span><br><span class="line"></span><br><span class="line">    System.out.println(name);<span class="comment">//tunan</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当把name传给upcase()方法的时候，实际上传递的是一个引用的拷贝。而该引用所指的对象其实一直待在单一的物理位置上，从未动过。</p><p>回到upcase()的定义，传入其中的引用有了名字s，只有upcase()运行的时候，局部引用s才存在。一旦upcase()运行结束，s就消失了。当然了，upcase()的返回值，其实只是最终结果的引用。这足已说明，upcase()返回的引用已经指向了一个新的对象name2，而原本的name则还在原地。</p><p>既然String类型的变量name没有变过，我们从源码的角度去看为什么没有改变</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">String</span></span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span>, <span class="title">Comparable</span>&lt;<span class="title">String</span>&gt;, <span class="title">CharSequence</span> </span>&#123;</span><br><span class="line">    <span class="comment">//使用字节数组存储字符串</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">char</span> value[];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//存储hash值</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> hash; <span class="comment">// Default to 0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以清楚的看到String类是一个final类，但这并不是String不可变的真正原因，继续看String实现了CharSequence接口</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">CharSequence</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">length</span><span class="params">()</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">char</span> <span class="title">charAt</span><span class="params">(<span class="keyword">int</span> index)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="function">CharSequence <span class="title">subSequence</span><span class="params">(<span class="keyword">int</span> start, <span class="keyword">int</span> end)</span></span>;</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>CharSequence是一个接口，它只包括length(), charAt(int index), subSequence(int start, int end)这几个API接口。同时除了String实现了CharSequence之外，StringBuffer和StringBuilder也实现了CharSequence接口。 </p><p>也就是说，CharSequence其实也就是定义了字符串操作的接口，其他具体的实现是由String、StringBuilder、StringBuffer完成的，String、StringBuilder、StringBuffer都可以转化为CharSequence类型。</p><p>继续看String类，<code>private final char value[];</code>这个final类型的字符型变量才是真正存储字符串的容器，也正是因为这个变量是final的，才真正决定了字符串不可变，也许你不相信，你可以说Stirng类也是final修饰的，也是不可变的，那么如果StringBuilder和StringBuffer也是final修饰的呢？</p><h2 id="String-Pool-的角度解析String不可变"><a href="#String-Pool-的角度解析String不可变" class="headerlink" title="String Pool 的角度解析String不可变"></a>String Pool 的角度解析String不可变</h2><p>JVM为了提升性能和减少内存开销，避免字符串的重复创建，其维护了一块特殊的内存空间，这就是我们今天要讨论的核心，即字符串池（String Pool）。</p><p>我们知道，在Java中有两种创建字符串对象的方式：</p><ol><li><p>采用字面值的方式赋值 </p></li><li><p>采用new关键字新建一个字符串对象。</p><p>这两种方式在性能和内存占用方面存在着差别。</p></li></ol><p>方式一：采用字面值的方式赋值，例如：</p><p><img src="https://yerias.github.io/java_img/1.png" alt=""></p><p>采用字面值的方式创建一个字符串时，JVM首先会去字符串池中查找是否存在”aaa”这个对象，如果不存在，则在字符串池中创建”aaa”这个对象，然后将池中”aaa”这个对象的引用地址返回给字符串常量str，这样str会指向池中”aaa”这个字符串对象；如果存在，则不创建任何对象，直接将池中”aaa”这个对象的地址返回，赋给字符串常量。</p><p><img src="https://yerias.github.io/java_img/3.jpg" alt=""></p><p> 在本例中，执行：str == str2 ，会得到以下结果：</p><p><img src="https://yerias.github.io/java_img/2.png" alt=""></p><p>这是因为，创建字符串对象str2时，字符串池中已经存在”aaa”这个对象，直接把对象”aaa”的引用地址返回给str2，这样str2指向了池中”aaa”这个对象，也就是说str和str2指向了同一个对象，因此语句System.out.println(str == str2)输出：true。</p><p>方式二：采用new关键字新建一个字符串对象，例如：</p><p><img src="https://yerias.github.io/java_img/4.png" alt=""></p><p>采用new关键字新建一个字符串对象时，JVM首先在字符串池中查找有没有”aaa”这个字符串对象，如果有，则不在池中再去创建”aaa”这个对象了，直接在堆中创建一个”aaa”字符串对象，然后将堆中的这个”aaa”对象的地址返回赋给引用str3，这样，str3就指向了堆中创建的这个”aaa”字符串对象；如果没有，则首先在字符串池中创建一个”aaa”字符串对象，然后再在堆中创建一个”aaa”字符串对象，然后将堆中这个”aaa”字符串对象的地址返回赋给str3引用，这样，str3指向了堆中创建的这个”aaa”字符串对象。</p><p><img src="https://yerias.github.io/java_img/6.jpg" alt=""></p><p>在这个例子中，执行：str3 == str4，得到以下结果：</p><p><img src="https://yerias.github.io/java_img/5.png" alt=""></p><p>因为，采用new关键字创建对象时，每次new出来的都是一个新的对象，也即是说引用str3和str4指向的是两个不同的对象，因此语句System.out.println(str3 == str4)输出：false。</p><p>字符串池的实现有一个前提条件：String对象是不可变的。因为这样可以保证多个引用可以同时指向字符串池中的同一个对象。如果字符串是可变的，那么一个引用操作改变了对象的值，对其他引用会有影响，这样显然是不合理的。</p><p><strong>Java语言规范（Java Language Specification）</strong>中对字符串做出了如下说明：每一个字符串常量都是指向一个字符串类实例的引用。字符串对象有一个固定值。字符串常量，或者一般的说，常量表达式中的字符串都被使用方法 String.intern进行保留来共享唯一的实例。</p><p>以上是Java语言规范中的原文，比较官方，用更通俗易懂的语言翻译过来主要说明了三点：</p><ol><li>每一个字符串常量都指向字符串池中或者堆内存中的一个字符串实例。</li><li>字符串对象值是固定的，一旦创建就不能再修改。</li><li>字符串常量或者常量表达式中的字符串都被方法String.intern()在字符串池中保留了唯一的实例。</li></ol><p><img src="https://yerias.github.io/java_img/7.jpg" alt=""></p><p>​        其他包</p><p>​            <img src="https://yerias.github.io/java_img/8.png" alt=""></p><p>​        结果</p><p>​                <img src="https://yerias.github.io/java_img/9.png" alt=""></p><p>结论：</p><ul><li>同一个包下同一个类中的字符串常量的引用指向同一个字符串对象；</li><li>同一个包下不同的类中的字符串常量的引用指向同一个字符串对象；</li><li>不同的包下不同的类中的字符串常量的引用仍然指向同一个字符串对象；</li><li>由常量表达式计算出的字符串是在编译时进行计算,然后被当作常量；</li><li>在运行时通过连接计算出的字符串是新创建的，因此是不同的；</li><li>通过计算生成的字符串显示调用intern方法后产生的结果与原来存在的同样内容的字符串常量是一样的。</li></ul><p>从上面的例子可以看出，字符串常量在<strong>编译时</strong>计算和在<strong>运行时</strong>计算，其执行过程是不同的，得到的结果也是不同的。我们来看看下面这段代码：</p><p><img src="https://yerias.github.io/java_img/10.png" alt=""></p><p> 代码输出如下：</p><p><img src="https://yerias.github.io/java_img/11.png" alt=""></p><p>为什么出现上面的结果呢？这是因为，字符串字面量拼接操作是在Java编译器编译期间就执行了，也就是说编译器编译时，直接把”java”、”language”和”specification”这三个字面量进行”+”操作得到一个”javalanguagespecification” 常量，并且直接将这个常量放入字符串池中，这样做实际上是一种优化，将3个字面量合成一个，避免了创建多余的字符串对象。而字符串引用的”+”运算是在Java运行期间执行的，即str + str2 + str3在程序执行期间才会进行计算，它会在堆内存中重新创建一个拼接后的字符串对象。总结来说就是：字面量”+”拼接是在编译期间进行的，拼接后的字符串存放在字符串池中；而字符串引用的”+”拼接运算实在运行时进行的，新创建的字符串存放在堆中。</p><p><strong>到这里我们也能理解了什么是字符串的不可变性</strong>，其本质是在字符串池中开辟了一块空间，字符串的地址不变，字符串变量重新赋值感觉是字符串变了，其实是在字符串池中开辟了另外一块空间，并且字符串的引用重新指向新的空间地址，而原来的字符串内容和内存地址在字符串池中没有改变过。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String name = <span class="string">"aaa"</span>;</span><br><span class="line">name = <span class="string">"bbb"</span>;</span><br><span class="line">System.out.println(name);</span><br></pre></td></tr></table></figure><p><img src="https://yerias.github.io/java_img/12.jpg" alt=""></p><p>字符串池的位置是在堆中，那么GC的时候<strong>字符串如何保证不被GC？</strong><br>为了优化空间，运行时实例创建的全局字符串常量池中有一个表，总是为池中每个唯一的字符串对象维护一个引用。这就意味着它们一直引用着字符串常量池中的对象，所以，在常量池中的这些字符串不会被垃圾收集器回收。</p><p>总结：字符串是常量，字符串池中的每个字符串对象只有唯一的一份，可以被多个引用所指向，避免了重复创建内容相同的字符串；通过字面值赋值创建的字符串对象存放在字符串池中，通过关键字new出来的字符串对象存放在堆中。</p><h2 id="String对象不可变性的优缺点"><a href="#String对象不可变性的优缺点" class="headerlink" title="String对象不可变性的优缺点"></a>String对象不可变性的优缺点</h2><p><strong>1.字符串常量池的需要</strong>.<br>字符串常量池可以将一些字符常量放在常量池中重复使用，避免每次都重新创建相同的对象、节省存储空间。但如果字符串是可变的，此时相同内容的String还指向常量池的同一个内存空间，当某个变量改变了该内存的值时，其他遍历的值也会发生改变。所以不符合常量池设计的初衷。</p><p><strong>2. 线程安全考虑</strong>。<br>同一个字符串实例可以被多个线程共享。这样便不用因为线程安全问题而使用同步。字符串自己便是线程安全的。</p><p><strong>3. 类加载器要用到字符串，不可变性提供了安全性，以便正确的类被加载</strong>。譬如你想加载java.sql.Connection类，而这个值被改成了myhacked.Connection，那么会对你的数据库造成不可知的破坏。</p><p><strong>4. 支持hash映射和缓存。</strong><br>因为字符串是不可变的，所以在它创建的时候hashcode就被缓存了，不需要重新计算。这就使得字符串很适合作为Map中的键，字符串的处理速度要快过其它的键对象。这就是HashMap中的键往往都使用字符串。</p><p>缺点：</p><ol><li>如果有对String对象值改变的需求，那么会创建大量的String对象(使用StringBuffer或者StringBuilder替代)。</li></ol><h2 id="String对象是否真的不可变"><a href="#String对象是否真的不可变" class="headerlink" title="String对象是否真的不可变"></a>String对象是否真的不可变</h2><p>String对象的不可变，其根本是内存地址的不可变，这在字符串池中有解析</p><p>虽然String对象将value设置为final，并且还通过各种机制保证其成员变量不可改变。但是还是可以通过反射机制的手段改变其值。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建字符串"Hello World"， 并赋给引用s</span></span><br><span class="line">String s = <span class="string">"hello world"</span>;</span><br><span class="line">System.out.println(<span class="string">"s = "</span> +s);</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取String类中的value字段(private final char value[];)</span></span><br><span class="line">Field valueFieldOfString  = String.class.getDeclaredField("value");</span><br><span class="line"></span><br><span class="line"><span class="comment">//改变value属性的访问权限</span></span><br><span class="line">valueFieldOfString .setAccessible(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取s对象上的value属性的值</span></span><br><span class="line"><span class="keyword">char</span>[] value = (<span class="keyword">char</span>[]) valueFieldOfString.get(s);</span><br><span class="line"></span><br><span class="line"><span class="comment">//改变value所引用的数组中的第5个字符</span></span><br><span class="line">value[<span class="number">5</span>] =<span class="string">'_'</span>;</span><br><span class="line">System.out.println(<span class="string">"s = "</span> +s);</span><br></pre></td></tr></table></figure><p>打印结果为：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">s = Hello World</span><br><span class="line">s = Hello_World</span><br></pre></td></tr></table></figure><p>发现String的值已经发生了改变。也就是说，通过反射是可以修改所谓的“不可变”对象的</p><h2 id="从源码的角度解析StringBuilder的可变"><a href="#从源码的角度解析StringBuilder的可变" class="headerlink" title="从源码的角度解析StringBuilder的可变"></a>从源码的角度解析StringBuilder的可变</h2><p>StringBuilder可以动态构造字符串，并且是线程不安全的，我们从源码的角度解析StringBuilder为什么可以动态构造字符串。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">StringBuilder</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">AbstractStringBuilder</span></span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span>, <span class="title">CharSequence</span></span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">    <span class="comment">//默认char容量16</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">StringBuilder</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(<span class="number">16</span>);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//指定了则使用父类的构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">StringBuilder</span><span class="params">(<span class="keyword">int</span> capacity)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(capacity);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>我们首先看到StringBuilder也是final修饰的， 和String一样，不仅如此StringBuffer也是final修饰的，下面将不再解释，它继承了AbstractStringBuilder，并且和String、StringBuffer一样，都实现了CharSequence接口</p><p>看构造方法默认容量是16，指定了容量则使用父类的构造方法，我们现在去看下父类中如何实现的</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">AbstractStringBuilder</span> <span class="keyword">implements</span> <span class="title">Appendable</span>, <span class="title">CharSequence</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 使用字节数组存储字符串</span></span><br><span class="line">    <span class="keyword">char</span>[] value;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 记录存储的字符串的长度</span></span><br><span class="line">    <span class="keyword">int</span> count;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 空构造方法</span></span><br><span class="line">    AbstractStringBuilder() &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 传入初始值的构造方法</span></span><br><span class="line">    AbstractStringBuilder(<span class="keyword">int</span> capacity) &#123;</span><br><span class="line">        value = <span class="keyword">new</span> <span class="keyword">char</span>[capacity];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>父类的构造方法中是new了一个指定长度的char字节数组，这说明StringBuilder底层也是使用字符数组保存字符串的，需要注意的是value的定义，和String类中的实现不同，这里没有private和final修饰，正是因为这点，所以StringBuilder是可变的，StringBuilder的value字节数组可以动态的改变大小。</p><p>我们已经知道了StringBuilder为什么可变，还需要注意的是它的append方法，该方法直接决定了StringBuilder如何追加字符串。也是和StringBuffer唯一不同的地方</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StringBuilder <span class="title">append</span><span class="params">(String str)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>.append(str);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>直接重写的父类方法</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> AbstractStringBuilder <span class="title">append</span><span class="params">(String str)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (str == <span class="keyword">null</span>)<span class="comment">//检查是否空</span></span><br><span class="line">        <span class="keyword">return</span> appendNull();</span><br><span class="line">    <span class="keyword">int</span> len = str.length();<span class="comment">//获得字符串长度</span></span><br><span class="line">    ensureCapacityInternal(count + len);<span class="comment">//检查容量/库容</span></span><br><span class="line">    str.getChars(<span class="number">0</span>, len, value, count);<span class="comment">//拷贝内容</span></span><br><span class="line">    count += len;<span class="comment">//增加长度</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;<span class="comment">//返回</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 扩容</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">newCapacity</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// overflow-conscious code</span></span><br><span class="line">    <span class="keyword">int</span> newCapacity = (value.length &lt;&lt; <span class="number">1</span>) + <span class="number">2</span>;<span class="comment">//扩容为原字节数组长度的两倍+2，注意不是count</span></span><br><span class="line">    <span class="keyword">if</span> (newCapacity - minCapacity &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        newCapacity = minCapacity;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (newCapacity &lt;= <span class="number">0</span> || MAX_ARRAY_SIZE - newCapacity &lt; <span class="number">0</span>)</span><br><span class="line">        ? hugeCapacity(minCapacity)</span><br><span class="line">        : newCapacity;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们发现append方法的底层是对字符数组内容的复制，并且容量不够时，是扩容为原字节数组长度的两倍+2，是字节数组，不是容量</p><h2 id="从源码的角度解析StringBuffer和StringBuilder的异同"><a href="#从源码的角度解析StringBuffer和StringBuilder的异同" class="headerlink" title="从源码的角度解析StringBuffer和StringBuilder的异同"></a>从源码的角度解析StringBuffer和StringBuilder的异同</h2><p>StringBuffer和StringBuilder的所有实现一模一样，包括继承的父类，实现的接口，扩容机制，value的定义，正是这些特性让他们两很像，同时也都支持动态构造字符串。</p><p>我们知道StringBuffer和StringBuilder最大的不同是线程安全性的问题，StringBuffer在所有以StringBuilder为基础的代码上，在重写父类的方法的同时加了synchronized修饰，保证了线程的安全</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//下面只是节选一些StringBuffer中的函数</span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(<span class="keyword">char</span> ch)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(<span class="keyword">char</span>[] chars)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(<span class="keyword">char</span>[] chars, <span class="keyword">int</span> start, <span class="keyword">int</span> length)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(Object obj)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(String string)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(StringBuffer sb)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(CharSequence s)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(CharSequence s, <span class="keyword">int</span> start, <span class="keyword">int</span> end)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">insert</span><span class="params">(<span class="keyword">int</span> index, <span class="keyword">char</span> ch)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">insert</span><span class="params">(<span class="keyword">int</span> index, <span class="keyword">char</span>[] chars)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">insert</span><span class="params">(<span class="keyword">int</span> index, <span class="keyword">char</span>[] chars, <span class="keyword">int</span> start, <span class="keyword">int</span> length)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">insert</span><span class="params">(<span class="keyword">int</span> index, String string)</span></span></span><br></pre></td></tr></table></figure><h2 id="编译器对String做出了哪些优化"><a href="#编译器对String做出了哪些优化" class="headerlink" title="编译器对String做出了哪些优化"></a>编译器对String做出了哪些优化</h2><p>String的不可变性会带来一定的效率问题。为String对象重载的 “+” 操作符就是一个例子。重载的意思是，一个操作符在应用于特定的类时，被赋予了特殊的意义。</p><p>我们用一段代码来验证 “+” 用来拼接String</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String mongo = <span class="string">"mongo"</span>;</span><br><span class="line">String s = <span class="string">"abc"</span> + mongo + <span class="string">"def"</span> +<span class="number">47</span>;</span><br><span class="line">System.out.println(s);<span class="comment">//abcmongodef47</span></span><br></pre></td></tr></table></figure><p>我们猜想一下字符串s的工作方式，它可能有一个append方法，首先s的内容是abc，然后新建一个字符串的内容是abcmongo，继续新建内容是abcmongodef的字符串，最后新建abcmongodef47的字符串，也许你会说为什么不是 “abc” + mongo + “def” +47 一起生成一个字符串然后赋值给s，但是我们不要忘记字符串String，它是一个类。</p><p>这种设计方法可以行的通，但是为了最终生成的String，产生了一大堆的需要GC的中间对象。这样的性能是非常糟糕的。</p><p>那么String是如何做优化的？我们使用JDK自带的工具javap来反编译以上代码，-c表示生成JVM字节码，删除没用的部分，剩下的内容如下</p><p><code>javap -c StringTest</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Compiled from <span class="string">"StringTest.java"</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">string</span>.<span class="title">StringTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(java.lang.String[])</span></span>;</span><br><span class="line">    Code:</span><br><span class="line">       0: ldc           #2                  // String mongo</span><br><span class="line">       <span class="number">2</span>: astore_1<span class="comment">// store mongo</span></span><br><span class="line">       3: new           #3                  // StringBuilder</span><br><span class="line">       <span class="number">6</span>: dup</span><br><span class="line">       7: invokespecial #4                  // StringBuilder."&lt;init&gt;":()V</span><br><span class="line">      10: ldc           #5                  // String abc</span><br><span class="line">      12: invokevirtual #6                  // StringBuilder.append：abc</span><br><span class="line">      <span class="number">15</span>: aload_1<span class="comment">// load mongo</span></span><br><span class="line">      16: invokevirtual #6                  // StringBuilder.append：mongo</span><br><span class="line">      19: ldc           #7                  // String def</span><br><span class="line">      21: invokevirtual #6                  // StringBuilder.append：def</span><br><span class="line">      <span class="number">24</span>: bipush        <span class="number">47</span></span><br><span class="line">      26: invokevirtual #8                  // StringBuilder.append：47</span><br><span class="line">      29: invokevirtual #9                  // StringBuilder.toString：abcmongodef47</span><br><span class="line">      <span class="number">32</span>: astore_2<span class="comment">// store s = abcmongodef47</span></span><br><span class="line">      33: getstatic     #10                 // Field System.out:Ljava/io/PrintStream;</span><br><span class="line">      <span class="number">36</span>: aload_2</span><br><span class="line">      37: invokevirtual #11                 // PrintStream.println:(Ljava/lang/String;)V</span><br><span class="line">      <span class="number">40</span>: <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>即使看不懂编译语句也不重要，我们需要注意的重点是：编译器自动引入了java.lang.StringBuilder类，虽然我们在源码中并没有使用StringBuilder类，但是编译器却自动使用了它，因为它更加高效。</p><p>现在也许你会觉得可以随意的使用String对象，反正编译器会自动优化性能，<strong>可是我们千万要记住一点，在循环的内部拼接字符串，并不会起到优化的效果。</strong></p><p>下面的程序采用两种方式生成String：方法一使用多个String对象；方法二中使用了StringBuilder。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WitherStringBuilder</span> </span>&#123;</span><br><span class="line"><span class="comment">//方法一</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">implicit</span><span class="params">(String[] fields)</span></span>&#123;</span><br><span class="line">        String result=<span class="string">""</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; fields.length; i++) &#123;</span><br><span class="line">            result += fields[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//方法二</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">explicit</span><span class="params">(String[] fields)</span></span>&#123;</span><br><span class="line">        StringBuilder result = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; fields.length; i++) &#123;</span><br><span class="line">            result.append(fields[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行javap -c WitherStringBuilder，可以看到两个方法对应的(简化过的)字节码，首先是implicit()方法：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> java.lang.<span class="function">String <span class="title">implicit</span><span class="params">(java.lang.String[])</span></span>;</span><br><span class="line">  Code:</span><br><span class="line">     0: ldc           #2                  // String</span><br><span class="line">     <span class="number">2</span>: astore_2</span><br><span class="line">     <span class="number">3</span>: iconst_0</span><br><span class="line">     <span class="number">4</span>: istore_3</span><br><span class="line">     <span class="number">5</span>: iload_3</span><br><span class="line">     <span class="number">6</span>: aload_1</span><br><span class="line">     <span class="number">7</span>: arraylength</span><br><span class="line">     <span class="number">8</span>: if_icmpge     <span class="number">38</span></span><br><span class="line">    11: new           #3                  // StringBuilder</span><br><span class="line">    <span class="number">14</span>: dup</span><br><span class="line">    15: invokespecial #4                  // StringBuilder."&lt;init&gt;":()</span><br><span class="line">    <span class="number">18</span>: aload_2</span><br><span class="line">    19: invokevirtual #5                  // StringBuilder.append:()</span><br><span class="line">    <span class="number">22</span>: aload_1</span><br><span class="line">    <span class="number">23</span>: iload_3</span><br><span class="line">    <span class="number">24</span>: aaload</span><br><span class="line">    25: invokevirtual #5                  //StringBuilder.append:()</span><br><span class="line">    28: invokevirtual #6                  // StringBuilder.toString:()</span><br><span class="line">    <span class="number">31</span>: astore_2</span><br><span class="line">    <span class="number">32</span>: iinc          <span class="number">3</span>, <span class="number">1</span></span><br><span class="line">    <span class="number">35</span>: goto          <span class="number">5</span></span><br><span class="line">    <span class="number">38</span>: aload_2</span><br><span class="line">    <span class="number">39</span>: areturn</span><br></pre></td></tr></table></figure><p>注意从第8行到第35行构成一个循环体。</p><p>第8行：对堆栈中的操作数进行 “大于或等于的整数比较运算”，循环结束时跳到第38行。</p><p>第35行：返回循环体的起始点(第5行)。</p><p>要注意的重点是：StringBuilder是在循环之内构造的，这意味着每经过循环一次，就会创建一个新的StringBuilder对象。这样的操作没有任何优化可言。</p><p>下面是explicit()方法对应的字节码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> java.lang.<span class="function">String <span class="title">explicit</span><span class="params">(java.lang.String[])</span></span>;</span><br><span class="line">  Code:</span><br><span class="line">     0: new           #3                  // StringBuilder</span><br><span class="line">     <span class="number">3</span>: dup</span><br><span class="line">     4: invokespecial #4                  // StringBuilder."&lt;init&gt;":()</span><br><span class="line">     <span class="number">7</span>: astore_2</span><br><span class="line">     <span class="number">8</span>: iconst_0</span><br><span class="line">     <span class="number">9</span>: istore_3</span><br><span class="line">    <span class="number">10</span>: iload_3</span><br><span class="line">    <span class="number">11</span>: aload_1</span><br><span class="line">    <span class="number">12</span>: arraylength</span><br><span class="line">    <span class="number">13</span>: if_icmpge     <span class="number">30</span></span><br><span class="line">    <span class="number">16</span>: aload_2</span><br><span class="line">    <span class="number">17</span>: aload_1</span><br><span class="line">    <span class="number">18</span>: iload_3</span><br><span class="line">    <span class="number">19</span>: aaload</span><br><span class="line">    20: invokevirtual #5                  // StringBuilder.append:()</span><br><span class="line">    <span class="number">23</span>: pop</span><br><span class="line">    <span class="number">24</span>: iinc          <span class="number">3</span>, <span class="number">1</span></span><br><span class="line">    <span class="number">27</span>: goto          <span class="number">10</span></span><br><span class="line">    <span class="number">30</span>: aload_2</span><br><span class="line">    31: invokevirtual #6                  // StringBuilder.toString:()</span><br><span class="line">    <span class="number">34</span>: areturn</span><br></pre></td></tr></table></figure><p>可以看到，不仅循环部分的代码更加简短，而且它只生成了一个StringBuilder对象。所以遇到循环内拼接字符串时在循环体的外部定义StringBuilder()可以大大提升程序的性能。当然，如果字符串操作简单的话，那么就可以信赖编译器的优化。</p><p>而且显示地创建StringBuilder还允许你预先为其指定大小。如果你已经知道最终的字符串大概多长，那预先指定StringBuilder的大小还可以避免多次重新分配缓冲。</p><hr><p>参考书籍：《Java编程思想(第4版)》</p><p>参考文章：</p><p><a href="https://www.cnblogs.com/kissazi2/p/3648671.html" target="_blank" rel="noopener">https://www.cnblogs.com/kissazi2/p/3648671.html</a></p><p><a href="https://www.cnblogs.com/xudong-bupt/p/3961159.html" target="_blank" rel="noopener">https://www.cnblogs.com/xudong-bupt/p/3961159.html</a></p><p><a href="https://www.cnblogs.com/fangfuhai/p/5500065.html" target="_blank" rel="noopener">https://www.cnblogs.com/fangfuhai/p/5500065.html</a></p><p><a href="https://www.cnblogs.com/jaylon/p/5721571.html" target="_blank" rel="noopener">https://www.cnblogs.com/jaylon/p/5721571.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nginx的简介&amp;安装&amp;常用操作</title>
      <link href="/2020/03/10/nginx/1/"/>
      <url>/2020/03/10/nginx/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>Nginx的功能介绍</li><li>Nginx的安装</li><li>Nginx常用操作</li></ol><h2 id="Nginx的功能介绍"><a href="#Nginx的功能介绍" class="headerlink" title="Nginx的功能介绍"></a>Nginx的功能介绍</h2><p>Nginx是一个轻量级、高性能、稳定性高、并发性好的HTTP和反向代理服务器。也是由于其的特性，其应用非常广。</p><h3 id="反向代理"><a href="#反向代理" class="headerlink" title="反向代理"></a>反向代理</h3><p>在说反向代理之前说一下正向代理：某些情况下，代理服务器代理我们用户去访问服务器，需要用户手动的设置代理服务器的ip和端口号。正向代理的最大特点是代理服务器在客户端</p><p>反向代理是用来代理服务器的，代理服务器代理我们要访问的目标服务器。代理服务器接受请求，然后将请求转发给内部网络的服务器(集群化)，并将从服务器上得到的结果返回给客户端，此时代理服务器对外就表现为一个服务器。反向代理的最大特点是代理服务器在服务器端</p><p><img src="https://yerias.github.io/nginx_img/1.jpg" alt=""></p><h3 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h3><p>负载均衡是在高并发情况下需要使用。其原理就是将数据流量分摊到多个服务器执行，减轻每台服务器的压力，多台服务器(集群)共同完成工作任务，从而提高了数据的吞吐量。</p><p>Nginx可使用的负载均衡策略有：轮询（默认）、权重、ip_hash、url_hash(第三方)、fair(第三方)</p><p><img src="https://yerias.github.io/nginx_img/2.jpg" alt=""></p><p>Ip hash算法，对客户端请求的ip进行hash操作，然后根据hash结果将同一个客户端ip的请求分发给同一台服务器进行处理，可以解决session不共享的问题。 </p><p><img src="https://yerias.github.io/nginx_img/3.jpg" alt=""></p><h3 id="动静分离"><a href="#动静分离" class="headerlink" title="动静分离"></a>动静分离</h3><p>Nginx提供的动静分离是指把动态请求和静态请求分离开，合适的服务器处理相应的请求，使整个服务器系统的性能、效率更高。</p><p>Nginx可以根据配置对不同的请求做不同转发，这是动态分离的基础。静态请求对应的静态资源可以直接放在Nginx上做缓冲，更好的做法是放在相应的缓冲服务器上。动态请求由相应的后端服务器处理。</p><p>常见的静态文件有js、css、html，常见的动态文件有jsp、servlet</p><h2 id="Nginx的安装"><a href="#Nginx的安装" class="headerlink" title="Nginx的安装"></a>Nginx的安装</h2><h3 id="安装所需环境"><a href="#安装所需环境" class="headerlink" title="安装所需环境"></a>安装所需环境</h3><p><strong>一. gcc 安装</strong><br>安装 nginx 需要先将官网下载的源码进行编译，编译依赖 gcc 环境，如果没有 gcc 环境，则需要安装：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install gcc-c++</span><br></pre></td></tr></table></figure><p><strong>二. PCRE pcre-devel 安装</strong><br>PCRE(Perl Compatible Regular Expressions) 是一个Perl库，包括 perl 兼容的正则表达式库。nginx 的 http 模块使用 pcre 来解析正则表达式，所以需要在 linux 上安装 pcre 库，pcre-devel 是使用 pcre 开发的一个二次开发库。nginx也需要此库。命令：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install -y pcre pcre-devel</span><br></pre></td></tr></table></figure><p><strong>三. zlib 安装</strong><br>zlib 库提供了很多种压缩和解压缩的方式， nginx 使用 zlib 对 http 包的内容进行 gzip ，所以需要在 Centos 上安装 zlib 库。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install -y zlib zlib-devel</span><br></pre></td></tr></table></figure><p><strong>四. OpenSSL 安装</strong><br>OpenSSL 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及 SSL 协议，并提供丰富的应用程序供测试或其它目的使用。<br>nginx 不仅支持 http 协议，还支持 https（即在ssl协议上传输http），所以需要在 Centos 安装 OpenSSL 库。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install -y openssl openssl-devel</span><br></pre></td></tr></table></figure><p><strong>五. 综合操作下载命令</strong></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install gcc-c++ pcre pcre-devel zlib zlib-devel openssl openssl-devel -y</span><br></pre></td></tr></table></figure><h3 id="下载安装Nginx"><a href="#下载安装Nginx" class="headerlink" title="下载安装Nginx"></a>下载安装Nginx</h3><ol><li><p>官网下载<code>.tar.gz</code>安装包，地址：<a href="https://nginx.org/en/download.html" target="_blank" rel="noopener">https://nginx.org/en/download.html</a></p></li><li><p>使用<code>wget</code>命令下载（推荐）。确保系统已经安装了wget，如果没有安装，执行 yum install wget 安装。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget -c https://nginx.org/download/nginx-1.12.0.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>解压依然是直接命令：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf nginx-1.12.0.tar.gz -C ../app</span><br><span class="line">cd nginx-1.12.0</span><br></pre></td></tr></table></figure></li><li><p>默认配置(默认是安装在/usr/local/nginx，不推介指定安装目录)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./configure</span><br></pre></td></tr></table></figure></li><li><p>编译安装(root用户)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure></li><li><p>查找安装路径</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">whereis nginx</span><br></pre></td></tr></table></figure><p>结果：/usr/local/nginx</p></li></ol><h2 id="Nginx常用操作"><a href="#Nginx常用操作" class="headerlink" title="Nginx常用操作"></a>Nginx常用操作</h2><ol><li><p>启动、停止nginx</p><p>cd /usr/local/nginx/sbin/</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./nginx #启动</span><br><span class="line">./nginx -s stop#停止</span><br><span class="line">./nginx -s quit#退出</span><br><span class="line">./nginx -s reload#重新加载配置文件</span><br></pre></td></tr></table></figure><p>启动时报80端口被占用:nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)</p><p>解决办法：安装net-tool 包：<code>yum install net-tools</code></p><p><strong>命令详解：</strong></p><p><code>./nginx -s quit</code>:此方式停止步骤是待nginx进程处理任务完毕进行停止。<br><code>./nginx -s stop</code>:此方式相当于先查出nginx进程id再使用kill命令强制杀掉进程。</p></li><li><p>重启Nginx</p><p>对 nginx 进行重启相当于先停止再启动，即先执行停止命令再执行启动命令。如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./nginx -s quit</span><br><span class="line">./nginx</span><br></pre></td></tr></table></figure></li><li><p>重新加载配置文件</p><p>当 ngin x的配置文件 nginx.conf 修改后，要想让配置生效需要重启 nginx，使用<code>-s reload</code>不用先停止 ngin x再启动 nginx 即可将配置信息在 nginx 中生效，如下:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./nginx -s reload</span><br></pre></td></tr></table></figure><p>启动成功后，可打开浏览器访问，默认端口80</p></li><li><p>配置开机自启</p><p>即在<code>rc.local</code>增加启动代码就可以了。</p><p>vi /etc/rc.local</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/usr/local/nginx/sbin/nginx</span><br></pre></td></tr></table></figure><p>设置执行权限：<code>chmod 755 rc.local</code></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HUE安装&amp;集成</title>
      <link href="/2020/03/09/hue/1/"/>
      <url>/2020/03/09/hue/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li><p>hue简介</p></li><li><p>安装maven</p></li><li><p>安装ant</p></li><li><p>安装hue</p></li><li><p>hue集成hdfs</p></li><li><p>hue集成yarn</p></li><li><p>hue集成hive</p></li><li><p>hue集成mysql</p></li><li><p>hue集成zookeeper</p></li><li><p>hue集成hbase</p></li><li><p>hue集成oozie</p></li><li><p>Shell脚本</p></li></ol><h2 id="hue简介"><a href="#hue简介" class="headerlink" title="hue简介"></a>hue简介</h2><p>HUE=Hadoop User Experience</p><p>Hue是一个开源的Apache Hadoop UI系统，由Cloudera Desktop演化而来，最后Cloudera公司将其贡献给Apache基金会的Hadoop社区，它是基于Python Web框架Django实现的。</p><p>通过使用Hue，可以在浏览器端的Web控制台上与Hadoop集群进行交互，来分析处理数据，例如操作HDFS上的数据，运行MapReduce Job，执行Hive的SQL语句，浏览HBase数据库等等。（说人话就是支持提供各种Web图形化界面的）</p><h2 id="安装maven"><a href="#安装maven" class="headerlink" title="安装maven"></a>安装maven</h2><ol><li>上传解压apache-maven-3.6.3-bin.tar到~/app目录下</li><li>配置环境变量</li><li>输入<code>mvn -version</code>测试是否安装成功</li></ol><h2 id="安装ant"><a href="#安装ant" class="headerlink" title="安装ant"></a>安装ant</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install ant -y</span><br></pre></td></tr></table></figure><h2 id="安装hue"><a href="#安装hue" class="headerlink" title="安装hue"></a>安装hue</h2><ol><li><p>hue相关网站</p><p>hue官网：<a href="http://gethue.com/" target="_blank" rel="noopener">http://gethue.com/</a></p><p>配置文档：<a href="http://archive.cloudera.com/cdh5/cdh/5/hue-3.9.0-cdh5.16.2" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hue-3.9.0-cdh5.16.2</a></p><p>源码：<a href="https://github.com/cloudera/hue" target="_blank" rel="noopener">https://github.com/cloudera/hue</a></p><p>这里我们直接用下载hue：<a href="http://archive.cloudera.com/cdh5/cdh/5/hue-3.9.0-cdh5.16.2.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hue-3.9.0-cdh5.16.2.tar.gz</a></p></li><li><p>安装hue所需要的依赖包</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install asciidoc cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-plain gcc gcc-c++ krb5-devel libffi-devel libtidy libxml2-devel libxslt-devel make mysql mysql-devel openldap-devel python-devel sqlite-devel openssl-devel gmp-devel -y</span><br></pre></td></tr></table></figure></li><li><p>解压安装Hue的tar包</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd ~/software</span><br><span class="line">tar -zxvf hue-3.7.0-cdh5.3.6.tar -C ~/app</span><br></pre></td></tr></table></figure></li><li><p>编译</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd hue-3.7.0-cdh5.3.6</span><br><span class="line">make apps</span><br></pre></td></tr></table></figure></li><li><p>修改hue.ini配置文件</p><p>进入到desktop/conf目录下,找到hue.ini文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">secret_key=jFE93j;2[290-eiw.KEiwN2s3['d;/.q[eIW^y#e=+Iei*@Mn&lt;qW5o</span><br><span class="line">http_host=hadoop</span><br><span class="line">http_port=8888</span><br><span class="line">time_zone=Asia/Shanghai</span><br></pre></td></tr></table></figure></li><li><p>启动</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">build/env/bin/supervisor</span><br></pre></td></tr></table></figure></li></ol><h2 id="hue集成hdfs"><a href="#hue集成hdfs" class="headerlink" title="hue集成hdfs"></a>hue集成hdfs</h2><ol><li><p>配置hdfs.site.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>配置core-site.xml</p><p>设置代理用户</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hadoop.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hadoop.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果你的Hadoop配置了<code>高可用</code>，则必须通过httpfs来访问，需要添加如下属性，反则则不必须。（如果HUE服务与Hadoop服务不在同一节点，则必须配置）</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.httpfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.httpfs.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置httpfs-site.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>httpfs.proxyuser.hue.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>httpfs.proxyuser.hue.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>以上两个属性主要用于HUE服务与Hadoop服务不在同一台节点上所必须的配置。</p></li><li><p>配置hue.ini</p><p>找到<code>[[hdfs_clusters]]</code>标签</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS服务器地址</span><br><span class="line">fs_defaultfs=hdfs://hadoop:9000</span><br><span class="line"></span><br><span class="line">如果开启了高可用，需要配置如下</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># logical_name=mycluster</span></span></span><br><span class="line"></span><br><span class="line">向HDFS发送命令的请求地址</span><br><span class="line">webhdfs_url=http://hadoop:50070/webhdfs/v1</span><br><span class="line"></span><br><span class="line">HADOOP的一些配置</span><br><span class="line">hadoop_conf_dir=/home/hadoop/app/hadoop/etc/hadoop</span><br><span class="line">hadoop_hdfs_home=/home/hadoop/app/hadoop</span><br><span class="line">hadoop_bin=/home/hadoop/app/hadoop/bin</span><br></pre></td></tr></table></figure></li><li><p>如果配置了高可用则启动hue前需要先启动httpfs服务</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/home/hadoop/app/hadoop/sbin/httpfs.sh start</span><br></pre></td></tr></table></figure></li></ol><h2 id="hue集成yarn"><a href="#hue集成yarn" class="headerlink" title="hue集成yarn"></a>hue集成yarn</h2><p>如果没有在hadoop中配置历史节点，需要先配置历史节点</p><p>mapred-site.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>启动历史服务器</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver</span></span><br></pre></td></tr></table></figure><p>找到<code>[[yarn_clusters]]</code>标签，涉及修改配置如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn服务的配置</span><br><span class="line">resourcemanager_host=192.168.56.86</span><br><span class="line">resourcemanager_port=8032</span><br><span class="line"></span><br><span class="line">是否将作业提交到此群集，并监控作业执行情况</span><br><span class="line">submit_to=True</span><br><span class="line"><span class="meta">#</span><span class="bash">logical_name=cluster-yarn1(如果开高可用的话)</span></span><br><span class="line"></span><br><span class="line">配置yarn资源管理的访问入口</span><br><span class="line">resourcemanager_api_url=http://hadoop:8088</span><br><span class="line">proxy_api_url=http://hadoop:8088</span><br><span class="line"></span><br><span class="line">历史服务器管理的入口，查看作业的历史运行情况</span><br><span class="line">history_server_api_url=http://hadoop:19888</span><br></pre></td></tr></table></figure><p><code>注意</code>：hue界面的超级用户名字需要和提交到yarn的用户名字相同</p><h2 id="hue集成hive"><a href="#hue集成hive" class="headerlink" title="hue集成hive"></a>hue集成hive</h2><p>配置hive-site.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>TCP绑定的主机<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.long.polling.timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>5000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>HiveServer2在响应使用长轮询的异步调用之前等待的时间（毫秒）<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://hadoop:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>指向的是运行metastore服务的主机<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置hue.ini，找到[beeswax]属性标签，涉及修改如下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[beeswax]</span><br><span class="line">hive_server_host=hadoop</span><br><span class="line">hive_server_port=10000</span><br><span class="line">hive_conf_dir=/home/hadoop/app/hive/conf</span><br></pre></td></tr></table></figure><p>启动</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/hive --service metastore &amp;</span><br><span class="line">bin/hive --service hiveserver2 1&gt;/dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p><code>注意：</code>如果设置了uris，在今后使用Hive时，那么必须启动如上两个命令，否则Hive无法正常启动。</p><h2 id="hue集成mysql"><a href="#hue集成mysql" class="headerlink" title="hue集成mysql"></a>hue集成mysql</h2><p>配置hue.ini，找到[[[mysql]]]标签，涉及修改如下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[[[mysql]]]</span><br><span class="line">nice_name="mysql"</span><br><span class="line">engine=mysql</span><br><span class="line">host=hadoop</span><br><span class="line">port=3306</span><br><span class="line">user=root</span><br><span class="line">password=root</span><br></pre></td></tr></table></figure><h2 id="hue集成Zookeeper"><a href="#hue集成Zookeeper" class="headerlink" title="hue集成Zookeeper"></a>hue集成Zookeeper</h2><p>配置hue.ini</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zookeeper]</span><br><span class="line">[[clusters]]</span><br><span class="line">[[[default]]]</span><br><span class="line">host_ports=hadoop:2181</span><br></pre></td></tr></table></figure><p>启动zookeeper</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zkServer.sh start</span><br><span class="line">zkServer.sh status</span><br></pre></td></tr></table></figure><h2 id="hue集成hbase"><a href="#hue集成hbase" class="headerlink" title="hue集成hbase"></a>hue集成hbase</h2><h2 id="hue集成oozie"><a href="#hue集成oozie" class="headerlink" title="hue集成oozie"></a>hue集成oozie</h2><h2 id="Shell脚本"><a href="#Shell脚本" class="headerlink" title="Shell脚本"></a>Shell脚本</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line">echo "启动httpfs服务"</span><br><span class="line">/home/hadoop/app/hadoop/sbin/httpfs.sh start</span><br><span class="line"></span><br><span class="line">echo "启动历史服务"</span><br><span class="line">/home/hadoop/app/hadoop/sbin/mr-jobhistory-daemon.sh start historyserver</span><br><span class="line"></span><br><span class="line">echo "启动hive元数据"</span><br><span class="line"><span class="meta">$</span><span class="bash">HIVE_HOME/bin/hive --service metastore &amp;</span></span><br><span class="line"></span><br><span class="line">echo "启动hive服务端"</span><br><span class="line"><span class="meta">$</span><span class="bash">HIVE_HOME/bin/hive --service hiveserver2 1 &gt; /dev/null 2&gt;&amp;1 &amp;</span></span><br><span class="line"></span><br><span class="line">echo "启动hue"</span><br><span class="line"><span class="meta">$</span><span class="bash">HUE_HOME/build/env/bin/supervisor 1 &gt; /dev/null 2&gt;&amp;1 &amp;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hue </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FastJson的使用</title>
      <link href="/2020/03/07/java/9/"/>
      <url>/2020/03/07/java/9/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近在开发过程中使用了大量的<code>json</code>作为前后端数据交换的方式,由于之前没有对<code>json</code>做过系统的学习,所有在使用过程中查阅了大量的文档与资料,这里主要记录了我在开发后对<code>json</code>以及<code>FastJson</code>使用的总结</p><h2 id="Json介绍"><a href="#Json介绍" class="headerlink" title="Json介绍"></a>Json介绍</h2><p><code>JSON</code>(javaScript Object Notation)是一种轻量级的数据交换格式。主要采用键值对(<code>{&quot;name&quot;: &quot;json&quot;}</code>)的方式来保存和表示数据。<code>JSON</code>是<code>JS</code>对象的字符串表示法，它使用文本表示一个<code>JS</code>对象的信息，本质上是一个字符串。更多简介见<a href="http://www.json.org/json-zh.html" target="_blank" rel="noopener">介绍JSON</a>。</p><h2 id="FastJson-简介"><a href="#FastJson-简介" class="headerlink" title="FastJson  简介"></a>FastJson  简介</h2><p>在日志解析,前后端数据传输交互中,经常会遇到字符串(String)与<code>json</code>,<code>XML</code>等格式相互转换与解析，其中<code>json</code>以跨语言，跨前后端的优点在开发中被频繁使用，基本上可以说是标准的数据交换格式。<a href="https://github.com/alibaba/FastJson" target="_blank" rel="noopener">fastjson</a>是一个java语言编写的高性能且功能完善的JSON库，它采用一种“假定有序快速匹配”的算法，把<code>JSON Parse</code> 的性能提升到了极致。它的接口简单易用，已经被广泛使用在缓存序列化，协议交互，Web输出等各种应用场景中。</p><h2 id="FastJson-常用-API"><a href="#FastJson-常用-API" class="headerlink" title="FastJson  常用 API"></a>FastJson  常用 API</h2><p>导入Jar包</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;com.alibaba&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;fastjson&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;1.2.47&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>FastJson API 入口类是<code>com.alibaba.FastJson.JSON</code>,常用的序列化操作都可以在<code>JSON</code>类上的静态方法直接完成。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Object <span class="title">parse</span><span class="params">(String text)</span></span>; <span class="comment">// 把JSON文本parse为JSONObject或者JSONArray </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> JSONObject <span class="title">parseObject</span><span class="params">(String text)</span>； <span class="comment">// 把JSON文本parse成JSONObject    </span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> &lt;T&gt; T <span class="title">parseObject</span><span class="params">(String text, Class&lt;T&gt; clazz)</span></span>; <span class="comment">// 把JSON文本parse为JavaBean </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> JSONArray <span class="title">parseArray</span><span class="params">(String text)</span></span>; <span class="comment">// 把JSON文本parse成JSONArray </span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> &lt;T&gt; <span class="function">List&lt;T&gt; <span class="title">parseArray</span><span class="params">(String text, Class&lt;T&gt; clazz)</span></span>; <span class="comment">//把JSON文本parse成JavaBean集合 </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String <span class="title">toJSONString</span><span class="params">(Object object)</span></span>; <span class="comment">// 将JavaBean序列化为JSON文本 </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String <span class="title">toJSONString</span><span class="params">(Object object, <span class="keyword">boolean</span> prettyFormat)</span></span>; <span class="comment">// 将JavaBean序列化为带格式的JSON文本 </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Object <span class="title">toJSON</span><span class="params">(Object javaObject)</span></span>; <span class="comment">//将JavaBean转换为JSONObject或者JSONArray。</span></span><br></pre></td></tr></table></figure><h3 id="使用方法举例"><a href="#使用方法举例" class="headerlink" title="使用方法举例"></a>使用方法举例</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//将JSON文本转换为java对象</span></span><br><span class="line"><span class="keyword">import</span> com.alibaba.FastJson.JSON;</span><br><span class="line">Model model = JSON.parseObject(jsonStr, Model<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure><h3 id="有关类库的一些说明"><a href="#有关类库的一些说明" class="headerlink" title="有关类库的一些说明"></a>有关类库的一些说明</h3><ul><li>JSONArray : 相当于List</li><li>JSONObject: 相当于Map&lt;String,Object&gt;</li></ul><h2 id="FastJson-使用实例"><a href="#FastJson-使用实例" class="headerlink" title="FastJson  使用实例"></a>FastJson  使用实例</h2><h3 id="Java对象与Json字符串的互转"><a href="#Java对象与Json字符串的互转" class="headerlink" title="Java对象与Json字符串的互转"></a>Java对象与Json字符串的互转</h3><p><code>User</code>测试类</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.json.user;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-03-07 09:54</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String username;</span><br><span class="line">    <span class="keyword">private</span> String password;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">User</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">User</span><span class="params">(String username, String password)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.username = username;</span><br><span class="line">        <span class="keyword">this</span>.password = password;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getUsername</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> username;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUsername</span><span class="params">(String username)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.username = username;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPassword</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> password;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPassword</span><span class="params">(String password)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.password = password;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>UserGroup</code>测试类</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.json.user;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-03-07 09:55</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserGroup</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> List&lt;User&gt; users= <span class="keyword">new</span> ArrayList&lt;User&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">UserGroup</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">UserGroup</span><span class="params">(String name, List&lt;User&gt; users)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.users = users;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;User&gt; <span class="title">getUsers</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> users;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUsers</span><span class="params">(List&lt;User&gt; users)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.users = users;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>fastJson</code>测试类</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.json.user;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-03-07 09:58</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestFastJosn</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        objectToJson();</span><br><span class="line">        JsonToObject();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//java对象转 json字符串</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">objectToJson</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="comment">//简单java类转json</span></span><br><span class="line">        User user = <span class="keyword">new</span> User(<span class="string">"tunan"</span>,<span class="string">"123456"</span>);</span><br><span class="line">        String userJson = JSON.toJSONString(user);</span><br><span class="line">        System.out.println(<span class="string">"简单java类转字符串： "</span>+userJson);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//List&lt;Object&gt;转Json字符串</span></span><br><span class="line">        User user1 = <span class="keyword">new</span> User(<span class="string">"xiaoqi"</span>, <span class="string">"961228"</span>);</span><br><span class="line">        User user2 = <span class="keyword">new</span> User(<span class="string">"tunan"</span>, <span class="string">"123456"</span>);</span><br><span class="line">        List&lt;User&gt; users = <span class="keyword">new</span> ArrayList&lt;User&gt;();</span><br><span class="line">        users.add(user1);</span><br><span class="line">        users.add(user2);</span><br><span class="line">        String listJson = JSON.toJSONString(users);</span><br><span class="line">        System.out.println(<span class="string">"List&lt;Object&gt;转Json字符串： "</span>+listJson);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//复杂java转json字符串</span></span><br><span class="line">        UserGroup userGroup = <span class="keyword">new</span> UserGroup(<span class="string">"student"</span>, users);</span><br><span class="line">        String userGroupJson = JSON.toJSONString(userGroup);</span><br><span class="line">        System.out.println(<span class="string">"复杂java转json字符串： "</span>+userGroupJson);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Json字符串 转Java类</span></span><br><span class="line">    <span class="comment">//注：字符串中使用双引号需要转义 (" --&gt; \"),这里使用的是单引号</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">JsonToObject</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * json字符串转简单java对象</span></span><br><span class="line"><span class="comment">         * 字符串：&#123;"password":"123456","username":"dmego"&#125;</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        String userJson = <span class="string">"&#123;'password':'123456','username':'dmego'&#125;"</span>;</span><br><span class="line">        User user = JSON.parseObject(userJson, User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        System.out.println(<span class="string">"json字符串转简单java对象: "</span>+user.toString());</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * json字符串转List&lt;Object&gt;对象</span></span><br><span class="line"><span class="comment">         * 字符串：[&#123;"password":"123123","username":"zhangsan"&#125;,&#123;"password":"321321","username":"lisi"&#125;]</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        String listUserJson = <span class="string">"[&#123;'password':'123123','username':'zhangsan'&#125;,&#123;'password':'321321','username':'lisi'&#125;]"</span>;</span><br><span class="line">        List&lt;User&gt; userList = JSON.parseArray(listUserJson, User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        System.out.println(<span class="string">"json字符串转List&lt;Object&gt;对象: "</span>+userList.toString());</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*json字符串转复杂java对象</span></span><br><span class="line"><span class="comment">         * 字符串：&#123;"name":"userGroup","users":[&#123;"password":"123123","username":"zhangsan"&#125;,&#123;"password":"321321","username":"lisi"&#125;]&#125;</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        String userGroupJson = <span class="string">"&#123;'name':'userGroup','users':[&#123;'password':'123123','username':'zhangsan'&#125;,&#123;'password':'321321','username':'lisi'&#125;]&#125;"</span>;</span><br><span class="line">        UserGroup userGroup = JSON.parseObject(userGroupJson, UserGroup<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        System.out.println(<span class="string">"json字符串转复杂java对象: "</span>+userGroup.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">简单java类转字符串： &#123;<span class="string">"password"</span>:<span class="string">"123456"</span>,<span class="string">"username"</span>:<span class="string">"tunan"</span>&#125;</span><br><span class="line">List&lt;Object&gt;转Json字符串： [&#123;<span class="string">"password"</span>:<span class="string">"961228"</span>,<span class="string">"username"</span>:<span class="string">"xiaoqi"</span>&#125;,&#123;<span class="string">"password"</span>:<span class="string">"123456"</span>,<span class="string">"username"</span>:<span class="string">"tunan"</span>&#125;]</span><br><span class="line">复杂java转json字符串： &#123;<span class="string">"name"</span>:<span class="string">"student"</span>,<span class="string">"users"</span>:[&#123;<span class="string">"password"</span>:<span class="string">"961228"</span>,<span class="string">"username"</span>:<span class="string">"xiaoqi"</span>&#125;,&#123;<span class="string">"password"</span>:<span class="string">"123456"</span>,<span class="string">"username"</span>:<span class="string">"tunan"</span>&#125;]&#125;</span><br><span class="line"></span><br><span class="line">json字符串转简单java对象: User&#123;username=<span class="string">'dmego'</span>, password=<span class="string">'123456'</span>&#125;</span><br><span class="line">json字符串转List&lt;Object&gt;对象: [User&#123;username=<span class="string">'zhangsan'</span>, password=<span class="string">'123123'</span>&#125;, User&#123;username=<span class="string">'lisi'</span>, password=<span class="string">'321321'</span>&#125;]</span><br><span class="line">json字符串转复杂java对象: UserGroup&#123;name=<span class="string">'userGroup'</span>, users=[User&#123;username=<span class="string">'zhangsan'</span>, password=<span class="string">'123123'</span>&#125;, User&#123;username=<span class="string">'lisi'</span>, password=<span class="string">'321321'</span>&#125;]&#125;</span><br></pre></td></tr></table></figure><p>通过结果看代码:</p><p>我们可以看到，在<code>JavaBean</code>转<code>Json</code>的时候，使用了<code>JSON.toJSONString()</code>方法</p><ol><li>该方法中传入的是对象，即生成对象的<code>Json</code>字符串</li><li>如果传入的是<code>List</code>，生成的就是对象的<code>Json</code>字符串数组</li><li>如果传入的是复杂类型既有对象，又有对象数组，那么生成的就是个<code>Json</code>对象，这个对象中包括对象的<code>Json</code>字符串和对象的<code>Json</code>字符串数组，<code>Json</code>的字符串数组中又包含对象的<code>Json</code>字符串。</li></ol><p>在<code>Json</code>转<code>JavaBean</code>的时候，会根据调用<code>JSON.parseObject()</code>和<code>JSON.parseArray()</code>的不同，输出不同的结果</p><ol><li>传入对象的<code>Json</code>字符串和需要生成的<code>Java</code>对象，需要调用<code>JSON.parseObject()</code>方法，输出的就是一个普通的对象</li><li>传入对象的<code>Json</code>字符串数组和要生成的<code>Java</code>对象，需要调用<code>JSON.parseArray()</code>方法，输出是一个<code>Java</code>对象数组</li><li>传入的即包含对象的<code>Json</code>字符串，又包含对象的<code>Json</code>字符串数组，那么需要调用一个<code>JSON.parseObject()</code>方法，该方法会输出一个复杂的Java对象，该对象既包括对象又包括对象数组。</li></ol><h2 id="FastJson解析复杂嵌套Json字符串"><a href="#FastJson解析复杂嵌套Json字符串" class="headerlink" title="FastJson解析复杂嵌套Json字符串"></a>FastJson解析复杂嵌套Json字符串</h2><p>这个实例是我在开发中用到的，先给出要解析的Json字符串</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">"id"</span>: <span class="string">"user_list"</span>,</span><br><span class="line">        <span class="string">"key"</span>: <span class="string">"id"</span>,</span><br><span class="line">        <span class="string">"tableName"</span>: <span class="string">"用户列表"</span>,</span><br><span class="line">        <span class="string">"className"</span>: <span class="string">"cn.dmego.domain.User"</span>,</span><br><span class="line">        <span class="string">"column"</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"key"</span>: <span class="string">"rowIndex"</span>,</span><br><span class="line">                <span class="string">"header"</span>: <span class="string">"序号"</span>,</span><br><span class="line">                <span class="string">"width"</span>: <span class="string">"50"</span>,</span><br><span class="line">                <span class="string">"allowSort"</span>: <span class="string">"false"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"key"</span>: <span class="string">"id"</span>,</span><br><span class="line">                <span class="string">"header"</span>: <span class="string">"id"</span>,</span><br><span class="line">                <span class="string">"hidden"</span>: <span class="string">"true"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"key"</span>: <span class="string">"name"</span>,</span><br><span class="line">                <span class="string">"header"</span>: <span class="string">"姓名"</span>,</span><br><span class="line">                <span class="string">"width"</span>: <span class="string">"100"</span>,</span><br><span class="line">                <span class="string">"allowSort"</span>: <span class="string">"true"</span></span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">"id"</span>: <span class="string">"role_list"</span>,</span><br><span class="line">        <span class="string">"key"</span>: <span class="string">"id"</span>,</span><br><span class="line">        <span class="string">"tableName"</span>: <span class="string">"角色列表"</span>,</span><br><span class="line">        <span class="string">"className"</span>: <span class="string">"cn.dmego.domain.Role"</span>,</span><br><span class="line">        <span class="string">"column"</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"key"</span>: <span class="string">"rowIndex"</span>,</span><br><span class="line">                <span class="string">"header"</span>: <span class="string">"序号"</span>,</span><br><span class="line">                <span class="string">"width"</span>: <span class="string">"50"</span>,</span><br><span class="line">                <span class="string">"allowSort"</span>: <span class="string">"false"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"key"</span>: <span class="string">"id"</span>,</span><br><span class="line">                <span class="string">"header"</span>: <span class="string">"id"</span>,</span><br><span class="line">                <span class="string">"hidden"</span>: <span class="string">"true"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"key"</span>: <span class="string">"name"</span>,</span><br><span class="line">                <span class="string">"header"</span>: <span class="string">"名称"</span>,</span><br><span class="line">                <span class="string">"width"</span>: <span class="string">"100"</span>,</span><br><span class="line">                <span class="string">"allowSort"</span>: <span class="string">"true"</span></span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>要想解析这种复杂的字符串，首先得先定义好与之相符的Java POJO 对象，经过观察，我们发现，这个是一个Json对象数组，每一个对象里包含了许多属性，其中还有一个属性的类型也是对象数组。所有，我们从里到外，先定义最里面的对象：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Column</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String key;</span><br><span class="line">    <span class="keyword">private</span> String header;</span><br><span class="line">    <span class="keyword">private</span> String width;</span><br><span class="line">    <span class="keyword">private</span> String allowSort;</span><br><span class="line">    <span class="keyword">private</span> String hidden;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Column&#123;"</span> +</span><br><span class="line">                <span class="string">"key='"</span> + key + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", header='"</span> + header + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", width='"</span> + width + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", allowSort='"</span> + allowSort + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", hidden='"</span> + hidden + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//这里省略部分getter与setter方法 </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>再定义外层的对象：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Query</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String id;</span><br><span class="line">    <span class="keyword">private</span> String key;</span><br><span class="line">    <span class="keyword">private</span> String tableName;</span><br><span class="line">    <span class="keyword">private</span> String className;</span><br><span class="line">    <span class="keyword">private</span> List&lt;Column&gt; column;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Query&#123;"</span> +</span><br><span class="line">                <span class="string">"id='"</span> + id + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", key='"</span> + key + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", tableName='"</span> + tableName + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", className='"</span> + className + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", column="</span> + column +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">     <span class="comment">//这里省略部分getter与setter方法 </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我的这个Json文件放置在类路径下，最后想将这个Json字符串转化为List对象，并且将column 对象数组转化为query对象里的List属性</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>: 将这个json字符串转化为List对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-03-07 10:47</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RoleTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        RoleTest roleTest = <span class="keyword">new</span> RoleTest();</span><br><span class="line">        List&lt;Query&gt; queries = roleTest.JsonToObject();</span><br><span class="line">        <span class="comment">//生成的是个List，需要循环输出</span></span><br><span class="line">        queries.forEach(System.out::println);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span>  List&lt;Query&gt; <span class="title">JsonToObject</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        ClassLoader loader = <span class="keyword">this</span>.getClass().getClassLoader();</span><br><span class="line">        InputStream in = loader.getResourceAsStream(<span class="string">"query.json"</span>);</span><br><span class="line">        <span class="comment">//这里是一次全部读出来了，大数据处理时需要按行读取</span></span><br><span class="line">        String jsonTxet = IOUtils.toString(in, <span class="string">"utf8"</span>);</span><br><span class="line">        List&lt;Query&gt; query = JSON.parseArray(jsonTxet, Query<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="keyword">return</span> query;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Query&#123;id=<span class="string">'user_list'</span>, key=<span class="string">'id'</span>, tableName=<span class="string">'用户列表'</span>, className=<span class="string">'cn.dmego.domain.User'</span>, column=[Column&#123;key=<span class="string">'rowIndex'</span>, header=<span class="string">'序号'</span>, width=<span class="string">'50'</span>, allowSort=<span class="string">'false'</span>, hidden=<span class="string">'null'</span>&#125;, Column&#123;key=<span class="string">'id'</span>, header=<span class="string">'id'</span>, width=<span class="string">'null'</span>, allowSort=<span class="string">'null'</span>, hidden=<span class="string">'true'</span>&#125;, Column&#123;key=<span class="string">'name'</span>, header=<span class="string">'姓名'</span>, width=<span class="string">'100'</span>, allowSort=<span class="string">'true'</span>, hidden=<span class="string">'null'</span>&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Query&#123;id=<span class="string">'role_list'</span>, key=<span class="string">'id'</span>, tableName=<span class="string">'角色列表'</span>, className=<span class="string">'cn.dmego.domain.Role'</span>, column=[Column&#123;key=<span class="string">'rowIndex'</span>, header=<span class="string">'序号'</span>, width=<span class="string">'50'</span>, allowSort=<span class="string">'false'</span>, hidden=<span class="string">'null'</span>&#125;, Column&#123;key=<span class="string">'id'</span>, header=<span class="string">'id'</span>, width=<span class="string">'null'</span>, allowSort=<span class="string">'null'</span>, hidden=<span class="string">'true'</span>&#125;, Column&#123;key=<span class="string">'name'</span>, header=<span class="string">'名称'</span>, width=<span class="string">'100'</span>, allowSort=<span class="string">'true'</span>, hidden=<span class="string">'null'</span>&#125;]&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala之高阶函数</title>
      <link href="/2020/03/03/scala/1/"/>
      <url>/2020/03/03/scala/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>sorted</li><li>sortBy</li><li>sortWith</li><li>flatten</li><li>map</li><li>flatMap</li><li>filter</li><li>groupBy</li><li>flod</li><li>reduce</li><li>wc</li></ol><h2 id="sorted"><a href="#sorted" class="headerlink" title="sorted"></a>sorted</h2><p>排序，默认升序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> array = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">    array.sorted</span><br></pre></td></tr></table></figure><h2 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h2><p>排序，指定排序规则</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> arrMap = <span class="type">Array</span>(('a', <span class="number">1</span>), ('b', <span class="number">4</span>), ('d', <span class="number">3</span>), ('c', <span class="number">2</span>))</span><br><span class="line">    arrMap.sortBy(_._1)</span><br><span class="line">    arrMap.sortBy(_._2)</span><br></pre></td></tr></table></figure><h2 id="sortWith"><a href="#sortWith" class="headerlink" title="sortWith"></a>sortWith</h2><p>排序，多个字段联合排序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> arrMap2 = <span class="type">Array</span>((<span class="string">"a"</span>, <span class="number">2</span>), (<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">2</span>), (<span class="string">"d"</span>, <span class="number">5</span>), (<span class="string">"c"</span>, <span class="number">4</span>))</span><br><span class="line">    arrMap2.sortWith((t1,t2)=&gt;&#123;</span><br><span class="line">      <span class="keyword">if</span> (!t1._1.equalsIgnoreCase(t2._1))&#123;</span><br><span class="line">        t1._1.compareTo(t2._1)&lt;<span class="number">0</span></span><br><span class="line">      &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        t1._2.compareTo(t2._2)&lt;<span class="number">0</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure><h2 id="flatten"><a href="#flatten" class="headerlink" title="flatten"></a>flatten</h2><p>扁平化</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> flatten = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), <span class="type">Array</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>))</span><br><span class="line">    flatten.flatten</span><br></pre></td></tr></table></figure><h2 id="map"><a href="#map" class="headerlink" title="map"></a>map</h2><p>映射</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> map = <span class="type">Array</span>(<span class="string">"this is a demo,hello world"</span>)</span><br><span class="line">    map.map(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    map.map(_.split(<span class="string">" "</span>)).flatten.map((_,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><h2 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h2><p>flatMap ==&gt; flatten + map</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> flatMap = <span class="type">Array</span>(<span class="string">"this is a demo"</span>, <span class="string">"hello word"</span>)</span><br><span class="line">    flatMap.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    flatMap.flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    flatMap.flatMap(line =&gt; (<span class="keyword">for</span> (i&lt;- line.split(<span class="string">" "</span>)) <span class="keyword">yield</span> (i,<span class="number">1</span>)))</span><br></pre></td></tr></table></figure><h2 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h2><p>过滤</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> filter = <span class="type">Array</span>(<span class="string">"this is a demo"</span>, <span class="string">"hello word"</span>)</span><br><span class="line">    filter.filter(_.contains(<span class="string">"hello"</span>))</span><br><span class="line">    filter.filterNot(_.contains(<span class="string">"hello"</span>))</span><br></pre></td></tr></table></figure><h2 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h2><p>分组</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> groupBy = <span class="type">Array</span>(<span class="string">"good"</span>, <span class="string">"good"</span>, <span class="string">"study"</span>)</span><br><span class="line">    groupBy.groupBy(x=&gt;x)</span><br><span class="line">    groupBy.groupBy(x=&gt;x).map(t=&gt;(t._1,t._2.size))</span><br></pre></td></tr></table></figure><h2 id="flod"><a href="#flod" class="headerlink" title="flod"></a>flod</h2><p>聚合，需要指定默认值</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> flod = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">4</span>)</span><br><span class="line">    flod.fold(<span class="number">10</span>)((a,b)=&gt;&#123;</span><br><span class="line">      println(a+<span class="string">"  :  "</span>+b)</span><br><span class="line">      a-b</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure><h2 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h2><p>聚合</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> reduce = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">4</span>)</span><br><span class="line">    reduce.reduce(_+_)</span><br><span class="line">    reduce.reduce(_-_)</span><br><span class="line">    reduce.reduceLeft(_+_)</span><br><span class="line">    reduce.reduceLeft(_-_)</span><br><span class="line">    reduce.reduceRight(_+_)</span><br><span class="line">    reduce.reduceRight(_-_)</span><br></pre></td></tr></table></figure><h2 id="wc"><a href="#wc" class="headerlink" title="wc"></a>wc</h2><p>方案1</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> wc = <span class="type">Array</span>(<span class="string">"this is demo"</span>, <span class="string">"good good study"</span>, <span class="string">"day day up"</span>)</span><br><span class="line">    wc.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .groupBy(word=&gt;word)</span><br><span class="line">      .toList</span><br><span class="line">      .map(t=&gt;(t._1,t._2.size))</span><br><span class="line">      .sortBy(_._1)</span><br><span class="line">      .mkString(<span class="string">","</span>)</span><br></pre></td></tr></table></figure><p>方案2</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">wc.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">     .map((_,<span class="number">1</span>))</span><br><span class="line">     .groupBy(_._1)</span><br><span class="line">     .toList</span><br><span class="line">     .map(t=&gt;(t._1,t._2.size))</span><br><span class="line">     .sortBy(_._1)</span><br><span class="line">     .mkString(<span class="string">"\t"</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zookeeper的安装&amp;使用</title>
      <link href="/2020/03/02/zookeeper/1/"/>
      <url>/2020/03/02/zookeeper/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>什么是分布式</li><li>为什么选择Zookeeper</li><li>Zookeeper的设计目标</li><li>Zookeeper的数据模型</li><li>安装Zookeeper</li><li>单节点配置Zookeeper</li><li>多节点配置Zookeeper</li><li>Zookeeper的常用命令</li><li>Zookeeper的监听</li><li>Zookeeper四字命令</li></ol><h2 id="什么是分布式"><a href="#什么是分布式" class="headerlink" title="什么是分布式"></a>什么是分布式</h2><p>分布式系统</p><p>分布式系统是由一组通过网络进行通信、为了完成共同的任务而协调工作的计算机节点组成的系统。分布式系统的出现是为了用廉价的、普通的机器完成单个计算机无法完成的计算、存储任务。其目的是<strong>利用更多的机器，处理更多的数据</strong>。</p><p>分布式协调技术</p><p>分布式系统的出现带来了分布式协调的问题，也就是我们如何对分布式系统中的进程进行调度，假设我们在某一台机器上面挂载了一个资源，然后其他物理分布的进程都要竞争这个资源，但是我们又不希望他们同时访问，这时候就需要一个协调器，让他们有序的来访问在这个资源。这个协调器就是我们经常提到的那个<strong>锁</strong>，比如说”进程-1”在使用该资源的时候，会先去获得锁，”进程1”获得锁以后会对该资源保持<strong>独占</strong>，这样其他进程就无法访问该资源，”进程1”用完该资源以后就将锁释放掉，让其他进程来获得锁，那么通过这个锁机制，我们就能保证了分布式系统中多个进程能够有序的访问该临界资源。那么我们把这个分布式环境下的这个锁叫作<strong>分布式锁</strong>。分布式锁也就是我们<strong>分布式协调技术</strong>实现的核心内容。</p><h2 id="为什么选择zookeeper"><a href="#为什么选择zookeeper" class="headerlink" title="为什么选择zookeeper"></a>为什么选择zookeeper</h2><p>ZooKeeper是一种为分布式应用所设计的高可用、高性能且一致的开源协调服务，它提供了一项基本服务：<strong>分布式锁服务</strong>。由于ZooKeeper的开源特性，后来我们的开发者在分布式锁的基础上，摸索了出了其他的使用方法：<strong>配置维护、组服务、分布式消息队列</strong>、<strong>分布式通知/协调</strong>等。</p><p>ZooKeeper在实现这些服务时，首先它设计一种新的<strong>数据结构——Znode</strong>，然后在该数据结构的基础上定义了一些<strong>原语</strong>，也就是一些关于该数据结构的一些操作。有了这些数据结构和原语还不够，因为我们的ZooKeeper是工作在一个分布式的环境下，我们的服务是通过消息以网络的形式发送给我们的分布式应用程序，所以还需要一个<strong>通知机制</strong>——Watcher机制。那么总结一下，ZooKeeper所提供的服务主要是通过：数据结构+原语+watcher机制，三个部分来实现的。</p><p>ZooKeeper<strong>性能上的特点</strong>决定了它能够用在大型的、分布式的系统当中。从<strong>可靠性</strong>方面来说，它并不会因为一个节点的错误而崩溃。除此之外，它<strong>严格的序列访问控制</strong>意味着复杂的控制原语可以应用在客户端上。ZooKeeper在一致性、可用性、容错性的保证，也是ZooKeeper的成功之处，它获得的一切成功都与它采用的协议——Zab协议是密不可分的，这些内容将会在后面介绍。</p><h2 id="Zookeeper的设计目标"><a href="#Zookeeper的设计目标" class="headerlink" title="Zookeeper的设计目标"></a>Zookeeper的设计目标</h2><p>ZooKeeper 很简单。ZooKeeper允许分布式进程通过共享的层次命名空间相互协调，该命名空间的组织类似于标准文件系统。名称空间由数据寄存器组成——用ZooKeeper的说法称为znodes——这些寄存器类似于文件和目录。与典型的用于存储的文件系统不同，ZooKeeper数据保存在内存中，这意味着ZooKeeper可以实现高吞吐量和低延迟数。</p><p>ZooKeeper实现非常重视高性能、高可用性和严格有序的访问。ZooKeeper的性能方面意味着它可以用于大型分布式系统。可靠性方面使它不会成为单点故障。严格的排序意味着复杂的同步原语可以在客户端实现。</p><p>ZooKeeper 是复制。与它所协调的分布式进程一样，ZooKeeper本身也打算在一组称为集合的主机上进行复制。</p><p><img src="http://zookeeper.apache.org/doc/current/images/zkservice.jpg" alt="Zookeeper集群部署"></p><p>组成ZooKeeper服务的服务器必须相互了解。它们在内存中维护状态映像，以及持久存储中的事务日志和快照。只要大多数服务器可用，ZooKeeper服务就可用。</p><p>客户端连接到单个ZooKeeper服务器。客户端维护一个TCP连接，通过它发送请求、获取响应、获取监视事件和发送心跳。如果到服务器的TCP连接中断，客户机将连接到另一台服务器。</p><p>ZooKeeper是有序的。ZooKeeper用一个数字来标记每个更新，这个数字反映了所有ZooKeeper事务的顺序。后续操作可以使用该顺序来实现更高级别的抽象，比如同步原语。</p><p>ZooKeeper很快。在“以读取为主”的工作负载中，它的速度特别快。ZooKeeper应用程序运行在数千台机器上，当读操作比写操作更常见时，它的性能最好，比率约为10:1。</p><h2 id="Zookeeper的数据模型"><a href="#Zookeeper的数据模型" class="headerlink" title="Zookeeper的数据模型"></a>Zookeeper的数据模型</h2><p>ZooKeeper拥有一个层次的命名空间，这个和标准的文件系统非常相似，如下图所示。</p><p><img src="https://images0.cnblogs.com/blog/671563/201411/301534562152768.png" alt="ZooKeeper数据模型与文件系统目录树"></p><p>从图中我们可以看出ZooKeeper的数据模型，在结构上和标准文件系统的非常相似，都是采用这种树形层次结构，ZooKeeper树中的每个节点被称为—Znode。和文件系统的目录树一样，ZooKeeper树中的每个节点可以拥有子节点。但也有不同之处：</p><ol><li><p>引用方式</p><p>Zonde通过<strong>路径引用</strong>，如同Unix中的文件路径。路径必须是绝对的，因此他们必须由斜杠字符来<strong>开头</strong>。除此以外，他们必须是唯一的，也就是说每一个路径只有一个表示，因此这些路径不能改变。在ZooKeeper中，路径由Unicode字符串组成，并且有一些限制。字符串”/zookeeper”用以保存管理信息，比如关键配额信息。</p></li><li><p>Znode结构</p><p>ZooKeeper命名空间中的Znode，兼具文件和目录两种特点。既像文件一样维护着数据、元信息、ACL、时间戳等数据结构，又像目录一样可以作为路径标识的一部分。图中的每个节点称为一个Znode。 每个Znode由3部分组成:</p><ul><li><p>stat：此为状态信息, 描述该Znode的版本, 权限等信息</p></li><li><p>data：与该Znode关联的数据</p></li><li><p>children：该Znode下的子节点</p></li></ul><p>ZooKeeper虽然可以关联一些数据，但并没有被设计为常规的数据库或者大数据存储，相反的是，它用来<strong>管理调度数据</strong>，比如分布式应用中的配置文件信息、状态信息、汇集位置等等。这些数据的共同特性就是它们都是很小的数据，通常以KB为大小单位。ZooKeeper的服务器和客户端都被设计为严格检查并限制每个Znode的数据大小至多1M，但常规使用中应该远小于此值。</p></li><li><p>数据访问</p><p>ZooKeeper中的每个节点存储的数据要被<strong>原子性的操作</strong>。也就是说读操作将获取与节点相关的所有数据，写操作也将替换掉节点的所有数据。另外，每一个节点都拥有自己的ACL(访问控制列表)，这个列表规定了用户的权限，即限定了特定用户对目标节点可以执行的操作。</p></li><li><p>节点类型</p><p>ZooKeeper中的节点有两种，分别为<strong>临时节点</strong>和<strong>永久节点</strong>。节点的类型在创建时即被确定，并且不能改变。</p><p>临时节点: </p><ul><li>该节点的生命周期依赖于创建它们的会话。一旦会话(Session)结束，临时节点将被自动删除，当然可以也可以手动删除。虽然每个临时的Znode都会绑定到一个客户端会话，但他们对所有的客户端还是可见的。另外，ZooKeeper的临时节点不允许拥有子节点。</li></ul><p>永久节点：</p><ul><li>该节点的生命周期不依赖于会话，并且只有在客户端显示执行删除操作的时候，他们才能被删除。</li></ul></li><li><p>顺序节点</p><p>当创建Znode的时候，用户可以请求在ZooKeeper的路径结尾添加一个<strong>递增的计数</strong>。这个计数<strong>对于此节点的父节点来说</strong>是唯一的，它的格式为”%10d”(10位数字，没有数值的数位用0补充，例如”0000000001”)。当计数值大于232-1时，计数器将溢出。</p></li><li><p>观察</p><p>客户端可以在节点上设置watch，我们称之为<strong>监视器</strong>。当节点状态发生改变时(Znode的增、删、改)将会触发watch所对应的操作。当watch被触发时，ZooKeeper将会向客户端发送且仅发送一条通知，因为watch只能被触发一次，这样可以减少网络流量。</p></li></ol><h2 id="安装Zookeeper"><a href="#安装Zookeeper" class="headerlink" title="安装Zookeeper"></a>安装Zookeeper</h2><ol><li><p>下载源码包()</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">http:<span class="comment">//archive.cloudera.com/cdh5/cdh/5/zookeeper-3.4.5-cdh5.16.2.tar.gz</span></span><br></pre></td></tr></table></figure></li><li><p>编译</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">yum install ant<span class="comment">//zk的编译不是使用maven的，而是使用ant</span></span><br><span class="line">ant <span class="keyword">package</span></span><br></pre></td></tr></table></figure><p>需要注意两个坑:</p><ol><li>需要指定jdk为1.7</li><li>远端仓库连不上，需要改路径</li></ol></li><li><p>配置环境变量</p></li></ol><h2 id="单节点配置Zookeeper"><a href="#单节点配置Zookeeper" class="headerlink" title="单节点配置Zookeeper"></a>单节点配置Zookeeper</h2><ol><li>修改zoo.cfg文件(需要从zoo_sample.cfg复制一份)</li><li>修改dataDir路径，默认放在/tmp目录下，dataDir主要用于保存Zookeeper中的数据</li><li>启动服务端 <code>bin/zkServer.sh start</code></li><li>启动客户端 <code>bin/zkCli.sh</code></li></ol><h2 id="多节点配置Zookeeper"><a href="#多节点配置Zookeeper" class="headerlink" title="多节点配置Zookeeper"></a>多节点配置Zookeeper</h2><ol><li><p>修改zoo.cfg文件(需要从zoo_sample.cfg复制一份)</p></li><li><p>修改dataDir路径，默认放在/tmp目录下，dataDir主要用于保存Zookeeper中的数据</p></li><li><p>在zoo.cfg文件中配置集群的id、主机和端口号，<code>集群共享内容</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">server<span class="number">.2</span>=hadoop102:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line">server<span class="number">.3</span>=hadoop103:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line">server<span class="number">.4</span>=hadoop104:<span class="number">2888</span>:<span class="number">3888</span></span><br></pre></td></tr></table></figure><p><code>server.A=B:C:D</code><br>A：标识第几号服务器，有一个myid要与之对应<br>B：服务器的地址<br>C：follower与leader通信的端口<br>D：选举leader时的端口</p></li><li><p>在dataDir指定的路径下创建myid文件，添加与之对应的服务器编号，即编号A，<code>集群唯一内容</code>，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。</p></li><li><p>启动集群服务端 <code>bin/zkServer.sh start</code></p></li><li><p>启动集群客户端 ，工作中客户端连接集群命令: </p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">./zkCli.sh -server hadoop102:<span class="number">2181</span>,hadoop103:<span class="number">2181</span>,hadoop104:<span class="number">2181</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="Zookeeper的常用命令"><a href="#Zookeeper的常用命令" class="headerlink" title="Zookeeper的常用命令"></a>Zookeeper的常用命令</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">stat 状态</span><br><span class="line">set修改</span><br><span class="line">ls 查看</span><br><span class="line">ls2 ls+get</span><br><span class="line">delete 删除</span><br><span class="line">rmr 递归删除</span><br><span class="line">get 得到</span><br><span class="line">create 创建</span><br></pre></td></tr></table></figure><ol><li><p><code>stat path [watch]</code>：数据的状态</p><p><code>[zk: localhost:2181(CONNECTED) 1] stat /tunan</code> </p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">cZxid = <span class="number">0x21</span><span class="comment">//节点的id号</span></span><br><span class="line">ctime = Mon Mar <span class="number">02</span> <span class="number">15</span>:<span class="number">27</span>:<span class="number">36</span> CST <span class="number">2020</span><span class="comment">//节点的创建时间</span></span><br><span class="line">mZxid = <span class="number">0x32</span><span class="comment">//节点修改的id号</span></span><br><span class="line">mtime = Mon Mar <span class="number">02</span> <span class="number">15</span>:<span class="number">36</span>:<span class="number">45</span> CST <span class="number">2020</span><span class="comment">//节点修改的时间</span></span><br><span class="line">pZxid = <span class="number">0x38</span><span class="comment">//最后更新的子节点的id</span></span><br><span class="line">cversion = <span class="number">9</span><span class="comment">//节点所拥有的子节点的被修改的版本号</span></span><br><span class="line">dataVersion = <span class="number">1</span><span class="comment">//节点数据版本号</span></span><br><span class="line">aclVersion = <span class="number">0</span><span class="comment">//节点所拥有的ACL版本号</span></span><br><span class="line">ephemeralOwner = <span class="number">0x0</span><span class="comment">//特殊标记 0x0: 永久节点</span></span><br><span class="line">dataLength = <span class="number">3</span><span class="comment">//数据的长度</span></span><br><span class="line">numChildren = <span class="number">5</span><span class="comment">//子节点的个数</span></span><br></pre></td></tr></table></figure></li><li><p><code>set path data [version]</code>：修改数据</p><p><code>[zk: localhost:2181(CONNECTED) 5] set /tunan/aaa 123456</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">cZxid = <span class="number">0x31</span><span class="comment">//这是一个子节点的id跟上面的父节点的id不同</span></span><br><span class="line">ctime = Mon Mar <span class="number">02</span> <span class="number">15</span>:<span class="number">35</span>:<span class="number">31</span> CST <span class="number">2020</span><span class="comment">//子节点的创建时间</span></span><br><span class="line">mZxid = <span class="number">0x3a</span><span class="comment">//子节点的修改id</span></span><br><span class="line">mtime = Tue Mar <span class="number">03</span> <span class="number">11</span>:<span class="number">28</span>:<span class="number">06</span> CST <span class="number">2020</span><span class="comment">//子节点的修改时间</span></span><br><span class="line">pZxid = <span class="number">0x31</span><span class="comment">//最后更新的子节点的id，我们现在更新的是这个节点，所以对应cZxid</span></span><br><span class="line">cversion = <span class="number">0</span><span class="comment">//节点所拥有的子节点的被修改的版本号，明显和父节点不同</span></span><br><span class="line">dataVersion = <span class="number">1</span><span class="comment">//数据被修改过了，所以版本号变了</span></span><br><span class="line">aclVersion = <span class="number">0</span><span class="comment">//节点所拥有的ACL版本号</span></span><br><span class="line">ephemeralOwner = <span class="number">0x0</span><span class="comment">//特殊标记 0x0: 永久节点</span></span><br><span class="line">dataLength = <span class="number">6</span><span class="comment">//数据的长度</span></span><br><span class="line">numChildren = <span class="number">0</span><span class="comment">//子节点的个数</span></span><br></pre></td></tr></table></figure></li><li><p><code>ls path [watch]</code>：查看节点</p><p><code>[zk: localhost:2181(CONNECTED) 8] ls /tunan</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[aaa, ccc, bbb, eee, ddd]</span><br></pre></td></tr></table></figure></li><li><p><code>ls2 path [watch]</code>：查看并获取节点内容，等同于get</p><p><code>[zk: localhost:2181(CONNECTED) 9] ls2 /tunan</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[aaa, ccc, bbb, eee, ddd]<span class="comment">//节点的数据内容</span></span><br></pre></td></tr></table></figure></li><li><p><code>delete path [version]</code>：删除节点</p><p><code>[zk: localhost:2181(CONNECTED) 10] delete /tunan/aaa</code><br><code>[zk: localhost:2181(CONNECTED) 11] ls /tunan</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[ccc, bbb, eee, ddd]</span><br></pre></td></tr></table></figure></li><li><p><code>rmr path</code>：递归删除节点</p><p><code>[zk: localhost:2181(CONNECTED) 12] rmr /tunan/ccc</code><br><code>[zk: localhost:2181(CONNECTED) 13] ls /tunan</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">[bbb, eee, ddd]</span><br></pre></td></tr></table></figure></li><li><p><code>get path [watch]</code>：获取节点内容，等同于ls2</p><p><code>[zk: localhost:2181(CONNECTED) 14] get /tunan/bbb</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="number">222</span></span><br></pre></td></tr></table></figure></li><li><p><code>create [-s] [-e] path data acl</code>：创建节点</p><p>-s：顺序创建</p><p><code>[zk: localhost:2181(CONNECTED) 0] create -s /tunan/bbb 12345</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Created /tunan/bbb0000000007</span><br></pre></td></tr></table></figure><p><code>[zk: localhost:2181(CONNECTED) 1] create -s /tunan/bbb 12345</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Created /tunan/bbb0000000008</span><br></pre></td></tr></table></figure><p>-e：临时创建</p><p><code>[zk: localhost:2181(CONNECTED) 5] create -e /tunan/fff 12345</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Created /tunan/fff</span><br></pre></td></tr></table></figure></li></ol><h2 id="Zookeeper的监听"><a href="#Zookeeper的监听" class="headerlink" title="Zookeeper的监听"></a>Zookeeper的监听</h2><ol><li><p>watch概述</p><p>ZooKeeper可以为所有的<strong>读操作</strong>设置watch，这些读操作包括：exists()、getChildren()及getData()。watch事件是<strong>一次性的触发器</strong>，当watch的对象状态发生改变时，将会触发此对象上watch所对应的事件。watch事件将被<strong>异步</strong>地发送给客户端，并且ZooKeeper为watch机制提供了有序的<strong>一致性保证</strong>。理论上，客户端接收watch事件的时间要快于其看到watch对象状态变化的时间。</p></li><li><p>watch类型</p><p>ZooKeeper所管理的watch可以分为两类：</p><ul><li><p>数据watch(data watches)：<strong>getData</strong>和<strong>exists</strong>负责设置数据watch</p></li><li><p>孩子watch(child watches)：<strong>getChildren</strong>负责设置孩子watch</p></li></ul><p>我们可以通过操作<strong>返回的数据</strong>来设置不同的watch：</p><ul><li>getData和exists：</li><li>getChildren：返回孩子列表</li></ul><p>因此</p><ol><li><p>一个成功的<strong>setData操作</strong>将触发Znode的数据watch</p></li><li><p>一个成功的<strong>create操作</strong>将触发Znode的数据watch以及孩子watch</p></li><li><p>一个成功的<strong>delete操作</strong>将触发Znode的数据watch以及孩子watch</p></li></ol></li><li><p>watch注册与处触发</p><p>图 6.1 watch设置操作及相应的触发器如图下图所示：</p><p><img src="https://images0.cnblogs.com/blog/671563/201411/301534579188980.png" alt="img"></p><ol><li>exists操作上的watch，在被监视的Znode<strong>创建</strong>、<strong>删除</strong>或<strong>数据更新</strong>时被触发。</li><li>getData操作上的watch，在被监视的Znode<strong>删除</strong>或<strong>数据更新</strong>时被触发。在被创建时不能被触发，因为只有</li><li>getChildren操作上的watch，在被监视的Znode的子节点<strong>创建</strong>或<strong>删除</strong>，或是这个Znode自身被<strong>删除</strong>时被触发。可以通过查看watch事件类型来区分是Znode，还是他的子节点被删除：NodeDelete表示Znode被删除，NodeDeletedChanged表示子节点被删除。</li></ol><p>Watch由客户端所连接的ZooKeeper服务器在本地维护，因此watch可以非常容易地设置、管理和分派。当客户端连接到一个新的服务器时，任何的会话事件都将可能触发watch。另外，当从服务器断开连接的时候，watch将不会被接收。但是，当一个客户端重新建立连接的时候，任何先前注册过的watch都会被重新注册。</p></li><li><p>需要注意的几点</p><p>Zookeeper的watch实际上要处理两类事件：</p><ul><li><p>连接状态事件(type=None, path=null)</p><p>这类事件不需要注册，也不需要我们连续触发，我们只要处理就行了。</p></li><li><p>节点事件</p><p>节点的建立，删除，数据的修改。它是one time trigger，我们需要不停的注册触发，还可能发生事件丢失的情况。</p></li></ul><p>上面2类事件都在Watch中处理，也就是重载的<strong>process(Event event)</strong></p><p>节点事件的触发，通过函数exists，getData或getChildren来处理这类函数，有双重作用：</p><ol><li>注册触发事件</li><li>函数本身的功能</li></ol><p>函数的本身的功能又可以用异步的回调函数来实现,重载processResult()过程中处理函数本身的的功能。</p></li><li><p>命令行操作</p><p>监听创建节点(<code>stat命令</code>)</p><p><code>stat /xiaoqi watch</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 13] create /xiaoqi 111</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeCreated path:/xiaoqi</span><br><span class="line">Created /xiaoqi</span><br></pre></td></tr></table></figure><p>监听修改节点(<code>stat命令</code>)</p><p><code>stat /xiaoqi watch</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 16] set /xiaoqi 1234</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeDataChanged path:/xiaoqi</span><br></pre></td></tr></table></figure><p>监听删除节点(<code>stat命令</code>)</p><p><code>stat /xiaoqi watch</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 20] delete /xiaoqi</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeDeleted path:/xiaoqi</span><br></pre></td></tr></table></figure><p>监听父节点删除子节点(<code>ls命令</code>)</p><p><code>ls /xiaoqi watch</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 29] create /xiaoqi/aaa 111</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/xiaoqi</span><br></pre></td></tr></table></figure><p>监听父节点创建子节点(<code>ls命令</code>)</p><p><code>ls /xiaoqi watch</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 31] delete /xiaoqi/aaa</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/xiaoqi</span><br></pre></td></tr></table></figure></li></ol><h2 id="Zookeeper四字命令"><a href="#Zookeeper四字命令" class="headerlink" title="Zookeeper四字命令"></a>Zookeeper四字命令</h2><p><a href="">官网</a></p><p>无须启动zk客户端，直接在命令行输入<code>echo {}| nc localhost 2181</code> 即可</p><p><code>stat:</code> 列出服务器和连接的客户机的简要信息。</p><p>echo stat | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Zookeeper version: <span class="number">3.4</span><span class="number">.5</span>-cdh5<span class="number">.16</span><span class="number">.2</span>--<span class="number">1</span>, built on <span class="number">06</span>/<span class="number">03</span>/<span class="number">2019</span> <span class="number">10</span>:<span class="number">40</span> GMT</span><br><span class="line">Clients:<span class="comment">//客户端</span></span><br><span class="line"> /<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">58056</span>[<span class="number">0</span>](queued=<span class="number">0</span>,recved=<span class="number">1</span>,sent=<span class="number">0</span>)</span><br><span class="line"> /<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">57020</span>[<span class="number">1</span>](queued=<span class="number">0</span>,recved=<span class="number">288</span>,sent=<span class="number">296</span>)</span><br><span class="line"></span><br><span class="line">Latency min/avg/max: <span class="number">0</span>/<span class="number">0</span>/<span class="number">20</span></span><br><span class="line">Received: <span class="number">454</span><span class="comment">//接收</span></span><br><span class="line">Sent: <span class="number">461</span><span class="comment">//发送</span></span><br><span class="line">Connections: <span class="number">2</span><span class="comment">//连接数</span></span><br><span class="line">Outstanding: <span class="number">0</span></span><br><span class="line">Zxid: <span class="number">0x53</span></span><br><span class="line">Mode: standalone<span class="comment">//模式</span></span><br><span class="line">Node count: <span class="number">17</span><span class="comment">//节点数</span></span><br></pre></td></tr></table></figure><p><code>ruok:</code> 测试服务器是否在非错误状态下运行。</p><p>echo ruok | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">imok</span><br></pre></td></tr></table></figure><p><code>dump:</code> 列出未完成的会话和临时节点。这只对leader有效。</p><p>echo dump | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SessionTracker dump:</span><br><span class="line"><span class="function">Session <span class="title">Sets</span> <span class="params">(<span class="number">3</span>)</span>:</span></span><br><span class="line"><span class="function">0 expire at Tue Mar 03 12:29:46 CST 2020:</span></span><br><span class="line"><span class="function">0 expire at Tue Mar 03 12:29:56 CST 2020:</span></span><br><span class="line"><span class="function">1 expire at Tue Mar 03 12:30:06 CST 2020:</span></span><br><span class="line"><span class="function">0x1709e65d9b00001</span></span><br><span class="line"><span class="function">ephemeral nodes dump:</span></span><br><span class="line"><span class="function">Sessions with <span class="title">Ephemerals</span> <span class="params">(<span class="number">1</span>)</span>:</span></span><br><span class="line"><span class="function">0x1709e65d9b00001:</span></span><br><span class="line"><span class="function">/tunan/fff<span class="comment">//临时节点</span></span></span><br></pre></td></tr></table></figure><p><code>conf:</code> 打印关于服务配置的详细信息。</p><p>echo conf | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">clientPort=<span class="number">2181</span><span class="comment">//端口</span></span><br><span class="line">dataDir=/home/hadoop/tmp/zookeeper/version-<span class="number">2</span><span class="comment">//数据目录</span></span><br><span class="line">dataLogDir=/home/hadoop/tmp/zookeeper/version-<span class="number">2</span><span class="comment">//日志目录</span></span><br><span class="line">tickTime=<span class="number">2000</span><span class="comment">//心跳时间，单位毫秒</span></span><br><span class="line">maxClientCnxns=<span class="number">60</span><span class="comment">//最大连接数</span></span><br><span class="line">minSessionTimeout=<span class="number">4000</span><span class="comment">//最小的超时时间</span></span><br><span class="line">maxSessionTimeout=<span class="number">40000</span><span class="comment">//最大的超时时间</span></span><br><span class="line">serverId=<span class="number">0</span><span class="comment">//服务器的id</span></span><br></pre></td></tr></table></figure><p><code>envi:</code> 打印出服务环境的细节</p><p>echo envi | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Environment:</span><br><span class="line">zookeeper.version=<span class="number">3.4</span><span class="number">.5</span>-cdh5<span class="number">.16</span><span class="number">.2</span>--<span class="number">1</span>, built on <span class="number">06</span>/<span class="number">03</span>/<span class="number">2019</span> <span class="number">10</span>:<span class="number">40</span> GMT</span><br><span class="line">host.name=aliyun</span><br><span class="line">java.version=<span class="number">1.8</span><span class="number">.0_144</span></span><br><span class="line">java.vendor=Oracle Corporation</span><br><span class="line">java.home=/usr/java/jdk1<span class="number">.8</span><span class="number">.0_144</span>/jre</span><br><span class="line">java<span class="class">.<span class="keyword">class</span>.<span class="title">path</span></span>=/home/hadoop/app/zookeeper/bin/../build/classes:/home/hadoop/app/zookeeper/bin/../build/lib<span class="comment">/*.jar:/home/hadoop/app/zookeeper/bin/../share/zookeeper/zookeeper-3.4.5-cdh5.16.2.jar:/home/hadoop/app/zookeeper/bin/../share/zookeeper/slf4j-log4j12-1.7.5.jar:/home/hadoop/app/zookeeper/bin/../share/zookeeper/slf4j-api-1.7.5.jar:/home/hadoop/app/zookeeper/bin/../share/zookeeper/netty-3.10.5.Final.jar:/home/hadoop/app/zookeeper/bin/../share/zookeeper/log4j-1.2.16.jar:/home/hadoop/app/zookeeper/bin/../share/zookeeper/jline-2.11.jar:/home/hadoop/app/zookeeper/bin/../src/java/lib/*.jar:/home/hadoop/app/zookeeper/bin/../conf:</span></span><br><span class="line"><span class="comment">java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</span></span><br><span class="line"><span class="comment">java.io.tmpdir=/tmp</span></span><br><span class="line"><span class="comment">java.compiler=&lt;NA&gt;</span></span><br><span class="line"><span class="comment">os.name=Linux</span></span><br><span class="line"><span class="comment">os.arch=amd64</span></span><br><span class="line"><span class="comment">os.version=3.10.0-514.26.2.el7.x86_64</span></span><br><span class="line"><span class="comment">user.name=hadoop</span></span><br><span class="line"><span class="comment">user.home=/home/hadoop</span></span><br><span class="line"><span class="comment">user.dir=/home/hadoop/app/zookeeper-3.4.5-cdh5.16.2/bin</span></span><br></pre></td></tr></table></figure><p><code>mntr:</code> 输出可用于监视集群健康状况的变量列表。</p><p>echo mntr | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">zk_version<span class="number">3.4</span><span class="number">.5</span>-cdh5<span class="number">.16</span><span class="number">.2</span>--<span class="number">1</span>, built on <span class="number">06</span>/<span class="number">03</span>/<span class="number">2019</span> <span class="number">10</span>:<span class="number">40</span> GMT</span><br><span class="line">zk_avg_latency<span class="number">0</span></span><br><span class="line">zk_max_latency<span class="number">20</span></span><br><span class="line">zk_min_latency<span class="number">0</span></span><br><span class="line">zk_packets_received<span class="number">494</span></span><br><span class="line">zk_packets_sent<span class="number">501</span></span><br><span class="line">zk_num_alive_connections<span class="number">2</span></span><br><span class="line">zk_outstanding_requests<span class="number">0</span></span><br><span class="line">zk_server_statestandalone</span><br><span class="line">zk_znode_count<span class="number">17</span></span><br><span class="line">zk_watch_count<span class="number">1</span></span><br><span class="line">zk_ephemerals_count<span class="number">1</span></span><br><span class="line">zk_approximate_data_size<span class="number">253</span></span><br><span class="line">zk_open_file_descriptor_count<span class="number">27</span></span><br><span class="line">zk_max_file_descriptor_count<span class="number">65535</span></span><br><span class="line">zk_fsync_threshold_exceed_count<span class="number">0</span></span><br></pre></td></tr></table></figure><p><code>wchs:</code> 按会话列出关于服务器监视的详细信息。</p><p>echo wchs | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span> connections watching <span class="number">1</span> paths</span><br><span class="line">Total watches:<span class="number">1</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Zookeeper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Zookeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban的安装&amp;使用&amp;坑</title>
      <link href="/2020/02/20/azkaban/1/"/>
      <url>/2020/02/20/azkaban/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>Azkaban的安装</li><li>Azkaban的使用</li><li>Git的安装</li></ol><h2 id="Git的安装"><a href="#Git的安装" class="headerlink" title="Git的安装"></a>Git的安装</h2><p>在安装Azkaban之前要安装Git</p><ol><li><p>获取github最新的Git安装包下载链接，进入Linux服务器，执行下载，命令为： </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https://github.com/git/git/archive/v2.17.0.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>压缩包解压，命令为：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -zxvf v2.17.0.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>安装编译源码所需依赖，命令为：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel gcc perl-ExtUtils-MakeMaker</span><br></pre></td></tr></table></figure><p>耐心等待安装，出现提示输入y即可；</p></li><li><p>安装依赖时，yum自动安装了Git，需要卸载旧版本Git，命令为：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum remove git -y</span><br></pre></td></tr></table></figure></li><li><p>进入解压后的文件夹，命令 cd git-2.17.0 ，然后执行编译，命令为:  </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">make prefix=/usr/local/git all</span><br></pre></td></tr></table></figure></li><li><p>安装Git至/usr/local/git路径，命令为:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">make prefix=/usr/local/git install</span><br></pre></td></tr></table></figure></li><li><p>打开环境变量配置文件，命令 vim /etc/profile ，在底部加上Git相关配置信息：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export GIT_HOME=/usr/local/git/bin</span><br><span class="line">export PATH=$GIT_HOME/bin/:$PATH</span><br></pre></td></tr></table></figure></li><li><p>输入命令 <code>git --version</code> ，查看安装的git版本，校验通过，安装成功。</p></li></ol><h2 id="Azkaban的安装"><a href="#Azkaban的安装" class="headerlink" title="Azkaban的安装"></a>Azkaban的安装</h2><ol><li><p>下载<a href="https://github.com/azkaban/azkaban/releases" target="_blank" rel="noopener">Azkaban</a></p></li><li><p>解压</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xzvf 3.81.0.tar.gz -C ../app/</span><br></pre></td></tr></table></figure></li><li><p>安装<a href="https://azkaban.readthedocs.io/en/latest/getStarted.html" target="_blank" rel="noopener">编译文档</a></p><p>在开始编译之前要下载名为<a href="https://services.gradle.org/distributions/gradle-4.6-all.zip" target="_blank" rel="noopener">gradle-4.6-all.zip</a>的包，如果直接让系统下载超级慢，下载好了放在同目录下，并且在<code>/azkaban/gradle/wrapper/gradle-wrapper.properties</code> 中注释下载路径，并指定已经下载好的路径</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">distributionUrl=gradle-4.6-all.zip</span><br><span class="line"><span class="meta">#</span><span class="bash">distributionUrl=https\://services.gradle.org/distributions/gradle-4.6-all.zip</span></span><br></pre></td></tr></table></figure><p>还需要安装gcc环境:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install -y gcc-c++*</span><br></pre></td></tr></table></figure><p>修改maven仓库为阿里云镜像</p><p><code>vim azkaban-3.81.0/build.gradle 54行左右</code> </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">maven &#123;</span><br><span class="line">//这里是阿里云</span><br><span class="line">url &apos;http://maven.aliyun.com/nexus/content/repositories/central/&apos;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>   正式编译</p>   <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./gradlew build installDist -x test</span><br></pre></td></tr></table></figure><p>   编译后生成三个文件</p>   <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">azkaban-exec-server</span><br><span class="line">azkaban-solo-server</span><br><span class="line">azkaban-web-server</span><br></pre></td></tr></table></figure><ol start="4"><li><p>执行<code>azkaban-solo-server</code></p><ol><li><p>解压<code>azkaban-solo-server</code>到<code>app</code>目录下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xzvf azkaban-solo-server/build/distributionsa/zkaban-solo-server-0.1.0-SNAPSHOT.tar.gz ~/app</span><br></pre></td></tr></table></figure></li><li><p>启动: <code>bin/start-solo.sh</code>，生成<code>AzkabanSingleServer</code>进程</p></li><li><p>修改配置(时区、用户)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">时区: default.timezone.id=Asia/Shanghai</span><br><span class="line">用户: &lt;user password="tunan" roles="admin" username="tunan"/&gt;</span><br></pre></td></tr></table></figure></li><li><p>web端登录，端口8081</p></li></ol></li><li><p>azkaban需要至少3G内存，如果内存不足3G，需要设置不检查内存</p><p>azkaban-web-server-2.7.0/plugins/jobtypes/commonprivate.properties</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">memCheck.enabled=false</span><br></pre></td></tr></table></figure></li></ol><h2 id="Azkaban的使用"><a href="#Azkaban的使用" class="headerlink" title="Azkaban的使用"></a>Azkaban的使用</h2><h3 id="单节点部署"><a href="#单节点部署" class="headerlink" title="单节点部署"></a>单节点部署</h3><p><a href="https://azkaban.readthedocs.io/en/latest/createFlows.html" target="_blank" rel="noopener">官方文档</a></p><ol><li><p>创建flow20.projec文件，写入信息: </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">azkaban-flow-version: 2.0</span><br></pre></td></tr></table></figure></li><li><p>创建basic.flow文件，写入信息:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: jobA</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: echo "This is an echoed text."</span><br></pre></td></tr></table></figure></li><li><p>压缩Archive.zip文件，在web页面中提交</p><p><code>坑</code>: 提交的用户和hadoop上的用户不同</p></li><li><p>多依赖案例</p><p>flow20.projec文件相同，basic.flow文件内容不同，文件名可不同</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: jobC</span><br><span class="line">    type: noop</span><br><span class="line">    # jobC depends on jobA and jobB</span><br><span class="line">    dependsOn:</span><br><span class="line">      - jobA</span><br><span class="line">      - jobB</span><br><span class="line"></span><br><span class="line">  - name: jobA</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: echo "This is an echoed text."</span><br><span class="line"></span><br><span class="line">  - name: jobB</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: pwd</span><br></pre></td></tr></table></figure></li><li><p>wc案例</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: tunan-wordcount</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: hadoop jar /home/hadoop/app/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar wordcount /azkaban/data/wc.txt /out</span><br></pre></td></tr></table></figure><ol><li>直接点击程序修改输出文件参数</li><li>使用Flow Parameters修改输出文件参数</li></ol></li><li><p>hive案例</p><p>除了固定的flow20.project文件，需要修改hive.flow文件和添加stat.sh文件</p><p>hive.flow</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: exe hive</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: sh stat.sh</span><br></pre></td></tr></table></figure><p>stat.sh</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line">hive -e "select * from offline_dw.dws_country_traffic"</span><br></pre></td></tr></table></figure><p>上传相同的文件，新版本会覆盖旧的版本</p></li><li><p>调度执行</p><p>在执行页面的左下角点击Schedule即可，并可在Scheduling中查看调度信息，然后在History中查看历史执行信息</p></li></ol><h3 id="多节点部署"><a href="#多节点部署" class="headerlink" title="多节点部署"></a>多节点部署</h3><ol><li><p>在mysql中创建数据库</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE DATABASE azkaban;</span><br></pre></td></tr></table></figure></li><li><p>创建用户</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE USER 'tunan'@'%' IDENTIFIED BY 'tunan';</span><br></pre></td></tr></table></figure></li><li><p>赋权给用户</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span>,<span class="keyword">INSERT</span>,<span class="keyword">UPDATE</span>,<span class="keyword">DELETE</span> <span class="keyword">ON</span> azkaban.* <span class="keyword">to</span> <span class="string">'tunan'</span>@<span class="string">'%'</span> <span class="keyword">WITH</span> <span class="keyword">GRANT</span> <span class="keyword">OPTION</span>;</span><br></pre></td></tr></table></figure></li><li><p>在mysql中生成需要的表</p><p>这里的source可能会遇到权限问题，解决的办法很多，如移动其他位置</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">use</span> azkaban;</span><br><span class="line">source /home/hadoop/app/azkaban/azkaban-db/build/<span class="keyword">install</span>/azkaban-db/<span class="keyword">create</span>-<span class="keyword">all</span>-<span class="keyword">sql</span><span class="number">-0.1</span><span class="number">.0</span>-SNAPSHOT.sql;</span><br></pre></td></tr></table></figure></li><li><p>执行SQL获取动态端口号</p><p><code>mysql&gt; select * from executors ;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">----+--------+-------+--------+</span></span><br><span class="line">| id | host   | port  | active |</span><br><span class="line">+<span class="comment">----+--------+-------+--------+</span></span><br><span class="line">|  1 | aliyun | 46418 |      0 |</span><br><span class="line">+<span class="comment">----+--------+-------+--------+</span></span><br></pre></td></tr></table></figure></li><li><p>激活执行器</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -G "http://aliyun:46418/executor?action=activate"</span><br></pre></td></tr></table></figure></li><li><p>把<code>azkaban-exec-server</code>和<code>azkaban-web-server</code>解压，并启动</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xzvf azkaban-exec-server/build/distributionsa/azkaban-exec-server-0.1.0-SNAPSHOT.tar.gz ~/app</span><br><span class="line">tar -xzvf azkaban-web-server/build/distributionsa/azkaban-web-server-0.1.0-SNAPSHOT.tar.gz ~/app</span><br></pre></td></tr></table></figure></li><li><p>登录web使用，使用方式和单节点一样</p></li></ol><h3 id="二次开发"><a href="#二次开发" class="headerlink" title="二次开发"></a>二次开发</h3><p>azkaban的二次开发需要调用az已经做好的接口</p><p><a href="https://azkaban.readthedocs.io/en/latest/ajaxApi.html#request-parameters-1" target="_blank" rel="noopener">官方文档</a></p><p>命令行获取session.id</p><p><code>curl -k -X POST --data &quot;action=login&amp;username=azkaban&amp;password=azkaban&quot; http://aliyun:8081</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "session.id" : "f1725f59-9d20-4fe2-b89e-3715ef85****",</span><br><span class="line">  "status" : "success"</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="常用功能"><a href="#常用功能" class="headerlink" title="常用功能"></a>常用功能</h2><ol><li>配置代理用户</li><li>配置hadoop.home</li><li>配置Spark作业提交</li></ol>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CentOS7安装CDH 第九章：CDH中安装Kafka</title>
      <link href="/2019/04/09/cdh/9/"/>
      <url>/2019/04/09/cdh/9/</url>
      
        <content type="html"><![CDATA[<h2 id="CDH官网Kafka的安装教程网址"><a href="#CDH官网Kafka的安装教程网址" class="headerlink" title="CDH官网Kafka的安装教程网址"></a>CDH官网Kafka的安装教程网址</h2><p><a href="https://www.cloudera.com/documentation/kafka/latest/topics/kafka_installing.html#concept_m2t_d45_4r" target="_blank" rel="noopener">点击进入官网</a></p><p><img src="https://yerias.github.io/cdh/kafka-1.png" alt=""></p><h2 id="下载对应的Kafka版本"><a href="#下载对应的Kafka版本" class="headerlink" title="下载对应的Kafka版本"></a>下载对应的Kafka版本</h2><ol><li><p>查看CDH和Kafka的版本对应列表：</p><p><a href="https://www.cloudera.com/documentation/enterprise/release-notes/topics/rn_consolidated_pcm.html#pcm_kafka" target="_blank" rel="noopener">点击进入官网</a></p></li></ol><p><img src="https://yerias.github.io/cdh/kafka-2.png" alt=""></p><ol start="2"><li><p>因为安装的CDH版本为5.10或5.12，故选择的Kafka版本为2.2.x和0.10.2，此时去网站找到对应的Kafka版本：</p><p><a href="https://www.cloudera.com/documentation/kafka/latest/topics/kafka_packaging.html#concept_fzg_phl_br" target="_blank" rel="noopener">点击进入官网</a></p></li></ol><p><img src="https://yerias.github.io/cdh/kafka-3.png" alt=""></p><ol start="3"><li>点击对应的下载地址，下载该Kafka的parcel包（需更改sha1的后缀名）：</li></ol><p><img src="https://yerias.github.io/cdh/kafka-4.png" alt=""></p><ol start="4"><li>最终的主节点上的kafka_parcel包(包的位置任意，最终都要移动到/var/www/html/kafka_parcel目录下)</li></ol><p><img src="https://yerias.github.io/cdh/kafka-4-2.jpg" alt=""></p><h2 id="安装Kafka服务"><a href="#安装Kafka服务" class="headerlink" title="安装Kafka服务"></a>安装Kafka服务</h2><ol><li><p>将主节点上Kafka的parcel包（3个文件）上传到/var/www/html/kafka_parcel目录下，需配置好https服务，请参考上述CDH安装时的方法配置，在浏览器上能访问到如下场景即可：</p><p>安装httpd: <code>yum install -y httpd</code></p></li></ol><p><img src="https://yerias.github.io/cdh/kafka-5.png" alt=""></p><ol start="2"><li>点击CDH主页面中的主机下面的Parcel按钮：</li></ol><p><img src="https://yerias.github.io/cdh/kafka-6.png" alt=""></p><ol start="3"><li>点击Parcel界面的配置按钮，配置Kafka的地址，该地址默认是官网地址，但在CDH的离线安装时已将所有的在线地址删除，所以在这加上Kafka的Parcel包的离线地址即可：</li></ol><p><img src="https://yerias.github.io/cdh/kafka-7.png" alt=""></p><ol start="4"><li>在Parcel界面，点击Kafka的下载按钮：</li></ol><p><img src="https://yerias.github.io/cdh/kafka-8.png" alt=""></p><ol start="5"><li>依次执行Kafka的分配和激活：</li></ol><p><img src="https://yerias.github.io/cdh/kafka-9.png" alt=""></p><p><img src="https://yerias.github.io/cdh/kafka-9-2.png" alt="kafka-9-2"></p><h2 id="将Kafka服务添加到CDH中"><a href="#将Kafka服务添加到CDH中" class="headerlink" title="将Kafka服务添加到CDH中"></a>将Kafka服务添加到CDH中</h2><ol><li>在CDH的主界面点击添加服务按钮，并选择Kafka服务：</li></ol><p><img src="https://yerias.github.io/cdh/kafka-10.png" alt=""></p><ol start="2"><li>给Kafka分配节点（Kafka后面2个服务一般情况下不选）：</li></ol><p><img src="https://yerias.github.io/cdh/kafka-11.png" alt=""></p><p><img src="https://yerias.github.io/cdh/kafka-11-2.png" alt=""></p><ol start="3"><li>Kafka的配置文件进行配置：</li></ol><p>配置Kafka的文件存放目录，因为Kafka是依赖Zookeeper的，所以Kafka的文件也是存放在Zookeeper的目录中，如果要卸载Kafka时，需要将这些Kafka的文件也删除，所以可以把Kafka的文件存放在一个目录中：</p><p>Kafka的文件存放目录：</p><p><img src="https://yerias.github.io/cdh/kafka-12.png" alt=""></p><p>进入Zookeeper的文件管理界面（命令行）：</p><p><img src="https://yerias.github.io/cdh/kafka-13.png" alt=""></p><p><img src="https://yerias.github.io/cdh/kafka-14.png" alt=""></p><p>因为Kafka是一个消息中间键，有将生产者生产的信息进行缓存的操作，所以在配置Kafka的数据存储目录时需要注意，将数据存放到一个比较大的磁盘中，该数据存放的目录如下配置所示：</p><p><img src="https://yerias.github.io/cdh/kafka-15.png" alt=""></p><p>在卸载重装Kafka时，需要将Zookeeper目录下的Kafka文件，以及Kafka数据存放的目录都清空，请注意是每个节点都要清空，否则不能重装。</p><ol start="5"><li>启动Kafka服务，会发现Kafka服务不能成功启动，报错如下：</li></ol><p><img src="https://yerias.github.io/cdh/kafka-16.png" alt=""></p><p>查看日志</p><p><img src="https://yerias.github.io/cdh/kafka-17.png" alt=""></p><p>此时为主机的内存不足，返回Kafka配置文件界面，修改memory中的Java Heap Size of Broker值为512M（如果机器内存充足，可以再大一些），如下：</p><p><img src="https://yerias.github.io/cdh/kafka-18.png" alt=""></p><p>修改之后去CDH的主界面重启Kafka，启动成功，如下所示：</p><p><img src="https://yerias.github.io/cdh/kafka-19.png" alt=""></p><h2 id="测试Kafka"><a href="#测试Kafka" class="headerlink" title="测试Kafka"></a>测试Kafka</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建topic</span></span><br><span class="line">./kafka-topics.sh \</span><br><span class="line">--create \</span><br><span class="line">--zookeeper cdh001:2181,cdh002:2181,cdh003:2181/kafka \</span><br><span class="line">--replication-factor 2 --partitions 3 --topic cloudera</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动生产者</span></span><br><span class="line">./kafka-console-producer.sh \</span><br><span class="line">--broker-list cdh001:9092 ,cdh002:9092 ,cdh003:9092  \</span><br><span class="line">--topic cloudera</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动消费者</span></span><br><span class="line">./kafka-console-consumer.sh \</span><br><span class="line">--bootstrap-server cdh001:9092 ,cdh002:9092 ,cdh003:9092  \</span><br><span class="line">--from-beginning \</span><br><span class="line">--topic cloudera</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除topic</span></span><br><span class="line">./kafka-topics.sh  \</span><br><span class="line">--delete \</span><br><span class="line">--zookeeper  cdh001:2181,cdh002:2181,cdh003:2181/kafka  \</span><br><span class="line">--topic cloudera</span><br></pre></td></tr></table></figure><h2 id="重装Kafka"><a href="#重装Kafka" class="headerlink" title="重装Kafka"></a>重装Kafka</h2><p>除了上文提到的要删除zookeeper中的kafka目录，还要删除Kafka的数据存储目录，即<code>/var/local/kafka/data</code></p>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GIT的常用操作&amp;GITHUB的常用操作&amp;在IDEA中使用GIT操作GITHUB</title>
      <link href="/2019/02/01/git/1/"/>
      <url>/2019/02/01/git/1/</url>
      
        <content type="html"><![CDATA[<h2 id="GIT实战操作"><a href="#GIT实战操作" class="headerlink" title="GIT实战操作"></a>GIT实战操作</h2><ol><li><p>创建版本库</p><p>在项目文件夹内，执行: git init</p></li><li><p>提交文件</p><p>新建文件后，通过git status 进行查看文件状态(可选)</p><p>将文件添加到暂存区  git add 文件名</p><p>或者也可以git commit –m “注释内容”, 直接带注释提交</p></li><li><p>查看文件提交记录</p><p>git log –pretty=oneline 文件名   进行查看历史记录</p></li><li><p>回退历史</p><p>git reset –hard HEAD~n 回退n次操作</p></li><li><p>版本穿越</p><p>进行查看历史记录的版本号，执行 git reflog 文件名</p><p>执行 git reset –hard 版本号</p></li><li><p>还原文件</p><p>git checkout – 文件名 </p></li><li><p>删除某个文件</p><p>先删除文件 git rm 文件名</p><p>再git add 再提交</p></li><li><p>创建分支</p><p>git branch &lt;分支名&gt;</p><p>git branch –v 查看分支</p></li><li><p>切换分支</p><p>git checkout –b &lt;分支名&gt;</p></li><li><p>合并分支</p><p>先切换到主干  git checkout master</p><p>git merge &lt;分支名&gt;</p></li><li><p>合并时冲突</p><p>程序合并时发生冲突系统会提示CONFLICT关键字，命令行后缀会进入MERGING状态，表示此时是解决冲突的状态。</p><p>然后修改冲突文件的内容，再次git add <file> 和git commit 提交后，后缀MERGING消失，说明冲突解决完成。</p></li></ol><h2 id="GITHUB实战操作"><a href="#GITHUB实战操作" class="headerlink" title="GITHUB实战操作"></a>GITHUB实战操作</h2><ol><li><p>搭建代码库</p><ul><li><p>git init</p></li><li><p>git config </p><ul><li>git config –global(全局) user.email “<a href="mailto:you@example.com" target="_blank" rel="noopener">you@example.com</a>”</li><li>git config –global(全局) user.name “Your Name”</li></ul></li></ul></li><li><p>提交代码到本地仓库</p><ul><li><p>git add 文件名</p></li><li><p>git commit –m “注释内容”</p></li></ul></li><li><p>GitHub准备工作：</p><ul><li><p>注册GitHub账号</p></li><li><p>在GitHub搭建项目</p></li></ul></li><li><p>推送代码到远端</p><ul><li><p>git remote add origin <url>(仓库地址)</p></li><li><p>git push origin master</p></li></ul></li><li><p>其他用户克隆</p><p>git clone <url></p></li><li><p>其他用户提交代码到本地仓库</p><ul><li><p>git add 文件名</p></li><li><p>git commit –m “注释内容”</p></li></ul></li><li><p>其他用户推送到远端仓库</p><ul><li>git push origin master</li></ul></li><li><p>其他用户拉取代码</p><ul><li>git pull origin master</li></ul></li><li><p>增加远程地址</p><ul><li>git remote add &lt;远端代号(origin)&gt;  &lt;远端地址&gt;</li></ul></li><li><p>推送到远程库</p><ul><li>git push  &lt;远端代号&gt;  &lt;本地分支名称&gt;</li></ul></li><li><p>合作开发权限</p><p><img src="https://yerias.github.io/git/%E6%B7%BB%E5%8A%A0%E5%90%88%E4%BD%9C%E4%BC%99%E4%BC%B41.jpg" alt="添加合作伙伴1"></p><p><img src="https://yerias.github.io/git/%E6%B7%BB%E5%8A%A0%E5%90%88%E4%BD%9C%E4%BC%99%E4%BC%B42.jpg" alt="添加合作伙伴2"></p></li><li><p>协作冲突</p><p>在上传或同步代码时，由于你和他人都改了同一文件的同一位置的代码，版本管理软件无法判断究竟以谁为准，就会报告冲突,需要程序员手工解决。</p><ul><li><p>修改合并</p></li><li><p>git add 文件名</p></li><li><p>git commit –m “注释内容”</p></li><li><p>git push origin master</p></li></ul></li></ol><h2 id="在IDEA中使用GIT"><a href="#在IDEA中使用GIT" class="headerlink" title="在IDEA中使用GIT"></a>在IDEA中使用GIT</h2><ol><li><p>配置</p><p>setting配置GIT</p><p><img src="https://yerias.github.io/git/%E9%85%8D%E7%BD%AEgit%E6%89%A7%E8%A1%8C%E7%A8%8B%E5%BA%8F.jpg" alt="配置git执行程序"></p></li><li><p>创建仓库</p><p>VCS配置账户密码创建仓库</p><p><img src="https://yerias.github.io/git/%E5%88%9B%E5%BB%BAgithub%E4%BB%93%E5%BA%93.jpg" alt="创建github仓库"></p></li><li><p>提交代码</p><p><img src="https://yerias.github.io/git/%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%811.jpg" alt="提交代码1"></p><p><img src="https://yerias.github.io/git/%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%812.jpg" alt="提交代码2"></p><p><img src="https://yerias.github.io/git/%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%813.jpg" alt="提交代码3"></p></li><li><p>同步代码</p><p><img src="https://yerias.github.io/git/%E5%90%8C%E6%AD%A5%E4%BB%A3%E7%A0%81.jpg" alt="同步代码"></p></li><li><p>克隆项目</p><p><img src="https://yerias.github.io/git/%E5%85%8B%E9%9A%86%E4%BB%A3%E7%A0%811.jpg" alt="克隆代码1"></p><p><img src="https://yerias.github.io/git/%E5%85%8B%E9%9A%86%E4%BB%A3%E7%A0%812.jpg" alt="克隆代码2"></p></li><li><p>解决版本冲突</p><p>代码添加到公共区间再次提交</p><p><img src="https://yerias.github.io/git/%E7%89%88%E6%9C%AC%E5%86%B2%E7%AA%81.jpg" alt="版本冲突"></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IO流&amp;比较器&amp;内部类&amp;Random</title>
      <link href="/2019/01/08/java/8/"/>
      <url>/2019/01/08/java/8/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>IO流</li><li>比较器</li><li>内部类</li><li>Random</li></ol><h2 id="IO流"><a href="#IO流" class="headerlink" title="IO流"></a>IO流</h2><p>Java中的流根据传输方向分为输入输出流，根据操作数据的不同又可以分为字节流和字符流</p><h3 id="字节流"><a href="#字节流" class="headerlink" title="字节流"></a>字节流</h3><p>所有的字节流都继承自InputStream接口和OutputStream接口</p><p>用于文件传输的是FileInputStream类和FileOutputStream类，传输的是字节，使用FileInputStream读取文件时，可以使用byte字节数组建立一个字节数组缓冲区，读取数据时read方法中传入一个字节数组，每次读取一个字节数组的数据，即可实现缓冲区读取数据</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">byte</span>[] buff = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</span><br><span class="line">in.read(buff)  <span class="comment">//数据读进buff中</span></span><br><span class="line">out.write(buff) <span class="comment">//write中传入buff写出数据</span></span><br></pre></td></tr></table></figure><p>在字节流的IO包中提供了两个带缓冲的字节流，分别是BufferedInputStream和BufferedOutputStream，他们的构造方法中分别接受InputStream和OutputStream，类型的参数作为对象，这两</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">new</span> BufferedInputStream(<span class="keyword">new</span> InputStream(filePath));</span><br></pre></td></tr></table></figure><h3 id="字符流"><a href="#字符流" class="headerlink" title="字符流"></a>字符流</h3><h2 id="比较器"><a href="#比较器" class="headerlink" title="比较器"></a>比较器</h2><h2 id="内部类"><a href="#内部类" class="headerlink" title="内部类"></a>内部类</h2><h2 id="Random"><a href="#Random" class="headerlink" title="Random"></a>Random</h2>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA.UTIL包下的TreeSet和迭代器快速失败的的源码解析以及多线程下故障现象、导致原因、以及解决办法和优化建议</title>
      <link href="/2019/01/05/java/6/"/>
      <url>/2019/01/05/java/6/</url>
      
        <content type="html"><![CDATA[<hr><p>该篇博客不适合小白，只做针对性的api源码解析，以及适合我自身的案例研究</p><hr><ol><li><p>使用多个线程往<code>ArrayList</code>中添加元素</p></li><li><p>故障现象</p></li><li><p>故障原因</p></li><li><p>解决方法</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HashMap1.8源码分析</title>
      <link href="/2019/01/04/java/5/"/>
      <url>/2019/01/04/java/5/</url>
      
        <content type="html"><![CDATA[<hr><p>该篇博客不适合小白，只做针对性的api源码解析，以及适合我自身的案例研究</p><hr><h3 id="HashMap构造函数"><a href="#HashMap构造函数" class="headerlink" title="HashMap构造函数"></a>HashMap构造函数</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashMap</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">AbstractMap</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt;</span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">Map</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt;, <span class="title">Cloneable</span>, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始容量</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_INITIAL_CAPACITY = <span class="number">1</span> &lt;&lt; <span class="number">4</span>; <span class="comment">// aka 16</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 最大容量2的30次方</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MAXIMUM_CAPACITY = <span class="number">1</span> &lt;&lt; <span class="number">30</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 负载因子</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">float</span> DEFAULT_LOAD_FACTOR = <span class="number">0.75f</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 链表转树的阈值</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TREEIFY_THRESHOLD = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 树转链表的阈值</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> UNTREEIFY_THRESHOLD = <span class="number">6</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 树最小的子节点数</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MIN_TREEIFY_CAPACITY = <span class="number">64</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 默认初始容量是16，负载因子是0.75</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">HashMap</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.loadFactor = DEFAULT_LOAD_FACTOR; <span class="comment">// all other fields defaulted</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 只传入初始容量</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">HashMap</span><span class="params">(<span class="keyword">int</span> initialCapacity)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>(initialCapacity, DEFAULT_LOAD_FACTOR);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 传入初始容量和负载因子</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HashMap</span><span class="params">(<span class="keyword">int</span> initialCapacity, <span class="keyword">float</span> loadFactor)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (initialCapacity &lt; <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Illegal initial capacity: "</span> +</span><br><span class="line">                                               initialCapacity);</span><br><span class="line">        <span class="keyword">if</span> (initialCapacity &gt; MAXIMUM_CAPACITY)</span><br><span class="line">            initialCapacity = MAXIMUM_CAPACITY;</span><br><span class="line">        <span class="keyword">if</span> (loadFactor &lt;= <span class="number">0</span> || Float.isNaN(loadFactor))</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Illegal load factor: "</span> +</span><br><span class="line">                                               loadFactor);</span><br><span class="line">        <span class="keyword">this</span>.loadFactor = loadFactor;</span><br><span class="line">        <span class="keyword">this</span>.threshold = tableSizeFor(initialCapacity);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 传入一个map集合</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HashMap</span><span class="params">(Map&lt;? extends K, ? extends V&gt; m)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.loadFactor = DEFAULT_LOAD_FACTOR;</span><br><span class="line">        putMapEntries(m, <span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="HashMap的数据结构"><a href="#HashMap的数据结构" class="headerlink" title="HashMap的数据结构"></a>HashMap的数据结构</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">Map</span>.<span class="title">Entry</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// hash值</span></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> hash;</span><br><span class="line">    <span class="comment">// key值</span></span><br><span class="line">    <span class="keyword">final</span> K key;</span><br><span class="line">    <span class="comment">// value值</span></span><br><span class="line">    V value;</span><br><span class="line">    <span class="comment">// 下一个Node的指针</span></span><br><span class="line">    Node&lt;K,V&gt; next;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 构造方法</span></span><br><span class="line">    Node(<span class="keyword">int</span> hash, K key, V value, Node&lt;K,V&gt; next) &#123;</span><br><span class="line">        <span class="keyword">this</span>.hash = hash;</span><br><span class="line">        <span class="keyword">this</span>.key = key;</span><br><span class="line">        <span class="keyword">this</span>.value = value;</span><br><span class="line">        <span class="keyword">this</span>.next = next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 得到key值</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> K <span class="title">getKey</span><span class="params">()</span>        </span>&#123; <span class="keyword">return</span> key; &#125;</span><br><span class="line">    <span class="comment">// 得到vaue值</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> V <span class="title">getValue</span><span class="params">()</span>      </span>&#123; <span class="keyword">return</span> value; &#125;</span><br><span class="line">    <span class="comment">// 重写toString</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> key + <span class="string">"="</span> + value; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// hashCode值等于key和value的hash值取反</span></span><br><span class="line">        <span class="keyword">return</span> Objects.hashCode(key) ^ Objects.hashCode(value);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 修改value</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> V <span class="title">setValue</span><span class="params">(V newValue)</span> </span>&#123;</span><br><span class="line">        V oldValue = value;</span><br><span class="line">        value = newValue;</span><br><span class="line">        <span class="keyword">return</span> oldValue;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断内容相等</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (o == <span class="keyword">this</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        <span class="keyword">if</span> (o <span class="keyword">instanceof</span> Map.Entry) &#123;</span><br><span class="line">            Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o;</span><br><span class="line">            <span class="keyword">if</span> (Objects.equals(key, e.getKey()) &amp;&amp;</span><br><span class="line">                Objects.equals(value, e.getValue()))</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="重温jdk1-7中如何触发死循环的"><a href="#重温jdk1-7中如何触发死循环的" class="headerlink" title="重温jdk1.7中如何触发死循环的"></a>重温jdk1.7中如何触发死循环的</h3><p>单线程情况下，rehash无问题。下图演示了单线程条件下的rehash过程</p><p><img src="https://yerias.github.io/java_img/java_1.png" alt="单线程rehash"></p><p>多线程并发下的rehash</p><p>这里假设有两个线程同时执行了put操作并引发了rehash，执行了transfer方法，并假设线程一进入transfer方法并执行完next = e.next后，因为线程调度所分配时间片用完而“暂停”，此时线程二完成了transfer方法的执行。此时状态如下。</p><p><img src="https://yerias.github.io/java_img/java_2.png" alt="多线程rehash"></p><p>接着线程1被唤醒，继续执行第一轮循环的剩余部分</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">e.next = newTable[<span class="number">1</span>] = <span class="keyword">null</span></span><br><span class="line">newTable[<span class="number">1</span>] = e = key(<span class="number">5</span>)</span><br><span class="line">e = next = key(<span class="number">9</span>)</span><br></pre></td></tr></table></figure><p>结果如下图所示</p><p><img src="https://yerias.github.io/java_img/java_3.png" alt="多线程rehash"></p><p>接着线程1被唤醒，继续执行第一轮循环的剩余部分</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">e.next = newTable[<span class="number">1</span>] = <span class="keyword">null</span></span><br><span class="line">newTable[<span class="number">1</span>] = e = key(<span class="number">5</span>)</span><br><span class="line">e = next = key(<span class="number">9</span>)</span><br></pre></td></tr></table></figure><p>结果如下图所示</p><p><img src="https://yerias.github.io/java_img/java_5.png" alt="多线程rehash"></p><p>接着执行下一轮循环，结果状态图如下所示</p><p><img src="https://yerias.github.io/java_img/java_4.png" alt="多线程rehash"></p><p>此时循环链表形成，并且key(11)无法加入到线程1的新数组。在下一次访问该链表时会出现死循环。</p><h3 id="resize"><a href="#resize" class="headerlink" title="resize()"></a>resize()</h3><p>初始化容量16，负载因子0.75，尾插法，扩容2倍，8个节点转树，6个节点转链表，不会产生死循环</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> Node&lt;K,V&gt;[] resize() &#123;</span><br><span class="line"><span class="comment">// 理解为node/hashmap</span></span><br><span class="line">    Node&lt;K,V&gt;[] oldTab = table;</span><br><span class="line"><span class="comment">// 拿到旧的hashmapnode的长度</span></span><br><span class="line">    <span class="keyword">int</span> oldCap = (oldTab == <span class="keyword">null</span>) ? <span class="number">0</span> : oldTab.length;</span><br><span class="line"><span class="comment">// 旧hashmap的扩容阈值</span></span><br><span class="line">    <span class="keyword">int</span> oldThr = threshold;</span><br><span class="line"><span class="comment">// 新hashmap的容量和阈值初始化为0</span></span><br><span class="line">    <span class="keyword">int</span> newCap, newThr = <span class="number">0</span>;</span><br><span class="line"><span class="comment">// 拿到新hashmap的容量</span></span><br><span class="line">    <span class="keyword">if</span> (oldCap &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">// 旧的hashmap容量大于最大值，则直接返回</span></span><br><span class="line"><span class="keyword">if</span> (oldCap &gt;= MAXIMUM_CAPACITY) &#123;</span><br><span class="line">            threshold = Integer.MAX_VALUE;</span><br><span class="line">            <span class="keyword">return</span> oldTab;</span><br><span class="line">        &#125;</span><br><span class="line"><span class="comment">// 旧的hashmap扩容为原来的两倍</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> ((newCap = oldCap &lt;&lt; <span class="number">1</span>) &lt; MAXIMUM_CAPACITY &amp;&amp;</span><br><span class="line">                 oldCap &gt;= DEFAULT_INITIAL_CAPACITY)</span><br><span class="line">            newThr = oldThr &lt;&lt; <span class="number">1</span>; </span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// 旧的hashmap长度等于0，但是阈值大于0，则把阈值赋值为hashmap的长度</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (oldThr &gt; <span class="number">0</span>) </span><br><span class="line">        newCap = oldThr;</span><br><span class="line"><span class="comment">// 如果旧的hashmap的长度和阈值都为0，则赋初始值</span></span><br><span class="line">    <span class="keyword">else</span> &#123;               </span><br><span class="line">        newCap = DEFAULT_INITIAL_CAPACITY;</span><br><span class="line">        newThr = (<span class="keyword">int</span>)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// 拿到新阈值的值 =&gt; 新容量*0.75</span></span><br><span class="line">    <span class="keyword">if</span> (newThr == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">float</span> ft = (<span class="keyword">float</span>)newCap * loadFactor;</span><br><span class="line">        newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (<span class="keyword">float</span>)MAXIMUM_CAPACITY ?</span><br><span class="line">                  (<span class="keyword">int</span>)ft : Integer.MAX_VALUE);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// 赋值给threshold</span></span><br><span class="line">    threshold = newThr;</span><br><span class="line">    <span class="meta">@SuppressWarnings</span>(&#123;<span class="string">"rawtypes"</span>,<span class="string">"unchecked"</span>&#125;)</span><br><span class="line"><span class="comment">// 创建一个新的容量的Node</span></span><br><span class="line">    Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])<span class="keyword">new</span> Node[newCap];</span><br><span class="line"><span class="comment">// 赋值给table</span></span><br><span class="line">    table = newTab;</span><br><span class="line"><span class="comment">// 尾插法，将旧的table的数据查询出来再插入新的table</span></span><br><span class="line">    <span class="keyword">if</span> (oldTab != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; oldCap; ++j) &#123;</span><br><span class="line">            Node&lt;K,V&gt; e;</span><br><span class="line">            <span class="keyword">if</span> ((e = oldTab[j]) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                oldTab[j] = <span class="keyword">null</span>;</span><br><span class="line">                <span class="keyword">if</span> (e.next == <span class="keyword">null</span>)</span><br><span class="line">                    newTab[e.hash &amp; (newCap - <span class="number">1</span>)] = e;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (e <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line">                    ((TreeNode&lt;K,V&gt;)e).split(<span class="keyword">this</span>, newTab, j, oldCap);</span><br><span class="line">                <span class="keyword">else</span> &#123; <span class="comment">// preserve order</span></span><br><span class="line">                    Node&lt;K,V&gt; loHead = <span class="keyword">null</span>, loTail = <span class="keyword">null</span>;</span><br><span class="line">                    Node&lt;K,V&gt; hiHead = <span class="keyword">null</span>, hiTail = <span class="keyword">null</span>;</span><br><span class="line">                    Node&lt;K,V&gt; next;</span><br><span class="line">                    <span class="keyword">do</span> &#123;</span><br><span class="line">                        next = e.next;</span><br><span class="line">                        <span class="keyword">if</span> ((e.hash &amp; oldCap) == <span class="number">0</span>) &#123;</span><br><span class="line">                            <span class="keyword">if</span> (loTail == <span class="keyword">null</span>)</span><br><span class="line">                                loHead = e;</span><br><span class="line">                            <span class="keyword">else</span></span><br><span class="line">                                loTail.next = e;</span><br><span class="line">                            loTail = e;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">else</span> &#123;</span><br><span class="line">                            <span class="keyword">if</span> (hiTail == <span class="keyword">null</span>)</span><br><span class="line">                                hiHead = e;</span><br><span class="line">                            <span class="keyword">else</span></span><br><span class="line">                                hiTail.next = e;</span><br><span class="line">                            hiTail = e;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125; <span class="keyword">while</span> ((e = next) != <span class="keyword">null</span>);</span><br><span class="line">                    <span class="keyword">if</span> (loTail != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        loTail.next = <span class="keyword">null</span>;</span><br><span class="line">                        newTab[j] = loHead;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span> (hiTail != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        hiTail.next = <span class="keyword">null</span>;</span><br><span class="line">                        newTab[j + oldCap] = hiHead;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// 返回新的Node</span></span><br><span class="line">    <span class="keyword">return</span> newTab;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="hash"><a href="#hash" class="headerlink" title="hash()"></a>hash()</h3><p>使用时异或求hash值，比取余更快更均匀</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">hash</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> h;</span><br><span class="line">    <span class="comment">// '^' 异或操作，相同则为0，不同则为1</span></span><br><span class="line">    <span class="keyword">return</span> (key == <span class="keyword">null</span>) ? <span class="number">0</span> : (h = key.hashCode()) ^ (h &gt;&gt;&gt; <span class="number">16</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="put"><a href="#put" class="headerlink" title="put()"></a>put()</h3><p>如果插入的位置为空则直接插入，如果有值但是key的hash或者内容相等，则覆盖，如果能转树则转树。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">put</span><span class="params">(K key, V value)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 调用putVal方法</span></span><br><span class="line">    <span class="keyword">return</span> putVal(hash(key), key, value, <span class="keyword">false</span>, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">final</span> V <span class="title">putVal</span><span class="params">(<span class="keyword">int</span> hash, K key, V value, <span class="keyword">boolean</span> onlyIfAbsent,</span></span></span><br><span class="line"><span class="function"><span class="params">               <span class="keyword">boolean</span> evict)</span> </span>&#123;</span><br><span class="line">    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; <span class="keyword">int</span> n, i;</span><br><span class="line">    <span class="comment">// 如果当前数组table为null，进行resize()初始化，n = resize的长度</span></span><br><span class="line">    <span class="keyword">if</span> ((tab = table) == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span>)</span><br><span class="line">        n = (tab = resize()).length;</span><br><span class="line">    <span class="comment">// 如果table[i]为空，那就把这个键值对放在table[i]</span></span><br><span class="line">    <span class="keyword">if</span> ((p = tab[i = (n - <span class="number">1</span>) &amp; hash]) == <span class="keyword">null</span>)</span><br><span class="line">        tab[i] = newNode(hash, key, value, <span class="keyword">null</span>);</span><br><span class="line">    <span class="comment">// 当另一个key的hash值已经存在时</span></span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        Node&lt;K,V&gt; e; K k;</span><br><span class="line">        <span class="comment">// 如果节点的key的hash值和容量都相同，则覆盖</span></span><br><span class="line">        <span class="keyword">if</span> (p.hash == hash &amp;&amp;</span><br><span class="line">            ((k = p.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line">            e = p;</span><br><span class="line">        <span class="comment">// Node转成树</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (p <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line">            e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(<span class="keyword">this</span>, tab, hash, key, value);</span><br><span class="line">        <span class="comment">// 遍历table[i]所对应的链表，直到最后一个节点的next为null或者有重复的key值</span></span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> binCount = <span class="number">0</span>; ; ++binCount) &#123;</span><br><span class="line">                <span class="keyword">if</span> ((e = p.next) == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    p.next = newNode(hash, key, value, <span class="keyword">null</span>);</span><br><span class="line">                    <span class="keyword">if</span> (binCount &gt;= TREEIFY_THRESHOLD - <span class="number">1</span>)</span><br><span class="line">                        treeifyBin(tab, hash);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (e.hash == hash &amp;&amp;</span><br><span class="line">                    ((k = e.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                p = e;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// key重复，替换value</span></span><br><span class="line">        <span class="keyword">if</span> (e != <span class="keyword">null</span>) &#123;</span><br><span class="line">            V oldValue = e.value;</span><br><span class="line">            <span class="keyword">if</span> (!onlyIfAbsent || oldValue == <span class="keyword">null</span>)</span><br><span class="line">                e.value = value;</span><br><span class="line">            afterNodeAccess(e);</span><br><span class="line">            <span class="keyword">return</span> oldValue;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    ++modCount;</span><br><span class="line">    <span class="comment">// 触发扩容</span></span><br><span class="line">    <span class="keyword">if</span> (++size &gt; threshold)</span><br><span class="line">        resize();</span><br><span class="line">    afterNodeInsertion(evict);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="get"><a href="#get" class="headerlink" title="get()"></a>get()</h3><p>首先通过<code>hash</code>函数找到索引，然后判断map为null，再判断table[i]是否等于key，然后在找与table相连的链表的key是否相等。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">get</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">Node&lt;K,V&gt; e;</span><br><span class="line"><span class="comment">// 如果拿出来的不为null，就返回value</span></span><br><span class="line"><span class="keyword">return</span> (e = getNode(hash(key), key)) == <span class="keyword">null</span> ? <span class="keyword">null</span> : e.value;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">final</span> Node&lt;K,V&gt; <span class="title">getNode</span><span class="params">(<span class="keyword">int</span> hash, Object key)</span> </span>&#123;</span><br><span class="line">Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; <span class="keyword">int</span> n; K k;</span><br><span class="line"><span class="keyword">if</span> ((tab = table) != <span class="keyword">null</span> &amp;&amp; (n = tab.length) &gt; <span class="number">0</span> &amp;&amp;</span><br><span class="line">(first = tab[(n - <span class="number">1</span>) &amp; hash]) != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="comment">// 如果是第一个元素就返回</span></span><br><span class="line"><span class="keyword">if</span> (first.hash == hash &amp;&amp; <span class="comment">// always check first node</span></span><br><span class="line">((k = first.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line"><span class="keyword">return</span> first;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ((e = first.next) != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="comment">//// 从树中拿</span></span><br><span class="line"><span class="keyword">if</span> (first <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line"><span class="keyword">return</span> ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key);</span><br><span class="line"><span class="keyword">do</span> &#123;</span><br><span class="line"><span class="comment">// 循环链表，key的hash值或者key的内容相同，则返回</span></span><br><span class="line"><span class="keyword">if</span> (e.hash == hash &amp;&amp;</span><br><span class="line">((k = e.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line"><span class="keyword">return</span> e;</span><br><span class="line">&#125; <span class="keyword">while</span> ((e = e.next) != <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 没找到 返回null</span></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>面试题：如果new HashMap(19)，bucket数组多大？</p><ul><li>HashMap的bucket 数组大小一定是2的幂，如果new的时候指定了容量且不是2的幂，实际容量会是最接近(大于)指定容量的2的幂，比如 new HashMap&lt;&gt;(19)，比19大且最接近的2的幂是32，实际容量就是32。</li></ul><p>基础知识</p><p><img src="https://yerias.github.io/java_img/%E4%BD%8D%E8%BF%90%E7%AE%97.png" alt="位运算"></p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HashMap多线程问题</title>
      <link href="/2019/01/04/java/4/"/>
      <url>/2019/01/04/java/4/</url>
      
        <content type="html"><![CDATA[<hr><p>该篇博客不适合小白，只做针对性的api源码解析，以及适合我自身的案例研究</p><h2 id="jdk1-7中的HashMap"><a href="#jdk1-7中的HashMap" class="headerlink" title="jdk1.7中的HashMap"></a>jdk1.7中的HashMap</h2><p>在jdk1.8中对HashMap做了很多优化，这里先分析在jdk1.7中的问题，相信大家都知道在jdk1.7多线程环境下HashMap容易出现死循环，这里我们先用代码来模拟出现死循环的情况：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="number">1</span> <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashMapTest</span> </span>&#123;</span><br><span class="line"> <span class="number">2</span></span><br><span class="line"> <span class="number">3</span>     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"> <span class="number">4</span>         HashMapThread thread0 = <span class="keyword">new</span> HashMapThread();</span><br><span class="line"> <span class="number">5</span>         HashMapThread thread1 = <span class="keyword">new</span> HashMapThread();</span><br><span class="line"> <span class="number">6</span>         HashMapThread thread2 = <span class="keyword">new</span> HashMapThread();</span><br><span class="line"> <span class="number">7</span>         HashMapThread thread3 = <span class="keyword">new</span> HashMapThread();</span><br><span class="line"> <span class="number">8</span>         HashMapThread thread4 = <span class="keyword">new</span> HashMapThread();</span><br><span class="line"> <span class="number">9</span>         thread0.start();</span><br><span class="line"><span class="number">10</span>         thread1.start();</span><br><span class="line"><span class="number">11</span>         thread2.start();</span><br><span class="line"><span class="number">12</span>         thread3.start();</span><br><span class="line"><span class="number">13</span>         thread4.start();</span><br><span class="line"><span class="number">14</span>     &#125;</span><br><span class="line"><span class="number">15</span> &#125;</span><br><span class="line"><span class="number">16</span></span><br><span class="line"><span class="number">17</span> <span class="class"><span class="keyword">class</span> <span class="title">HashMapThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line"><span class="number">18</span>     <span class="keyword">private</span> <span class="keyword">static</span> AtomicInteger ai = <span class="keyword">new</span> AtomicInteger();</span><br><span class="line"><span class="number">19</span>     <span class="keyword">private</span> <span class="keyword">static</span> Map&lt;Integer, Integer&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="number">21</span>     <span class="meta">@Override</span></span><br><span class="line"><span class="number">22</span>     <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="number">23</span>         <span class="keyword">while</span> (ai.get() &lt; <span class="number">1000000</span>) &#123;</span><br><span class="line"><span class="number">24</span>             map.put(ai.get(), ai.get());</span><br><span class="line"><span class="number">25</span>             ai.incrementAndGet();</span><br><span class="line"><span class="number">26</span>         &#125;</span><br><span class="line"><span class="number">27</span>     &#125;</span><br><span class="line"><span class="number">28</span> &#125;</span><br></pre></td></tr></table></figure><p>上述代码比较简单，就是开多个线程不断进行put操作，并且HashMap与AtomicInteger都是全局共享的。在多运行几次该代码后，出现如下死循环情形：</p><p><img src="https://yerias.github.io/java_img/hashmap_1.jpg" alt=""></p><p>其中有几次还会出现数组越界的情况：</p><p><img src="https://yerias.github.io/java_img/hashmap_2.jpg" alt=""></p><p>这里我们着重分析为什么会出现死循环的情况，通过jps和jstack命名查看死循环情况，结果如下：</p><p><img src="https://yerias.github.io/java_img/hashmap_3.jpg" alt=""></p><p>从堆栈信息中可以看到出现死循环的位置，通过该信息可明确知道死循环发生在HashMap的扩容函数中，根源在<strong>transfer函数</strong>中，jdk1.7中HashMap的transfer函数如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="number">1</span>    <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Entry[] newTable, <span class="keyword">boolean</span> rehash)</span> </span>&#123;</span><br><span class="line"> <span class="number">2</span>         <span class="keyword">int</span> newCapacity = newTable.length;</span><br><span class="line"> <span class="number">3</span>         <span class="keyword">for</span> (Entry&lt;K,V&gt; e : table) &#123;</span><br><span class="line"> <span class="number">4</span>             <span class="keyword">while</span>(<span class="keyword">null</span> != e) &#123;</span><br><span class="line"> <span class="number">5</span>                 Entry&lt;K,V&gt; next = e.next;</span><br><span class="line"> <span class="number">6</span>                 <span class="keyword">if</span> (rehash) &#123;</span><br><span class="line"> <span class="number">7</span>                     e.hash = <span class="keyword">null</span> == e.key ? <span class="number">0</span> : hash(e.key);</span><br><span class="line"> <span class="number">8</span>                 &#125;</span><br><span class="line"> <span class="number">9</span>                 <span class="keyword">int</span> i = indexFor(e.hash, newCapacity);</span><br><span class="line"><span class="number">10</span>                 e.next = newTable[i];</span><br><span class="line"><span class="number">11</span>                 newTable[i] = e;</span><br><span class="line"><span class="number">12</span>                 e = next;</span><br><span class="line"><span class="number">13</span>             &#125;</span><br><span class="line"><span class="number">14</span>         &#125;</span><br><span class="line"><span class="number">15</span>     &#125;</span><br></pre></td></tr></table></figure><p>总结下该函数的主要作用：</p><p>在对table进行扩容到newTable后，需要将原来数据转移到newTable中，注意10-12行代码，这里可以看出在转移元素的过程中，使用的是头插法，也就是<strong>链表的顺序会翻转</strong>，这里也是形成死循环的关键点。下面进行详细分析。</p><h2 id="扩容造成死循环分析过程"><a href="#扩容造成死循环分析过程" class="headerlink" title="扩容造成死循环分析过程"></a>扩容造成死循环分析过程</h2><p>这里假设</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">#1.hash算法为简单的用key mod链表的大小。</span><br><span class="line"></span><br><span class="line">#2.最开始hash表size=2，key=3,7,5，则都在table[1]中。</span><br><span class="line"></span><br><span class="line">#3.然后进行resize，使size变成4。</span><br></pre></td></tr></table></figure><p>未resize前的数据结构如下：</p><p><img src="https://yerias.github.io/java_img/hashmap_4.jpg" alt=""></p><p>如果在单线程环境下，最后的结果如下：</p><p><img src="https://yerias.github.io/java_img/hashmap_5.jpg" alt=""></p><p>这里的转移过程，不再进行详述，只要理解transfer函数在做什么，其转移过程以及如何对链表进行反转应该不难。</p><p>然后在多线程环境下，假设有两个线程A和B都在进行put操作。线程A在执行到transfer函数中第11行代码处挂起，因为该函数在这里分析的地位非常重要，因此再次贴出来。</p><p><img src="https://yerias.github.io/java_img/hashmap_6.jpg" alt=""></p><p>此时线程A中运行结果如下：</p><p><img src="https://yerias.github.io/java_img/hashmap_7.jpg" alt=""></p><p>线程A挂起后，此时线程B正常执行，并完成resize操作，结果如下：</p><p><img src="https://yerias.github.io/java_img/hashmap_8.jpg" alt=""></p><p><strong>这里需要特别注意的点：由于线程B已经执行完毕，根据Java内存模型，现在newTable和table中的Entry都是主存中最新值：7.next=3，3.next=null。</strong></p><p>此时切换到线程A上，在线程A挂起时内存中值如下：e=3，next=7，newTable[3]=null，代码执行过程如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">newTable[<span class="number">3</span>]=e ----&gt; newTable[<span class="number">3</span>]=<span class="number">3</span></span><br><span class="line">e=next ----&gt; e=<span class="number">7</span></span><br></pre></td></tr></table></figure><p>此时结果如下：</p><p><img src="https://yerias.github.io/java_img/hashmap_9.jpg" alt=""></p><p>继续循环：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">e=<span class="number">7</span></span><br><span class="line">next=e.next ----&gt; next=<span class="number">3</span>【从主存中取值】</span><br><span class="line">e.next=newTable[<span class="number">3</span>] ----&gt; e.next=<span class="number">3</span>【从主存中取值】</span><br><span class="line">newTable[<span class="number">3</span>]=e ----&gt; newTable[<span class="number">3</span>]=<span class="number">7</span></span><br><span class="line">e=next ----&gt; e=<span class="number">3</span></span><br></pre></td></tr></table></figure><p>结果如下：</p><p><img src="https://yerias.github.io/java_img/hashmap_10.jpg" alt=""></p><p>再次进行循环：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">e=<span class="number">3</span></span><br><span class="line">next=e.next ----&gt; next=<span class="keyword">null</span></span><br><span class="line">e.next=newTable[<span class="number">3</span>] ----&gt; e.next=<span class="number">7</span> 即：<span class="number">3</span>.next=<span class="number">7</span></span><br><span class="line">newTable[<span class="number">3</span>]=e ----&gt; newTable[<span class="number">3</span>]=<span class="number">3</span></span><br><span class="line">e=next ----&gt; e=<span class="keyword">null</span></span><br></pre></td></tr></table></figure><p>注意此次循环：e.next=7，而在上次循环中7.next=3，出现环形链表，并且此时e=null循环结束。</p><p>结果如下：</p><p><img src="https://yerias.github.io/java_img/hashmap_11.jpg" alt=""></p><p>在后续操作中只要涉及轮询hashmap的数据结构，就会在这里发生死循环，造成悲剧。</p><h2 id="扩容造成数据丢失分析过程"><a href="#扩容造成数据丢失分析过程" class="headerlink" title="扩容造成数据丢失分析过程"></a>扩容造成数据丢失分析过程</h2><p>遵照上述分析过程，初始时：</p><p><img src="https://yerias.github.io/java_img/hashmap_12.jpg" alt=""></p><p>线程A和线程B进行put操作，同样线程A挂起：</p><p><img src="https://yerias.github.io/java_img/hashmap_13.jpg" alt=""></p><p>此时线程A的运行结果如下：</p><p><img src="https://yerias.github.io/java_img/hashmap_14.jpg" alt=""></p><p>此时线程B已获得CPU时间片，并完成resize操作：</p><p><img src="https://yerias.github.io/java_img/hashmap_15.jpg" alt=""></p><p>同样注意由于线程B执行完成，newTable和table都为最新值：<strong>5.next=null</strong>。</p><p>此时切换到线程A，在线程A挂起时：<strong>e=7，next=5，newTable[3]=null。</strong></p><p>执行newtable[i]=e，就将<strong>7放在了table[3]</strong>的位置，此时next=5。接着进行下一次循环：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">e=<span class="number">5</span></span><br><span class="line">next=e.next ----&gt; next=<span class="keyword">null</span>，从主存中取值</span><br><span class="line">e.next=newTable[<span class="number">1</span>] ----&gt; e.next=<span class="number">5</span>，从主存中取值</span><br><span class="line">newTable[<span class="number">1</span>]=e ----&gt; newTable[<span class="number">1</span>]=<span class="number">5</span></span><br><span class="line">e=next ----&gt; e=<span class="keyword">null</span></span><br></pre></td></tr></table></figure><p>将5放置在table[1]位置，此时e=null循环结束，<strong>3元素丢失</strong>，并形成<strong>环形链表</strong>。并在后续操作hashmap时造成死循环。</p><p><img src="https://yerias.github.io/java_img/hashmap_16.jpg" alt=""></p><h2 id="jdk1-8中HashMap"><a href="#jdk1-8中HashMap" class="headerlink" title="jdk1.8中HashMap"></a>jdk1.8中HashMap</h2><p>在jdk1.8中对HashMap进行了优化，在发生hash碰撞，不再采用头插法方式，而是直接插入链表尾部，因此不会出现环形链表的情况，但是在多线程的情况下仍然不安全，这里我们看jdk1.8中HashMap的put操作源码：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span>  <span class="function"><span class="keyword">final</span> V <span class="title">putVal</span><span class="params">(<span class="keyword">int</span> hash, K key, V value, <span class="keyword">boolean</span> onlyIfAbsent,</span></span></span><br><span class="line"><span class="function"><span class="params"> <span class="number">2</span>                    <span class="keyword">boolean</span> evict)</span> </span>&#123;</span><br><span class="line"> <span class="number">3</span>         Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; <span class="keyword">int</span> n, i;</span><br><span class="line"> <span class="number">4</span>         <span class="keyword">if</span> ((tab = table) == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span>)</span><br><span class="line"> <span class="number">5</span>             n = (tab = resize()).length;</span><br><span class="line"> <span class="number">6</span>         <span class="keyword">if</span> ((p = tab[i = (n - <span class="number">1</span>) &amp; hash]) == <span class="keyword">null</span>) <span class="comment">// 如果没有hash碰撞则直接插入元素</span></span><br><span class="line"> <span class="number">7</span>             tab[i] = newNode(hash, key, value, <span class="keyword">null</span>);</span><br><span class="line"> <span class="number">8</span>         <span class="keyword">else</span> &#123;</span><br><span class="line"> <span class="number">9</span>             Node&lt;K,V&gt; e; K k;</span><br><span class="line"><span class="number">10</span>             <span class="keyword">if</span> (p.hash == hash &amp;&amp;</span><br><span class="line"><span class="number">11</span>                 ((k = p.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line"><span class="number">12</span>                 e = p;</span><br><span class="line"><span class="number">13</span>             <span class="keyword">else</span> <span class="keyword">if</span> (p <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line"><span class="number">14</span>                 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(<span class="keyword">this</span>, tab, hash, key, value);</span><br><span class="line"><span class="number">15</span>             <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="number">16</span>                 <span class="keyword">for</span> (<span class="keyword">int</span> binCount = <span class="number">0</span>; ; ++binCount) &#123;</span><br><span class="line"><span class="number">17</span>                     <span class="keyword">if</span> ((e = p.next) == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="number">18</span>                         p.next = newNode(hash, key, value, <span class="keyword">null</span>);</span><br><span class="line"><span class="number">19</span>                         <span class="keyword">if</span> (binCount &gt;= TREEIFY_THRESHOLD - <span class="number">1</span>) <span class="comment">// -1 for 1st</span></span><br><span class="line"><span class="number">20</span>                             treeifyBin(tab, hash);</span><br><span class="line"><span class="number">21</span>                         <span class="keyword">break</span>;</span><br><span class="line"><span class="number">22</span>                     &#125;</span><br><span class="line"><span class="number">23</span>                     <span class="keyword">if</span> (e.hash == hash &amp;&amp;</span><br><span class="line"><span class="number">24</span>                         ((k = e.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line"><span class="number">25</span>                         <span class="keyword">break</span>;</span><br><span class="line"><span class="number">26</span>                     p = e;</span><br><span class="line"><span class="number">27</span>                 &#125;</span><br><span class="line"><span class="number">28</span>             &#125;</span><br><span class="line"><span class="number">29</span>             <span class="keyword">if</span> (e != <span class="keyword">null</span>) &#123; <span class="comment">// existing mapping for key</span></span><br><span class="line"><span class="number">30</span>                 V oldValue = e.value;</span><br><span class="line"><span class="number">31</span>                 <span class="keyword">if</span> (!onlyIfAbsent || oldValue == <span class="keyword">null</span>)</span><br><span class="line"><span class="number">32</span>                     e.value = value;</span><br><span class="line"><span class="number">33</span>                 afterNodeAccess(e);</span><br><span class="line"><span class="number">34</span>                 <span class="keyword">return</span> oldValue;</span><br><span class="line"><span class="number">35</span>             &#125;</span><br><span class="line"><span class="number">36</span>         &#125;</span><br><span class="line"><span class="number">37</span>         ++modCount;</span><br><span class="line"><span class="number">38</span>         <span class="keyword">if</span> (++size &gt; threshold)</span><br><span class="line"><span class="number">39</span>             resize();</span><br><span class="line"><span class="number">40</span>         afterNodeInsertion(evict);</span><br><span class="line"><span class="number">41</span>         <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line"><span class="number">42</span>     &#125;</span><br></pre></td></tr></table></figure><p>这是jdk1.8中HashMap中put操作的主函数， 注意第6行代码，如果没有hash碰撞则会直接插入元素。如果线程A和线程B同时进行put操作，刚好这两条不同的数据hash值一样，并且该位置数据为null，所以这线程A、B都会进入第6行代码中。假设一种情况，线程A进入后还未进行数据插入时挂起，而线程B正常执行，从而正常插入数据，然后线程A获取CPU时间片，此时线程A不用再进行hash判断了，问题出现：线程A会把线程B插入的数据给<strong>覆盖</strong>，发生线程不安全。</p><p>这里只是简要分析下jdk1.8中HashMap出现的线程不安全问题的体现，后续将会对java的集合框架进行总结，到时再进行具体分析。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>首先HashMap是不安全的，其主要体现:</p><ol><li>在jdk1.7中，多线程环境下，扩容时会造成环形链表或丢失数据</li><li>在jdk18中，多线程环境下，会发生数据覆盖的情况</li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA.UTIL包下的LINKEDLIST和迭代器快速失败的源码解析以及多线程下故障现象、导致原因、以及解决办法和优化建议</title>
      <link href="/2019/01/03/java/3/"/>
      <url>/2019/01/03/java/3/</url>
      
        <content type="html"><![CDATA[<hr><p>该篇博客不适合小白，只做针对性的api源码解析，以及适合我自身的案例研究</p><hr><ol><li><p>使用多个线程往<code>LinkedList</code>中添加元素</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">50</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">        list.add(UUID.randomUUID().toString().substring(<span class="number">0</span>,<span class="number">8</span>));</span><br><span class="line">        System.out.println(list);</span><br><span class="line">    &#125;,String.valueOf(i)).start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>故障现象</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Exception in thread <span class="string">"3"</span> java.util.ConcurrentModificationException</span><br></pre></td></tr></table></figure><p>我们知道<code>LinkedList</code>是线程不安全的，当多线程操作时，线程操作迭代器的同时其他线程改变了元素的值就会产生<code>ConcurrentModificationException</code>异常，<code>ConcurrentModificationException</code>是在操作<code>Iterator</code>时抛出的异常</p></li><li><p>故障原因</p><p>从前一篇的<code>ArrayList</code>中的解析中得出，我们最终都会经过迭代器中的代码检查<code>modCount</code> 和 <code>expectedModCount</code>的值相不相等</p><p>所以我们这里就不追踪栈异常信息，而直接看看源码中的操作是如何让<code>modCount</code> 和<code>expectedModCount</code>不相等的</p><ol><li><p>首先找出问题的关键代码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">      <span class="keyword">private</span> Node&lt;E&gt; lastReturned;<span class="comment">//最近一次返回的节点，也是当前持有的节点</span></span><br><span class="line">      <span class="keyword">private</span> Node&lt;E&gt; next;<span class="comment">//对下一个元素的引用</span></span><br><span class="line">      <span class="keyword">private</span> <span class="keyword">int</span> nextIndex;<span class="comment">//下一个节点的索引</span></span><br><span class="line">      <span class="keyword">private</span> <span class="keyword">int</span> expectedModCount = modCount;</span><br><span class="line">      </span><br><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">          checkForComodification();<span class="comment">//每次添加元素前，都检查一次modCount和expectedModCount是否相等，如果不相等就直接返回ConcurrentModificationException异常，产生快速失败,多线程环境下这里通不过</span></span><br><span class="line">          <span class="keyword">if</span> (!hasNext())</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> NoSuchElementException();</span><br><span class="line">      </span><br><span class="line">          lastReturned = next;<span class="comment">//当前节点--&gt;下一个节点</span></span><br><span class="line">          next = next.next;<span class="comment">//下下个节点的指正往前移一个节点</span></span><br><span class="line">          nextIndex++;<span class="comment">//index++</span></span><br><span class="line">          <span class="keyword">return</span> lastReturned.item; <span class="comment">//返回移动后的节点的数据</span></span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">checkForComodification</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (modCount != expectedModCount)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>在解析如何修改<code>modCount</code> 值之前我们应该弄明白，<code>LinkedList</code>中的<code>Node</code>节点是如何实现的</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="comment">//私有节点类Node，在1.8版本之前叫做Entry</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span>&lt;<span class="title">E</span>&gt; </span>&#123;</span><br><span class="line">       E item;<span class="comment">//存储的元素</span></span><br><span class="line">       Node&lt;E&gt; next;<span class="comment">//后继结点</span></span><br><span class="line">       Node&lt;E&gt; prev;<span class="comment">//前驱结点</span></span><br><span class="line">      </span><br><span class="line">       <span class="comment">// 前驱结点、存储的元素和后继结点作为参数的构造方法</span></span><br><span class="line">       Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123;</span><br><span class="line">           <span class="keyword">this</span>.item = element;</span><br><span class="line">           <span class="keyword">this</span>.next = next;</span><br><span class="line">           <span class="keyword">this</span>.prev = prev;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></li><li><p>源码中是如何修改<code>modCount</code> 的值的(不包括作为队列和双端队列的方法)(开始怼源码)</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">transient</span> <span class="keyword">int</span> size = <span class="number">0</span>;  <span class="comment">//元素数量</span></span><br><span class="line"><span class="keyword">transient</span> Node&lt;E&gt; first;  <span class="comment">//首节点引用，为null</span></span><br><span class="line"><span class="keyword">transient</span> Node&lt;E&gt; last;  <span class="comment">//尾结点引用，为null</span></span><br><span class="line">      </span><br><span class="line"><span class="comment">//追加一个元素到列表的末尾</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">add</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//把元素存放到链表的末尾</span></span><br><span class="line">       linkLast(e);</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//尾部添加元素</span></span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">linkLast</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//获取当前尾节点的引用</span></span><br><span class="line">       <span class="keyword">final</span> Node&lt;E&gt; l = last;</span><br><span class="line">       <span class="comment">//构建一个新节点，prev的值为l(尾节点)、节点数据为e、next的值为null</span></span><br><span class="line">       <span class="keyword">final</span> Node&lt;E&gt; newNode = <span class="keyword">new</span> Node&lt;&gt;(l, e, <span class="keyword">null</span>);</span><br><span class="line">       <span class="comment">//新节点作为尾结点</span></span><br><span class="line">       last = newNode;</span><br><span class="line">       <span class="comment">//如果原尾节点为null</span></span><br><span class="line">       <span class="keyword">if</span> (l == <span class="keyword">null</span>)</span><br><span class="line">           <span class="comment">//即原链表为null，链表的首节点也是newNode</span></span><br><span class="line">           first = newNode;</span><br><span class="line">       <span class="keyword">else</span></span><br><span class="line">           <span class="comment">//否则，原节点的next设置为newNode</span></span><br><span class="line">           l.next = newNode;</span><br><span class="line">       <span class="comment">//数量+1</span></span><br><span class="line">       size++;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//将指定元素插入到列表中的指定位置。</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> index, E element)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查index是否在有效范围内</span></span><br><span class="line">       checkPositionIndex(index);</span><br><span class="line"><span class="comment">//如果index==size，说明添加的位置是末尾</span></span><br><span class="line">       <span class="keyword">if</span> (index == size)</span><br><span class="line">           <span class="comment">//在末尾添加元素</span></span><br><span class="line">           linkLast(element);</span><br><span class="line">       <span class="keyword">else</span></span><br><span class="line">           <span class="comment">//把element元素插入到index指定的节点位置</span></span><br><span class="line">           linkBefore(element, node(index));</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//检查index是否在有效范围内</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">checkPositionIndex</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (!isPositionIndex(index))</span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> IndexOutOfBoundsException(outOfBoundsMsg(index));</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">isPositionIndex</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">return</span> index &gt;= <span class="number">0</span> &amp;&amp; index &lt;= size;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//返回指定元素索引处的(非空)节点。(注意这里有技巧)</span></span><br><span class="line">   <span class="function">Node&lt;E&gt; <span class="title">node</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//如果index小于size的1/2长度</span></span><br><span class="line">       <span class="keyword">if</span> (index &lt; (size &gt;&gt; <span class="number">1</span>)) &#123;</span><br><span class="line">           <span class="comment">//获取头结点的引用</span></span><br><span class="line">           Node&lt;E&gt; x = first;</span><br><span class="line">           <span class="comment">//从前到后遍历index个节点，最后返回</span></span><br><span class="line">           <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; index; i++)</span><br><span class="line">               x = x.next;</span><br><span class="line">           <span class="keyword">return</span> x;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="comment">//否则获取最后一个节点的引用</span></span><br><span class="line">           Node&lt;E&gt; x = last;</span><br><span class="line">           <span class="comment">//从后到前遍历index个节点，最后返回</span></span><br><span class="line">           <span class="keyword">for</span> (<span class="keyword">int</span> i = size - <span class="number">1</span>; i &gt; index; i--)</span><br><span class="line">               x = x.prev;</span><br><span class="line">           <span class="keyword">return</span> x;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//在非空节点succ之前插入元素e。</span></span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">linkBefore</span><span class="params">(E e, Node&lt;E&gt; succ)</span> </span>&#123;</span><br><span class="line"><span class="comment">//获取succ节点(index)的前一个节点的引用</span></span><br><span class="line">       <span class="keyword">final</span> Node&lt;E&gt; pred = succ.prev;</span><br><span class="line">       <span class="comment">//创建一个新的节点，维护的新节点的前一个节点是pred、元素本身、和元素的下一个节点succ</span></span><br><span class="line">       <span class="keyword">final</span> Node&lt;E&gt; newNode = <span class="keyword">new</span> Node&lt;&gt;(pred, e, succ);</span><br><span class="line">       <span class="comment">//succ节点的上一个节点指向newNode</span></span><br><span class="line">       succ.prev = newNode;</span><br><span class="line">       <span class="keyword">if</span> (pred == <span class="keyword">null</span>)</span><br><span class="line">           <span class="comment">//如果pred节点为null，则说明没有值，则newNode就是第一个节点</span></span><br><span class="line">           first = newNode;</span><br><span class="line">       <span class="keyword">else</span></span><br><span class="line">           <span class="comment">//否则pred的下一个节点指向newNode</span></span><br><span class="line">           pred.next = newNode;</span><br><span class="line">       <span class="comment">//数量+1</span></span><br><span class="line">       size++;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//将指定集合中的所有元素追加到这个列表</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">addAll</span><span class="params">(Collection&lt;? extends E&gt; c)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//调用了插入集合元素的方法，指定了index=size</span></span><br><span class="line">       <span class="keyword">return</span> addAll(size, c);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//将指定集合中的所有元素插入其中列表</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">addAll</span><span class="params">(<span class="keyword">int</span> index, Collection&lt;? extends E&gt; c)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查index是否有效</span></span><br><span class="line">       checkPositionIndex(index);</span><br><span class="line"><span class="comment">//先把集合转换成数组，这也就限定了集合必须是List下的实现类</span></span><br><span class="line">       Object[] a = c.toArray();</span><br><span class="line">       <span class="comment">//获取集合的长度</span></span><br><span class="line">       <span class="keyword">int</span> numNew = a.length;</span><br><span class="line">       <span class="comment">//如果集合的长度为0，即为空</span></span><br><span class="line">       <span class="keyword">if</span> (numNew == <span class="number">0</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">       </span><br><span class="line"><span class="comment">//定义两个节点，succ指向当前需要插入节点的位置，pred指向其前一个节点</span></span><br><span class="line">    Node&lt;E&gt; pred, succ;</span><br><span class="line">       <span class="comment">//如果在尾部插入元素</span></span><br><span class="line">       <span class="keyword">if</span> (index == size) &#123;</span><br><span class="line">           <span class="comment">//当前节点为空</span></span><br><span class="line">           succ = <span class="keyword">null</span>;</span><br><span class="line">           <span class="comment">//当前节点的上一个节点就是最后一个节点</span></span><br><span class="line">           pred = last;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="comment">//否则获取index位置的节点指向succ</span></span><br><span class="line">           succ = node(index);</span><br><span class="line">           <span class="comment">//index位置节点的前一个引用指向pred</span></span><br><span class="line">           pred = succ.prev;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="comment">//遍历集合，每一个节点都做重复创建，添加引用</span></span><br><span class="line">       <span class="keyword">for</span> (Object o : a) &#123;</span><br><span class="line">           <span class="meta">@SuppressWarnings</span>(<span class="string">"unchecked"</span>) E e = (E) o;</span><br><span class="line">           <span class="comment">//创建节点，该节点的pred指向pred节点(最初的index节点的前一个节点)，元素本身e，下一个节点指向null。</span></span><br><span class="line">           Node&lt;E&gt; newNode = <span class="keyword">new</span> Node&lt;&gt;(pred, e, <span class="keyword">null</span>);</span><br><span class="line">           <span class="comment">//判断是否为链表头</span></span><br><span class="line">           <span class="keyword">if</span> (pred == <span class="keyword">null</span>)</span><br><span class="line">               first = newNode;</span><br><span class="line">           <span class="keyword">else</span></span><br><span class="line">               <span class="comment">//pred节点的next也指向新节点</span></span><br><span class="line">               pred.next = newNode;</span><br><span class="line">           <span class="comment">//新节点又继续作为pred节点存在</span></span><br><span class="line">           pred = newNode;</span><br><span class="line">       &#125;</span><br><span class="line"><span class="comment">//集合遍历完成后，如果当前节点为空节点，即在链表的末尾添加元素，就把pred指向尾结点，succ节点不存在</span></span><br><span class="line">       <span class="keyword">if</span> (succ == <span class="keyword">null</span>) &#123;</span><br><span class="line">           last = pred;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="comment">//如果是在链表中间插入的集合，当前节点succ是作位pred的next元素存在</span></span><br><span class="line">           pred.next = succ;</span><br><span class="line">           <span class="comment">//同时pred指向succ的上一个节点引用，相互引用</span></span><br><span class="line">           succ.prev = pred;</span><br><span class="line">       &#125;</span><br><span class="line"><span class="comment">//size+numNew个数</span></span><br><span class="line">       size += numNew;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除列表中指定位置的元素。</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> E <span class="title">remove</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查index是否有效</span></span><br><span class="line">       checkElementIndex(index);</span><br><span class="line">       <span class="comment">//删除index节点的元素</span></span><br><span class="line">       <span class="keyword">return</span> unlink(node(index));</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//删除节点</span></span><br><span class="line">   <span class="function">E <span class="title">unlink</span><span class="params">(Node&lt;E&gt; x)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//获取节点的元素</span></span><br><span class="line">       <span class="keyword">final</span> E element = x.item;</span><br><span class="line">       <span class="comment">//获取元素的下一个节点的引用</span></span><br><span class="line">       <span class="keyword">final</span> Node&lt;E&gt; next = x.next;</span><br><span class="line">       <span class="comment">//获取元素的上一个节点的引用</span></span><br><span class="line">       <span class="keyword">final</span> Node&lt;E&gt; prev = x.prev;</span><br><span class="line">      </span><br><span class="line">       <span class="comment">//如果上一个节点等于null，则说明前面没有节点，把next节点的下一个节点引用为第一个节点</span></span><br><span class="line">       <span class="keyword">if</span> (prev == <span class="keyword">null</span>) &#123;</span><br><span class="line">           first = next;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="comment">//否则把next节点引用为prev节点的next指向的节点</span></span><br><span class="line">           prev.next = next;</span><br><span class="line">           <span class="comment">//x节点的上一个节点置空</span></span><br><span class="line">           x.prev = <span class="keyword">null</span>;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="comment">//如果next节点等于null，为空值，就直接把prev赋值为last节点</span></span><br><span class="line">       <span class="keyword">if</span> (next == <span class="keyword">null</span>) &#123;</span><br><span class="line">           last = prev;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="comment">//否则把next节点的上一个节点指向prev</span></span><br><span class="line">           next.prev = prev;</span><br><span class="line">           <span class="comment">//x节点的下一个节点置空</span></span><br><span class="line">           x.next = <span class="keyword">null</span>;</span><br><span class="line">       &#125;</span><br><span class="line"><span class="comment">//最后把x的数据置空</span></span><br><span class="line">       x.item = <span class="keyword">null</span>;</span><br><span class="line">       size--;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">       <span class="comment">//返回已经删除的元素值</span></span><br><span class="line">       <span class="keyword">return</span> element;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//从列表中删除指定元素的第一个匹配项</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">remove</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//如果对象o等于null(这里说明linkedList允许存放null值)</span></span><br><span class="line">       <span class="keyword">if</span> (o == <span class="keyword">null</span>) &#123;</span><br><span class="line">           <span class="comment">//从头到尾遍历找出第一个符合数据为null的节点</span></span><br><span class="line">           <span class="keyword">for</span> (Node&lt;E&gt; x = first; x != <span class="keyword">null</span>; x = x.next) &#123;</span><br><span class="line">               <span class="keyword">if</span> (x.item == <span class="keyword">null</span>) &#123;</span><br><span class="line">                   <span class="comment">//删除节点</span></span><br><span class="line">                   unlink(x);</span><br><span class="line">                   <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="comment">//否则 从头到尾遍历找出第一个符合数据为null的节点</span></span><br><span class="line">           <span class="keyword">for</span> (Node&lt;E&gt; x = first; x != <span class="keyword">null</span>; x = x.next) &#123;</span><br><span class="line">               <span class="comment">//找出符合qeuals条件的对象</span></span><br><span class="line">               <span class="keyword">if</span> (o.equals(x.item)) &#123;</span><br><span class="line">                   <span class="comment">//删除</span></span><br><span class="line">                   unlink(x);</span><br><span class="line">                   <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//从列表中删除所有元素。</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">//注意使用的是for循环遍历全部节点</span></span><br><span class="line">           <span class="keyword">for</span> (Node&lt;E&gt; x = first; x != <span class="keyword">null</span>; ) &#123;</span><br><span class="line">               <span class="comment">//每个节点的所有信息全部置空</span></span><br><span class="line">               Node&lt;E&gt; next = x.next;</span><br><span class="line">               x.item = <span class="keyword">null</span>;</span><br><span class="line">               x.next = <span class="keyword">null</span>;</span><br><span class="line">               x.prev = <span class="keyword">null</span>;</span><br><span class="line">               <span class="comment">//循环赋值</span></span><br><span class="line">               x = next;</span><br><span class="line">           &#125;</span><br><span class="line">      <span class="comment">//首节点和尾结点赋值为null</span></span><br><span class="line">           first = last = <span class="keyword">null</span>;</span><br><span class="line">           size = <span class="number">0</span>;</span><br><span class="line">           modCount++;</span><br><span class="line">       &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//获取指定位置节点元素</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> E <span class="title">get</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查index是否有效</span></span><br><span class="line">       checkElementIndex(index);</span><br><span class="line">       <span class="comment">//返回index节点的数据</span></span><br><span class="line">       <span class="keyword">return</span> node(index).item;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//将列表中指定位置的元素替换为指定元素。</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> E <span class="title">set</span><span class="params">(<span class="keyword">int</span> index, E element)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查index是否有效</span></span><br><span class="line">       checkElementIndex(index);</span><br><span class="line">       <span class="comment">//获取index的节点</span></span><br><span class="line">       Node&lt;E&gt; x = node(index);</span><br><span class="line">       <span class="comment">//获取节点的item</span></span><br><span class="line">       E oldVal = x.item;</span><br><span class="line"><span class="comment">//新的元素替换旧的元素</span></span><br><span class="line">       x.item = element;</span><br><span class="line">       <span class="comment">//返回旧元素</span></span><br><span class="line">       <span class="keyword">return</span> oldVal;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//将双向链表转换成数组</span></span><br><span class="line">   <span class="keyword">public</span> Object[] toArray() &#123;</span><br><span class="line">       <span class="comment">//创建一个Size大小的数组</span></span><br><span class="line">       Object[] result = <span class="keyword">new</span> Object[size];</span><br><span class="line">       <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">       <span class="comment">//循环遍历所有节点</span></span><br><span class="line">       <span class="keyword">for</span> (Node&lt;E&gt; x = first; x != <span class="keyword">null</span>; x = x.next)</span><br><span class="line">           <span class="comment">//将节点数据存入数组</span></span><br><span class="line">           result[i++] = x.item;</span><br><span class="line">       <span class="comment">//返回数组</span></span><br><span class="line">       <span class="keyword">return</span> result;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>和<code>ArrayList</code>一样，无论<code>add()</code>、<code>remove()</code>，还是<code>clear()</code>，只要涉及到修改集合中的元素个数时，都会改变<code>modCount</code>(全局)的值。由此回到<code>next()</code>方法中我们发现当 ‘A’ 线程正在做迭代器遍历操作时，<code>modCount</code>赋值给了<code>expectedModCount</code>，每次调用<code>next()</code>方法都会做一次<code>modCount != expectedModCount</code>的校验，此时线程 ’B‘ 进来了调用了add方法修改了<code>modCount</code>的值，此时<code>modCount</code>变成了N+1，判断为false，抛出<code>ConcurrentModificationException</code>异常，产生fail-fast事件 。</p></li></ol></li><li><p>fail-fast`事件</p><p>当多个线程对同一个集合进行操作的时候，某线程访问集合的过程中，该集合的内容被其他线程所改变(即其它线程通过<code>add</code>、<code>remove</code>、<code>clear</code>等方法，改变了<code>modCount</code>的值)；这时，就会抛出<code>ConcurrentModificationException</code>异常。与此对应的安全失败，在后面再解析。</p></li><li><p>解决方法</p><p>使用<code>Collections.synchronizedList()</code>方法包装，但是效率低下，这种方法实际上只是将原来非线程安全的<code>LinkedList</code>中的方法加上一个<code>synchronized</code>同步代码块 (哭了。。。)</p></li><li><p>优化建议</p><p>在多线程下如果有按数据索引访问元素的情形，采用<code>Collections.synchronizedList(new LinkedList&lt;&gt;())</code>方法</p></li><li><p>LinkedList的常用API</p><table><thead><tr><th>方法</th><th>描述</th></tr></thead><tbody><tr><td><code>add(E e)</code></td><td>将指定的元素添加到列表的末尾。</td></tr><tr><td><code>addFirst(E e)</code></td><td>在此列表的开始处插入指定的元素。</td></tr><tr><td><code>addLast(E e)</code></td><td>将指定的元素列表的结束。</td></tr><tr><td><code>get(int index)</code></td><td>返回此列表中指定位置的元素。</td></tr><tr><td><code>remove(int index)</code></td><td>移除此列表中指定位置的元素。</td></tr><tr><td><code>size()</code></td><td>返回此列表中元素的数目。</td></tr><tr><td><code>toArray()</code></td><td>返回一个数组，包含在这个列表中的所有元素在适当的顺序（从第一个到最后一个元素）。</td></tr><tr><td><code>clear()</code></td><td>从这个列表中移除所有的元素。</td></tr><tr><td><code>indexOf(Object o)</code></td><td>返回此列表中指定元素的第一个出现的索引</td></tr><tr><td><code>peek()</code></td><td>检索，但不删除，此列表的头（第一个元素）。</td></tr><tr><td><code>poll()</code></td><td>检索并删除此列表的头（第一个元素）。</td></tr><tr><td><code>pop()</code></td><td>从这个列表所表示的堆栈中弹出一个元素。</td></tr><tr><td><code>push(E e)</code></td><td>将一个元素推到由该列表所表示的堆栈上。</td></tr><tr><td><code>offer(E e)</code></td><td>将指定的元素添加到列表的尾部（最后一个元素）。</td></tr></tbody></table></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA.UTIL包下的ARRAYLIST和迭代器快速失败的源码解析以及多线程下故障现象、导致原因、以及解决办法和优化建议</title>
      <link href="/2019/01/02/java/2/"/>
      <url>/2019/01/02/java/2/</url>
      
        <content type="html"><![CDATA[<hr><p>该篇博客不适合小白，只做针对性的api源码解析，以及适合我自身的案例研究</p><hr><ol><li><p>使用多个线程往<code>ArrayList</code>中添加元素</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建资源集合</span></span><br><span class="line">List&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建30个线程添加元素</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">30</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">        list.add(UUID.randomUUID().toString().substring(<span class="number">0</span>,<span class="number">8</span>));</span><br><span class="line">        System.out.println(list);</span><br><span class="line">    &#125;).start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>故障现象</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Exception in thread <span class="string">"2"</span> java.util.ConcurrentModificationException</span><br></pre></td></tr></table></figure><p>我们知道<code>ArrayList</code>是线程不安全的，当多线程操作时，线程操作迭代器的同时其他线程改变了元素的值就会产生<code>ConcurrentModificationException</code>异常，<code>ConcurrentModificationException</code>是在操作<code>Iterator</code>时抛出的异常</p></li><li><p>故障原因</p><p>java中的<code>java.util</code>包下的类全部都是快速失败的，那么为什么在多线程操作ArrayList的时候会出现<code>ConcurrentModificationException</code>异常呢？</p><p>在我们的案例中，每个线程添加一次元素我们就打印一次集合中的元素，通过源码追踪，得出下面的内容</p><ol><li><p>打印内容，经过第二行代码，String调用了valueOf(x)方法</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">println</span><span class="params">(Object x)</span> </span>&#123;</span><br><span class="line">    String s = String.valueOf(x);</span><br><span class="line">    <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">        print(s);</span><br><span class="line">        newLine();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>ValueOf(Object x)方法是把Object类型的对象x转换成String类型，判断不为null，进入obj也就是我们的集合</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">valueOf</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (obj == <span class="keyword">null</span>) ? <span class="string">"null"</span> : obj.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>集合中的toString方法定义在了AbstractCollection类中，我们的错误出现在了 <code>E e = it.next();</code>获取元素这里，继续追踪错误。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Iterator&lt;E&gt; it = iterator();<span class="comment">//创建迭代器对象</span></span><br><span class="line">    <span class="keyword">if</span> (! it.hasNext())<span class="comment">//判断集合是否为空</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"[]"</span>;</span><br><span class="line">      </span><br><span class="line">    StringBuilder sb = <span class="keyword">new</span> StringBuilder();<span class="comment">//创建StringBuilder动态构造字符串</span></span><br><span class="line">    sb.append(<span class="string">'['</span>);</span><br><span class="line">    <span class="keyword">for</span> (;;) &#123;<span class="comment">//死循环</span></span><br><span class="line">        E e = it.next();<span class="comment">//获取元素 E ==&gt; String 是传进来的泛型</span></span><br><span class="line">        sb.append(e == <span class="keyword">this</span> ? <span class="string">"(this Collection)"</span> : e);<span class="comment">//添加元素</span></span><br><span class="line">        <span class="keyword">if</span> (! it.hasNext())<span class="comment">//循环换了就返回集合的字符串类型</span></span><br><span class="line">            <span class="keyword">return</span> sb.append(<span class="string">']'</span>).toString();</span><br><span class="line">        sb.append(<span class="string">','</span>).append(<span class="string">' '</span>); <span class="comment">//这里是没有return之前会进来，添加分隔符</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>下面是迭代器中的next()方法</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="comment">// 用来记录List修改的次数：每修改一次(添加/删除等操作)，将modCount+1</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">transient</span> <span class="keyword">int</span> modCount = <span class="number">0</span>;</span><br><span class="line">      </span><br><span class="line"><span class="keyword">int</span> cursor;       <span class="comment">// 下一个要返回的元素的索引</span></span><br><span class="line">      <span class="keyword">int</span> lastRet = -<span class="number">1</span>; <span class="comment">// 最后一个返回元素的索引;-1:没有</span></span><br><span class="line">      <span class="keyword">int</span> expectedModCount = modCount;<span class="comment">//这是非常关键的一步，把modCount赋值给expectedModCount，用来在迭代器遍历时next()和remove()方法中做校验</span></span><br><span class="line">      </span><br><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">          checkForComodification();<span class="comment">//每次添加元素前，都检查一次modCount和expectedModCount是否相等，如果不相等就直接返回ConcurrentModificationException异常，产生快速失败,多线程环境下这里通不过</span></span><br><span class="line">          <span class="keyword">int</span> i = cursor;</span><br><span class="line">          <span class="keyword">if</span> (i &gt;= size)</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> NoSuchElementException();</span><br><span class="line">          Object[] elementData = ArrayList.<span class="keyword">this</span>.elementData;</span><br><span class="line">          <span class="keyword">if</span> (i &gt;= elementData.length)</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</span><br><span class="line">          cursor = i + <span class="number">1</span>;</span><br><span class="line">          <span class="keyword">return</span> (E) elementData[lastRet = i];</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">checkForComodification</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (modCount != expectedModCount)<span class="comment">//多线程环境下，modCount和expectedModCount不相等</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>那么在多线程环境下是如何让<code>modCount</code> != <code>expectedModCount</code>的呢？</p><p>我们先看源码中是如何修改<code>modCount</code> 的值的(开始怼源码)</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="comment">// list中容量变化时，对应的同步函数</span></span><br><span class="line"><span class="keyword">transient</span> Object[] elementData; <span class="comment">// 保存了添加到ArrayList中的元素</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;<span class="comment">//默认是空元素</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_CAPACITY = <span class="number">10</span>;<span class="comment">//初始容量</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">ensureCapacity</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">int</span> minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA)</span><br><span class="line">           <span class="comment">//元素列表不等于默认列表</span></span><br><span class="line">           ? <span class="number">0</span></span><br><span class="line">           <span class="comment">//元素列表等于默认列表，返回默认初始值10</span></span><br><span class="line">           : DEFAULT_CAPACITY;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">if</span> (minCapacity &gt; minExpand) &#123;<span class="comment">//传入的容量大于最小扩展容量(minExpand)，则调用ensureExplicitCapacity()方法传入minCapacity</span></span><br><span class="line">           ensureExplicitCapacity(minCapacity);</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//判断元素列表是否为初始的元素列表，返回默认容量10和传入的容量中较大的值</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">calculateCapacity</span><span class="params">(Object[] elementData, <span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123;</span><br><span class="line">           <span class="keyword">return</span> Math.max(DEFAULT_CAPACITY, minCapacity);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> minCapacity;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//JDK1.8 中，add、addAll方法会先调用者方法判断是否需要扩容</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">ensureCapacityInternal</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//把元素列表和扩展的容量传入到calculateCapacity方法，做一个判断</span></span><br><span class="line">       ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//list中容量变化时，modCount+1</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">ensureExplicitCapacity</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">      </span><br><span class="line">       <span class="comment">// 如果minCapacity大于elementData的长度，则进行扩容</span></span><br><span class="line">       <span class="keyword">if</span> (minCapacity - elementData.length &gt; <span class="number">0</span>)</span><br><span class="line">           <span class="comment">//调用grow扩容</span></span><br><span class="line">           grow(minCapacity);</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MAX_ARRAY_SIZE = Integer.MAX_VALUE - <span class="number">8</span>;</span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">grow</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//旧容量</span></span><br><span class="line">       <span class="keyword">int</span> oldCapacity = elementData.length;</span><br><span class="line">       <span class="comment">//新容量=旧容量+旧容量的1/2 ==&gt;扩容1.5倍</span></span><br><span class="line">       <span class="keyword">int</span> newCapacity = oldCapacity + (oldCapacity &gt;&gt; <span class="number">1</span>);</span><br><span class="line">       <span class="comment">//如果计算出来的新容量比传进来的容量小，则以传入的容量为准</span></span><br><span class="line">       <span class="keyword">if</span> (newCapacity - minCapacity &lt; <span class="number">0</span>)</span><br><span class="line">           newCapacity = minCapacity;</span><br><span class="line">       <span class="comment">//如果新容量大于MAX_ARRAY_SIZE(Integer.MAX_VALUE - 8)，则把新容量交给hugeCapacity方法</span></span><br><span class="line">       <span class="keyword">if</span> (newCapacity - MAX_ARRAY_SIZE &gt; <span class="number">0</span>)</span><br><span class="line">           newCapacity = hugeCapacity(minCapacity);<span class="comment">//hugeCapacity实际上是做了一次内存溢出的判断，因为MAX_ARRAY_SIZE的容量已经接近溢出的边缘</span></span><br><span class="line">       <span class="comment">//Arrays.copyOf()方法放入elementData的元素，并把容量扩容为newCapacity</span></span><br><span class="line">       elementData = Arrays.copyOf(elementData, newCapacity);</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//主要检查内存是否溢出</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">hugeCapacity</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">       <span class="comment">// 内存溢出</span></span><br><span class="line">       <span class="keyword">if</span> (minCapacity &lt; <span class="number">0</span>) </span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> OutOfMemoryError();</span><br><span class="line">       <span class="keyword">return</span> (minCapacity &gt; MAX_ARRAY_SIZE) ?</span><br><span class="line">           Integer.MAX_VALUE :</span><br><span class="line">           MAX_ARRAY_SIZE;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line">   <span class="comment">// 添加元素到队列最后</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">add</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//判断是否需要扩容和modCount+1,以及判断内存溢出</span></span><br><span class="line">       ensureCapacityInternal(size + <span class="number">1</span>);  </span><br><span class="line">       <span class="comment">//将元素添加到数组末尾</span></span><br><span class="line">       elementData[size++] = e;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">   <span class="comment">// 添加元素到指定的位置</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> index, E element)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查index的范围</span></span><br><span class="line">       rangeCheckForAdd(index);</span><br><span class="line"><span class="comment">//判断是否需要扩容和modCount+1,以及判断内存溢出</span></span><br><span class="line">       ensureCapacityInternal(size + <span class="number">1</span>);  </span><br><span class="line">       <span class="comment">//把原数组的index位置移动到目标数组的index+1的位置,长度=数组长度-插入位置，这样就把index位置空出来了，涉及到内存操作，贼慢</span></span><br><span class="line">       System.arraycopy(elementData, index, elementData, index + <span class="number">1</span>,</span><br><span class="line">                        size - index);</span><br><span class="line">       <span class="comment">//把元素插入到数组中的index位置</span></span><br><span class="line">       elementData[index] = element;</span><br><span class="line">       size++;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//指定位置的范围检查，适用于add和addAll.</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">rangeCheckForAdd</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (index &gt; size || index &lt; <span class="number">0</span>)</span><br><span class="line">           <span class="comment">//throw new 下标越界异常</span></span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> IndexOutOfBoundsException(outOfBoundsMsg(index));</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line">   <span class="comment">// 添加集合</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">addAll</span><span class="params">(Collection&lt;? extends E&gt; c)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//把集合转换成一个Object类型的数组</span></span><br><span class="line">       Object[] a = c.toArray();</span><br><span class="line">       <span class="comment">//获集合的长度，用作扩容和复制</span></span><br><span class="line">       <span class="keyword">int</span> numNew = a.length;</span><br><span class="line">       <span class="comment">//判断是否需要扩容和modCount+1,以及判断内存溢出</span></span><br><span class="line">       ensureCapacityInternal(size + numNew);</span><br><span class="line">       <span class="comment">//把集合从0位置开始移动到目标数组的size位置,长度就等于集合的长度</span></span><br><span class="line">       System.arraycopy(a, <span class="number">0</span>, elementData, size, numNew);</span><br><span class="line">       <span class="comment">//计算size</span></span><br><span class="line">       size += numNew;</span><br><span class="line">       <span class="keyword">return</span> numNew != <span class="number">0</span>;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//类似于addAll，插入的位置变了而已</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">addAll</span><span class="params">(<span class="keyword">int</span> index, Collection&lt;? extends E&gt; c)</span> </span>&#123;&#125;</span><br><span class="line">  </span><br><span class="line">      </span><br><span class="line">   <span class="comment">// 删除指定位置的元素 </span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> E <span class="title">remove</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查范围</span></span><br><span class="line">       rangeCheck(index);</span><br><span class="line"><span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">       <span class="comment">//找出index位置的元素</span></span><br><span class="line">       E oldValue = elementData(index);</span><br><span class="line"><span class="comment">//计算数组从index到size的长度，-1是为了减去index的位置</span></span><br><span class="line">       <span class="keyword">int</span> numMoved = size - index - <span class="number">1</span>;</span><br><span class="line">       <span class="comment">//如果计算后的长度大于0，则使用System.arraycopy复制index后的元素向前移动一位，内存操作，贼慢</span></span><br><span class="line">       <span class="keyword">if</span> (numMoved &gt; <span class="number">0</span>)</span><br><span class="line">           System.arraycopy(elementData, index+<span class="number">1</span>, elementData, index,numMoved);</span><br><span class="line">        <span class="comment">//size--,并赋值为null</span></span><br><span class="line">       elementData[--size] = <span class="keyword">null</span>; <span class="comment">// clear to let GC do its work</span></span><br><span class="line"><span class="comment">//返回删除元素后的数组</span></span><br><span class="line">       <span class="keyword">return</span> oldValue;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//检查索引的范围</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">rangeCheck</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (index &gt;= size)</span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> IndexOutOfBoundsException(outOfBoundsMsg(index));</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">   <span class="comment">// 快速删除指定位置的元素,和remove(int index)类似，省去了范围校验</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">fastRemove</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">       <span class="keyword">int</span> numMoved = size - index - <span class="number">1</span>;</span><br><span class="line">       <span class="keyword">if</span> (numMoved &gt; <span class="number">0</span>)</span><br><span class="line">           System.arraycopy(elementData, index+<span class="number">1</span>, elementData, index,numMoved);</span><br><span class="line">       <span class="comment">//原来这里也会有GC回收</span></span><br><span class="line">       elementData[--size] = <span class="keyword">null</span>;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line">   <span class="comment">// 清空集合</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">      </span><br><span class="line">       <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++)</span><br><span class="line">           <span class="comment">//这里也会有GC回收</span></span><br><span class="line">           elementData[i] = <span class="keyword">null</span>;</span><br><span class="line">      </span><br><span class="line">       size = <span class="number">0</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   ...</span><br></pre></td></tr></table></figure><p>一遍源码读下来，发现无论是<code>add()</code>、<code>remove()</code>，还是<code>clear()</code>，只要涉及到修改集合中的元素个数时，都会改变<code>modCount</code>(全局)的值。</p><p>由此回到<code>next()</code>方法中我们发现当 ‘A’ 线程正在做迭代器遍历操作时，<code>modCount</code>赋值给了<code>expectedModCount</code>，每次调用<code>next()</code>方法都会做一次<code>modCount != expectedModCount</code>的校验，此时线程 ’B‘ 进来了调用了add方法修改了<code>modCount</code>的值，此时<code>modCount</code>变成了N+1，判断为false，抛出<code>ConcurrentModificationException</code>异常，产生fail-fast事件 。</p></li></ol></li><li><p><code>fail-fast</code>事件</p><p>当多个线程对同一个集合进行操作的时候，某线程访问集合的过程中，该集合的内容被其他线程所改变(即其它线程通过<code>add</code>、<code>remove</code>、<code>clear</code>等方法，改变了<code>modCount</code>的值)；这时，就会抛出<code>ConcurrentModificationException</code>异常。与此对应的安全失败，在后面再解析。</p></li><li><p>解决方法</p><p>经过源码解析<code>ConcurrentModificationException</code>异常是因为多个线程同时调用add()方法导致的，解决的办法有一下三种:</p><ol><li><p>使用<code>Vector()</code>替代<code>ArrayList()</code>，但是这种方法效率低下，因为<code>Vector()</code>的几乎所有方法都加上了<code>synchronized</code>修饰符，<code>synchronized</code>保证了在同一时刻最多只有一个线程访问该段代码，虽然jdk1.5引入了自旋锁、锁粗化、轻量级锁和偏向锁，但还是太重，效率很低。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;String&gt; list = <span class="keyword">new</span> Vector&lt;String&gt;());</span><br></pre></td></tr></table></figure></li><li><p>使用<code>Collections.synchronizedList()</code>包装<code>ArrayList</code>，但是这种方法实际上只是将原来非线程安全的<code>ArrayList</code>中的方法加上一个<code>synchronized</code>同步代码块 (哭了。。。)</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;String&gt; list = Collections.synchronizedList(<span class="keyword">new</span> ArrayList&lt;String&gt;());</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> Object mutex;     <span class="comment">// 对象锁</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">get</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line"><span class="keyword">synchronized</span> (mutex) &#123;<span class="comment">//同步代码块</span></span><br><span class="line">        <span class="keyword">return</span> list.get(index);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">set</span><span class="params">(<span class="keyword">int</span> index, E element)</span> </span>&#123;</span><br><span class="line"><span class="keyword">synchronized</span> (mutex) &#123;<span class="comment">//同步代码块</span></span><br><span class="line">        <span class="keyword">return</span> list.set(index, element);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure></li><li><p>第三种也是推介的一种方法就是使用java.util.concurrent包下的CopyOnWriteArrayList解决，俗称写时复制机制，是读写分离的一种实现，这种方法在后面将详细源码解析。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;String&gt; list = <span class="keyword">new</span> CopyOnWriteArrayList&lt;String&gt;();</span><br></pre></td></tr></table></figure></li></ol></li><li><p>优化建议</p><p>在多线程下如果有按数据索引访问元素的情形，采用<code>CopyOnWriteArrayList()</code>方法</p></li><li><p>ArrayList的常用API</p><table><thead><tr><th>方法</th><th>描述</th></tr></thead><tbody><tr><td>add(E e)</td><td>将指定的元素列表的结束。</td></tr><tr><td>addAll(Collection c)</td><td>追加指定集合的所有元素到这个列表的末尾，按他们的指定集合的迭代器返回。</td></tr><tr><td>clear()</td><td>从这个列表中移除所有的元素。</td></tr><tr><td>contains(Object o)</td><td>返回 <code>true</code>如果这个列表包含指定元素。</td></tr><tr><td>get(int index)</td><td>返回此列表中指定位置的元素。</td></tr><tr><td>iterator()</td><td>在这个列表中的元素上返回一个正确的顺序。</td></tr><tr><td>remove(int index)</td><td>移除此列表中指定位置的元素。</td></tr><tr><td>set(int index, E element)</td><td>用指定元素替换此列表中指定位置的元素。</td></tr><tr><td>size()</td><td>返回此列表中元素的数目</td></tr><tr><td>toArray()</td><td>返回一个数组，包含在这个列表中的所有元素在适当的顺序</td></tr></tbody></table></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lock可重入锁与函数式接口Runnable接口的lambda编程方式</title>
      <link href="/2019/01/01/java/1/"/>
      <url>/2019/01/01/java/1/</url>
      
        <content type="html"><![CDATA[<h2 id="多线程企业级Demo"><a href="#多线程企业级Demo" class="headerlink" title="多线程企业级Demo"></a>多线程企业级Demo</h2><ol><li><p>前提</p><p>所有的多线程开开发遵循一个规则:</p><p><code>在高内聚低耦合的前提下，线程--&gt;操作--&gt;资源类</code></p><p>在这个条件下我们写一个卖票的Demo，三个售票员卖出30张票</p></li><li><p>资源类</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//资源类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Ticket</span> </span>&#123;</span><br><span class="line">    <span class="comment">//创建一个可重入的lock锁</span></span><br><span class="line">    Lock lock = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line">    <span class="comment">//总30张票</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> number = <span class="number">30</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sale</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        lock.lock();<span class="comment">//上锁</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (number &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                System.out.println(Thread.currentThread().getName() + <span class="string">"\t卖出第"</span> + (number--) + <span class="string">"\t张票，还剩下"</span> + number);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            lock.unlock();<span class="comment">//解锁</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>线程</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">40</span>; i++) &#123;</span><br><span class="line">            <span class="comment">//待操作的代码</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;,<span class="string">"a"</span>).start();</span><br></pre></td></tr></table></figure><p><code>注意:</code> 我们查看Runnable()接口的源代码发现，这是一个函数式接口，下面提供源码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@FunctionalInterface</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所以我们可以使用lambda表达式简化操作</p></li><li><p>操作</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//new资源类</span></span><br><span class="line">Ticket ticket = <span class="keyword">new</span> Ticket();</span><br><span class="line"><span class="keyword">new</span> Thread(() -&gt; &#123; <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">40</span>; i++) ticket.sale();&#125;,<span class="string">"A卖票员"</span>).start();</span><br><span class="line"><span class="keyword">new</span> Thread(() -&gt; &#123; <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">40</span>; i++) ticket.sale();&#125;,<span class="string">"B卖票员"</span>).start();</span><br><span class="line"><span class="keyword">new</span> Thread(() -&gt; &#123; <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">40</span>; i++) ticket.sale();&#125;,<span class="string">"C卖票员"</span>).start();</span><br></pre></td></tr></table></figure></li><li><p>结果：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">A卖票员卖出第<span class="number">30</span>张票，还剩下<span class="number">29</span></span><br><span class="line">A卖票员卖出第<span class="number">29</span>张票，还剩下<span class="number">28</span></span><br><span class="line">A卖票员卖出第<span class="number">28</span>张票，还剩下<span class="number">27</span></span><br><span class="line">...</span><br><span class="line">A卖票员卖出第<span class="number">1</span>张票，还剩下<span class="number">0</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="Lambda表达式与函数式接口"><a href="#Lambda表达式与函数式接口" class="headerlink" title="Lambda表达式与函数式接口"></a>Lambda表达式与函数式接口</h2><p>在jdk1.8中，引入了函数式接口，函数式接口中只能声明一个抽象方法，lambda表达式可以直接使用这个接口</p><ol><li><p>定义函数式接口</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@FunctionalInterface</span><span class="comment">//显式指定 当接口中只有一条抽象方法时，默认是函数式接口</span></span><br><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">Foo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>使用lambda表达式实现</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="comment">//指定接口的实现</span></span><br><span class="line">    Foo foo = () -&gt; System.out.println(<span class="string">"hello FunctionInterface.."</span>);</span><br><span class="line">    foo.sayHello();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>还需要知道的是在jdk1.8中，引入了default方法修饰接口，并且可以在接口中声明static方法，但是必须实现方法，这里和我们印象中的java接口有点不同</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//@FunctionalInterface  //声明函数式接口 只有一条抽象方法时可以省略</span></span><br><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">Foo</span> </span>&#123;</span><br><span class="line">    <span class="comment">//声明抽象方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//声明默认方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">default</span> <span class="keyword">int</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span>  x+y;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//声明静态方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">dec</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> x-y;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义类，没有实现接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FuncitonInterfaceDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//Foo对象接收实现的接口</span></span><br><span class="line">        Foo foo = () -&gt; System.out.println(<span class="string">"hello FunctionInterface.."</span>);</span><br><span class="line">        foo.sayHello();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//Foo对象的引用调用add方法</span></span><br><span class="line">        System.out.println(foo.add(<span class="number">10</span>, <span class="number">5</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//接口名直接调用static修饰的dec方法</span></span><br><span class="line">        System.out.println(Foo.dec(<span class="number">10</span>, <span class="number">5</span>));</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume源代码二次开发Source&amp;Sink&amp;Interceptor&amp;Channel的事物保证</title>
      <link href="/2018/12/05/flume/5/"/>
      <url>/2018/12/05/flume/5/</url>
      
        <content type="html"><![CDATA[<ol><li>Agent架构</li><li>自定义Source</li><li>自定义Sink</li><li>自定义Interceptor</li><li>Channel的事物保证</li></ol><h2 id="Agent架构"><a href="#Agent架构" class="headerlink" title="Agent架构"></a>Agent架构</h2><p><img src="https://yerias.github.io/flume_img/Agent.jpg" alt="Agent"></p><h2 id="自定义Source"><a href="#自定义Source" class="headerlink" title="自定义Source"></a>自定义Source</h2><p>提示：当不会写的时候，看源码是个不错的选择</p><p>在自定义Flume的组件之前， IDEA需要引入Flume的依赖</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Source的目的是从外部客户端接收数据并将其存储到配置的Channels中。一个Source可以获得它自己的ChannelProcessor的一个实例来连续处理一个Event，该Event在一个Channel本地事务中被提交。</p><p>在异常的情况下，所需的Channel将传播异常，所有Channel将回滚它们的Event，但之前在其他Channel上处理的Event将保持提交。</p><p>自定义Source可以在数据源里直接产生数据，产生的数据你可以定制化(前缀、后缀)</p><p>自定义Source类，参考官网<a href="http://flume.apache.org/releases/content/1.9.0/FlumeDeveloperGuide.html#source" target="_blank" rel="noopener">demo/github</a></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> tunan.hadoop.flume.sources;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.EventDeliveryException;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.PollableSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.event.SimpleEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.source.AbstractSource;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySource</span> <span class="keyword">extends</span> <span class="title">AbstractSource</span> <span class="keyword">implements</span> <span class="title">Configurable</span>, <span class="title">PollableSource</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 自定义属性，前缀和后缀</span></span><br><span class="line">    <span class="keyword">private</span> String prefix;</span><br><span class="line">    <span class="keyword">private</span> String suffix;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 处理Event</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>&#123;</span><br><span class="line">        <span class="comment">// 自定义状态属性</span></span><br><span class="line">        Status status = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 模拟产生数据</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            <span class="comment">// 创建Event</span></span><br><span class="line">            SimpleEvent event = <span class="keyword">new</span> SimpleEvent();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 把数据设置到Body中去，注意header为空</span></span><br><span class="line">            event.setBody((prefix + i + suffix).getBytes());</span><br><span class="line">            <span class="comment">// 开始处理Evetn</span></span><br><span class="line">            getChannelProcessor().processEvent(event);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 成功</span></span><br><span class="line">            status = Status.READY;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            <span class="comment">// 失败，回退</span></span><br><span class="line">            status = Status.BACKOFF;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 返回状态的结果</span></span><br><span class="line">        <span class="keyword">return</span> status;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取Agent中传入的参数信息</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.prefix = context.getString(<span class="string">"prefix"</span>,<span class="string">"TUNAN"</span>);</span><br><span class="line">        <span class="keyword">this</span>.suffix = context.getString(<span class="string">"suffix"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将jar包上传到$FLUME_HOME/lib下，并修改配置文件中的Source，修改Type、添加类中自定义的参数</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 我们在类中自定义的Source参数和类的全限定名</span></span><br><span class="line">a1.sources.r1.type = tunan.hadoop.flume.sources.MySource</span><br><span class="line">a1.sources.r1.prefix = tunan:</span><br><span class="line">a1.sources.r1.suffix = -6639</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--conf /home/hadoop/app/flume/conf \</span><br><span class="line">--conf-file /home/hadoop/app/flume/script/MySource.conf \</span><br><span class="line">--name a1 \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>查看结果</p><p><img src="https://yerias.github.io/flume_img/mysource.jpg" alt="mysource"></p><h2 id="自定义Sink"><a href="#自定义Sink" class="headerlink" title="自定义Sink"></a>自定义Sink</h2><p>提示：当不会写的时候，看源码是个不错的选择</p><p>Sink的目的是从Channel中提取Event并将它们转发到流中的下一个Flume Agent或将它们存储在外部存储库中。</p><p>正如在Flume属性文件中配置的那样，一个Sink仅与一个Channel相关联。</p><p>有一个与每个配置的Sink相关联的SinkRunner实例，当Flume框架调用SinkRunner.start()时，会创建一个新线程来驱动Sink(使用SinkRunner)。这个线程管理Sink的生命周期。Sink需要实现start()和stop()方法，它们是生命周期感知接口的一部分。start()方法应该初始化Sink，并使其处于可以将Event转发到下一个目的地的状态。process()方法应该执行从Channel中提取Event并转发Event的核心处理。stop()方法应该做必要的清理工作(例如释放资源)。</p><p>从Channel拿到数据(Event)，把数据输出我们自定义的Sink中去，架构为：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nc source ==&gt; memory channel ==&gt; MySink</span><br></pre></td></tr></table></figure><p>注意： nc可以保证消息有序，telnet不能保证消息有序</p><p>代码实现：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> tunan.hadoop.flume.sink;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.sink.AbstractSink;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySink</span> <span class="keyword">extends</span> <span class="title">AbstractSink</span> <span class="keyword">implements</span> <span class="title">Configurable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 拿到logger</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger logger =  LoggerFactory.getLogger(MySink<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 属性作为参数的前缀和后缀</span></span><br><span class="line">    <span class="keyword">private</span> String prefix;</span><br><span class="line">    <span class="keyword">private</span> String suffix;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从channel中获取数据发送到目的地</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>&#123;</span><br><span class="line">        Status status;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取channel</span></span><br><span class="line">        Channel ch = getChannel();</span><br><span class="line">        <span class="comment">// 获取事物</span></span><br><span class="line">        Transaction txn = ch.getTransaction();</span><br><span class="line">        <span class="comment">// 开启事物</span></span><br><span class="line">        txn.begin();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 拿到event</span></span><br><span class="line">            Event event;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">do</span> &#123;   <span class="comment">// 一直等待，直到拿到不为空的Event</span></span><br><span class="line">                event = ch.take();</span><br><span class="line">            &#125; <span class="keyword">while</span> ((event == <span class="keyword">null</span>));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// body是个字节数组转换成字符串</span></span><br><span class="line">            String body = <span class="keyword">new</span> String(event.getBody());</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 控制台打印</span></span><br><span class="line">            logger.error(prefix + body + suffix);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 提交事务</span></span><br><span class="line">            txn.commit();</span><br><span class="line">            status = Status.READY;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">            <span class="comment">// 回滚事务</span></span><br><span class="line">            txn.rollback();</span><br><span class="line">            status = Status.BACKOFF;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (t <span class="keyword">instanceof</span> Error) &#123;</span><br><span class="line">                <span class="keyword">throw</span> (Error)t;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            txn.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> status;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从Agent中拿到参数</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.prefix = context.getString(<span class="string">"prefix"</span>,<span class="string">"Tunan"</span>);</span><br><span class="line">        <span class="keyword">this</span>.suffix = context.getString(<span class="string">"suffix"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>jar包上传到$FLUME_HOME\lib下，并修改配置文件中的Sink，修改Type、添加类中自定义的参数</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="comment"># 我们在类中自定义的Sink参数和类的全限定名</span></span></span><br><span class="line">a1.sinks.k1.type = tunan.hadoop.flume.sink.MySink</span><br><span class="line">a1.sinks.k1.prefix = tunan-sink:</span><br><span class="line">a1.sinks.k1.suffix = -flie</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>发送消息</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ nc localhost 44444</span><br><span class="line">1</span><br><span class="line">OK</span><br><span class="line">2</span><br><span class="line">OK</span><br><span class="line">3</span><br><span class="line">OK</span><br><span class="line">..</span><br><span class="line">0</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><p>查看结果</p><p><img src="https://yerias.github.io/flume_img/image-20200502115329892.png" alt=""></p><h2 id="自定义Interceptor"><a href="#自定义Interceptor" class="headerlink" title="自定义Interceptor"></a>自定义Interceptor</h2><p>提示：当不会写的时候，看源码是个不错的选择</p><p>Flume具有修改/删除运行中的Event的能力。这是在拦截器的帮助下完成的。拦截器是实现<code>org.apache.flum .interceptor. interceptor</code>接口的类。拦截器可以根据拦截器开发人员选择的任何标准修改甚至删除Event。</p><p>Flume支持链式的拦截器。这是通过在配置中指定拦截器builder类名列表来实现的。截取程序被指定为source配置中的空白分隔列表。指定拦截器的顺序与调用它们的顺序相同。一个拦截器返回的Event列表被传递到链中的下一个拦截器。</p><p>拦截器可以修改或删除Event。如果拦截器需要删除Event，它只会在其返回的列表中不返回该Event。如果它要删除所有Event，那么它只返回一个空列表。</p><p>需求：Flume接进来的数据都在一起，有些业务线的数据比较重要，单独拉出来，这里自定义拦截器，并配合<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#multiplexing-channel-selector" target="_blank" rel="noopener">Multiplexing Channel Selector</a>将body包含<code>gifshow</code>的数据单独拿出来</p><p><img src="https://yerias.github.io/flume_img/Flume%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8B%A6%E6%88%AA%E5%99%A8.jpg" alt="Flume自定义拦截器"></p><p>代码实现自定义拦截器</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> tunan.hadoop.flume.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Event;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.interceptor.Interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyInterceptor</span> <span class="keyword">implements</span> <span class="title">Interceptor</span> </span>&#123;</span><br><span class="line"><span class="comment">// 自定义List用来处理批量Event</span></span><br><span class="line">    <span class="keyword">private</span>  List&lt;Event&gt; newEvents;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化设置</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        newEvents = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 拦截单个Event</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        Map&lt;String, String&gt; headers = event.getHeaders();</span><br><span class="line">        String body = <span class="keyword">new</span> String(event.getBody());</span><br><span class="line">        <span class="keyword">if</span> (body.contains(<span class="string">"gifshow"</span>)) &#123;</span><br><span class="line">            <span class="comment">// 自定义头信息</span></span><br><span class="line">            headers.put(<span class="string">"type"</span>, <span class="string">"gifshow"</span>);</span><br><span class="line">        <span class="comment">// 自定义头信息</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            headers.put(<span class="string">"type"</span>, <span class="string">"other"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 拦截多个Event处理</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Event&gt; <span class="title">intercept</span><span class="params">(List&lt;Event&gt; events)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 每次进来初始化List</span></span><br><span class="line">        newEvents.clear();</span><br><span class="line">        Iterator&lt;Event&gt; iter = events.iterator();</span><br><span class="line">        <span class="keyword">while</span> (iter.hasNext())&#123;</span><br><span class="line">            Event next = iter.next();</span><br><span class="line">            <span class="comment">// event传递给单个处理，并添加到新的List</span></span><br><span class="line">            newEvents.add(intercept(next));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 返回拦截后的Event</span></span><br><span class="line">        <span class="keyword">return</span> newEvents;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 源码在 HostInterceptor，需要添加一个静态内部类，并且名为Builder</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Builder</span> <span class="keyword">implements</span> <span class="title">Interceptor</span>.<span class="title">Builder</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Interceptor <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> MyInterceptor();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>配置第一个Agent</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 自定义拦截器</span></span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = tunan.hadoop.flume.interceptor.MyIntercepto$Builder</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> multiplexing selector</span></span><br><span class="line">a1.sources.r1.selector.type = multiplexing</span><br><span class="line">a1.sources.r1.selector.header = type</span><br><span class="line"><span class="meta">#</span><span class="bash"> 与自定义拦截器中设置的头信息对应</span></span><br><span class="line">a1.sources.r1.selector.mapping.figshow = c1</span><br><span class="line">a1.sources.r1.selector.mapping.other = c2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop</span><br><span class="line">a1.sinks.k1.port = 4441</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop</span><br><span class="line">a1.sinks.k2.port = 4442</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure><p>配置第二个Agent</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = hadoop</span><br><span class="line">a1.sources.r1.port = 4441</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>配置第三个Agent</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = hadoop</span><br><span class="line">a1.sources.r1.port = 4442</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>启动Agent，先启动Agent2和Agent3，不然会报错</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--conf /home/hadoop/app/flume/conf \</span><br><span class="line">--conf-file /home/hadoop/app/flume/script/Interceptor_3.conf \</span><br><span class="line">--name a1 \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--conf /home/hadoop/app/flume/conf \</span><br><span class="line">--conf-file /home/hadoop/app/flume/script/Interceptor_2.conf \</span><br><span class="line">--name a1 \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--conf /home/hadoop/app/flume/conf \</span><br><span class="line">--conf-file /home/hadoop/app/flume/script/Interceptor_1.conf \</span><br><span class="line">--name a1 \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>发送消息</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ nc localhost 44444</span><br><span class="line">gifshow</span><br><span class="line">OK</span><br><span class="line">aaaaa</span><br><span class="line">OK</span><br><span class="line">aaaagifshow</span><br><span class="line">OK</span><br><span class="line">figshow</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><p>查看结果</p><p><img src="https://yerias.github.io/flume_img/other.jpg" alt=""></p><p><img src="https://yerias.github.io/flume_img/gifshow.jpg" alt=""></p><h2 id="Channel的事物保证"><a href="#Channel的事物保证" class="headerlink" title="Channel的事物保证"></a>Channel的事物保证</h2><p>Transaction interface 是Flume可靠性的基础，所有主要组件(即Source、Sink、Channel)必须使用Flume Transaction。</p><p>Transaction 在连接Channel中实现，连接到Channel的每个Source和Sink都必须获得一个Transaction 对象。</p><p>Source使用ChannelProcessor来管理Transaction ，而Sinks 通过其配置的Channel显式地管理Transaction。</p><p>每个阶段的操作Event(将其放入Channel中)或提取Event(将其从Channel中取出)的操作在活动必须在Transaction中完成。</p><p>![Channel Trancastion](<a href="https://yerias.github.io/flume_img/Channel">https://yerias.github.io/flume_img/Channel</a> Trancastion.jpg)</p><p>所有的事物管理由MemoryChannel类来做，具体可以查看源码</p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume源代码二次开发debug</title>
      <link href="/2018/12/04/flume/4/"/>
      <url>/2018/12/04/flume/4/</url>
      
        <content type="html"><![CDATA[<h3 id="1-下载安装Flume，配置flume"><a href="#1-下载安装Flume，配置flume" class="headerlink" title="1.下载安装Flume，配置flume"></a>1.下载安装Flume，配置flume</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop conf]$ cat exec_memory_kafka.conf</span><br><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the custom <span class="built_in">exec</span> <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = com.ruozedata.prewarning.ExecSourceJSON</span><br><span class="line">a1.sources.r1.command = tail -F /home/hadoop/flume-test-log hadoop-cmf-hdfs-NAMENODE-ruozedata001.log.out</span><br><span class="line">a1.sources.r1.hostname = hadoop</span><br><span class="line">a1.sources.r1.servicename = namenode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">a1.sinks.k1.kafka.topic = PREWARNING</span><br><span class="line">a1.sinks.k1.kafka.bootstrap.servers = hadoop:9090,hadoop:9091,hadoop:9092</span><br><span class="line">a1.sinks.k1.kafka.flumeBatchSize = 6000</span><br><span class="line">a1.sinks.k1.kafka.producer.acks = all</span><br><span class="line">a1.sinks.k1.kafka.producer.linger.ms = 1</span><br><span class="line">a1.sinks.ki.kafka.producer.compression.type = snappy</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.keep-alive = 90</span><br><span class="line">a1.channels.c1.capacity = 2000000</span><br><span class="line">a1.channels.c1.transactionCapacity = 6000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h3 id="2-拉取源码拉取源码到idea"><a href="#2-拉取源码拉取源码到idea" class="headerlink" title="2.拉取源码拉取源码到idea"></a>2.拉取源码拉取源码到idea</h3><p>地址：<a href="https://github.com/apache/flume" target="_blank" rel="noopener">https://github.com/apache/flume</a></p><h3 id="3-将自己写的代码放入到源码中"><a href="#3-将自己写的代码放入到源码中" class="headerlink" title="3.将自己写的代码放入到源码中"></a>3.将自己写的代码放入到源码中</h3><p><img src="https://yerias.github.io/flume_img/flume-debug.png" alt="flume-debug"></p><h3 id="4-自己写的代码打包后放到远端服务器的lib目录下"><a href="#4-自己写的代码打包后放到远端服务器的lib目录下" class="headerlink" title="4.自己写的代码打包后放到远端服务器的lib目录下"></a>4.自己写的代码打包后放到远端服务器的lib目录下</h3><p><img src="https://yerias.github.io/flume_img/flume-jar.png" alt="flume-jar"></p><p>注意：除了这种方法，还可以直接修改源码，然后一起打包上传到服务器上</p><h3 id="5-启动flume服务"><a href="#5-启动flume服务" class="headerlink" title="5.启动flume服务"></a>5.启动flume服务</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nohup  /home/hadoop/app/apache-flume-1.7.0-bin/bin/flume-ng agent \</span><br><span class="line">-c /home/hadoop/app/apache-flume-1.7.0-bin/conf \</span><br><span class="line">-f /home/hadoop/MonitoringProject/exec_memory_kafka.properties \</span><br><span class="line">-n a1 \</span><br><span class="line">-Dflume.root.logger=DEBUG,console -Xmx20m -Xdebug -Xrunjdwp:transport=dt_socket,address=8000,server=y,suspend=y</span><br></pre></td></tr></table></figure><h3 id="6-配置启动应用"><a href="#6-配置启动应用" class="headerlink" title="6.配置启动应用"></a>6.配置启动应用</h3><p><img src="https://yerias.github.io/flume_img/%E9%85%8D%E7%BD%AE.png" alt="配置"></p><p>打上断点，启动后即可进入断点</p><p><img src="https://yerias.github.io/flume_img/%E6%9F%A5%E7%9C%8B.png" alt="查看"></p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume的Channel选择器&amp;Flume的Sink选择器&amp;Channel的两种类型</title>
      <link href="/2018/12/03/flume/3/"/>
      <url>/2018/12/03/flume/3/</url>
      
        <content type="html"><![CDATA[<h2 id="Flume的Channel选择器"><a href="#Flume的Channel选择器" class="headerlink" title="Flume的Channel选择器"></a>Flume的Channel选择器</h2><p>Flume的Channel选择器有<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#replicating-channel-selector-default" target="_blank" rel="noopener">Replicating Channel Selector (default)</a>和<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#multiplexing-channel-selector" target="_blank" rel="noopener">Multiplexing Channel Selector</a>，作用分别是复制和多路分发，默认用的复制</p><p>下面我们用两个案例分别实现Replicating Channel Selector和Multiplexing Channel Selector</p><h3 id="Replicating-Channel-Selector"><a href="#Replicating-Channel-Selector" class="headerlink" title="Replicating Channel Selector"></a>Replicating Channel Selector</h3><ol><li><p>需要实现的功能</p><p><img src="https://yerias.github.io/flume_img/ReplicatingChannel.png" alt="ReplicatingChannel"></p></li><li><p>代码实现</p></li></ol><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2#两个sink</span><br><span class="line">a1.channels = c1 c2 #两个channel</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat#nc输入</span><br><span class="line">a1.sources.r1.bind = 0.0.0.0</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k2.type = logger#第一个sink</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = hdfs#第二个sink</span><br><span class="line">a1.sinks.k1.hdfs.path = /flume/%y-%m-%d/%H/%M</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = replicating-</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">a1.sinks.k1.hdfs.fileType=DataStream</span><br><span class="line"></span><br><span class="line">a1.sources.r1.selector.type = replicating#选择器</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory#第一个memory</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type = memory#第二个memory</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1 c2//source连上两个channel</span><br><span class="line">a1.sinks.k1.channel = c1//channel1连上sink1</span><br><span class="line">a1.sinks.k2.channel = c2//channel2连上sink2</span><br></pre></td></tr></table></figure><ol start="3"><li>执行命令</li></ol><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/nc-replicating-logger_and_hdfs.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><ol start="4"><li><p>发送消息</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">telnet localhost 44444</span><br></pre></td></tr></table></figure><p>结果：成功</p></li></ol><h3 id="Multiplexing-Channel-Selector"><a href="#Multiplexing-Channel-Selector" class="headerlink" title="Multiplexing Channel Selector"></a>Multiplexing Channel Selector</h3><ol><li><p>需要实现的功能</p><p><img src="https://yerias.github.io/flume_img/multiplexingChannel.jpg" alt="multiplexingChannel"></p><p>数据从Source到Channel中间会经过一个<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#static-interceptor" target="_blank" rel="noopener">拦截器</a>，拦截器中的Key和Value参数被添加到了所有的Event上，在经过<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#multiplexing-channel-selector" target="_blank" rel="noopener">选择器</a>的时候，会根据拦截器中的Event所带的Value值的不同发送到不同的Sink</p></li><li><p>代码实现</p><p>nc-memory-arvo1.conf</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">a1.sources.r1.port = <span class="number">44441</span></span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun</span><br><span class="line">a1.sinks.k1.port = <span class="number">55555</span></span><br><span class="line"></span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = <span class="keyword">static</span></span><br><span class="line">a1.sources.r1.interceptors.i1.key = state</span><br><span class="line">a1.sources.r1.interceptors.i1.value = UA</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>nc-memory-arvo2.conf</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">a1.sources.r1.port = <span class="number">44442</span></span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun</span><br><span class="line">a1.sinks.k1.port = <span class="number">55555</span></span><br><span class="line"></span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = <span class="keyword">static</span></span><br><span class="line">a1.sources.r1.interceptors.i1.key = state</span><br><span class="line">a1.sources.r1.interceptors.i1.value = UB</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>vim nc-memory-arvo3.conf</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">a1.sources.r1.port = <span class="number">44443</span></span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun</span><br><span class="line">a1.sinks.k1.port = <span class="number">55555</span></span><br><span class="line"></span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = <span class="keyword">static</span></span><br><span class="line">a1.sources.r1.interceptors.i1.key = state</span><br><span class="line">a1.sources.r1.interceptors.i1.value = UC</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>vim avro-memory-multi.conf</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2 k3</span><br><span class="line">a1.channels = c1 c2 c3</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = <span class="number">55555</span></span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = logger</span><br><span class="line"></span><br><span class="line">a1.sinks.k3.type = hdfs</span><br><span class="line">a1.sinks.k3.hdfs.path = /flume/%y-%m-%d/%H/%M</span><br><span class="line">a1.sinks.k3.hdfs.filePrefix = multiplexing-</span><br><span class="line">a1.sinks.k3.hdfs.useLocalTimeStamp = <span class="keyword">true</span></span><br><span class="line">a1.sinks.k3.hdfs.fileType=DataStream</span><br><span class="line">a1.sinks.k3.hdfs.writeFormat=Text</span><br><span class="line"></span><br><span class="line">a1.sources.r1.selector.type = multiplexing</span><br><span class="line">a1.sources.r1.selector.header = state</span><br><span class="line">a1.sources.r1.selector.mapping.UA = c1</span><br><span class="line">a1.sources.r1.selector.mapping.UB = c2</span><br><span class="line">a1.sources.r1.selector.<span class="keyword">default</span> = c3</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c3.type = memory</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1 c2 c3</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br><span class="line">a1.sinks.k3.channel = c3</span><br></pre></td></tr></table></figure></li><li><p>执行命令</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/nc-memory-arvo1.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/nc-memory-arvo2.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/nc-memory-arvo3.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/avro-memory-multi.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li><li><p>发送消息</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">telnet aliyun <span class="number">44441</span></span><br><span class="line">telnet aliyun <span class="number">44442</span></span><br><span class="line">telnet aliyun <span class="number">44443</span></span><br></pre></td></tr></table></figure><p>结果：成功</p></li></ol><h2 id="Flume的Sink选择器"><a href="#Flume的Sink选择器" class="headerlink" title="Flume的Sink选择器"></a>Flume的Sink选择器</h2><p>Flume的Sink选择器常用的有两种：<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#failover-sink-processor" target="_blank" rel="noopener">Failover Sink Processor</a>和<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#load-balancing-sink-processor" target="_blank" rel="noopener">Load balancing Sink Processor</a></p><h3 id="Failover-Sink-Processor"><a href="#Failover-Sink-Processor" class="headerlink" title="Failover Sink Processor"></a>Failover Sink Processor</h3><p>Failover Sink Processor可以在Agent中的Sink端做一个类似于灾备的Sink组，官方文档中介绍Failover Sink Processor维护一个按优先级排序的Sink列表，确保只要有一个可用的Sink，就会处理Event。</p><p>Failover Sink Processor的工作方式是将多个Sink组成Sinks组，他们有一个优先级的顺序关系，优先级大的先被激活。如果Sink在发送Event时失败，则下一个具有最高优先级的Sink将会用于发送Event。例如，优先级为100的接收器在优先级为80的接收器之前被激活。如果没有指定优先级，则根据在配置中指定Sink的顺序确定优先级。</p><table><thead><tr><th align="left">Property Name</th><th align="left">Default</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left"><strong>sinks</strong></td><td align="left">–</td><td align="left">Space-separated list of sinks that are participating in the group</td></tr><tr><td align="left"><strong>processor.type</strong></td><td align="left"><code>default</code></td><td align="left">The component type name, needs to be <code>failover</code></td></tr><tr><td align="left"><strong>processor.priority.</strong></td><td align="left">–</td><td align="left">Priority value. <sinkName> must be one of the sink instances associated with the current sink group A higher priority value Sink gets activated earlier. A larger absolute value indicates higher priority</td></tr><tr><td align="left">processor.maxpenalty</td><td align="left">30000</td><td align="left">The maximum backoff period for the failed Sink (in millis)</td></tr></tbody></table><ol><li><p>需要实现的功能</p><p>​    ![Failover Sink Processor](<a href="https://yerias.github.io/flume_img/Failover">https://yerias.github.io/flume_img/Failover</a> Sink Processor.jpg)</p><p>Agent1发送Event，如果Sink组中的任意一个Sink接收Event失败，其他的Sink激活继续接收</p></li><li><p>代码实现</p><p>nc-memory-avro.conf</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinkgroups.g1.processor.type = failover</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k1 = 5</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k2 = 10</span><br><span class="line">a1.sinkgroups.g1.processor.maxpenalty = 10000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun</span><br><span class="line">a1.sinks.k1.port = 55551</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = aliyun</span><br><span class="line">a1.sinks.k2.port = 55552</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure><p>avro1-memory-logger.conf </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = 55551</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>avro2-memory-logger.conf </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = 55552</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>执行命令</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/failover/nc-memory-avro.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/failover/avro1-memory-logger.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/failover/avro2-memory-logger.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li><li><p>发送消息</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">telnet aliyun 44444</span><br></pre></td></tr></table></figure><p>结果：成功</p></li></ol><h3 id="Load-balancing-Sink-Processor"><a href="#Load-balancing-Sink-Processor" class="headerlink" title="Load balancing Sink Processor"></a>Load balancing Sink Processor</h3><p>Load balancing Sink Processor提供了在多个Sink 上实现负载均衡的能力。它维护一个活动Sink的索引列表，发送的Event必须分布在这个列表上。实现支持通过round_robin或random分配负载。默认为round_robin类型，但是可以通过配置覆盖。自定义选择机制是通过继承AbstractSinkSelector的自定义类来支持的。</p><p>调用时，选择器使用其配置的选择机制选择下一个Sink 调用它。对于round_robin和random，如果选择的Sink 不能传递Event，处理器将通过其配置的选择机制选择下一个可用的Sink 。这种方法不会将失败的Sink加入黑名单，而是继续乐观地尝试每个可用的Sink。如果所有的Sink调用都导致失败，则整个程序运行失败</p><p>如果启用了backoff，Sink处理器将把失败的Sink列入黑名单，超过给定的时间后删除他们。当超时结束时，如果Sink仍然没有响应，超时将以指数方式增加，以避免在没有响应的Sink上陷入长时间的等待。禁用此功能后，在循环中，所有失败的Sink负载将按行传递到下一个Sink，因此不是均匀的。</p><table><thead><tr><th align="left">Property Name</th><th align="left">Default</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left"><strong>processor.sinks</strong></td><td align="left">–</td><td align="left">Space-separated list of sinks that are participating in the group</td></tr><tr><td align="left"><strong>processor.type</strong></td><td align="left"><code>default</code></td><td align="left">The component type name, needs to be <code>load_balance</code></td></tr><tr><td align="left">processor.backoff</td><td align="left">false</td><td align="left">Should failed sinks be backed off exponentially.</td></tr><tr><td align="left">processor.selector</td><td align="left"><code>round_robin</code></td><td align="left">Selection mechanism. Must be either <code>round_robin</code>, <code>random</code> or FQCN of custom class that inherits from <code>AbstractSinkSelector</code></td></tr><tr><td align="left">processor.selector.maxTimeOut</td><td align="left">30000</td><td align="left">Used by backoff selectors to limit exponential backoff (in milliseconds)</td></tr></tbody></table><ol><li><p>需要实现的功能</p><p>![Load balancing Sink Processor](<a href="https://yerias.github.io/flume_img/Load">https://yerias.github.io/flume_img/Load</a> balancing Sink Processor.jpg)</p><p>Agent1发送Event，Sink组中的每个Sink根据配置的选择器机制选择发送到哪一个Sink</p></li><li><p>代码实现</p><p>nc-memory-avro.conf</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">a1.sinkgroups.g1.processor.backoff = true</span><br><span class="line">a1.sinkgroups.g1.processor.selector = random</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun</span><br><span class="line">a1.sinks.k1.port = 55551</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = aliyun</span><br><span class="line">a1.sinks.k2.port = 55552</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure><p>avro1-memory-logger.conf </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = 55551</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>avro2-memory-logger.conf</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = 55552</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>执行命令</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/load_balancing/nc-memory-avro.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/load_balancing/avro1-memory-logger.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/load_balancing/avro2-memory-logger.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li><li><p>发送消息</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">telnet aliyun 44444</span><br></pre></td></tr></table></figure><p>结果：成功</p></li></ol><h2 id="Channel的两种类型"><a href="#Channel的两种类型" class="headerlink" title="Channel的两种类型"></a>Channel的两种类型</h2><p>Channel有File和Memory两种类型，Memory的特点是使用内存，速度快，但是安全性没有保障；File的特点是数据都会写进文件，速度慢，但是安全性高。</p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>案例&amp;Flume单源单出口&amp;Flume单源多出口&amp;Flume多源单出口</title>
      <link href="/2018/12/02/flume/2/"/>
      <url>/2018/12/02/flume/2/</url>
      
        <content type="html"><![CDATA[<h2 id="安装地址"><a href="#安装地址" class="headerlink" title="安装地址"></a>安装地址</h2><ol><li><p>Flume官网地址</p><p><a href="http://flume.apache.org/" target="_blank" rel="noopener">http://flume.apache.org/</a></p></li><li><p>文档查看地址</p><p><a href="http://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="noopener">http://flume.apache.org/FlumeUserGuide.html</a></p></li><li><p>下载地址</p><p><a href="http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.16.2-src.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.16.2-src.tar.gz</a></p></li></ol><h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><ol><li><p>将apache-flume-1.7.0-bin.tar.gz上传到linux的/opt/software目录下</p></li><li><p>解压apache-flume-1.7.0-bin.tar.gz到/opt/module/目录下</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun software]$ tar -zxf apache-flume-1.7.0-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li><li><p>修改apache-flume-1.7.0-bin的名称为flume</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun module]$ mv apache-flume-1.7.0-bin flume</span><br></pre></td></tr></table></figure></li><li><p>将flume/conf下的flume-env.sh.template文件修改为flume-env.sh，并配置flume-env.sh文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun conf]$ mv flume-env.sh.template flume-env.sh</span><br><span class="line">[aliyun@aliyun conf]$ vi flume-env.sh</span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure></li></ol><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><ol><li><p>安装netcat工具</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun software]$ sudo yum install -y nc</span><br></pre></td></tr></table></figure></li><li><p>判断44444端口是否被占用</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume-telnet]$ sudo netstat -tunlp | grep 44444</span><br></pre></td></tr></table></figure></li><li><p>创建Flume Agent配置文件flume-netcat-logger.conf</p><p>在flume目录下创建job文件夹并进入job文件夹。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ mkdir job</span><br><span class="line">[aliyun@aliyun flume]$ cd job/</span><br></pre></td></tr></table></figure></li><li><p>在job文件夹下创建Flume Agent配置文件flume-netcat-logger.conf。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ vim flume-netcat-logger.conf</span><br></pre></td></tr></table></figure></li><li><p>添加内容如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent    #a1表示agent的名称</span><br><span class="line">a1.sources = r1#r1表示a1的输入源</span><br><span class="line">a1.sinks = k1#k1表示a1的输出目的地</span><br><span class="line">a1.channels = c1#c1表示a1的缓冲区</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat#表示a1的输入源类型为netcat端口类型</span><br><span class="line">a1.sources.r1.bind = localhost#表示a1的监听的主机</span><br><span class="line">a1.sources.r1.port = 44444#表示a1的监听的端口号</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger#表示a1的输出目的地是控制台logger类型</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory#表示a1的channel类型是memory内存型</span><br><span class="line">a1.channels.c1.capacity = 1000#表示a1的channel总容量1000个event</span><br><span class="line">a1.channels.c1.transactionCapacity = 10#表示a1的channel传输时收集到了100条event以后再去提交事务</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1#将r1和c1连接起来</span><br><span class="line">a1.sinks.k1.channel = c1#将k1和c1连接起来</span><br></pre></td></tr></table></figure></li><li><p>先开启flume监听端口</p><p>第一种写法：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>第二种写法：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ bin/flume-ng agent -c conf/ -n a1 –f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>参数说明：</p><p>​        <code>--conf conf/：</code>表示配置文件存储在conf/目录</p><p>​        <code>--name a1：</code>表示给agent起名为a1</p><pre><code>`--conf-file job/flume-netcat.conf ：`flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。</code></pre><p>​        <code>--Dflume.root.logger==INFO,console ：</code>-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error。</p></li><li><p>使用netcat工具向本机的44444端口发送内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun ~]$ nc localhost 44444</span><br><span class="line">hello </span><br><span class="line">aliyun</span><br></pre></td></tr></table></figure></li><li><p>在Flume监听页面观察接收数据情况</p></li></ol><h2 id="实时读取本地文件到HDFS案例"><a href="#实时读取本地文件到HDFS案例" class="headerlink" title="实时读取本地文件到HDFS案例"></a>实时读取本地文件到HDFS案例</h2><p>案例需求：实时监控Hive日志，并上传到HDFS中</p><p><img src="https://yerias.github.io/flume_img/%E5%8D%95%E6%BA%90%E5%8D%95%E5%87%BA%E5%8F%A31.jpg" alt="单源单出口1"></p><ol><li><p>给Flume的lib目录下添加aliyun相关的jar包</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">commons-configuration-1.6.jar</span><br><span class="line">aliyun-auth-2.7.2.jar</span><br><span class="line">aliyun-common-2.7.2.jar</span><br><span class="line">aliyun-hdfs-2.7.2.jar</span><br><span class="line">commons-io-2.4.jar</span><br><span class="line">htrace-core-3.1.0-incubating.jar</span><br></pre></td></tr></table></figure></li><li><p>创建flume-file-hdfs.conf文件</p><p>创建文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ touch flume-file-hdfs.conf</span><br></pre></td></tr></table></figure><p>注：要想读取Linux系统中的文件，就得按照Linux命令的规则执行命令。由于Hive日志在Linux系统中所以读取文件的类型选择：exec即execute执行的意思。表示执行Linux命令来读取文件。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ vim flume-file-hdfs.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources = r2#定义source</span><br><span class="line">a2.sinks = k2#定义sink</span><br><span class="line">a2.channels = c2#定义channels</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a2.sources.r2.type = exec#定义source类型为exec可执行命令的</span><br><span class="line">a2.sources.r2.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line">a2.sources.r2.shell = /bin/bash -c#执行shell脚本的绝对路径</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k2.type = hdfs</span><br><span class="line">a2.sinks.k2.hdfs.path = hdfs://aliyun:9000/flume/%Y%m%d/%H</span><br><span class="line">a2.sinks.k2.hdfs.filePrefix = logs-#上传文件的前缀</span><br><span class="line">a2.sinks.k2.hdfs.round = true#是否按照时间滚动文件夹</span><br><span class="line">a2.sinks.k2.hdfs.roundValue = 1#多少时间单位创建一个新的文件夹</span><br><span class="line">a2.sinks.k2.hdfs.roundUnit = hour#重新定义时间单位</span><br><span class="line">a2.sinks.k2.hdfs.useLocalTimeStamp = true#是否使用本地时间戳</span><br><span class="line">a2.sinks.k2.hdfs.batchSize = 1000#积攒多少个Event才flush到HDFS一次</span><br><span class="line">a2.sinks.k2.hdfs.fileType = DataStream#设置文件类型，可支持压缩</span><br><span class="line">a2.sinks.k2.hdfs.rollInterval = 60#多久生成一个新的文件</span><br><span class="line">a2.sinks.k2.hdfs.rollSize = 134217700#设置每个文件的滚动大小</span><br><span class="line">a2.sinks.k2.hdfs.rollCount = 0#文件的滚动与Event数量无关</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a2.channels.c2.type = memory</span><br><span class="line">a2.channels.c2.capacity = 1000</span><br><span class="line">a2.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r2.channels = c2</span><br><span class="line">a2.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：</p><p>对于所有与时间相关的转义序列，Event Header中必须存在以 “timestamp”的key（除非hdfs.useLocalTimeStamp设置为true，此方法会使用TimestampInterceptor自动添加timestamp）。</p></li><li><p>执行监控配置</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/flume-file-hdfs.conf</span><br></pre></td></tr></table></figure></li><li><p>开启aliyun和Hive并操作Hive产生日志</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun aliyun-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[aliyun@aliyun103 aliyun-2.7.2]$ sbin/start-yarn.sh</span><br><span class="line"></span><br><span class="line">[aliyun@aliyun hive]$ bin/hive</span><br><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure></li><li><p>在HDFS上查看文件。</p></li></ol><h2 id="实时读取目录文件到HDFS案例"><a href="#实时读取目录文件到HDFS案例" class="headerlink" title="实时读取目录文件到HDFS案例"></a>实时读取目录文件到HDFS案例</h2><p>案例需求：使用Flume监听整个目录的文件</p><p><img src="https://yerias.github.io/flume_img/%E5%8D%95%E6%BA%90%E5%8D%95%E5%87%BA%E5%8F%A32.jpg" alt="单源单出口2"></p><ol><li><p>创建配置文件flume-dir-hdfs.conf</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ touch flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure></li><li><p>打开文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ vim flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure></li><li><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a3.sources = r3#定义sources</span><br><span class="line">a3.sinks = k3#定义sink</span><br><span class="line">a3.channels = c3#定义channel</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a3.sources.r3.type = spooldir#定义souce类型为目录</span><br><span class="line">a3.sources.r3.spoolDir = /opt/module/flume/upload#定义监控目录</span><br><span class="line">a3.sources.r3.fileSuffix = .COMPLETED#定义文件上传完的后缀</span><br><span class="line">a3.sources.r3.fileHeader = true#是否有文件头</span><br><span class="line">a3.sources.r3.ignorePattern = ([^ ]*\.tmp)#忽略所有以.tmp结尾的文件，不上传</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a3.sinks.k3.type = hdfs#sink类型为hdfs</span><br><span class="line">a3.sinks.k3.hdfs.pat=hdfs://aliyun:9000/flume/upload/%Y%m%d/%H  #文件上传到hdfs的路径</span><br><span class="line"></span><br><span class="line">a3.sinks.k3.hdfs.filePrefix = upload-#上传文件到hdfs的前缀</span><br><span class="line">a3.sinks.k3.hdfs.round = true#是否按照时间滚动文件夹</span><br><span class="line">a3.sinks.k3.hdfs.roundValue = 1#多少时间单位创建一个新的文件夹</span><br><span class="line">a3.sinks.k3.hdfs.roundUnit = hour#重新定义时间单位</span><br><span class="line">a3.sinks.k3.hdfs.useLocalTimeStamp = true#是否使用本地时间戳</span><br><span class="line">a3.sinks.k3.hdfs.batchSize = 100#积攒多少个Event才flush到HDFS一次</span><br><span class="line">a3.sinks.k3.hdfs.fileType = DataStream#设置文件类型，可支持压缩</span><br><span class="line">a3.sinks.k3.hdfs.rollInterval = 60#多久生成一个新的文件</span><br><span class="line">a3.sinks.k3.hdfs.rollSize = 134217700#设置每个文件的滚动大小大概是128M</span><br><span class="line">a3.sinks.k3.hdfs.rollCount = 0#文件的滚动与Event数量无关</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a3.channels.c3.type = memory</span><br><span class="line">a3.channels.c3.capacity = 1000</span><br><span class="line">a3.channels.c3.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a3.sources.r3.channels = c3</span><br><span class="line">a3.sinks.k3.channel = c3</span><br></pre></td></tr></table></figure></li><li><p>启动监控文件夹命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure><p>说明： 在使用Spooling Directory Source时</p><ol><li>不要在监控目录中创建并持续修改文件</li><li>上传完成的文件会以.COMPLETED结尾</li><li>被监控文件夹每500毫秒扫描一次文件变动</li></ol></li><li><p>向upload文件夹中添加文件</p><p>在/opt/module/flume目录下创建upload文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ mkdir upload</span><br></pre></td></tr></table></figure><p>向upload文件夹中添加文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun upload]$ touch aliyun.txt</span><br><span class="line">[aliyun@aliyun upload]$ touch aliyun.tmp</span><br><span class="line">[aliyun@aliyun upload]$ touch aliyun.log</span><br></pre></td></tr></table></figure></li><li><p>查看HDFS上的数据</p></li><li><p>等待1s，再次查询upload文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun upload]$ ll</span><br><span class="line">总用量 0</span><br><span class="line">-rw-rw-r--. 1 aliyun aliyun 0 5月  20 22:31 aliyun.log.COMPLETED</span><br><span class="line">-rw-rw-r--. 1 aliyun aliyun 0 5月  20 22:31 aliyun.tmp</span><br><span class="line">-rw-rw-r--. 1 aliyun aliyun 0 5月  20 22:31 aliyun.txt.COMPLETED</span><br></pre></td></tr></table></figure></li></ol><h2 id="单数据源多出口案例-选择器"><a href="#单数据源多出口案例-选择器" class="headerlink" title="单数据源多出口案例(选择器)"></a>单数据源多出口案例(选择器)</h2><p>案例需求：使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。同时Flume-1将变动内容传递给Flume-3，Flume-3负责输出到Local FileSystem。</p><p><img src="https://yerias.github.io/flume_img/%E5%8D%95%E6%BA%90%E5%A4%9A%E5%87%BA%E5%8F%A31.jpg" alt="单源多出口1"></p><ol><li><p>准备工作</p><p>在/opt/module/flume/job目录下创建group1文件夹</p><p><code>[hadoop@aliyun102 job]$ cd group1/</code></p><p>在/opt/module/datas/目录下创建flume3文件夹</p><p><code>[hadoop@aliyun102 datas]$ mkdir flume3</code></p></li><li><p>创建flume-file-flume.conf</p><p>配置1个接收日志文件的source和两个channel、两个sink，分别输送给flume-flume-hdfs和flume-flume-dir。</p><p>创建配置文件并打开</p><p><code>[hadoop@aliyun102 group1]$ touch flume-file-flume.conf</code></p><p><code>[hadoop@aliyun102 group1]$ vim flume-file-flume.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"></span><br><span class="line"># 将数据流复制给所有channel</span><br><span class="line">a1.sources.r1.selector.type = replicating</span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"> </span><br><span class="line"># Describe the sink</span><br><span class="line"># sink端的avro是一个数据发送者</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun102 </span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = aliyun102</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure><p>注：Avro是由aliyun创始人Doug Cutting创建的一种语言无关的数据序列化和RPC框架。</p><p>注：RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。</p></li><li><p>创建flume-flume-hdfs.conf</p><p>配置上级Flume输出的Source，输出是到HDFS的Sink。</p><p>创建配置文件并打开</p><p><code>[hadoop@aliyun102 group1]$ touch flume-flume-hdfs.conf</code></p><p><code>[hadoop@aliyun102 group1]$ vim flume-flume-hdfs.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line"># source端的avro是一个数据接收服务</span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = aliyun102</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k1.type = hdfs</span><br><span class="line">a2.sinks.k1.hdfs.path = hdfs://aliyun102:9000/flume2/%Y%m%d/%H</span><br><span class="line">#上传文件的前缀</span><br><span class="line">a2.sinks.k1.hdfs.filePrefix = flume2-</span><br><span class="line">#是否按照时间滚动文件夹</span><br><span class="line">a2.sinks.k1.hdfs.round = true</span><br><span class="line">#多少时间单位创建一个新的文件夹</span><br><span class="line">a2.sinks.k1.hdfs.roundValue = 1</span><br><span class="line">#重新定义时间单位</span><br><span class="line">a2.sinks.k1.hdfs.roundUnit = hour</span><br><span class="line">#是否使用本地时间戳</span><br><span class="line">a2.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">#积攒多少个Event才flush到HDFS一次</span><br><span class="line">a2.sinks.k1.hdfs.batchSize = 100</span><br><span class="line">#设置文件类型，可支持压缩</span><br><span class="line">a2.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">#多久生成一个新的文件</span><br><span class="line">a2.sinks.k1.hdfs.rollInterval = 600</span><br><span class="line">#设置每个文件的滚动大小大概是128M</span><br><span class="line">a2.sinks.k1.hdfs.rollSize = 134217700</span><br><span class="line">#文件的滚动与Event数量无关</span><br><span class="line">a2.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>创建flume-flume-dir.conf</p><p>配置上级Flume输出的Source，输出是到本地目录的Sink。</p><p>创建配置文件并打开</p><p><code>[hadoop@aliyun102 group1]$ touch flume-flume-dir.conf</code></p><p><code>[hadoop@aliyun102 group1]$ vim flume-flume-dir.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = aliyun102</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a3.sinks.k1.type = file_roll</span><br><span class="line">a3.sinks.k1.sink.directory = /opt/module/data/flume3</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure><p>提示：输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录。</p></li><li><p>执行配置文件</p><p>分别开启对应配置文件：flume-flume-dir，flume-flume-hdfs，flume-file-flume。</p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group1/flume-flume-dir.conf</code></p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group1/flume-flume-hdfs.conf</code></p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group1/flume-file-flume.conf</code></p></li><li><p>启动aliyun和Hive</p><p><code>[hadoop@aliyun102 aliyun-2.7.2]$ sbin/start-dfs.sh</code></p><p><code>[hadoop@aliyun103 aliyun-2.7.2]$ sbin/start-yarn.sh</code></p><p><code>[hadoop@aliyun102 hive]$ bin/hive</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure></li><li><p>检查HDFS上数据</p></li><li><p>检查/opt/module/datas/flume3目录中数据</p><p><code>[hadoop@aliyun102 flume3]$ ll</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">总用量 8</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 5942 5月 22 00:09 1526918887550-3</span><br></pre></td></tr></table></figure></li></ol><h2 id="单数据源多出口案例-Sink组"><a href="#单数据源多出口案例-Sink组" class="headerlink" title="单数据源多出口案例(Sink组)"></a>单数据源多出口案例(Sink组)</h2><p>案例需求：使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。同时Flume-1将变动内容传递给Flume-3，Flume-3也负责存储到HDFS </p><p><img src="https://yerias.github.io/flume_img/%E5%8D%95%E6%BA%90%E5%A4%9A%E5%87%BA%E5%8F%A32.jpg" alt="单源多出口2"></p><ol><li><p>准备工作</p><p>在/opt/module/flume/job目录下创建group2文件夹</p><p><code>[hadoop@aliyun102 job]$ cd group2/</code></p></li><li><p>创建flume-netcat-flume.conf</p><p>配置1个接收日志文件的source和1个channel、两个sink，分别输送给flume-flume-console1和flume-flume-console2。</p><p>创建配置文件并打开</p><p><code>[hadoop@aliyun102 group2]$ touch flume-netcat-flume.conf</code></p><p><code>[hadoop@aliyun102 group2]$ vim flume-netcat-flume.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">a1.sinkgroups.g1.processor.backoff = true</span><br><span class="line">a1.sinkgroups.g1.processor.selector = round_robin</span><br><span class="line">a1.sinkgroups.g1.processor.selector.maxTimeOut=10000</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun102</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = aliyun102</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure><p>注：Avro是由aliyun创始人Doug Cutting创建的一种语言无关的数据序列化和RPC框架。</p><p>注：RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。</p></li><li><p>创建flume-flume-console1.conf</p><p>配置上级Flume输出的Source，输出是到本地控制台。</p><p>创建配置文件并打开</p><p><code>[hadoop@aliyun102 group2]$ touch flume-flume-console1.conf</code></p><p><code>[hadoop@aliyun102 group2]$ vim flume-flume-console1.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = aliyun102</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>创建flume-flume-console2.conf</p><p>配置上级Flume输出的Source，输出是到本地控制台。</p><p>创建配置文件并打开</p><p><code>[hadoop@aliyun102 group2]$ touch flume-flume-console2.conf</code></p><p><code>[hadoop@aliyun102 group2]$ vim flume-flume-console2.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = aliyun102</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure></li><li><p>执行配置文件</p><p>分别开启对应配置文件：flume-flume-console2，flume-flume-console1，flume-netcat-flume。</p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group2/flume-flume-console2.conf -Dflume.root.logger=INFO,console</code></p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group2/flume-flume-console1.conf -Dflume.root.logger=INFO,console</code></p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group2/flume-netcat-flume.conf</code></p></li><li><p>使用netcat工具向本机的44444端口发送内容</p><p><code>$ nc localhost 44444</code></p></li><li><p>查看Flume2及Flume3的控制台打印</p></li></ol><h2 id="多数据源汇总案例"><a href="#多数据源汇总案例" class="headerlink" title="多数据源汇总案例"></a>多数据源汇总案例</h2><p>案例需求：</p><p>aliyun103上的Flume-1监控文件/opt/module/group.log，</p><p>aliyun102上的Flume-2监控某一个端口的数据流，</p><p>Flume-1与Flume-2将数据发送给aliyun104上的Flume-3，Flume-3将最终数据打印到控制台。</p><p><img src="https://yerias.github.io/flume_img/%E5%A4%9A%E6%BA%90%E5%8D%95%E5%87%BA%E5%8F%A3.jpg" alt="多源单出口"></p><ol><li><p>准备工作</p><p>分发Flume</p><p><code>[hadoop@aliyun102 module]$ xsync flume</code></p><p>在aliyun102、aliyun103以及aliyun104的/opt/module/flume/job目录下创建一个group3文件夹。</p><p><code>[hadoop@aliyun102 job]$ mkdir group3</code></p><p><code>[hadoop@aliyun103 job]$ mkdir group3</code></p><p><code>[hadoop@aliyun104 job]$ mkdir group3</code></p></li><li><p>创建flume1-logger-flume.conf</p><p>配置Source用于监控hive.log文件，配置Sink输出数据到下一级Flume。</p><p>在aliyun103上创建配置文件并打开</p><p><code>[hadoop@aliyun103 group3]$ touch flume1-logger-flume.conf</code></p><p><code>[hadoop@aliyun103 group3]$ vim flume1-logger-flume.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/group.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun104</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>创建flume2-netcat-flume.conf</p><p>配置Source监控端口44444数据流，配置Sink数据到下一级Flume：</p><p>在aliyun102上创建配置文件并打开</p><p><code>[hadoop@aliyun102 group3]$ touch flume2-netcat-flume.conf</code></p><p><code>[hadoop@aliyun102 group3]$ vim flume2-netcat-flume.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a2.sources.r1.type = netcat</span><br><span class="line">a2.sources.r1.bind = aliyun102</span><br><span class="line">a2.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k1.type = avro</span><br><span class="line">a2.sinks.k1.hostname = aliyun104</span><br><span class="line">a2.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>创建flume3-flume-logger.conf</p><p>配置source用于接收flume1与flume2发送过来的数据流，最终合并后sink到控制台。</p><p>在aliyun104上创建配置文件并打开</p><p><code>[hadoop@aliyun104 group3]$ touch flume3-flume-logger.conf</code></p><p><code>[hadoop@aliyun104 group3]$ vim flume3-flume-logger.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = aliyun104</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line"># Describe the sink</span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>执行配置文件</p><p>分别开启对应配置文件：flume3-flume-logger.conf，flume2-netcat-flume.conf，flume1-logger-flume.conf。</p><p><code>[hadoop@aliyun104 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group3/flume3-flume-logger.conf -Dflume.root.logger=INFO,console</code></p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group3/flume2-netcat-flume.conf</code></p><p><code>[hadoop@aliyun103 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group3/flume1-logger-flume.conf</code></p></li><li><p>在aliyun103上向/opt/module目录下的group.log追加内容</p><p><code>[hadoop@aliyun103 module]$ echo &#39;hello&#39; &gt; group.log</code></p></li><li><p>在aliyun102上向44444端口发送数据</p><p><code>[hadoop@aliyun102 flume]$ telnet aliyun102 44444</code></p></li><li><p>检查aliyun104上数据</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume架构摸排</title>
      <link href="/2018/12/01/flume/1/"/>
      <url>/2018/12/01/flume/1/</url>
      
        <content type="html"><![CDATA[<h2 id="Flume定义"><a href="#Flume定义" class="headerlink" title="Flume定义"></a>Flume定义</h2><p>Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单。</p><h2 id="Flume的优点"><a href="#Flume的优点" class="headerlink" title="Flume的优点"></a>Flume的优点</h2><ol><li><p>可以和任意存储进程集成。</p></li><li><p>输入的的数据速率大于写入目的存储的速率，flume会进行缓冲，减小hdfs的压力。</p></li><li><p>flume中的事务基于channel，使用了两个事务模型（sender + receiver），确保消息被可靠发送。</p></li></ol><p>Flume使用两个独立的事务分别负责从soucrce到channel，以及从channel到sink的事件传递。一旦事务中所有的数据全部成功提交到channel，那么source才认为该数据读取完成。同理，只有成功被sink写出去的数据，才会从channel中移除。</p><h2 id="Flume组成架构"><a href="#Flume组成架构" class="headerlink" title="Flume组成架构"></a>Flume组成架构</h2><p><img src="https://yerias.github.io/flume_img/flume.jpg" alt="flume架构图"></p><ul><li><p>Agent</p><p>Agent是一个JVM进程，它以事件的形式将数据从源头送至目的。</p><p>Agent主要有3个部分组成，Source、Channel、Sink。</p></li><li><p>Source</p><p>Source是负责接收数据到Flume Agent的组件。</p><p>Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。</p></li><li><p>Channel</p><p>Channel是位于Source和Sink之间的缓冲区。因此，Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。</p><p>Flume自带两种Channel：Memory Channel和File Channel。</p><ol><li><p>Memory Channel是内存中的队列。Memory Channel在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么Memory Channel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。</p></li><li><p>File Channel将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。</p></li></ol></li><li><p>Sink</p><p>Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。</p><p>Sink是完全事务性的。在从Channel批量删除数据之前，每个Sink用Channel启动一个事务。批量事件一旦成功写出到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。</p><p>Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。</p></li><li><p>Event</p><p>传输单元，Flume数据传输的基本单元，以事件的形式将数据从源头送至目的地。 Event由可选的header和载有数据的一个byte array 构成。Header是容纳了key-value字符串对的HashMap。</p></li></ul><h2 id="Flume拓扑结构"><a href="#Flume拓扑结构" class="headerlink" title="Flume拓扑结构"></a>Flume拓扑结构</h2><h3 id="Flume-Agent连接"><a href="#Flume-Agent连接" class="headerlink" title="Flume Agent连接"></a>Flume Agent连接</h3><p><img src="https://yerias.github.io/flume_img/flume2.jpg" alt="flume2"></p><p>这种模式是将多个flume给顺序连接起来了，从最初的source开始到最终sink传送的目的存储系统。此模式不建议桥接过多的flume数量， flume数量过多不仅会影响传输速率，而且一旦传输过程中某个节点flume宕机，会影响整个传输系统。</p><h3 id="单source，多channel、sink"><a href="#单source，多channel、sink" class="headerlink" title="单source，多channel、sink"></a>单source，多channel、sink</h3><p><img src="https://yerias.github.io/flume_img/flume3.jpg" alt="flume3"></p><p>Flume支持将事件流向一个或者多个目的地。这种模式将数据源复制到多个channel中，每个channel都有相同的数据，sink可以选择传送的不同的目的地。</p><h3 id="Flume负载均衡"><a href="#Flume负载均衡" class="headerlink" title="Flume负载均衡"></a>Flume负载均衡</h3><p><img src="https://yerias.github.io/flume_img/flume4.jpg" alt="flume4"></p><p>Flume支持使用将多个sink逻辑上分到一个sink组，flume将数据发送到不同的sink，主要解决负载均衡和故障转移问题。</p><h3 id="Flume-Agent聚合"><a href="#Flume-Agent聚合" class="headerlink" title="Flume Agent聚合"></a>Flume Agent聚合</h3><p><img src="https://yerias.github.io/flume_img/flume5.jpg" alt="flume5"></p><p>这种模式是我们最常见的，也非常实用，日常web应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用flume的这种组合方式能很好的解决这一问题，每台服务器部署一个flume采集日志，传送到一个集中收集日志的flume，再由此flume上传到hdfs、hive、hbase、jms等，进行日志分析。</p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SQOOP安装&amp;RDBMS导入HDFS&amp;RDBMS导入HIVE&amp;HDFS导入RDBMS&amp;HIVE导入RDBMS&amp;SQOOP的ETL案例&amp;在SHELL中操作MYSQL</title>
      <link href="/2018/12/01/sqoop/1/"/>
      <url>/2018/12/01/sqoop/1/</url>
      
        <content type="html"><![CDATA[<p>首先抛出两个场景</p><ol><li>数据数据在RDBMS中，你想使用Hive进行处理，怎么做</li><li>使用Hive统计分析好了，数据还在Hive中，如何导到RDBMS中</li></ol><h2 id="Sqoop安装"><a href="#Sqoop安装" class="headerlink" title="Sqoop安装"></a>Sqoop安装</h2><ol><li><p>下载并解压</p><p>下载地址：<a href="http://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.16.2.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.16.2.tar.gz</a></p><p>上传安装包sqoop-1.4.6-cdh5.16.2.tar.gz到主机中</p><p>解压sqoop安装包到指定目录，如：$ tar -zxf sqoop-1.4.6-cdh5.16.2.tar.gz -C /opt/module/</p></li><li><p>修改配置文件</p><p>重命名配置文件</p><p><code>$ mv sqoop-env-template.sh sqoop-env.sh</code></p><p>修改配置文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HADOOP_COMMON_HOME=/opt/module/hadoop</span><br><span class="line">export HADOOP_MAPRED_HOME=/opt/module/hadoop</span><br><span class="line">export HIVE_HOME=/opt/module/hive</span><br><span class="line">export ZOOKEEPER_HOME=/opt/module/zookeeper</span><br><span class="line">export ZOOCFGDIR=/opt/module/zookeeper</span><br><span class="line">export HBASE_HOME=/opt/module/hbase</span><br></pre></td></tr></table></figure></li><li><p>拷贝JDBC驱动</p><p><code>$ cp mysql-connector-java-5.1.27-bin.jar /opt/module/sqoop/lib/</code></p></li><li><p>验证Sqoop</p><p><code>$ bin/sqoop help</code></p><p>出现一些Warning警告（警告信息已省略），并伴随着帮助命令的输出：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Available commands:</span><br><span class="line">  codegen            Generate code to interact with database records</span><br><span class="line">  create-hive-table  Import a table definition into Hive</span><br><span class="line">  eval               Evaluate a SQL statement and display the results</span><br><span class="line">  export             Export an HDFS directory to a database table</span><br><span class="line">  help               List available commands</span><br><span class="line">  import             Import a table from a database to HDFS</span><br><span class="line">  import-all-tables  Import tables from a database to HDFS</span><br><span class="line">  import-mainframe   Import datasets from a mainframe server to HDFS</span><br><span class="line">  job                Work with saved jobs</span><br><span class="line">  list-databases     List available databases on a server</span><br><span class="line">  list-tables        List available tables in a database</span><br><span class="line">  merge              Merge results of incremental imports</span><br><span class="line">  metastore          Run a standalone Sqoop metastore</span><br><span class="line">  version            Display version information</span><br></pre></td></tr></table></figure></li><li><p>测试Sqoop是否能够成功连接数据库</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop \</span><br><span class="line">list-databases \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/ \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root</span></span><br></pre></td></tr></table></figure><p>数据库用到的参数:</p><ul><li>“\“： 代表换行</li><li>“connect”：连接数据库</li><li>username：用户</li><li>password：密码</li></ul><p>显示所有的数据库列表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">information_schema</span><br><span class="line">leyou1</span><br><span class="line">metastore</span><br><span class="line">mypython</span><br><span class="line">mysql</span><br><span class="line">performance_schema</span><br><span class="line">sys</span><br><span class="line">travel</span><br><span class="line">tunan</span><br></pre></td></tr></table></figure></li></ol><h2 id="RDBMS到HDFS"><a href="#RDBMS到HDFS" class="headerlink" title="RDBMS到HDFS"></a>RDBMS到HDFS</h2><h3 id="MySQL准备表和数据"><a href="#MySQL准备表和数据" class="headerlink" title="MySQL准备表和数据"></a>MySQL准备表和数据</h3><ol><li><p>确定mysql服务开启正常</p></li><li><p>在mysql中创建库表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> company;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> company.staff(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">4</span>) primary <span class="keyword">key</span> <span class="keyword">not</span> <span class="literal">null</span> auto_increment, </span><br><span class="line">    <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">255</span>), </span><br><span class="line">    sex <span class="built_in">varchar</span>(<span class="number">255</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure></li><li><p>插入数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> company.staff(<span class="keyword">name</span>, sex) <span class="keyword">values</span>(<span class="string">'Thomas'</span>, <span class="string">'Male'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> company.staff(<span class="keyword">name</span>, sex) <span class="keyword">values</span>(<span class="string">'Catalina'</span>, <span class="string">'FeMale'</span>);</span><br></pre></td></tr></table></figure></li><li><p>查看数据</p><p><code>select * from staff;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">----+----------+--------+</span></span><br><span class="line">| id | name     | sex    |</span><br><span class="line">+<span class="comment">----+----------+--------+</span></span><br><span class="line">|  1 | Thomas   | Male   |</span><br><span class="line">|  2 | Catalina | FeMale |</span><br><span class="line">+<span class="comment">----+----------+--------+</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="将MySQL数据导入到HDFS"><a href="#将MySQL数据导入到HDFS" class="headerlink" title="将MySQL数据导入到HDFS"></a>将MySQL数据导入到HDFS</h3><h4 id="全部导入"><a href="#全部导入" class="headerlink" title="全部导入"></a>全部导入</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://aliyun:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table staff \</span><br><span class="line">--target-dir /user/company \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &quot;\t&quot;</span><br></pre></td></tr></table></figure><p>全部导入用到的参数:</p><ul><li>table：指定被导入的表名</li><li>target-dir：指定导入路径</li><li>delete-target-dir：如果目标目录存在就删除它</li><li>num-mappers：mapper的个数</li><li>fields-terminated-by：指定字段分隔符</li></ul><h4 id="查询导入-query"><a href="#查询导入-query" class="headerlink" title="查询导入(query)"></a>查询导入(query)</h4><p>query参数就可以让用户随意写sql语句来查询了。query和table参数是互斥的。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://aliyun:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--target-dir /user/company \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &quot;\t&quot; \</span><br><span class="line">--query &apos;select name,sex from staff where id &lt;=1 and $CONDITIONS;&apos;</span><br></pre></td></tr></table></figure><ul><li>query：指定查询SQL where条件要有$CONDITIONS</li></ul><p>注意:  <code>must contain &#39;$CONDITIONS&#39; in WHERE clause.</code></p><p>如果query后使用的是双引号，则$CONDITIONS前必须加转移符，防止shell识别为自己的变量。</p><h4 id="导入指定列-columns"><a href="#导入指定列-columns" class="headerlink" title="导入指定列(columns)"></a>导入指定列(columns)</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--target-dir /user/company \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--num-mappers 1 \</span></span><br><span class="line"><span class="comment">--fields-terminated-by "\t" \</span></span><br><span class="line"><span class="comment">--columns id,sex \</span></span><br><span class="line"><span class="comment">--table staff</span></span><br></pre></td></tr></table></figure><ul><li>columns：指定导入的列</li></ul><h4 id="筛选查询导入数据-where"><a href="#筛选查询导入数据-where" class="headerlink" title="筛选查询导入数据(where)"></a>筛选查询导入数据(where)</h4><p>where参数可以进行一些简单的筛选</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--target-dir /user/company \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--num-mappers 1 \</span></span><br><span class="line"><span class="comment">--fields-terminated-by "\t" \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--where "id=1"</span></span><br></pre></td></tr></table></figure><ul><li>where：指定查询过滤条件</li></ul><h4 id="增量导入"><a href="#增量导入" class="headerlink" title="增量导入"></a>增量导入</h4><p>增量导入的一个场景就是昨天导入了一批数据，今天又增加了部分数据，现在要把这部分数据也导入到hdfs中。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://localhost:3306/company \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--target-dir /user/company/ \</span></span><br><span class="line"><span class="comment">--num-mappers 1 \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--null-string "" \</span></span><br><span class="line"><span class="comment">--null-non-string "0" \</span></span><br><span class="line"><span class="comment">--check-column "id" \</span></span><br><span class="line"><span class="comment">--incremental append \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line"><span class="comment">--last-value 0</span></span><br></pre></td></tr></table></figure><ul><li>null-string：字符串为null怎么处理</li><li>null-non-string：其他类型为null怎么处理</li><li>check-column：根据哪一行做增量导入</li><li>last-value：开始增量导入的上个位置</li></ul><h2 id="RDBMS导入到Hive"><a href="#RDBMS导入到Hive" class="headerlink" title="RDBMS导入到Hive"></a>RDBMS导入到Hive</h2><ol><li><p>在导入hive之前先在hive创建一样的表结构</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> staff(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">4</span>) , </span><br><span class="line">    <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">255</span>), </span><br><span class="line">    sex <span class="built_in">varchar</span>(<span class="number">255</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure></li><li><p>使用sqoop导入数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--hive-overwrite \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--null-string "" \</span></span><br><span class="line"><span class="comment">--null-non-string "0" \</span></span><br><span class="line"><span class="comment">--hive-import \</span></span><br><span class="line"><span class="comment">--hive-database default \</span></span><br><span class="line"><span class="comment">--hive-table staff \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line"><span class="comment">--num-mappers 1</span></span><br></pre></td></tr></table></figure><ul><li><p>hive-import：数据从关系数据库中导入到hive表中</p></li><li><p>hive-overwrite：覆盖掉在hive表中已经存在的数据</p></li><li><p>hive-table：后面接hive表,默认使用MySQL的表名</p></li><li><p>如果导入的是分区表，需要指定分区的key和value</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--hive-partition-key key \</span></span><br><span class="line"><span class="comment">--hive-partition-value value \</span></span><br></pre></td></tr></table></figure></li></ul><p>该过程分为两步，第一步将数据导入到HDFS，第二步将导入到HDFS的数据迁移到Hive仓库，第一步默认的临时目录是/user/hadoop/表名</p></li><li><p>查看hive表中的数据:</p><p><code>select * from staff;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">id|name    |sex   |</span><br><span class="line"><span class="comment">--|--------|------|</span></span><br><span class="line"> 1|Thomas  |Male  |</span><br><span class="line"> 2|Catalina|FeMale|</span><br></pre></td></tr></table></figure></li></ol><h2 id="HDFS导出到RDBMS"><a href="#HDFS导出到RDBMS" class="headerlink" title="HDFS导出到RDBMS"></a>HDFS导出到RDBMS</h2><ol><li><p>首先保证MySQL创建了一张和Hive一样表结构的表用来接收数据</p><p>注意表结构和分隔符都要一样</p></li><li><p>写Sqoop代码(批量导入)</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line">-Dsqoop.export.records.per.statement=10 \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--export-dir /user/company/ \</span></span><br><span class="line"><span class="comment">--null-string "" \</span></span><br><span class="line"><span class="comment">--null-non-string "0" \</span></span><br><span class="line"><span class="comment">--columns "id,name" \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure><ul><li><code>Dsqoop.export.records.per.statement</code>：批量更新，每隔10条提交一次 </li><li>export-dir：导出的hdfs目录</li><li>table：导入的表名</li><li>columns：指定导入的列</li></ul><p><code>注意:</code> MySQL中如果表不存在，不会自动创建</p></li></ol><h2 id="Hive导出到RDBMS"><a href="#Hive导出到RDBMS" class="headerlink" title="Hive导出到RDBMS"></a>Hive导出到RDBMS</h2><ol><li><p>首先保证MySQL创建了一张和Hive一样表结构的表用来接收数据</p><p>注意表结构和分隔符都要一样</p></li><li><p>写Sqoop代码</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--num-mappers 1 \</span></span><br><span class="line"><span class="comment">--export-dir /user/hive/warehouse/staff \</span></span><br><span class="line"><span class="comment">--input-fields-terminated-by "\t"</span></span><br></pre></td></tr></table></figure><ul><li><p>export-dir：指定被导出的目录</p></li><li><p>input-fields-terminated-by：导入的分隔符格式，和导入的fields-terminated-by有区别</p></li></ul><p><code>注意:</code> Mysql中如果表不存在，不会自动创建</p></li><li><p>查看MySQL数据库中的数据</p><p><code>select * from staff;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">----+----------+--------+</span></span><br><span class="line">| id | name     | sex    |</span><br><span class="line">+<span class="comment">----+----------+--------+</span></span><br><span class="line">|  1 | Thomas   | Male   |</span><br><span class="line">|  2 | Catalina | FeMale |</span><br><span class="line">|  3 | tunan    | Male   |</span><br><span class="line">+<span class="comment">----+----------+--------+</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="Sqoop的综合操作"><a href="#Sqoop的综合操作" class="headerlink" title="Sqoop的综合操作"></a>Sqoop的综合操作</h2><p>需求：emp和dept表是在MySQL，把MySQL数据抽取到Hive进行统计分析，然后把统计的结果回写到MySQL中</p><ol><li><p>在Hive中创建与MySQL中emp和dept表相对应的表emp_hive，dept_hive</p><p><code>emp_hive表</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> emp_hive(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">job <span class="keyword">string</span>,</span><br><span class="line">mgr <span class="built_in">int</span>,</span><br><span class="line">hiredate <span class="keyword">string</span>,</span><br><span class="line">sal <span class="keyword">double</span>,</span><br><span class="line">comm <span class="keyword">double</span>,</span><br><span class="line">deptno <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p><code>dept_hive表</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> dept_hive(</span><br><span class="line">deptno <span class="built_in">int</span>,</span><br><span class="line">dname <span class="keyword">string</span>,</span><br><span class="line">loc <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure></li><li><p>创建在Hive中的中间结果表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> mid_hive(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">deptno <span class="built_in">int</span>,</span><br><span class="line">dname <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure></li><li><p>将MySQL中的emp表中的数据传到emp_hive中</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/tunan \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--table emp \</span></span><br><span class="line"><span class="comment">--hive-overwrite \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--null-string "-" \</span></span><br><span class="line"><span class="comment">--null-non-string "0" \</span></span><br><span class="line"><span class="comment">--hive-import \</span></span><br><span class="line"><span class="comment">--hive-database default \</span></span><br><span class="line"><span class="comment">--hive-table emp_hive \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure></li><li><p>将MySQL中的dept表中的数据传到dept_hive</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/tunan \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--table dept \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--null-string "-" \</span></span><br><span class="line"><span class="comment">--null-non-string '0' \</span></span><br><span class="line"><span class="comment">--hive-import \</span></span><br><span class="line"><span class="comment">--hive-database default \</span></span><br><span class="line"><span class="comment">--hive-table dept_hive \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure></li><li><p>将Hive中的表进行业务处理</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> </span><br><span class="line">mid_hive</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">e.empno,e.ename,d.deptno,d.dname</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">emp_hive  e</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">dept_hive  d</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">e.deptno=d.deptno</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">*</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">mid_hive;</span><br></pre></td></tr></table></figure></li><li><p>查看中间表的数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> mid_hive;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |deptno|dname     |</span><br><span class="line"><span class="comment">-----|------|------|----------|</span></span><br><span class="line"> 7369|SMITH |    20|RESEARCH  |</span><br><span class="line"> 7499|ALLEN |    30|SALES     |</span><br><span class="line"> 7521|WARD  |    30|SALES     |</span><br><span class="line"> 7566|JONES |    20|RESEARCH  |</span><br><span class="line"> 7654|MARTIN|    30|SALES     |</span><br><span class="line"> 7698|BLAKE |    30|SALES     |</span><br><span class="line"> 7782|CLARK |    10|ACCOUNTING|</span><br><span class="line"> 7788|SCOTT |    20|RESEARCH  |</span><br><span class="line"> 7839|KING  |    10|ACCOUNTING|</span><br><span class="line"> 7844|TURNER|    30|SALES     |</span><br><span class="line"> 7876|ADAMS |    20|RESEARCH  |</span><br><span class="line"> 7900|JAMES |    30|SALES     |</span><br><span class="line"> 7902|FORD  |    20|RESEARCH  |</span><br><span class="line"> 7934|MILLER|    10|ACCOUNTING|</span><br></pre></td></tr></table></figure></li><li><p>在MySQL中创建返回数据的表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="string">`mid`</span>(</span><br><span class="line">empno <span class="built_in">int</span>(<span class="number">11</span>),</span><br><span class="line">ename <span class="built_in">varchar</span>(<span class="number">20</span>),</span><br><span class="line">deptno <span class="built_in">int</span>(<span class="number">11</span>),</span><br><span class="line">dname <span class="built_in">varchar</span>(<span class="number">20</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p>将处理好的数据用Sqoop发回MySQL</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/tunan \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--table mid \</span></span><br><span class="line">-m 1 \</span><br><span class="line"><span class="comment">--export-dir /user/hive/warehouse/mid_hive \</span></span><br><span class="line"><span class="comment">--input-fields-terminated-by '\t'</span></span><br></pre></td></tr></table></figure></li><li><p>查看MySQL中已经发回的数据</p><p><code>select * from mid;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">-------+--------+--------+------------+</span></span><br><span class="line">| empno | ename  | deptno | dname      |</span><br><span class="line">+<span class="comment">-------+--------+--------+------------+</span></span><br><span class="line">|  7369 | SMITH  |     20 | RESEARCH   |</span><br><span class="line">|  7499 | ALLEN  |     30 | SALES      |</span><br><span class="line">|  7521 | WARD   |     30 | SALES      |</span><br><span class="line">|  7566 | JONES  |     20 | RESEARCH   |</span><br><span class="line">|  7654 | MARTIN |     30 | SALES      |</span><br><span class="line">|  7698 | BLAKE  |     30 | SALES      |</span><br><span class="line">|  7782 | CLARK  |     10 | ACCOUNTING |</span><br><span class="line">|  7788 | SCOTT  |     20 | RESEARCH   |</span><br><span class="line">|  7839 | KING   |     10 | ACCOUNTING |</span><br><span class="line">|  7844 | TURNER |     30 | SALES      |</span><br><span class="line">|  7876 | ADAMS  |     20 | RESEARCH   |</span><br><span class="line">|  7900 | JAMES  |     30 | SALES      |</span><br><span class="line">|  7902 | FORD   |     20 | RESEARCH   |</span><br><span class="line">|  7934 | MILLER |     10 | ACCOUNTING |</span><br><span class="line">+<span class="comment">-------+--------+--------+------------+</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="shell操作数据库"><a href="#shell操作数据库" class="headerlink" title="shell操作数据库"></a>shell操作数据库</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql -uroot -pruozedata &lt;&lt;EOF</span><br><span class="line"><span class="keyword">use</span> sqoop;</span><br><span class="line"><span class="keyword">truncate</span> etl_result;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS Block损坏恢复</title>
      <link href="/2018/10/10/PE/2/"/>
      <url>/2018/10/10/PE/2/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">上传:</span><br><span class="line">-bash-4.2$ hdfs dfs -mkdir /blockrecover</span><br><span class="line">-bash-4.2$ echo &quot;www.ruozedata.com&quot; &gt; ruozedata.md</span><br><span class="line"></span><br><span class="line">-bash-4.2$ hdfs dfs -put ruozedata.md /blockrecover</span><br><span class="line">-bash-4.2$ hdfs dfs -ls /blockrecover</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 hdfs supergroup         18 2019-03-03 14:42 /blockrecover/ruozedata.md</span><br><span class="line">-bash-4.2$ </span><br><span class="line"></span><br><span class="line">校验: 健康状态</span><br><span class="line">-bash-4.2$ hdfs fsck /</span><br><span class="line">Connecting to namenode via http://yws76:50070/fsck?ugi=hdfs&amp;path=%2F</span><br><span class="line">FSCK started by hdfs (auth:SIMPLE) from /192.168.0.76 for path / at Sun Mar 03 14:44:44 CST 2019</span><br><span class="line">...............................................................................Status: HEALTHY</span><br><span class="line"> Total size:    50194618424 B</span><br><span class="line"> Total dirs:    354</span><br><span class="line"> Total files:   1079</span><br><span class="line"> Total symlinks:                0</span><br><span class="line"> Total blocks (validated):      992 (avg. block size 50599413 B)</span><br><span class="line"> Minimally replicated blocks:   992 (100.0 %)</span><br><span class="line"> Over-replicated blocks:        0 (0.0 %)</span><br><span class="line"> Under-replicated blocks:       0 (0.0 %)</span><br><span class="line"> Mis-replicated blocks:         0 (0.0 %)</span><br><span class="line"> Default replication factor:    3</span><br><span class="line"> Average block replication:     3.0</span><br><span class="line"> Corrupt blocks:                0</span><br><span class="line"> Missing replicas:              0 (0.0 %)</span><br><span class="line"> Number of data-nodes:          3</span><br><span class="line"> Number of racks:               1</span><br><span class="line">FSCK ended at Sun Mar 03 14:44:45 CST 2019 in 76 milliseconds</span><br><span class="line"></span><br><span class="line">The filesystem under path &apos;/&apos; is HEALTHY</span><br><span class="line">-bash-4.2$</span><br></pre></td></tr></table></figure><h3 id="直接DN节点上删除文件一个block的一个副本-3副本"><a href="#直接DN节点上删除文件一个block的一个副本-3副本" class="headerlink" title="直接DN节点上删除文件一个block的一个副本(3副本)"></a>直接DN节点上删除文件一个block的一个副本(3副本)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">删除块和meta文件:</span><br><span class="line">[root@yws87 subdir135]# rm -rf blk_1075808214 blk_1075808214_2068515.meta</span><br><span class="line"></span><br><span class="line">直接重启HDFS，直接模拟损坏效果，然后fsck检查:</span><br><span class="line">-bash-4.2$ hdfs fsck /</span><br><span class="line">Connecting to namenode via http://yws77:50070/fsck?ugi=hdfs&amp;path=%2F</span><br><span class="line">FSCK started by hdfs (auth:SIMPLE) from /192.168.0.76 for path / at Sun Mar 03 16:02:04 CST 2019</span><br><span class="line">.</span><br><span class="line">/blockrecover/ruozedata.md:  Under replicated BP-1513979236-192.168.0.76-1514982530341:blk_1075808214_2068515. Target Replicas is 3 but found 2 live replica(s), 0 decommissioned replica(s), 0 decommissioning replica(s).</span><br><span class="line">...............................................................................Status: HEALTHY</span><br><span class="line"> Total size:    50194618424 B</span><br><span class="line"> Total dirs:    354</span><br><span class="line"> Total files:   1079</span><br><span class="line"> Total symlinks:                0</span><br><span class="line"> Total blocks (validated):      992 (avg. block size 50599413 B)</span><br><span class="line"> Minimally replicated blocks:   992 (100.0 %)</span><br><span class="line"> Over-replicated blocks:        0 (0.0 %)</span><br><span class="line"> Under-replicated blocks:       1 (0.10080645 %)</span><br><span class="line"> Mis-replicated blocks:         0 (0.0 %)</span><br><span class="line"> Default replication factor:    3</span><br><span class="line"> Average block replication:     2.998992</span><br><span class="line"> Corrupt blocks:                0</span><br><span class="line"> Missing replicas:              1 (0.033602152 %)</span><br><span class="line"> Number of data-nodes:          3</span><br><span class="line"> Number of racks:               1</span><br><span class="line">FSCK ended at Sun Mar 03 16:02:04 CST 2019 in 148 milliseconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The filesystem under path &apos;/&apos; is HEALTHY</span><br><span class="line">-bash-4.2$</span><br></pre></td></tr></table></figure><h3 id="手动修复hdfs-debug"><a href="#手动修复hdfs-debug" class="headerlink" title="手动修复hdfs debug"></a>手动修复hdfs debug</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-bash-4.2$ hdfs |grep debug</span><br><span class="line">没有输出debug参数的任何信息结果！</span><br><span class="line">故hdfs命令帮助是没有debug的，但是确实有hdfs debug这个组合命令，切记。</span><br><span class="line"></span><br><span class="line">修复命令:</span><br><span class="line">-bash-4.2$ hdfs debug  recoverLease  -path /blockrecover/ruozedata.md -retries 10</span><br><span class="line">recoverLease SUCCEEDED on /blockrecover/ruozedata.md</span><br><span class="line">-bash-4.2$ </span><br><span class="line"></span><br><span class="line">直接DN节点查看，block文件和meta文件恢复:</span><br><span class="line">[root@yws87 subdir135]# ll</span><br><span class="line">total 8</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 56 Mar  3 14:28 blk_1075808202</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 14:28 blk_1075808202_2068503.meta</span><br><span class="line">[root@yws87 subdir135]# ll</span><br><span class="line">total 24</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 56 Mar  3 14:28 blk_1075808202</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 14:28 blk_1075808202_2068503.meta</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 18 Mar  3 15:23 blk_1075808214</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 15:23 blk_1075808214_2068515.meta</span><br></pre></td></tr></table></figure><h3 id="自动修复"><a href="#自动修复" class="headerlink" title="自动修复"></a>自动修复</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">当数据块损坏后，DN节点执行directoryscan操作之前，都不会发现损坏；</span><br><span class="line">也就是directoryscan操作是间隔6h</span><br><span class="line">dfs.datanode.directoryscan.interval : 21600</span><br><span class="line"></span><br><span class="line">在DN向NN进行blockreport前，都不会恢复数据块;</span><br><span class="line">也就是blockreport操作是间隔6h</span><br><span class="line">dfs.blockreport.intervalMsec : 21600000</span><br><span class="line"></span><br><span class="line">当NN收到blockreport才会进行恢复操作。</span><br></pre></td></tr></table></figure><p>具体参考生产上HDFS（CDH5.12.0）对应的版本的文档参数:<a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.12.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.12.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>生产上本人一般倾向于使用 手动修复方式，但是前提要手动删除损坏的block块。</p><p>切记，是删除损坏block文件和meta文件，而不是删除hdfs文件。</p><p>当然还可以先把文件get下载，然后hdfs删除，再对应上传。</p><p>切记删除不要执行: hdfs fsck / -delete 这是删除损坏的文件， 那么数据不就丢了嘛；除非无所谓丢数据，或者有信心从其他地方可以补数据到hdfs！</p><h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><ul><li>那么如何确定一个文件的损失的块位置，哪几种方法呢？</li><li>CDH的配置里搜索没有这两个参数，怎么调整生效呢？</li></ul><p>转载来源: [<a href="https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/]" target="_blank" rel="noopener">https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/]</a>(<a href="https://ruozedata.github.io/2019/06/06/生产HDFS" target="_blank" rel="noopener">https://ruozedata.github.io/2019/06/06/生产HDFS</a> Block损坏恢复最佳实践(含思考题)/)</p>]]></content>
      
      
      <categories>
          
          <category> 生产故障案例 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生产故障案例 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DataNode OOM溢出</title>
      <link href="/2018/10/09/PE/1/"/>
      <url>/2018/10/09/PE/1/</url>
      
        <content type="html"><![CDATA[<h3 id="DataNode的内存溢出报错"><a href="#DataNode的内存溢出报错" class="headerlink" title="DataNode的内存溢出报错"></a>DataNode的内存溢出报错</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2017-12-17 23:58:14,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1437036909-![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.36-1509097205664:blk_1074725940_987917, type=HAS_DOWNSTREAM_IN_PIPELINE terminating</span><br><span class="line">2017-12-17 23:58:31,425 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode is out of memory. Will retry in 30 seconds.</span><br><span class="line">java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">at java.lang.Thread.start0(Native Method)</span><br><span class="line">at java.lang.Thread.start(Thread.java:714)</span><br><span class="line">at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:154)</span><br><span class="line">at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">2017-12-17 23:59:01,426 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode is out of memory. Will retry in 30 seconds.</span><br><span class="line">java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">at java.lang.Thread.start0(Native Method)</span><br><span class="line">at java.lang.Thread.start(Thread.java:714)</span><br><span class="line">at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:154)</span><br><span class="line">at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">2017-12-17 23:59:05,520 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode is out of memory. Will retry in 30 seconds.</span><br><span class="line">java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">at java.lang.Thread.start0(Native Method)</span><br><span class="line">at java.lang.Thread.start(Thread.java:714)</span><br><span class="line">at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:154)</span><br><span class="line">at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">2017-12-17 23:59:31,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1437036909-![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.36-1509097205664:blk_1074725951_987928 src: /![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.54:40478 dest: /![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.48:50010</span><br></pre></td></tr></table></figure><h3 id="CDH查看DataNode的内存情况"><a href="#CDH查看DataNode的内存情况" class="headerlink" title="CDH查看DataNode的内存情况"></a>CDH查看DataNode的内存情况</h3><p><img src="https://yerias.github.io/hadoop_img/20191205092342.jpg" alt="datanode内存使用图"></p><h3 id="明明1G的内存都没有使用，为什么会报OOM？"><a href="#明明1G的内存都没有使用，为什么会报OOM？" class="headerlink" title="明明1G的内存都没有使用，为什么会报OOM？"></a>明明1G的内存都没有使用，为什么会报OOM？</h3><p>可以确定是操作系统哪里设置错了，我想应该是把产品环境的某个参数配置错了，系统本身的影响肯定不会有了，因为产品环境上我们只create了800左右个线程，就OOM了，那应该就是配置的问题了</p><p>解决方法：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1.</span><br><span class="line"></span><br><span class="line">echo "kernel.threads-max=196605" &gt;&gt; /etc/sysctl.conf</span><br><span class="line">echo "kernel.pid_max=196605" &gt;&gt; /etc/sysctl.conf</span><br><span class="line">echo "vm.max_map_count=393210" &gt;&gt; /etc/sysctl.conf</span><br><span class="line">sysctl -p </span><br><span class="line"></span><br><span class="line">2.</span><br><span class="line"></span><br><span class="line">/etc/security/limits.conf</span><br><span class="line">* soft nofile 196605</span><br><span class="line">* hard nofile 196605</span><br><span class="line">* soft nproc 196605</span><br><span class="line">* hard nproc 196605</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 生产故障案例 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生产故障案例 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux下MySQL进程死掉的可能解决方案</title>
      <link href="/2018/10/08/mysql/5/"/>
      <url>/2018/10/08/mysql/5/</url>
      
        <content type="html"><![CDATA[<p>linux下mysql进程死掉，且无法启动mysql服务，查看myql日志，发现如下日志：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">2019-10-10 18:11:03 9772 [Note] InnoDB: Initializing buffer pool, size = 128.0M</span><br><span class="line">InnoDB: mmap(136019968 bytes) failed; errno 12</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] InnoDB: Cannot allocate memory for the buffer pool</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Plugin 'InnoDB' init function returned error.</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Unknown/unsupported storage engine: InnoDB</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Aborting</span><br></pre></td></tr></table></figure><p>其中InnoDB: mmap(136019968 bytes) failed; errno 12是关键的错误信息。<br>从网上查资料，有人说修改innodb_buffer_pool_size，经过测试无效。<br>有人说是swap分区为0导致的此错误，使用free -m命令查看系统内存，发现swap确实为0。使用如下命令建立一个临时的swap分区：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">d if=/dev/zero of=/swap bs=1M count=512  //创建一个swap文件，大小为512M</span><br><span class="line">mkswap /swap                              //将swap文件变为swap分区文件</span><br><span class="line">swapon /swap                              //将其映射为swap分区</span><br></pre></td></tr></table></figure><p>此时使用<code>free -m</code>命令即可看到swap分区已存在了，然后启动mysql服务即可。<br>为了保证下次系统启动后，此swap分区被自动加载，需要修改系统的fstab文件，操作如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">vi /etc/fstab</span><br><span class="line">//在其中添加如下一行</span><br><span class="line">/swap swap swap defaults 0 0</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL中的Top N</title>
      <link href="/2018/10/08/mysql/4/"/>
      <url>/2018/10/08/mysql/4/</url>
      
        <content type="html"><![CDATA[<h3 id="切入点"><a href="#切入点" class="headerlink" title="切入点"></a>切入点</h3><p>MySQL没有获取Top N的这种函数，但是在MySQL中求Top N又是必须掌握的点</p><p>比如查询分组后的最大值、最小值所在的整行记录或者分组后的Top N行记录</p><p>下面我们就如何在MySQL中求Top N做出深度的思考和验证</p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>测试表结构如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">&gt;&gt; CREATE TABLE `student` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `name` varchar(20) DEFAULT NULL,</span><br><span class="line">  `course` varchar(20) DEFAULT NULL,</span><br><span class="line">  `score` int(11) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8</span><br></pre></td></tr></table></figure><p> 插入数据：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">&gt;&gt; insert into student(name,course,score)</span><br><span class="line"><span class="keyword">values</span></span><br><span class="line">(<span class="string">'张三'</span>,<span class="string">'语文'</span>,<span class="number">80</span>),</span><br><span class="line">(<span class="string">'李四'</span>,<span class="string">'语文'</span>,<span class="number">90</span>),</span><br><span class="line">(<span class="string">'王五'</span>,<span class="string">'语文'</span>,<span class="number">93</span>),</span><br><span class="line">(<span class="string">'张三'</span>,<span class="string">'数学'</span>,<span class="number">77</span>),</span><br><span class="line">(<span class="string">'李四'</span>,<span class="string">'数学'</span>,<span class="number">68</span>),</span><br><span class="line">(<span class="string">'王五'</span>,<span class="string">'数学'</span>,<span class="number">99</span>),</span><br><span class="line">(<span class="string">'张三'</span>,<span class="string">'英语'</span>,<span class="number">90</span>),</span><br><span class="line">(<span class="string">'李四'</span>,<span class="string">'英语'</span>,<span class="number">50</span>),</span><br><span class="line">(<span class="string">'王五'</span>,<span class="string">'英语'</span>,<span class="number">89</span>);</span><br></pre></td></tr></table></figure><p>查看结果：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">&gt;&gt; select * from student;</span><br><span class="line">+<span class="comment">----+--------+--------+-------+</span></span><br><span class="line">| id | name   | course | score |</span><br><span class="line">+<span class="comment">----+--------+--------+-------+</span></span><br><span class="line">|  1 | 张三   | 语文   |    80 |</span><br><span class="line">|  2 | 李四   | 语文   |    90 |</span><br><span class="line">|  3 | 王五   | 语文   |    93 |</span><br><span class="line">|  4 | 张三   | 数学   |    77 |</span><br><span class="line">|  5 | 李四   | 数学   |    68 |</span><br><span class="line">|  6 | 王五   | 数学   |    99 |</span><br><span class="line">|  7 | 张三   | 英语   |    90 |</span><br><span class="line">|  8 | 李四   | 英语   |    50 |</span><br><span class="line">|  9 | 王五   | 英语   |    89 |</span><br><span class="line">+<span class="comment">----+--------+--------+-------+</span></span><br></pre></td></tr></table></figure><h3 id="TOP-1"><a href="#TOP-1" class="headerlink" title="TOP 1"></a>TOP 1</h3><p>查询每门课程分数最高的学生以及成绩</p><ol><li><p>我们先拆分题目，这是一题查询分组求最大值的题目，拆分后的题目是：查询 每门课程 分数最高 的学生以及成绩</p><p>我们首先按照常规思路来写SQL: </p><p>select 学生姓名，学生分数</p><p>group by 课程</p><p>max(分数) </p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">s.name,s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">s.course;</span><br></pre></td></tr></table></figure><p>得出的查询结果是:</p><table><thead><tr><th>name</th><th>course</th><th>score</th></tr></thead><tbody><tr><td>张三</td><td>数学</td><td>99</td></tr><tr><td>张三</td><td>英语</td><td>90</td></tr><tr><td>张三</td><td>语文</td><td>93</td></tr></tbody></table><h4 id="问题-为什么姓名都是张三？课程和对应的成绩又全是对的？"><a href="#问题-为什么姓名都是张三？课程和对应的成绩又全是对的？" class="headerlink" title="问题: 为什么姓名都是张三？课程和对应的成绩又全是对的？"></a>问题: 为什么姓名都是张三？课程和对应的成绩又全是对的？</h4><p>我预测是因为没有把姓名加入group的分组字段，那么我们把姓名加入group的分组字段后试试看</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">s.name,s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">s.name,s.course;</span><br></pre></td></tr></table></figure><p>得出的查询结果是:</p><table><thead><tr><th>name</th><th>course</th><th>score</th></tr></thead><tbody><tr><td>张三</td><td>数学</td><td>77</td></tr><tr><td>张三</td><td>英语</td><td>90</td></tr><tr><td>张三</td><td>语文</td><td>80</td></tr><tr><td>李四</td><td>数学</td><td>68</td></tr><tr><td>李四</td><td>英语</td><td>50</td></tr><tr><td>李四</td><td>语文</td><td>90</td></tr><tr><td>王五</td><td>数学</td><td>99</td></tr><tr><td>王五</td><td>英语</td><td>89</td></tr><tr><td>王五</td><td>语文</td><td>93</td></tr></tbody></table><p>结果还是不对，这次把所有的字段都查询出来了，字段的排序规则是先按姓名分组，再按课程分组，因为课程是唯一的，所以跟直接查询的结果一样。</p></li><li><p>我们回到上一步，上一步的课程和成绩对应上了，姓名没有对应上，我们干脆就不要姓名和，拿课程和成绩作为一张表再和自己联结一次，以课程和成绩作为过滤字段，说不定就能得到想要的姓名字段。</p><p>思路：</p><ol><li><p>先课程分组求出最高的分数</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">s.course;</span><br></pre></td></tr></table></figure></li><li><p>把前面得出的结果作为表t再自联结一次</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">s.name,s.score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student s</span><br><span class="line"><span class="keyword">join</span> t </span><br><span class="line"><span class="keyword">on</span> s.course=t.course <span class="keyword">and</span> s.score=t.score;</span><br></pre></td></tr></table></figure></li><li><p>把t替换成查询出来的结果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">s.name,s.course,s.score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student s</span><br><span class="line"><span class="keyword">join</span> (<span class="keyword">select</span> </span><br><span class="line">s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">s.course) t </span><br><span class="line"><span class="keyword">on</span> s.course=t.course <span class="keyword">and</span> s.score=t.score;</span><br></pre></td></tr></table></figure><p>得出的查询结果是:</p><table><thead><tr><th>name</th><th>course</th><th>score</th></tr></thead><tbody><tr><td>王五</td><td>语文</td><td>93</td></tr><tr><td>王五</td><td>数学</td><td>99</td></tr><tr><td>张三</td><td>英语</td><td>90</td></tr></tbody></table><p>和原数据比较，这就是我们要得到的每门课程的top1。</p></li></ol></li></ol><h3 id="TOP-N"><a href="#TOP-N" class="headerlink" title="TOP N"></a>TOP N</h3><p>查询每门课程前两名的学生以及成绩</p><p>首先求Top 1的方法不适用与Top N，然后毫无头绪。。。</p><p>翻看其他人的博客后，发现求Top N的核心是: <code>自联结表的需求字段比较，也就是自己跟自己比较，然后把比较的结果求count()，最后控制过滤的记录数即可</code></p><h4 id="注意-自己和自己比较，可以通过联结和子查询，我们这里使用子查询"><a href="#注意-自己和自己比较，可以通过联结和子查询，我们这里使用子查询" class="headerlink" title="注意:  自己和自己比较，可以通过联结和子查询，我们这里使用子查询"></a>注意:  自己和自己比较，可以通过联结和子查询，我们这里使用子查询</h4><p>思路:</p><ol><li><p>首先是子查询，然后是自己和自己比较，得出一个count()值，最后使用where过滤这个count值</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">a.name,a.course,a.score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student a</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line"><span class="number">2</span>&gt;(<span class="keyword">select</span> <span class="keyword">count</span>(b.score) <span class="keyword">from</span> student b <span class="keyword">where</span> a.course=b.course <span class="keyword">and</span> a.score&lt;b.score)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> a.course <span class="keyword">desc</span>,a.score <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><p>梳理这段SQL，select字段不难，from字段不难，order by字段不难，难就难在where字段，我们先不看为什么使用2大于这个子查询，先把注意力放在子查询的where字段中的两个表成绩比较，实际上理解了为什么这么比较这题就解出来了。</p></li><li><p>我们画图来解释。。。只是做为示范，所以只取一个课程的成绩比较，其他的一样</p><p><img src="https://yerias.github.io/hadoop_img/20191205012529.jpg" alt="a.score&lt;b.score比较图"></p><p>可以看出，表a中的成绩越大，满足<code>a.score&lt;b.score</code>的次数越少，<code>where条件过滤count()的值越少越满足Top N的条件，可以根据where条件灵活控制过滤的记录数,我们这里是2，即取Top 2的记录。</code></p><h4 id="再提出一个问题，为什么要用a-score-lt-b-score而不是a-score-gt-b-score？"><a href="#再提出一个问题，为什么要用a-score-lt-b-score而不是a-score-gt-b-score？" class="headerlink" title="再提出一个问题，为什么要用a.score&lt;b.score而不是a.score&gt;b.score？"></a>再提出一个问题，为什么要用<code>a.score&lt;b.score</code>而不是<code>a.score&gt;b.score</code>？</h4><p>通过结果可以倒推出来，我们<code>select</code>语句中要的是表a，根据题意表a必定是比较中较大的值。如果使用<code>a.score&gt;b.score</code>，where条件限制的是满足最少的条件，把表a中最大的值给过滤了，那么得出的的count()结果是反的。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataWarehouse </tag>
            
            <tag> MySQL </tag>
            
            <tag> Rank </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据重刷机制(抛砖引玉)</title>
      <link href="/2018/10/07/mysql/%E6%95%B0%E6%8D%AE%E9%87%8D%E5%88%B7%E6%9C%BA%E5%88%B6/"/>
      <url>/2018/10/07/mysql/%E6%95%B0%E6%8D%AE%E9%87%8D%E5%88%B7%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<h4 id="先抛出几个问题"><a href="#先抛出几个问题" class="headerlink" title="先抛出几个问题"></a>先抛出几个问题</h4><ol><li><p>存储是不是基石？</p></li><li><p>假如存储不挂，数据真的准确吗？</p></li><li><p>存储挂了，数据还准确吗？</p></li><li><p>如何校验是否正确？如何让其正确？机制是不是必须有？</p></li></ol><p>注：<code>sqoop</code>抽数据，无<code>error</code>丢数据的概率很小</p><p>数据质量校验：数据量校验 <code>count</code>相同吗？<code>count</code>相同内容相同吗？</p><p>数据量相同–&gt;数据量不同 重刷机制 补or删 <code>spark</code> 95%–&gt;数据内容不同？ 抽样 5%</p><h4 id="现在重点理解一下重刷机制"><a href="#现在重点理解一下重刷机制" class="headerlink" title="现在重点理解一下重刷机制"></a>现在重点理解一下重刷机制</h4><p>背景：用<code>count</code>校验上下游的数据不准确</p><p>引入重刷机制：通过对上下游的两个表求<code>full outer join</code>来对比字段的<code>null</code>值</p><p>上游表a</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| id   | data  | age  |</span><br><span class="line">| <span class="comment">---- | ----- | ---- |</span></span><br><span class="line">| 1    | data1 | 18   |</span><br><span class="line">| 2    | data2 | 19   |</span><br><span class="line">| 3    | data3 | 20   |</span><br><span class="line">| 7    | data7 | 22   |</span><br></pre></td></tr></table></figure><p>下游表b</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| id   | data  | age  |</span><br><span class="line">| <span class="comment">---- | ----- | ---- |</span></span><br><span class="line">| 1    | data1 | 18   |</span><br><span class="line">| 2    | data2 | 19   |</span><br><span class="line">| 3    | data3 | 20   |</span><br><span class="line">| 7    | data7 | 22   |</span><br></pre></td></tr></table></figure><p>我们发现表 a 和表 b 对比 表 a 少了 5 、6 多了 7 ，表 b 少了 2 、 7 多了 6，我们现在对两个表做 <code>full outer join</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| aid  | data   | age  | bid  |</span><br><span class="line">| <span class="comment">---- | ------ | ---- | ---- |</span></span><br><span class="line">| 1    | ruoze1 | 18   | 1    |</span><br><span class="line">| 2    | ruoze2 | 19   | null |</span><br><span class="line">| 3    | ruoze3 | 20   | 3    |</span><br><span class="line">| 7    | ruoze7 | 22   | null |</span><br><span class="line">| null | null   | null | 5    |</span><br><span class="line">| null | null   | null | 6    |</span><br></pre></td></tr></table></figure><p>以表 a 为标准，对生成后的大表做筛选，分别查找 <code>aid</code> 和 <code>bid</code> 为 <code>null</code> 的记录</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">from</span> t <span class="keyword">where</span> aid=<span class="literal">null</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">from</span> t <span class="keyword">where</span> bid=<span class="literal">null</span></span><br></pre></td></tr></table></figure><p>发现 <code>bid</code>为 5 、 6 的行 <code>aid</code> 为 <code>null</code>，说明 <code>bid</code> 下游数据多了，根据 <code>bid</code> 重新构建</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> b <span class="keyword">where</span> bid=<span class="number">5</span>     </span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> b <span class="keyword">where</span> bid=<span class="number">6</span></span><br></pre></td></tr></table></figure><p>发现 <code>aid</code> 为 2 、 7 的 <code>bid</code> 为<code>null</code>，说明 <code>bid</code> 下游数据少了，根据 <code>aid</code> 重新构建</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="number">2</span> ruoze2 <span class="number">19</span> </span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="number">7</span> ruoze7 <span class="number">22</span></span><br></pre></td></tr></table></figure><p>经过重新构建也就是重刷后的数据是</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| aid  | data   | age  | bid  |</span><br><span class="line">| <span class="comment">---- | ------ | ---- | ---- |</span></span><br><span class="line">| 1    | ruoze1 | 18   | 1    |</span><br><span class="line">| 2    | ruoze2 | 19   | 2    |</span><br><span class="line">| 3    | ruoze3 | 20   | 3    |</span><br><span class="line">| 7    | ruoze7 | 22   | 7    |</span><br></pre></td></tr></table></figure><h4 id="深度思考："><a href="#深度思考：" class="headerlink" title="深度思考："></a>深度思考：</h4><p><code>full outer join</code> 其实就是先 <code>left join</code> 和后 <code>right join</code> 的两个结果，为 <code>null</code> 的刚好是缺少的或者多的，而交集是上下游都有的数据，需要做的是 <code>left join</code> 为 <code>null</code> 做 <code>insert</code> 或者 <code>delete</code>，还是 <code>right join</code> 为 null 做 <code>insert</code> 或者 <code>delete</code>。</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataWarehouse </tag>
            
            <tag> Data </tag>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>掌握where、group、join语句和写SQL</title>
      <link href="/2018/10/03/mysql/3/"/>
      <url>/2018/10/03/mysql/3/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 sql的where各种条件</li><li>整理 sql的group</li><li>整理 sql的join</li><li>04txt文件的案例 9句sql</li><li>整理刚才分享的小知识点</li><li>补充资料文件夹 去看看执行</li><li>彩蛋 视频 sql</li></ol><h3 id="1-整理-sql-的-where-各种条件"><a href="#1-整理-sql-的-where-各种条件" class="headerlink" title="1.整理 sql 的 where 各种条件"></a>1.整理 sql 的 where 各种条件</h3><p><code>where 子句:</code> 如需有条件地从表中选取数据，可将 where 子句添加到 SELECT 语句。</p><p>语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> 列名称 <span class="keyword">FROM</span> 表名称 <span class="keyword">WHERE</span> 列 运算符 值</span><br></pre></td></tr></table></figure><p>下面的运算符可在 where 子句中使用：</p><table><thead><tr><th>操作符</th><th>描述</th></tr></thead><tbody><tr><td>=</td><td>等于</td></tr><tr><td>&lt;&gt;</td><td>不等于</td></tr><tr><td>&gt;</td><td>大于</td></tr><tr><td>&lt;</td><td>小于</td></tr><tr><td>&gt;=</td><td>大于等于</td></tr><tr><td>&lt;=</td><td>小于等于</td></tr><tr><td>BETWEEN</td><td>在某个范围内</td></tr><tr><td>LIKE</td><td>搜索某种模式</td></tr></tbody></table><h3 id="2-整理-sql-的-group"><a href="#2-整理-sql-的-group" class="headerlink" title="2.整理 sql 的 group"></a>2.整理 sql 的 group</h3><p>聚合函数 (比如 SUM) 常常需要添加 group by语句。</p><p><code>group by语句:</code> group by语句用于结合聚合函数，根据一个或多个列对结果集进行分组。</p><p>语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name, aggregate_function(column_name)</span><br><span class="line"><span class="keyword">FROM</span> table_name</span><br><span class="line"><span class="keyword">where</span> column_name <span class="keyword">operator</span> <span class="keyword">value</span></span><br><span class="line"><span class="keyword">group</span> bycolumn_name</span><br></pre></td></tr></table></figure><h3 id="3-整理-sql-的-join"><a href="#3-整理-sql-的-join" class="headerlink" title="3.整理 sql 的 join"></a>3.整理 sql 的 join</h3><p>join分为inner join、left join、right join，分别表示内联结，左联结，右联结</p><p><code>inner join:</code> 在表中存在至少一个匹配时，INNER JOIN 关键字返回行。</p><p>语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name(s)</span><br><span class="line"><span class="keyword">FROM</span> table_name1</span><br><span class="line"><span class="keyword">INNER</span> <span class="keyword">JOIN</span> table_name2 </span><br><span class="line"><span class="keyword">ON</span> table_name1.column_name=table_name2.column_name</span><br></pre></td></tr></table></figure><p><code>注意:</code> inner 与 join是相同的。</p><p><code>left join:</code> left join 关键字会从左表 (table_name1) 那里返回所有的行，即使在右表 (table_name2) 中没有匹配的行。</p><p>语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name(s)</span><br><span class="line"><span class="keyword">FROM</span> table_name1</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> table_name2 </span><br><span class="line"><span class="keyword">ON</span> table_name1.column_name=table_name2.column_name</span><br></pre></td></tr></table></figure><p><code>注意:</code> 在某些数据库中， left join 称为 left outer join。</p><p><code>right join</code> right join 关键字会右表 (table_name2) 那里返回所有的行，即使在左表 (table_name1) 中没有匹配的行。</p><p>语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name(s)</span><br><span class="line"><span class="keyword">FROM</span> table_name1</span><br><span class="line"><span class="keyword">right</span> <span class="keyword">join</span> table_name2 </span><br><span class="line"><span class="keyword">ON</span> table_name1.column_name=table_name2.column_name</span><br></pre></td></tr></table></figure><p><code>注意:</code> 在某些数据库中， right join 称为 right outer join。</p><h3 id="4-04txt文件的案例9句sql"><a href="#4-04txt文件的案例9句sql" class="headerlink" title="4. 04txt文件的案例9句sql"></a>4. 04txt文件的案例9句sql</h3><ul><li>查询出部门编号为30的所有员工的编号和姓名</li><li>找出部门编号为10中所有经理，和部门编号为20中所有销售员的详细资料。</li><li>查询所有员工详细信息，用工资降序排序，如果工资相同使用入职日期升序排序</li><li>列出薪金大于1500的各种工作及从事此工作的员工人数。</li><li>列出在销售部工作的员工的姓名，假定不知道销售部的部门编号。</li><li>查询姓名以S开头的\以S结尾\包含S字符\第二个字母为L __</li><li>查询每种工作的最高工资、最低工资、人数</li><li>列出薪金高于公司平均薪金的所有员工号，员工姓名，所在部门名称，上级领导，工资，工资等级</li><li>列出薪金高于在部门30工作的 所有/任何一个员工的薪金的员工姓名和薪金、部门名称。</li></ul><h3 id="5-整理刚才分享的小知识点"><a href="#5-整理刚才分享的小知识点" class="headerlink" title="5.整理刚才分享的小知识点"></a>5.整理刚才分享的小知识点</h3><ol><li><p>关于count()的使用细节</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">使用count(id)替换count(*)的使用，可以提升性能</span><br></pre></td></tr></table></figure></li><li><p>sum()与count()的区别与联想</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sum()计算统计字段的和，count()统计字段的数量，容易混淆，建议使用sum()联想count()，使用count()联想sum()，并区分</span><br></pre></td></tr></table></figure></li><li><p>sql语句的执行顺序</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> yyyyy <span class="keyword">from</span> rzdata</span><br><span class="line"><span class="keyword">where</span> xxx</span><br><span class="line"><span class="keyword">group</span> byxxx <span class="keyword">having</span> xxx </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> xxx</span><br><span class="line"><span class="keyword">limit</span> xxx ;</span><br></pre></td></tr></table></figure></li><li><p>all和any的区别</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">all()的用法是表示要满足字段中所有值即最大值，any()的用法表示的是满足字段值任意一个值即最小值</span><br></pre></td></tr></table></figure></li><li><p>聚合函数中的null值</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">如果在进行数值计算的时候，字段中存在null值，则计算的结果是null值，在进行数值运算的时候使用IFNULL(expression, alt_value)替换null值为0，则能计算出结果</span><br><span class="line">如果第一个参数的表达式 expression 为 NULL，则返回第二个参数的备用值。</span><br></pre></td></tr></table></figure></li><li><p>unoin和union all的区别</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">union的用法是把联合查询中的语句如果整体重复则去重，unoin all不会去重</span><br></pre></td></tr></table></figure></li></ol><h3 id="6-补充资料文件夹-去看看执行"><a href="#6-补充资料文件夹-去看看执行" class="headerlink" title="6.补充资料文件夹 去看看执行"></a>6.补充资料文件夹 去看看执行</h3><p>　　资料文件中</p><h3 id="7-彩蛋-视频-sql"><a href="#7-彩蛋-视频-sql" class="headerlink" title="7.彩蛋 视频 sql"></a>7.彩蛋 视频 sql</h3><p>　　百度云中</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataWarehouse </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>掌握MySQL的建表规范、DDL语句和权限操作</title>
      <link href="/2018/10/02/mysql/2/"/>
      <url>/2018/10/02/mysql/2/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 建表规范</li><li>整理 DDL语句的</li><li>整理 三句话</li></ol><h3 id="首先看一个建表例子，再去研究应该遵循哪些规范"><a href="#首先看一个建表例子，再去研究应该遵循哪些规范" class="headerlink" title="首先看一个建表例子，再去研究应该遵循哪些规范"></a>首先看一个建表例子，再去研究应该遵循哪些规范</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> rzdata(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">not</span> <span class="literal">null</span> auto_increment,</span><br><span class="line"></span><br><span class="line"><span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">200</span>),</span><br><span class="line">age  <span class="built_in">int</span>(<span class="number">3</span>),</span><br><span class="line"></span><br><span class="line">createuser <span class="built_in">varchar</span>(<span class="number">200</span>) ,</span><br><span class="line">createtime <span class="built_in">timestamp</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">default</span> <span class="keyword">current_timestamp</span>,</span><br><span class="line">updateuser <span class="built_in">varchar</span>(<span class="number">200</span>) ,</span><br><span class="line">updatetime <span class="built_in">timestamp</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">default</span> <span class="keyword">current_timestamp</span> <span class="keyword">on</span> <span class="keyword">update</span> <span class="keyword">current_timestamp</span>,</span><br><span class="line"></span><br><span class="line">primary <span class="keyword">key</span> (<span class="keyword">id</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure><ol><li><p>表名</p><p>不能是中文，不能是汉语拼音 ，不然很low</p></li><li><p>风格统一</p><p>统一所有表的风格，可以从已有的表中查看，或者找leader检查，方便后期维护</p></li><li><p>第一个字段</p><p>第一个字段必须是id，并且自增长，是主键，没有意义 –&gt;拓展: 为什么？</p></li><li><p>主键</p><p>一张表只有一个主键，primary key == unique+not null</p></li><li><p>后四个字段</p><p>后四个字段包括：用户、创建时间、修改用户、修改时间</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">createuser varchar(200) ,</span><br><span class="line">createtime timestamp not null default current_timestamp,</span><br><span class="line">updateuser varchar(200) ,</span><br><span class="line">updatetime timestamp not null default current_timestamp on <span class="keyword">update</span> <span class="keyword">current_timestamp</span>,</span><br></pre></td></tr></table></figure></li><li><p>业务字段</p><p>业务字段需要唯一存在，使用unique约束，如订单号</p><p>业务字段都必须加上注释</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">COMMENT</span> <span class="string">'用户名称'</span></span><br></pre></td></tr></table></figure></li><li><p>字符集CHARSET</p><p>查看字符集</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">&gt;&gt; show variables like '%char%';</span><br><span class="line">+<span class="comment">--------------------------+---------------------------------------------------------------+</span></span><br><span class="line">| Variable_name            | Value                                                         |</span><br><span class="line">+<span class="comment">--------------------------+---------------------------------------------------------------+</span></span><br><span class="line">| character_set_client     | utf8                                                          |</span><br><span class="line">| character_set_connection | utf8                                                          |</span><br><span class="line">| character_set_database   | latin1                                                        |</span><br><span class="line">| character_set_filesystem | binary                                                        |</span><br><span class="line">| character_set_results    | utf8                                                          |</span><br><span class="line">| character_set_server     | latin1                                                        |</span><br><span class="line">| character_set_system     | utf8                                                          |</span><br><span class="line">| character_sets_dir       | /usr/local/mysql-5.6.23-linux-glibc2.5-x86_64/share/charsets/ |</span><br><span class="line">+<span class="comment">--------------------------+---------------------------------------------------------------+</span></span><br><span class="line">8 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li></ol><h3 id="DDL语句以及需要注意的点"><a href="#DDL语句以及需要注意的点" class="headerlink" title="DDL语句以及需要注意的点"></a>DDL语句以及需要注意的点</h3><p><code>查询语句：</code>select 查询字段 from 表 ;</p><p>注意：</p><ol><li><p>生产环境下不要用 * 代替所有字段</p><p>错误示范：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from tb_user;</span><br><span class="line">+<span class="comment">----+----------+----------------------------------+-------------+---------------------+----------------------------------+</span></span><br><span class="line">| id | username | password                         | phone       | created             | salt                             |</span><br><span class="line">+<span class="comment">----+----------+----------------------------------+-------------+---------------------+----------------------------------+</span></span><br><span class="line">| 28 | zhangsan | e21d44f200365b57fab2641cd31226d4 | 13600527634 | 2018-05-25 17:52:03 | 05b0f203987e49d2b72b20b95e0e57d9 |</span><br><span class="line">| 30 | leyou    | 4de9a93b3f95d468874a3c1bf3b25a48 | 15855410440 | 2018-09-30 11:37:30 | 4565613d4b0e434cb496d4eb87feb45f |</span><br><span class="line">+<span class="comment">----+----------+----------------------------------+-------------+---------------------+----------------------------------+</span></span><br><span class="line">2 rows in <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>正确示范：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; select username,password from tb_user;</span><br><span class="line">+<span class="comment">----------+----------------------------------+</span></span><br><span class="line">| username | password                         |</span><br><span class="line">+<span class="comment">----------+----------------------------------+</span></span><br><span class="line">| zhangsan | e21d44f200365b57fab2641cd31226d4 |</span><br><span class="line">| leyou    | 4de9a93b3f95d468874a3c1bf3b25a48 |</span><br><span class="line">+<span class="comment">----------+----------------------------------+</span></span><br><span class="line">2 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>查询语句如果数据量特别大必须使用where 或者 limit，否则需要使用大量的资源</p><p>错误示范：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,image,letter <span class="keyword">from</span> tb_brand;</span><br></pre></td></tr></table></figure><p>正确示范：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,image,letter <span class="keyword">from</span> tb_brand <span class="keyword">limit</span> <span class="number">100</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,image,letter <span class="keyword">from</span> tb_brand <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">100</span>;</span><br></pre></td></tr></table></figure></li></ol><p><code>新增语句：</code>insert into 表名（字段1，字段2…） values（数据1，数据2…）;</p><p>注意：在表名后加上对应要添加的字段名</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tb_brand (<span class="keyword">name</span>，letter) <span class="keyword">values</span>(tunan，T);</span><br></pre></td></tr></table></figure><p><code>修改语句：</code>update 表名 set 修改后的字段 where 条件；</p><p>注意：一定要加上条件，否则是全局修改</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> tb_brand <span class="keyword">set</span> <span class="keyword">name</span>=<span class="string">"xiaoqi"</span> <span class="keyword">where</span> <span class="keyword">id</span> = <span class="number">1</span>;</span><br></pre></td></tr></table></figure><p><code>删除语句：</code>delete from 表名 where 条件；</p><p>注意：一定要加上条件，否则是全局删除；</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> tb_brand tb <span class="keyword">where</span> <span class="keyword">id</span> = <span class="number">1</span>;</span><br></pre></td></tr></table></figure><h3 id="当某条SQL验证拖累进程时怎么办？"><a href="#当某条SQL验证拖累进程时怎么办？" class="headerlink" title="当某条SQL验证拖累进程时怎么办？"></a>当某条SQL验证拖累进程时怎么办？</h3><p>使用 show processlist；查看mysql中的 sql 进程</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; show processlist;</span><br><span class="line">+<span class="comment">-----+------+---------------------+--------+---------+------+----------+------------------+</span></span><br><span class="line">| Id  | User | Host                | db     | Command | Time | State    | Info             |</span><br><span class="line">+<span class="comment">-----+------+---------------------+--------+---------+------+----------+------------------+</span></span><br><span class="line">| 272 | root | localhost           | leyou1 | Query   |    0 | starting | <span class="keyword">show</span> <span class="keyword">processlist</span> |</span><br><span class="line">| <span class="number">273</span> | root | <span class="number">121.62</span><span class="number">.184</span><span class="number">.34</span>:<span class="number">56629</span> | leyou1 | <span class="keyword">Sleep</span>   |  <span class="number">448</span> |          | <span class="literal">NULL</span>             |</span><br><span class="line">| <span class="number">274</span> | root | <span class="number">121.62</span><span class="number">.184</span><span class="number">.34</span>:<span class="number">56631</span> | leyou1 | <span class="keyword">Sleep</span>   |  <span class="number">592</span> |          | <span class="literal">NULL</span>             |</span><br><span class="line">+<span class="comment">-----+------+---------------------+--------+---------+------+----------+------------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>然后根据 id 删除即可</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">kill</span> <span class="keyword">id</span>;</span><br></pre></td></tr></table></figure><h3 id="必须要记住的三条命令"><a href="#必须要记住的三条命令" class="headerlink" title="必须要记住的三条命令"></a>必须要记住的三条命令</h3><p><code>修改密码：</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> <span class="keyword">user</span> <span class="keyword">set</span> <span class="keyword">password</span>=<span class="keyword">password</span>(<span class="string">'密码'</span>) <span class="keyword">where</span> <span class="keyword">user</span>=<span class="string">'用户名'</span>;</span><br></pre></td></tr></table></figure><p><code>修改权限:</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> 用户名@<span class="string">'%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'密码'</span>;</span><br></pre></td></tr></table></figure><p><code>刷新权限：</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataWarehouse </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL二进制部署和DBeaver连接MySQL</title>
      <link href="/2018/10/01/mysql/1/"/>
      <url>/2018/10/01/mysql/1/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>二进制部署mysql</li><li>重新部署</li><li>部署dbeaver 打通 mysql</li></ol><h3 id="二进制部署mysql"><a href="#二进制部署mysql" class="headerlink" title="二进制部署mysql"></a>二进制部署mysql</h3><p>[<a href="https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt]" target="_blank" rel="noopener">https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt]</a>(<a href="https://github.com/Hackeruncle/MySQL/blob/master/MySQL" target="_blank" rel="noopener">https://github.com/Hackeruncle/MySQL/blob/master/MySQL</a> 5.6.23 Install.txt)</p><p>京东云下部署代码：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">scripts/mysql_install_db  \</span><br><span class="line"><span class="comment">--user=mysqladmin \</span></span><br><span class="line"><span class="comment">--basedir=/usr/local/mysql \</span></span><br><span class="line"><span class="comment">--datadir=/usr/local/mysql/data</span></span><br></pre></td></tr></table></figure><p>必须要装三个环境：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">yum <span class="keyword">install</span> -y perl</span><br><span class="line">yum <span class="keyword">install</span> -y autoconf</span><br><span class="line">yum <span class="keyword">install</span> -y libaio</span><br></pre></td></tr></table></figure><h3 id="重新部署"><a href="#重新部署" class="headerlink" title="重新部署"></a>重新部署</h3><ol><li><p>删除压缩文件和数据文件</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">rm -rf arch<span class="comment">/* data/*</span></span><br></pre></td></tr></table></figure></li><li><p>重置执行脚本文件</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">scripts/mysql_install_db  \</span><br><span class="line"><span class="comment">--user=mysqladmin \</span></span><br><span class="line"><span class="comment">--basedir=/usr/local/mysql \</span></span><br><span class="line"><span class="comment">--datadir=/usr/local/mysql/data</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="部署dbeaver-打通-mysql"><a href="#部署dbeaver-打通-mysql" class="headerlink" title="部署dbeaver 打通 mysql"></a>部署dbeaver 打通 mysql</h3><p>修改用户密码：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> <span class="keyword">user</span> <span class="keyword">set</span> <span class="keyword">password</span>=<span class="keyword">password</span>(<span class="string">'ruozedata'</span>) <span class="keyword">where</span> <span class="keyword">user</span>=<span class="string">'root'</span>;</span><br></pre></td></tr></table></figure><p>查看用户权限信息：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">user</span>,<span class="keyword">password</span>,host <span class="keyword">from</span> <span class="keyword">user</span>;</span><br></pre></td></tr></table></figure><p>给用户添加权限：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> root@<span class="string">'%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'ruozedata'</span>;</span><br><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;　　<span class="comment">#刷新权限</span></span><br></pre></td></tr></table></figure><p>DBeaver连接Mysql：</p><p><img src="https://img2018.cnblogs.com/i-beta/1657732/201911/1657732-20191121181225170-233024101.png" alt="DBeaver连接设置"> </p><p> <img src="https://img2018.cnblogs.com/i-beta/1657732/201911/1657732-20191121181334218-1241892255.png" alt="DBeaver连接添加jar包"></p><h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><p>为了保证 ip 地址的安全性和可用性</p><p>　　1. 在 linux 的 /etc/hosts 文件中配置 内网ip 地址和主机名的映射环境，这样在shell脚本或者代码中使用主机名替代 ip 使用</p><p>　　2. 如果是云主机，在windows中的hosts文件中配置外网ip地址和主机名的映射</p><p><code>注意：</code>hosts文件中的前两行切记不能删，否则可能带来bug</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataWarehouse </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的test命令</title>
      <link href="/2018/09/30/linux/shell/9/"/>
      <url>/2018/09/30/linux/shell/9/</url>
      
        <content type="html"><![CDATA[<p>Shell中的 test 命令用于检查某个条件是否成立，它可以进行<code>数值</code>、<code>字符</code>和<code>文件</code>三个方面的测试。</p><h3 id="数值测试"><a href="#数值测试" class="headerlink" title="数值测试"></a>数值测试</h3><table><thead><tr><th align="left">参数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">-eq</td><td align="left">等于则为真</td></tr><tr><td align="left">-ne</td><td align="left">不等于则为真</td></tr><tr><td align="left">-gt</td><td align="left">大于则为真</td></tr><tr><td align="left">-ge</td><td align="left">大于等于则为真</td></tr><tr><td align="left">-lt</td><td align="left">小于则为真</td></tr><tr><td align="left">-le</td><td align="left">小于等于则为真</td></tr></tbody></table><p>实例演示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">num1=100</span><br><span class="line">num2=100</span><br><span class="line">if test $[num1] -eq $[num2]</span><br><span class="line">then</span><br><span class="line">    echo '两个数相等！'</span><br><span class="line">else</span><br><span class="line">    echo '两个数不相等！'</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">两个数相等！</span><br></pre></td></tr></table></figure><p>代码中的 [] 执行基本的算数运算，如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">a=5</span><br><span class="line">b=6</span><br><span class="line"></span><br><span class="line">result=$[a+b] # 注意等号两边不能有空格</span><br><span class="line">echo "result 为： $result"</span><br></pre></td></tr></table></figure><p>结果为:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">result 为： 11</span><br></pre></td></tr></table></figure><h3 id="字符串测试"><a href="#字符串测试" class="headerlink" title="字符串测试"></a>字符串测试</h3><table><thead><tr><th align="left">参数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">=</td><td align="left">等于则为真</td></tr><tr><td align="left">!=</td><td align="left">不相等则为真</td></tr><tr><td align="left">-z 字符串</td><td align="left">字符串的长度为零则为真</td></tr><tr><td align="left">-n 字符串</td><td align="left">字符串的长度不为零则为真</td></tr></tbody></table><p>实例演示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">num1="ru1noob"</span><br><span class="line">num2="runoob"</span><br><span class="line">if test $num1 = $num2</span><br><span class="line">then</span><br><span class="line">    echo '两个字符串相等!'</span><br><span class="line">else</span><br><span class="line">    echo '两个字符串不相等!'</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">两个字符串不相等!</span><br></pre></td></tr></table></figure><h3 id="文件测试"><a href="#文件测试" class="headerlink" title="文件测试"></a>文件测试</h3><table><thead><tr><th align="left">参数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">-e 文件名</td><td align="left">如果文件存在则为真</td></tr><tr><td align="left">-r 文件名</td><td align="left">如果文件存在且可读则为真</td></tr><tr><td align="left">-w 文件名</td><td align="left">如果文件存在且可写则为真</td></tr><tr><td align="left">-x 文件名</td><td align="left">如果文件存在且可执行则为真</td></tr><tr><td align="left">-s 文件名</td><td align="left">如果文件存在且至少有一个字符则为真</td></tr><tr><td align="left">-d 文件名</td><td align="left">如果文件存在且为目录则为真</td></tr><tr><td align="left">-f 文件名</td><td align="left">如果文件存在且为普通文件则为真</td></tr><tr><td align="left">-c 文件名</td><td align="left">如果文件存在且为字符型特殊文件则为真</td></tr><tr><td align="left">-b 文件名</td><td align="left">如果文件存在且为块特殊文件则为真</td></tr></tbody></table><p>实例演示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /bin</span><br><span class="line">if test -e ./bash</span><br><span class="line">then</span><br><span class="line">    echo '文件已存在!'</span><br><span class="line">else</span><br><span class="line">    echo '文件不存在!'</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">文件已存在!</span><br></pre></td></tr></table></figure><p>另外，Shell还提供了与( -a )、或( -o )、非( ! )三个逻辑操作符用于将测试条件连接起来，其优先级为：”!”最高，”-a”次之，”-o”最低。例如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /bin</span><br><span class="line">if test -e ./notFile -o -e ./bash</span><br><span class="line">then</span><br><span class="line">    echo '至少有一个文件存在!'</span><br><span class="line">else</span><br><span class="line">    echo '两个文件都不存在'</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">至少有一个文件存在!</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的流程控制</title>
      <link href="/2018/09/29/linux/shell/8/"/>
      <url>/2018/09/29/linux/shell/8/</url>
      
        <content type="html"><![CDATA[<p>和Java、PHP等语言不一样，shell的流程控制不可缺少</p><h3 id="判断语句"><a href="#判断语句" class="headerlink" title="判断语句"></a>判断语句</h3><h4 id="if"><a href="#if" class="headerlink" title="if"></a>if</h4><p>if 语句语法格式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if condition</span><br><span class="line">then</span><br><span class="line">    command1 </span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN </span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>写成一行（适用于终端命令提示符）：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if [ $(ps -ef | grep -c "ssh") -gt 1 ]; then echo "true"; fi</span><br></pre></td></tr></table></figure><p>末尾的fi就是if倒过来拼写，后面还会遇到类似的。</p><h4 id="if-else"><a href="#if-else" class="headerlink" title="if else"></a>if else</h4><p>if else 语法格式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if condition</span><br><span class="line">then</span><br><span class="line">    command1 </span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN</span><br><span class="line">else</span><br><span class="line">    command</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h4 id="if-else-if-else"><a href="#if-else-if-else" class="headerlink" title="if else-if else"></a>if else-if else</h4><p>if else-if else 语法格式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if condition1</span><br><span class="line">then</span><br><span class="line">    command1</span><br><span class="line">elif condition2 </span><br><span class="line">then </span><br><span class="line">    command2</span><br><span class="line">else</span><br><span class="line">    commandN</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>以下实例判断两个变量是否相等：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a=10</span><br><span class="line">b=20</span><br><span class="line">if [ $a == $b ]</span><br><span class="line">then</span><br><span class="line">   echo "a 等于 b"</span><br><span class="line">elif [ $a -gt $b ]</span><br><span class="line">then</span><br><span class="line">   echo "a 大于 b"</span><br><span class="line">elif [ $a -lt $b ]</span><br><span class="line">then</span><br><span class="line">   echo "a 小于 b"</span><br><span class="line">else</span><br><span class="line">   echo "没有符合的条件"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a 小于 b</span><br></pre></td></tr></table></figure><p>if else语句经常与test命令结合使用，如下所示：</p><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">num1=$[<span class="number">2</span>*<span class="number">3</span>]</span><br><span class="line">num2=$[<span class="number">1</span>+<span class="number">5</span>]</span><br><span class="line"><span class="keyword">if</span> test $[num1] <span class="nomarkup">-eq</span> $[num2]</span><br><span class="line">then</span><br><span class="line">    echo <span class="string">'两个数字相等!'</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    echo <span class="string">'两个数字不相等!'</span></span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">两个数字相等!</span><br></pre></td></tr></table></figure><hr><h3 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h3><h4 id="for循环"><a href="#for循环" class="headerlink" title="for循环"></a>for循环</h4><p>与其他编程语言类似，Shell支持for循环。</p><p>for循环一般格式为：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for var in item1 item2 ... itemN</span><br><span class="line">do</span><br><span class="line">    command1</span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>写成一行：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for var in item1 item2 ... itemN; do command1; command2… done;</span><br></pre></td></tr></table></figure><p>当变量值在列表里，for循环即执行一次所有命令，使用变量名获取列表中的当前取值。命令可为任何有效的shell命令和语句。in列表可以包含替换、字符串和文件名。</p><p>in列表是可选的，如果不用它，for循环使用命令行的位置参数。</p><p>例如，顺序输出当前列表中的数字：</p><p><code>方法1:</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for var in 1 2 3 4 5 </span><br><span class="line">do</span><br><span class="line">    echo "The value is: $var"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p><code>方法2:</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for var in &#123;1..5&#125;</span><br><span class="line">do</span><br><span class="line">    echo "The value is: $var"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p><code>方法3:</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for ((var=1;var&lt;=5;var++))</span><br><span class="line">do</span><br><span class="line">    echo "The value is: $var"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">The value is: 1</span><br><span class="line">The value is: 2</span><br><span class="line">The value is: 3</span><br><span class="line">The value is: 4</span><br><span class="line">The value is: 5</span><br></pre></td></tr></table></figure><p>顺序输出字符串中的字符：</p><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> str <span class="keyword">in</span> <span class="string">'This is a string'</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    echo <span class="variable">$str</span></span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">This is a string</span><br></pre></td></tr></table></figure><h4 id="while-语句"><a href="#while-语句" class="headerlink" title="while 语句"></a>while 语句</h4><p>while循环用于不断执行一系列命令，也用于从输入文件中读取数据；命令通常为测试条件。其格式为：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">while condition</span><br><span class="line">do</span><br><span class="line">    command</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>以下是一个基本的while循环，测试条件是：如果int小于等于5，那么条件返回真。int从0开始，每次循环处理时，int加1。运行上述脚本，返回数字1到5，然后终止。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">int=1</span><br><span class="line">while(( $int&lt;=5 ))</span><br><span class="line">do</span><br><span class="line">    echo $int</span><br><span class="line">    let "int++"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>运行脚本，输出：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td></tr></table></figure><p>以上实例使用了 Bash let 命令，let 命令是 BASH 中用于计算的工具，用于执行一个或多个表达式，变量计算中不需要加上 $ 来表示变量。如果表达式中包含了空格或其他特殊字符，则必须引起来。</p><p>while循环可用于读取键盘信息。下面的例子中，输入信息被设置为变量FILM，按<Ctrl-D>结束循环。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo '按下 &lt;CTRL-D&gt; 退出'</span><br><span class="line">echo -n '输入你最喜欢的网站名: '</span><br><span class="line">while read FILM</span><br><span class="line">do</span><br><span class="line">    echo "是的！$FILM 是一个好网站"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>运行脚本，输出类似下面：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">按下 &lt;CTRL-D&gt; 退出</span><br><span class="line">输入你最喜欢的网站名:菜鸟教程</span><br><span class="line">是的！菜鸟教程 是一个好网站</span><br></pre></td></tr></table></figure><h4 id="无限循环"><a href="#无限循环" class="headerlink" title="无限循环"></a>无限循环</h4><p>无限循环语法格式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">while :</span><br><span class="line">do</span><br><span class="line">    command</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">while true</span><br><span class="line">do</span><br><span class="line">    command</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for (( ; ; ))</span><br></pre></td></tr></table></figure><h3 id="开关语句"><a href="#开关语句" class="headerlink" title="开关语句"></a>开关语句</h3><h4 id="case"><a href="#case" class="headerlink" title="case"></a>case</h4><p>Shell case语句为多选择语句。可以用case语句匹配一个值与一个模式，如果匹配成功，执行相匹配的命令。case语句格式如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">case 值 in</span><br><span class="line">模式1)</span><br><span class="line">    command1</span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN</span><br><span class="line">    ;;</span><br><span class="line">模式2）</span><br><span class="line">    command1</span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><p>case工作方式如上所示。取值后面必须为单词in，每一模式必须以右括号结束。取值可以为变量或常数。匹配发现取值符合某一模式后，其间所有命令开始执行直至 ;;。</p><p>取值将检测匹配的每一个模式。一旦模式匹配，则执行完匹配模式相应命令后不再继续其他模式。如果无一匹配模式，使用星号 * 捕获该值，再执行后面的命令。</p><p>下面的脚本提示输入1到4，与每一种模式进行匹配：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo '输入 1 到 4 之间的数字:'</span><br><span class="line">echo '你输入的数字为:'</span><br><span class="line">read aNum</span><br><span class="line">case $aNum in</span><br><span class="line">    1)  echo '你选择了 1'</span><br><span class="line">    ;;</span><br><span class="line">    2)  echo '你选择了 2'</span><br><span class="line">    ;;</span><br><span class="line">    3)  echo '你选择了 3'</span><br><span class="line">    ;;</span><br><span class="line">    4)  echo '你选择了 4'</span><br><span class="line">    ;;</span><br><span class="line">    *)  echo '你没有输入 1 到 4 之间的数字'</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><p>输入不同的内容，会有不同的结果，例如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">输入 1 到 4 之间的数字:</span><br><span class="line">你输入的数字为:</span><br><span class="line">3</span><br><span class="line">你选择了 3</span><br></pre></td></tr></table></figure><h3 id="跳出循环"><a href="#跳出循环" class="headerlink" title="跳出循环"></a>跳出循环</h3><p>在循环过程中，有时候需要在未达到循环结束条件时强制跳出循环，Shell使用两个命令来实现该功能：break和continue。</p><h4 id="break命令"><a href="#break命令" class="headerlink" title="break命令"></a>break命令</h4><p>break命令允许跳出所有循环（终止执行后面的所有循环）。</p><p>下面的例子中，脚本进入死循环直至用户输入数字大于5。要跳出这个循环，返回到shell提示符下，需要使用break命令。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">while :</span><br><span class="line">do</span><br><span class="line">    echo -n "输入 1 到 5 之间的数字:"</span><br><span class="line">    read aNum</span><br><span class="line">    case $aNum in</span><br><span class="line">        1|2|3|4|5) echo "你输入的数字为 $aNum!"</span><br><span class="line">        ;;</span><br><span class="line">        *) echo "你输入的数字不是 1 到 5 之间的! 游戏结束"</span><br><span class="line">            break</span><br><span class="line">        ;;</span><br><span class="line">    esac</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>执行以上代码，输出结果为：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">输入 1 到 5 之间的数字:3</span><br><span class="line">你输入的数字为 3!</span><br><span class="line">输入 1 到 5 之间的数字:7</span><br><span class="line">你输入的数字不是 1 到 5 之间的! 游戏结束</span><br></pre></td></tr></table></figure><h4 id="continue"><a href="#continue" class="headerlink" title="continue"></a>continue</h4><p>continue命令与break命令类似，只有一点差别，它不会跳出所有循环，仅仅跳出当前循环。</p><p>对上面的例子进行修改：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">while :</span><br><span class="line">do</span><br><span class="line">    echo -n "输入 1 到 5 之间的数字: "</span><br><span class="line">    read aNum</span><br><span class="line">    case $aNum in</span><br><span class="line">        1|2|3|4|5) echo "你输入的数字为 $aNum!"</span><br><span class="line">        ;;</span><br><span class="line">        *) echo "你输入的数字不是 1 到 5 之间的!"</span><br><span class="line">            continue</span><br><span class="line">            echo "游戏结束"</span><br><span class="line">        ;;</span><br><span class="line">    esac</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>运行代码发现，当输入大于5的数字时，该例中的循环不会结束，语句 <strong>echo “游戏结束”</strong> 永远不会被执行。</p><hr><h4 id="esac"><a href="#esac" class="headerlink" title="esac"></a>esac</h4><p>case的语法和C family语言差别很大，它需要一个esac（就是case反过来）作为结束标记，每个case分支用右圆括号，用两个分号表示break。</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的printf命令</title>
      <link href="/2018/09/28/linux/shell/7/"/>
      <url>/2018/09/28/linux/shell/7/</url>
      
        <content type="html"><![CDATA[<p>printf 命令模仿 C 程序库（library）里的 printf() 程序。</p><p>printf 由 POSIX 标准所定义，因此使用 printf 的脚本比使用 echo 移植性好。</p><p>printf 使用引用文本或空格分隔的参数，外面可以在 printf 中使用格式化字符串，还可以制定字符串的宽度、左右对齐方式等。默认 printf 不会像 echo 自动添加换行符，我们可以手动添加 \n。</p><p>printf 命令的语法：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">printf  format-string  [arguments...]</span><br></pre></td></tr></table></figure><p><strong>参数说明：</strong></p><ul><li><strong>format-string:</strong> 为格式控制字符串</li><li><strong>arguments:</strong> 为参数列表。</li></ul><p>实例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">"Hello, Shell"</span></span></span><br><span class="line">Hello, Shell</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">printf</span> <span class="string">"Hello, Shell\n"</span></span></span><br><span class="line">Hello, Shell</span><br><span class="line"><span class="meta">$</span></span><br></pre></td></tr></table></figure><p>接下来,我来用一个脚本来体现printf的强大功能：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">printf "%-10s %-8s %-4s\n" 姓名 性别 体重kg  </span><br><span class="line">printf "%-10s %-8s %-4.2f\n" 郭靖 男 66.1234 </span><br><span class="line">printf "%-10s %-8s %-4.2f\n" 杨过 男 48.6543 </span><br><span class="line">printf "%-10s %-8s %-4.2f\n" 郭芙 女 47.9876</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">姓名     性别   体重kg</span><br><span class="line">郭靖     男      66.12</span><br><span class="line">杨过     男      48.65</span><br><span class="line">郭芙     女      47.99</span><br></pre></td></tr></table></figure><p>%s %c %d %f都是格式替代符</p><p>%-10s 指一个宽度为10个字符（-表示左对齐，没有则表示右对齐），任何字符都会被显示在10个字符宽的字符内，如果不足则自动以空格填充，超过也会将内容全部显示出来。</p><p>%-4.2f 指格式化为小数，其中.2指保留2位小数。</p><p>更多实例：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> format-string为双引号</span></span><br><span class="line">printf "%d %s\n" 1 "abc"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 单引号与双引号效果一样 </span></span><br><span class="line">printf '%d %s\n' 1 "abc" </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 没有引号也可以输出</span></span><br><span class="line">printf %s abcdef</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 格式只指定了一个参数，但多出的参数仍然会按照该格式输出，format-string 被重用</span></span><br><span class="line">printf %s abc def</span><br><span class="line"></span><br><span class="line">printf "%s\n" abc def</span><br><span class="line"></span><br><span class="line">printf "%s %s %s\n" a b c d e f g h i j</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果没有 arguments，那么 %s 用NULL代替，%d 用 0 代替</span></span><br><span class="line">printf "%s and %d \n"</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1 abc</span><br><span class="line">1 abc</span><br><span class="line">abcdefabcdefabc</span><br><span class="line">def</span><br><span class="line">a b c</span><br><span class="line">d e f</span><br><span class="line">g h i</span><br><span class="line">j  </span><br><span class="line"> and 0</span><br></pre></td></tr></table></figure><h3 id="printf的转义序列"><a href="#printf的转义序列" class="headerlink" title="printf的转义序列"></a>printf的转义序列</h3><table><thead><tr><th align="left">序列</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">\a</td><td align="left">警告字符，通常为ASCII的BEL字符</td></tr><tr><td align="left">\b</td><td align="left">后退</td></tr><tr><td align="left">\c</td><td align="left">抑制（不显示）输出结果中任何结尾的换行字符（只在%b格式指示符控制下的参数字符串中有效），而且，任何留在参数里的字符、任何接下来的参数以及任何留在格式字符串中的字符，都被忽略</td></tr><tr><td align="left">\f</td><td align="left">换页（formfeed）</td></tr><tr><td align="left">\n</td><td align="left">换行</td></tr><tr><td align="left">\r</td><td align="left">回车（Carriage return）</td></tr><tr><td align="left">\t</td><td align="left">水平制表符</td></tr><tr><td align="left">\v</td><td align="left">垂直制表符</td></tr><tr><td align="left">\</td><td align="left">一个字面上的反斜杠字符</td></tr><tr><td align="left">\ddd</td><td align="left">表示1到3位数八进制值的字符。仅在格式字符串中有效</td></tr><tr><td align="left">\0ddd</td><td align="left">表示1到3位的八进制值字符</td></tr></tbody></table><p>实例:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">printf</span> <span class="string">"a string, no processing:&lt;%s&gt;\n"</span> <span class="string">"A\nB"</span></span></span><br><span class="line">a string, no processing:&lt;A\nB&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">printf</span> <span class="string">"a string, no processing:&lt;%b&gt;\n"</span> <span class="string">"A\nB"</span></span></span><br><span class="line">a string, no processing:&lt;A</span><br><span class="line"><span class="meta">B&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">printf</span> <span class="string">"www.runoob.com \a"</span></span></span><br><span class="line">www.runoob.com $                  #不换行</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的echo命令</title>
      <link href="/2018/09/27/linux/shell/6/"/>
      <url>/2018/09/27/linux/shell/6/</url>
      
        <content type="html"><![CDATA[<p>Shell 的 echo 指令与 PHP 的 echo 指令类似，都是用于字符串的输出。命令格式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo string</span><br></pre></td></tr></table></figure><p>您可以使用echo实现更复杂的输出格式控制。</p><h3 id="显示普通字符串"><a href="#显示普通字符串" class="headerlink" title="显示普通字符串:"></a>显示普通字符串:</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "It is a test"</span><br></pre></td></tr></table></figure><p>这里的双引号完全可以省略，以下命令与上面实例效果一致：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo It is a test</span><br></pre></td></tr></table></figure><h3 id="显示转义字符"><a href="#显示转义字符" class="headerlink" title="显示转义字符"></a>显示转义字符</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "\"It is a test\""</span><br></pre></td></tr></table></figure><p>结果将是:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">"It is a test"</span><br></pre></td></tr></table></figure><p>同样，双引号也可以省略</p><h3 id="显示变量"><a href="#显示变量" class="headerlink" title="显示变量"></a>显示变量</h3><p>read 命令从标准输入中读取一行,并把输入行的每个字段的值指定给 shell 变量</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line">read name </span><br><span class="line">echo "$name It is a test"</span><br></pre></td></tr></table></figure><p>以上代码保存为 test.sh，name 接收标准输入的变量，结果将是:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@www ~]# sh test.sh</span><br><span class="line">OK                     #标准输入</span><br><span class="line">OK It is a test        #输出</span><br></pre></td></tr></table></figure><h3 id="显示换行"><a href="#显示换行" class="headerlink" title="显示换行"></a>显示换行</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo -e "OK! \n" # -e 开启转义</span><br><span class="line">echo "It is a test"</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">OK!</span><br><span class="line"></span><br><span class="line">It is a test</span><br></pre></td></tr></table></figure><h3 id="显示不换行"><a href="#显示不换行" class="headerlink" title="显示不换行"></a>显示不换行</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line">echo -e "OK! \c" # -e 开启转义 \c 不换行</span><br><span class="line">echo "It is a test"</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">OK! It is a test</span><br></pre></td></tr></table></figure><h3 id="显示结果定向至文件"><a href="#显示结果定向至文件" class="headerlink" title="显示结果定向至文件"></a>显示结果定向至文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "It is a test" &gt; myfile</span><br></pre></td></tr></table></figure><h3 id="原样输出字符串，不进行转义或取变量-用单引号"><a href="#原样输出字符串，不进行转义或取变量-用单引号" class="headerlink" title="原样输出字符串，不进行转义或取变量(用单引号)"></a>原样输出字符串，不进行转义或取变量(用单引号)</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo '$name\"'</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">name\"</span></span><br></pre></td></tr></table></figure><h3 id="显示命令执行结果"><a href="#显示命令执行结果" class="headerlink" title="显示命令执行结果"></a>显示命令执行结果</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo `date`</span><br></pre></td></tr></table></figure><p><strong>注意：</strong> 这里使用的是反引号 <strong>`</strong>, 而不是单引号 <strong>‘</strong>。</p><p>结果将显示当前日期</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Thu Jul 24 10:08:46 CST 2014</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的运算符</title>
      <link href="/2018/09/26/linux/shell/5/"/>
      <url>/2018/09/26/linux/shell/5/</url>
      
        <content type="html"><![CDATA[<p>Shell 和其他编程语言一样，支持多种运算符，包括：</p><ul><li>算数运算符</li><li>关系运算符</li><li>布尔运算符</li><li>字符串运算符</li><li>文件测试运算符</li></ul><p>原生bash不支持简单的数学运算，但是可以通过其他命令来实现，例如 awk 和 expr，expr 最常用。</p><p>expr 是一款表达式计算工具，使用它能完成表达式的求值操作。</p><p>例如，两个数相加(注意使用的是反引号 ` 而不是单引号 ‘)：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">val=`expr 2 + 2`</span><br><span class="line">echo &quot;两数之和为 : $val&quot;</span><br></pre></td></tr></table></figure><p>运行实例</p><p>执行脚本，输出结果如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">两数之和为 : 4</span><br></pre></td></tr></table></figure><p>两点注意：</p><ul><li>表达式和运算符之间要有空格，例如 2+2 是不对的，必须写成 2 + 2，这与我们熟悉的大多数编程语言不一样。</li><li>完整的表达式要被 <code></code> 包含，注意这个字符不是常用的单引号，在 Esc 键下边。</li></ul><h3 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h3><p>下表列出了常用的算术运算符，假定变量 a 为 10，变量 b 为 20：</p><table><thead><tr><th align="left">运算符</th><th align="left">说明</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">+</td><td align="left">加法</td><td align="left"><code>expr $a + $b</code> 结果为 30。</td></tr><tr><td align="left">-</td><td align="left">减法</td><td align="left"><code>expr $a - $b</code> 结果为 -10。</td></tr><tr><td align="left">*</td><td align="left">乘法</td><td align="left"><code>expr $a * $b</code> 结果为  200。</td></tr><tr><td align="left">/</td><td align="left">除法</td><td align="left"><code>expr $b / $a</code> 结果为 2。</td></tr><tr><td align="left">%</td><td align="left">取余</td><td align="left"><code>expr $b % $a</code> 结果为 0。</td></tr><tr><td align="left">=</td><td align="left">赋值</td><td align="left">a=$b 将把变量 b 的值赋给 a。</td></tr><tr><td align="left">==</td><td align="left">相等。用于比较两个数字，相同则返回 true。</td><td align="left">[ $a == $b ] 返回 false。</td></tr><tr><td align="left">!=</td><td align="left">不相等。用于比较两个数字，不相同则返回 true。</td><td align="left">[ $a != $b ] 返回 true。</td></tr></tbody></table><p>注意：条件表达式要放在方括号之间，并且要有空格，例如: [$a==$b] 是错误的，必须写成 [ $a == $b ]。</p><p>算术运算符实例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a=10</span><br><span class="line">b=20</span><br><span class="line"></span><br><span class="line">val=`expr $a + $b`</span><br><span class="line">echo "a + b : $val"</span><br><span class="line"></span><br><span class="line">val=`expr $a - $b`</span><br><span class="line">echo "a - b : $val"</span><br><span class="line"></span><br><span class="line">val=`expr $a * $b`</span><br><span class="line">echo "a * b : $val"</span><br><span class="line"></span><br><span class="line">val=`expr $b / $a`</span><br><span class="line">echo "b / a : $val"</span><br><span class="line"></span><br><span class="line">val=`expr $b % $a`</span><br><span class="line">echo "b % a : $val"</span><br><span class="line"></span><br><span class="line">if [ $a == $b ]</span><br><span class="line">then</span><br><span class="line">  echo "a 等于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a != $b ]</span><br><span class="line">then</span><br><span class="line">  echo "a 不等于 b"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a + b : 30</span><br><span class="line">a - b : -10</span><br><span class="line">a * b : 200</span><br><span class="line">b / a : 2</span><br><span class="line">b % a : 0</span><br><span class="line">a 不等于 b</span><br></pre></td></tr></table></figure><p>注意：</p><ul><li>乘号()前边必须加反斜杠()才能实现乘法运算；</li><li>if…then…fi 是条件语句，后续将会讲解。</li><li>在 MAC 中 shell 的 expr 语法是：$((表达式))，此处表达式中的 “” 不需要转义符号 “&quot; 。</li></ul><h3 id="关系运算符"><a href="#关系运算符" class="headerlink" title="关系运算符"></a>关系运算符</h3><p>关系运算符只支持数字，不支持字符串，除非字符串的值是数字。</p><p>下表列出了常用的关系运算符，假定变量 a 为 10，变量 b 为 20：</p><table><thead><tr><th align="left">运算符</th><th align="left">说明</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">-eq</td><td align="left">检测两个数是否相等，相等返回 true。</td><td align="left">[ $a -eq $b ] 返回 false。</td></tr><tr><td align="left">-ne</td><td align="left">检测两个数是否不相等，不相等返回 true。</td><td align="left">[ $a -ne $b ] 返回 true。</td></tr><tr><td align="left">-gt</td><td align="left">检测左边的数是否大于右边的，如果是，则返回 true。</td><td align="left">[ $a -gt $b ] 返回 false。</td></tr><tr><td align="left">-lt</td><td align="left">检测左边的数是否小于右边的，如果是，则返回 true。</td><td align="left">[ $a -lt $b ] 返回 true。</td></tr><tr><td align="left">-ge</td><td align="left">检测左边的数是否大于等于右边的，如果是，则返回 true。</td><td align="left">[ $a -ge $b ] 返回 false。</td></tr><tr><td align="left">-le</td><td align="left">检测左边的数是否小于等于右边的，如果是，则返回 true。</td><td align="left">[ $a -le $b ] 返回 true。</td></tr></tbody></table><p>关系运算符实例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a=10</span><br><span class="line">b=20</span><br><span class="line"></span><br><span class="line">if [ $a -eq $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a -eq $b : a 等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a -eq $b: a 不等于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a -ne $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a -ne $b: a 不等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a -ne $b : a 等于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a -gt $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a -gt $b: a 大于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a -gt $b: a 不大于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a -lt $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a -lt $b: a 小于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a -lt $b: a 不小于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a -ge $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a -ge $b: a 大于或等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a -ge $b: a 小于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a -le $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a -le $b: a 小于或等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a -le $b: a 大于 b"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">10 -eq 20: a 不等于 b</span><br><span class="line">10 -ne 20: a 不等于 b</span><br><span class="line">10 -gt 20: a 不大于 b</span><br><span class="line">10 -lt 20: a 小于 b</span><br><span class="line">10 -ge 20: a 小于 b</span><br><span class="line">10 -le 20: a 小于或等于 b</span><br></pre></td></tr></table></figure><h3 id="布尔运算符"><a href="#布尔运算符" class="headerlink" title="布尔运算符"></a>布尔运算符</h3><p>下表列出了常用的布尔运算符，假定变量 a 为 10，变量 b 为 20：</p><table><thead><tr><th align="left">运算符</th><th align="left">说明</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">!</td><td align="left">非运算，表达式为 true 则返回 false，否则返回 true。</td><td align="left">[ ! false ] 返回 true。</td></tr><tr><td align="left">-o</td><td align="left">或运算，有一个表达式为 true 则返回 true。</td><td align="left">[ $a -lt 20 -o $b -gt 100 ] 返回 true。</td></tr><tr><td align="left">-a</td><td align="left">与运算，两个表达式都为 true 才返回 true。</td><td align="left">[ $a -lt 20 -a $b -gt 100 ] 返回 false。</td></tr></tbody></table><p>布尔运算符实例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a=10</span><br><span class="line">b=20</span><br><span class="line"></span><br><span class="line">if [ $a != $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a != $b : a 不等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a == $b: a 等于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a -lt 100 -a $b -gt 15 ]</span><br><span class="line">then</span><br><span class="line">  echo "$a 小于 100 且 $b 大于 15 : 返回 true"</span><br><span class="line">else</span><br><span class="line">  echo "$a 小于 100 且 $b 大于 15 : 返回 false"</span><br><span class="line">fi</span><br><span class="line">if [ $a -lt 100 -o $b -gt 100 ]</span><br><span class="line">then</span><br><span class="line">  echo "$a 小于 100 或 $b 大于 100 : 返回 true"</span><br><span class="line">else</span><br><span class="line">  echo "$a 小于 100 或 $b 大于 100 : 返回 false"</span><br><span class="line">fi</span><br><span class="line">if [ $a -lt 5 -o $b -gt 100 ]</span><br><span class="line">then</span><br><span class="line">  echo "$a 小于 5 或 $b 大于 100 : 返回 true"</span><br><span class="line">else</span><br><span class="line">  echo "$a 小于 5 或 $b 大于 100 : 返回 false"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">10 != 20 : a 不等于 b</span><br><span class="line">10 小于 100 且 20 大于 15 : 返回 true</span><br><span class="line">10 小于 100 或 20 大于 100 : 返回 true</span><br><span class="line">10 小于 5 或 20 大于 100 : 返回 false</span><br></pre></td></tr></table></figure><h3 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h3><p>以下介绍 Shell 的逻辑运算符，假定变量 a 为 10，变量 b 为 20:</p><table><thead><tr><th align="left">运算符</th><th align="left">说明</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">&amp;&amp;</td><td align="left">逻辑的 AND</td><td align="left">[[ $a -lt 100 &amp;&amp; $b -gt 100 ]] 返回 false</td></tr><tr><td align="left">||</td><td align="left">逻辑的 OR</td><td align="left">[[ $a -lt 100 || $b -gt 100 ]] 返回 true</td></tr></tbody></table><p>算符实例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a=10</span><br><span class="line">b=20</span><br><span class="line"></span><br><span class="line">if [[ $a -lt 100 &amp;&amp; $b -gt 100 ]]</span><br><span class="line">then</span><br><span class="line">  echo "返回 true"</span><br><span class="line">else</span><br><span class="line">  echo "返回 false"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [[ $a -lt 100 || $b -gt 100 ]]</span><br><span class="line">then</span><br><span class="line">  echo "返回 true"</span><br><span class="line">else</span><br><span class="line">  echo "返回 false"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">返回 false</span><br><span class="line">返回 true</span><br></pre></td></tr></table></figure><h3 id="字符串运算符"><a href="#字符串运算符" class="headerlink" title="字符串运算符"></a>字符串运算符</h3><p>下表列出了常用的字符串运算符，假定变量 a 为 “abc”，变量 b 为 “efg”：</p><table><thead><tr><th align="left">运算符</th><th align="left">说明</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">=</td><td align="left">检测两个字符串是否相等，相等返回 true。</td><td align="left">[ $a = $b ] 返回 false。</td></tr><tr><td align="left">!=</td><td align="left">检测两个字符串是否相等，不相等返回 true。</td><td align="left">[ $a != $b ] 返回 true。</td></tr><tr><td align="left">-z</td><td align="left">检测字符串长度是否为0，为0返回 true。</td><td align="left">[ -z $a ] 返回 false。</td></tr><tr><td align="left">-n</td><td align="left">检测字符串长度是否为0，不为0返回 true。</td><td align="left">[ -n “$a” ] 返回 true。</td></tr><tr><td align="left">$</td><td align="left">检测字符串是否为空，不为空返回 true。</td><td align="left">[ $a ] 返回 true。</td></tr></tbody></table><p>字符串运算符实例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a="abc"</span><br><span class="line">b="efg"</span><br><span class="line"></span><br><span class="line">if [ $a = $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a = $b : a 等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a = $b: a 不等于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a != $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a != $b : a 不等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a != $b: a 等于 b"</span><br><span class="line">fi</span><br><span class="line">if [ -z $a ]</span><br><span class="line">then</span><br><span class="line">  echo "-z $a : 字符串长度为 0"</span><br><span class="line">else</span><br><span class="line">  echo "-z $a : 字符串长度不为 0"</span><br><span class="line">fi</span><br><span class="line">if [ -n "$a" ]</span><br><span class="line">then</span><br><span class="line">  echo "-n $a : 字符串长度不为 0"</span><br><span class="line">else</span><br><span class="line">  echo "-n $a : 字符串长度为 0"</span><br><span class="line">fi</span><br><span class="line">if [ $a ]</span><br><span class="line">then</span><br><span class="line">  echo "$a : 字符串不为空"</span><br><span class="line">else</span><br><span class="line">  echo "$a : 字符串为空"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">abc = efg: a 不等于 b</span><br><span class="line">abc != efg : a 不等于 b</span><br><span class="line">-z abc : 字符串长度不为 0</span><br><span class="line">-n abc : 字符串长度不为 0</span><br><span class="line">abc : 字符串不为空</span><br></pre></td></tr></table></figure><h3 id="文件测试运算符"><a href="#文件测试运算符" class="headerlink" title="文件测试运算符"></a>文件测试运算符</h3><p>文件测试运算符用于检测 Unix 文件的各种属性。</p><p>属性检测描述如下：</p><table><thead><tr><th align="left">操作符</th><th align="left">说明</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">-b file</td><td align="left">检测文件是否是块设备文件，如果是，则返回 true。</td><td align="left">[ -b $file ] 返回 false。</td></tr><tr><td align="left">-c file</td><td align="left">检测文件是否是字符设备文件，如果是，则返回 true。</td><td align="left">[ -c $file ] 返回 false。</td></tr><tr><td align="left"><code>-d file</code></td><td align="left">检测文件是否是目录，如果是，则返回 true。</td><td align="left">[ -d $file ] 返回 false。</td></tr><tr><td align="left"><code>-f file</code></td><td align="left">检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回 true。</td><td align="left">[ -f $file ] 返回 true。</td></tr><tr><td align="left">-g file</td><td align="left">检测文件是否设置了 SGID 位，如果是，则返回 true。</td><td align="left">[ -g $file ] 返回 false。</td></tr><tr><td align="left">-k file</td><td align="left">检测文件是否设置了粘着位(Sticky Bit)，如果是，则返回 true。</td><td align="left">[ -k $file ] 返回 false。</td></tr><tr><td align="left">-p file</td><td align="left">检测文件是否是有名管道，如果是，则返回 true。</td><td align="left">[ -p $file ] 返回 false。</td></tr><tr><td align="left">-u file</td><td align="left">检测文件是否设置了 SUID 位，如果是，则返回 true。</td><td align="left">[ -u $file ] 返回 false。</td></tr><tr><td align="left"><code>-r file</code></td><td align="left">检测文件是否可读，如果是，则返回 true。</td><td align="left">[ -r $file ] 返回 true。</td></tr><tr><td align="left"><code>-w file</code></td><td align="left">检测文件是否可写，如果是，则返回 true。</td><td align="left">[ -w $file ] 返回 true。</td></tr><tr><td align="left"><code>-x file</code></td><td align="left">检测文件是否可执行，如果是，则返回 true。</td><td align="left">[ -x $file ] 返回 true。</td></tr><tr><td align="left"><code>-s file</code></td><td align="left">检测文件是否为空（文件大小是否大于0），不为空返回 true。</td><td align="left">[ -s $file ] 返回 true。</td></tr><tr><td align="left"><code>-e file</code></td><td align="left">检测文件（包括目录）是否存在，如果是，则返回 true。</td><td align="left">[ -e $file ] 返回 true。</td></tr></tbody></table><p>其他检查符：</p><ul><li>-S: 判断某文件是否 socket。</li><li>-L: 检测文件是否存在并且是一个符号链接。</li></ul><p>变量 file 表示文件 /var/www/runoob/test.sh，它的大小为 100 字节，具有 rwx 权限。下面的代码，将检测该文件的各种属性：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">file="/var/www/runoob/test.sh"</span><br><span class="line">if [ -r $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件可读"</span><br><span class="line">else</span><br><span class="line">  echo "文件不可读"</span><br><span class="line">fi</span><br><span class="line">if [ -w $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件可写"</span><br><span class="line">else</span><br><span class="line">  echo "文件不可写"</span><br><span class="line">fi</span><br><span class="line">if [ -x $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件可执行"</span><br><span class="line">else</span><br><span class="line">  echo "文件不可执行"</span><br><span class="line">fi</span><br><span class="line">if [ -f $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件为普通文件"</span><br><span class="line">else</span><br><span class="line">  echo "文件为特殊文件"</span><br><span class="line">fi</span><br><span class="line">if [ -d $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件是个目录"</span><br><span class="line">else</span><br><span class="line">  echo "文件不是个目录"</span><br><span class="line">fi</span><br><span class="line">if [ -s $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件不为空"</span><br><span class="line">else</span><br><span class="line">  echo "文件为空"</span><br><span class="line">fi</span><br><span class="line">if [ -e $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件存在"</span><br><span class="line">else</span><br><span class="line">  echo "文件不存在"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">文件可读</span><br><span class="line">文件可写</span><br><span class="line">文件可执行</span><br><span class="line">文件为普通文件</span><br><span class="line">文件不是个目录</span><br><span class="line">文件不为空</span><br><span class="line">文件存在</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的数组</title>
      <link href="/2018/09/25/linux/shell/4/"/>
      <url>/2018/09/25/linux/shell/4/</url>
      
        <content type="html"><![CDATA[<p>数组中可以存放多个值。Bash Shell 只支持一维数组（不支持多维数组），初始化时不需要定义数组大小（与 PHP 类似）。</p><p>与大部分编程语言类似，数组元素的下标由0开始。</p><p>Shell 数组用括号来表示，元素用”空格”符号分割开，语法格式如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">array_name=(value1 ... valuen)</span><br></pre></td></tr></table></figure><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">my_array=(A B "C" D)</span><br></pre></td></tr></table></figure><p>我们也可以使用下标来定义数组:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">array_name[0]=value0</span><br><span class="line">array_name[1]=value1</span><br><span class="line">array_name[2]=value2</span><br></pre></td></tr></table></figure><h3 id="读取数组"><a href="#读取数组" class="headerlink" title="读取数组"></a>读取数组</h3><p>读取数组元素值的一般格式是：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">&#123;array_name[index]&#125;</span></span><br></pre></td></tr></table></figure><h3 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">my_array=(A B "C" D)</span><br><span class="line"></span><br><span class="line">echo "第一个元素为: $&#123;my_array[0]&#125;"</span><br><span class="line">echo "第二个元素为: $&#123;my_array[1]&#125;"</span><br><span class="line">echo "第三个元素为: $&#123;my_array[2]&#125;"</span><br><span class="line">echo "第四个元素为: $&#123;my_array[3]&#125;"</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod +x test.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./test.sh</span></span><br><span class="line">第一个元素为: A</span><br><span class="line">第二个元素为: B</span><br><span class="line">第三个元素为: C</span><br><span class="line">第四个元素为: D</span><br></pre></td></tr></table></figure><h3 id="获取数组中的所有元素"><a href="#获取数组中的所有元素" class="headerlink" title="获取数组中的所有元素"></a>获取数组中的所有元素</h3><p>使用@ 或 * 可以获取数组中的所有元素，例如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">my_array[0]=A</span><br><span class="line">my_array[1]=B</span><br><span class="line">my_array[2]=C</span><br><span class="line">my_array[3]=D</span><br><span class="line"></span><br><span class="line">echo "数组的元素为: $&#123;my_array[*]&#125;"</span><br><span class="line">echo "数组的元素为: $&#123;my_array[@]&#125;"</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod +x test.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./test.sh</span></span><br><span class="line">数组的元素为: A B C D</span><br><span class="line">数组的元素为: A B C D</span><br></pre></td></tr></table></figure><h3 id="获取数组的长度"><a href="#获取数组的长度" class="headerlink" title="获取数组的长度"></a>获取数组的长度</h3><p>获取数组长度的方法与获取字符串长度的方法相同，例如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">my_array[0]=A</span><br><span class="line">my_array[1]=B</span><br><span class="line">my_array[2]=C</span><br><span class="line">my_array[3]=D</span><br><span class="line"></span><br><span class="line">echo "数组元素个数为: $&#123;#my_array[*]&#125;"</span><br><span class="line">echo "数组元素个数为: $&#123;#my_array[@]&#125;"</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod +x test.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./test.sh</span></span><br><span class="line">数组元素个数为: 4</span><br><span class="line">数组元素个数为: 4</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的参数传递</title>
      <link href="/2018/09/24/linux/shell/3/"/>
      <url>/2018/09/24/linux/shell/3/</url>
      
        <content type="html"><![CDATA[<p>我们可以在执行 Shell 脚本时，向脚本传递参数，脚本内获取参数的格式为：<strong>$n</strong>。<strong>n</strong> 代表一个数字，1 为执行脚本的第一个参数，2 为执行脚本的第二个参数，以此类推……</p><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><p>以下实例我们向脚本传递三个参数，并分别输出，其中 <strong>$0</strong> 为执行的文件名：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "Shell 传递参数实例！";</span><br><span class="line">echo "执行的文件名：$0";</span><br><span class="line">echo "第一个参数为：$1";</span><br><span class="line">echo "第二个参数为：$2";</span><br><span class="line">echo "第三个参数为：$3";</span><br></pre></td></tr></table></figure><p>为脚本设置可执行权限，并执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod +x test.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./test.sh 1 2 3</span></span><br><span class="line">Shell 传递参数实例！</span><br><span class="line">执行的文件名：./test.sh</span><br><span class="line">第一个参数为：1</span><br><span class="line">第二个参数为：2</span><br><span class="line">第三个参数为：3</span><br></pre></td></tr></table></figure><p>另外，还有几个特殊字符用来处理参数：</p><table><thead><tr><th align="left">参数处理</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">$#</td><td align="left">传递到脚本的参数个数</td></tr><tr><td align="left">$*</td><td align="left">以一个单字符串显示所有向脚本传递的参数。 如”$*”用「”」括起来的情况、以”$1 $2 … $n”的形式输出所有参数。</td></tr><tr><td align="left">$$</td><td align="left">脚本运行的当前进程ID号</td></tr><tr><td align="left">$!</td><td align="left">后台运行的最后一个进程的ID号</td></tr><tr><td align="left">$@</td><td align="left">与$*相同，但是使用时加引号，并在引号中返回每个参数。 如”$@”用「”」括起来的情况、以”$1” “$2” … “$n” 的形式输出所有参数。</td></tr><tr><td align="left">$-</td><td align="left">显示Shell使用的当前选项，与set命令功能相同。</td></tr><tr><td align="left">$?</td><td align="left">显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误。</td></tr></tbody></table><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "Shell 传递参数实例！";</span><br><span class="line">echo "第一个参数为：$1";</span><br><span class="line"></span><br><span class="line">echo "参数个数为：$#";</span><br><span class="line">echo "传递的参数作为一个字符串显示：$*";</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod +x test.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./test.sh 1 2 3</span></span><br><span class="line">Shell 传递参数实例！</span><br><span class="line">第一个参数为：1</span><br><span class="line">参数个数为：3</span><br><span class="line">传递的参数作为一个字符串显示：1 2 3</span><br></pre></td></tr></table></figure><p>$* 与 $@ 区别：</p><ul><li>相同点：都是引用所有参数。</li><li>不同点：只有在双引号中体现出来。假设在脚本运行时写了三个参数 1、2、3，，则 “ * “ 等价于 “1 2 3”（传递了一个参数），而 “@” 等价于 “1” “2” “3”（传递了三个参数）。</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "-- \$* 演示 ---"</span><br><span class="line">for i in "$*"; do</span><br><span class="line">    echo $i</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">echo "-- \$@ 演示 ---"</span><br><span class="line">for i in "$@"; do</span><br><span class="line">    echo $i</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod +x test.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./test.sh 1 2 3</span></span><br><span class="line">-- $* 演示 ---</span><br><span class="line">1 2 3</span><br><span class="line">-- $@ 演示 ---</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的字符串</title>
      <link href="/2018/09/23/linux/shell/2/"/>
      <url>/2018/09/23/linux/shell/2/</url>
      
        <content type="html"><![CDATA[<p>字符串是shell编程中最常用最有用的数据类型（除了数字和字符串，也没啥其它类型好用了），字符串可以用单引号，也可以用双引号，也可以不用引号。单双引号的区别跟PHP类似。</p><h3 id="单引号"><a href="#单引号" class="headerlink" title="单引号"></a>单引号</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">str='this is a string'</span><br></pre></td></tr></table></figure><p>单引号字符串的限制：</p><ul><li>单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的；</li><li>单引号字串中不能出现单独一个的单引号（对单引号使用转义符后也不行），但可成对出现，作为字符串拼接使用。</li></ul><h3 id="双引号"><a href="#双引号" class="headerlink" title="双引号"></a>双引号</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">your_name='runoob'</span><br><span class="line">str="Hello, I know you are \"$your_name\"! \n"</span><br><span class="line">echo -e $str</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Hello, I know you are "runoob"!</span><br></pre></td></tr></table></figure><p>双引号的优点：</p><ul><li>双引号里可以有变量</li><li>双引号里可以出现转义字符</li></ul><h3 id="拼接字符串"><a href="#拼接字符串" class="headerlink" title="拼接字符串"></a>拼接字符串</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">your_name="runoob"</span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用双引号拼接</span></span><br><span class="line">greeting="hello, "$your_name" !"</span><br><span class="line">greeting_1="hello, $&#123;your_name&#125; !"</span><br><span class="line">echo $greeting  $greeting_1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用单引号拼接</span></span><br><span class="line">greeting_2='hello, '$your_name' !'</span><br><span class="line">greeting_3='hello, $&#123;your_name&#125; !'</span><br><span class="line">echo $greeting_2  $greeting_3</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hello, runoob ! hello, runoob !</span><br><span class="line">hello, runoob ! hello, $&#123;your_name&#125; !</span><br></pre></td></tr></table></figure><h3 id="获取字符串长度"><a href="#获取字符串长度" class="headerlink" title="获取字符串长度"></a>获取字符串长度</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">string="abcd"</span><br><span class="line">echo $&#123;#string&#125; #输出 4</span><br></pre></td></tr></table></figure><h3 id="提取子字符串"><a href="#提取子字符串" class="headerlink" title="提取子字符串"></a>提取子字符串</h3><p>以下实例从字符串第 <strong>2</strong> 个字符开始截取 <strong>4</strong> 个字符：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">string="runoob is a great site"</span><br><span class="line">echo $&#123;string:1:4&#125; # 输出 unoo</span><br></pre></td></tr></table></figure><h3 id="查找子字符串"><a href="#查找子字符串" class="headerlink" title="查找子字符串"></a>查找子字符串</h3><p>查找字符 <strong>i</strong> 或 <strong>o</strong> 的位置(哪个字母先出现就计算哪个)：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">string="runoob is a great site"</span><br><span class="line">echo `expr index "$string" io`  # 输出 4</span><br></pre></td></tr></table></figure><p><strong>注意：</strong> 以上脚本中 <strong>`</strong> 是反引号，而不是单引号 <strong>‘</strong>，不要看错了哦。</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的变量</title>
      <link href="/2018/09/22/linux/shell/1/"/>
      <url>/2018/09/22/linux/shell/1/</url>
      
        <content type="html"><![CDATA[<h3 id="第一个Shell脚本"><a href="#第一个Shell脚本" class="headerlink" title="第一个Shell脚本"></a>第一个Shell脚本</h3><p>打开文本编辑器(可以使用 vi/vim 命令来创建文件)，新建一个文件 test.sh，扩展名为 sh（sh代表shell），扩展名并不影响脚本执行，见名知意就好，如果你用 php 写 shell 脚本，扩展名就用 php 好了。</p><p>输入一些代码，第一行一般是这样：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">echo "Hello World !"</span><br></pre></td></tr></table></figure><p><code>#! 是一个约定的标记，它告诉系统这个脚本需要什么解释器来执行，即使用哪一种 Shell。</code></p><p>echo 命令用于向窗口输出文本。</p><h3 id="运行-Shell-脚本的两种方法"><a href="#运行-Shell-脚本的两种方法" class="headerlink" title="运行 Shell 脚本的两种方法"></a>运行 Shell 脚本的两种方法</h3><ol><li><p>作为可执行程序</p><p>将上面的代码保存为 test.sh，并 cd 到相应目录：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chmod +x ./test.sh  #使脚本具有执行权限</span><br><span class="line">./test.sh  #执行脚本</span><br></pre></td></tr></table></figure><p>注意，一定要写成 <strong>./test.sh</strong>，而不是 <strong>test.sh</strong>，运行其它二进制的程序也一样，直接写 test.sh，linux 系统会去 PATH 里寻找有没有叫 test.sh 的，而只有 /bin, /sbin, /usr/bin，/usr/sbin 等在 PATH 里，你的当前目录通常不在 PATH 里，所以写成 test.sh 是会找不到命令的，要用 ./test.sh 告诉系统说，就在当前目录找。</p></li><li><p>作为解释器参数</p><p>这种运行方式是，直接运行解释器，其参数就是 shell 脚本的文件名，如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/bin/sh test.sh</span><br><span class="line">/bin/php test.php</span><br></pre></td></tr></table></figure><p>这种方式运行的脚本，可以省略第一行指定解释器信息。</p></li></ol><h3 id="Shell-变量"><a href="#Shell-变量" class="headerlink" title="Shell 变量"></a>Shell 变量</h3><p>定义变量时，变量名不加美元符号（$，PHP语言中变量需要），如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">your_name="runoob.com"</span><br></pre></td></tr></table></figure><p>注意，<code>变量名和等号之间不能有空格</code>，这可能和你熟悉的所有编程语言都不一样。同时，变量名的命名须遵循如下规则：</p><ul><li>命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。</li><li>中间不能有空格，可以使用下划线（_）。</li><li>不能使用标点符号。</li><li>不能使用bash里的关键字（可用help命令查看保留关键字）。</li></ul><p>有效的 Shell 变量名示例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">RUNOOB</span><br><span class="line">LD_LIBRARY_PATH</span><br><span class="line">_var</span><br><span class="line">var2</span><br></pre></td></tr></table></figure><p>无效的变量命名：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">?var=123</span><br><span class="line">user*name=runoob</span><br></pre></td></tr></table></figure><p>除了显式地直接赋值，还可以用语句给变量赋值，如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for file in `ls /etc`</span><br><span class="line">或</span><br><span class="line">for file in $(ls /etc)</span><br></pre></td></tr></table></figure><p>以上语句将 /etc 下目录的文件名循环出来。</p><h4 id="使用变量"><a href="#使用变量" class="headerlink" title="使用变量"></a>使用变量</h4><p>使用一个定义过的变量，只要在变量名前面加美元符号即可，如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">your_name="qinjx"</span><br><span class="line">echo $your_name</span><br><span class="line">echo $&#123;your_name&#125;</span><br></pre></td></tr></table></figure><p>变量名外面的花括号是可选的，加不加都行，加花括号是为了帮助解释器识别变量的边界，比如下面这种情况：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for skill in Ada Coffe Action Java; do</span><br><span class="line">    echo "I am good at $&#123;skill&#125;Script"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>如果不给skill变量加花括号，写成echo “I am good at $skillScript”，解释器就会把$skillScript当成一个变量（其值为空），代码执行结果就不是我们期望的样子了。</p><p>推荐给所有变量加上花括号，这是个好的编程习惯。</p><p>已定义的变量，可以被重新定义，如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">your_name="tom"</span><br><span class="line">echo $your_name</span><br><span class="line">your_name="alibaba"</span><br><span class="line">echo $your_name</span><br></pre></td></tr></table></figure><p>这样写是合法的，但注意，第二次赋值的时候不能写$your_name=”alibaba”，使用变量的时候才加美元符（$）。</p><h4 id="只读变量"><a href="#只读变量" class="headerlink" title="只读变量"></a>只读变量</h4><p>使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。</p><p>下面的例子尝试更改只读变量，结果报错：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">myUrl="http://www.google.com"</span><br><span class="line">readonly myUrl</span><br><span class="line">myUrl="http://www.runoob.com"</span><br></pre></td></tr></table></figure><p>运行脚本，结果如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/bin/sh: NAME: This variable is read only.</span><br></pre></td></tr></table></figure><h4 id="删除变量"><a href="#删除变量" class="headerlink" title="删除变量"></a>删除变量</h4><p>使用 unset 命令可以删除变量。语法：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">unset variable_name</span><br></pre></td></tr></table></figure><p>变量被删除后不能再次使用。unset 命令不能删除只读变量。</p><p>实例: </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line">myUrl="http://www.runoob.com"</span><br><span class="line">unset myUrl</span><br><span class="line">echo $myUrl</span><br></pre></td></tr></table></figure><p>以上实例执行将没有任何输出。</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>后台执行、crontab调度和软连接的使用场景</title>
      <link href="/2018/09/21/linux/5/"/>
      <url>/2018/09/21/linux/5/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 后台执行脚本</li><li>整理 rundeck 视频 部署</li><li>整理 crontab 每隔10s</li><li>整理 软连接 场景 坑</li></ol><h3 id="整理后台执行脚本"><a href="#整理后台执行脚本" class="headerlink" title="整理后台执行脚本"></a>整理后台执行脚本</h3><p>后台执行后命令有三个，分别是：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">./test.sh &amp;</span><br><span class="line">nohup ./test.sh &amp; </span><br><span class="line">nohup ./test.sh &gt; /root/test.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>一般使用第三条</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# nohup ./show.sh &gt;&gt; ./show.log 2&gt;&amp;1 &amp;　　　　#输出重定向</span><br><span class="line">[4] 18637</span><br><span class="line">[root@aliyun ~]# </span><br><span class="line">[root@aliyun ~]# tail -F show.log 　　　　#实时接收输出内容</span><br><span class="line">nohup: ignoring input</span><br><span class="line">Thu Nov 21 17:07:58 CST 2019</span><br><span class="line">Thu Nov 21 17:08:08 CST 2019</span><br><span class="line">Thu Nov 21 17:08:18 CST 2019</span><br><span class="line">Thu Nov 21 17:08:28 CST 2019</span><br><span class="line">Thu Nov 21 17:08:48 CST 2019</span><br></pre></td></tr></table></figure><h3 id="整理-rundeck-视频-部署"><a href="#整理-rundeck-视频-部署" class="headerlink" title="整理 rundeck 视频 部署"></a>整理 rundeck 视频 部署</h3><p><a href="https://www.bilibili.com/video/av35466584?from=search&amp;seid=1197620829255678947" target="_blank" rel="noopener">https://www.bilibili.com/video/av35466584?from=search&amp;seid=1197620829255678947</a></p><h3 id="整理-crontab-每隔10s"><a href="#整理-crontab-每隔10s" class="headerlink" title="整理 crontab 每隔10s"></a>整理 crontab 每隔10s</h3><p>Linux自带的任务调度工具 crontab 的调度单位分别是 分、时、日、周、月 最小的划分粒度是分钟，因此不能解决秒级别的调度问题，</p><p>* 代表每次，如 * / 6 代表每6分钟执行一次</p><p>但是换一种思路，我可以把调度代码包在循环体中，这个循环体执行6次，每次sleep 10s ，加起来就是分钟，即每分钟执行6次，每次间隔10秒</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">for((i=0;i&lt;6;i++));</span><br><span class="line">do</span><br><span class="line">        date</span><br><span class="line">        sleep 10s</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>这样打印出来的结果是间隔10秒</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ./show.sh </span><br><span class="line">Thu Nov 21 17:19:23 CST 2019</span><br><span class="line">Thu Nov 21 17:19:33 CST 2019</span><br><span class="line">Thu Nov 21 17:19:43 CST 2019</span><br><span class="line">Thu Nov 21 17:19:53 CST 2019</span><br><span class="line">Thu Nov 21 17:20:03 CST 2019</span><br><span class="line">Thu Nov 21 17:20:13 CST 2019</span><br></pre></td></tr></table></figure><h3 id="4-整理-软连接-场景-坑"><a href="#4-整理-软连接-场景-坑" class="headerlink" title="4.整理 软连接 场景 坑"></a>4.整理 软连接 场景 坑</h3><p>软连接的使用: ln -s 源文件路径 目标文件路径</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ln -s test.txt test</span><br><span class="line">[root@aliyun ~]# ll</span><br><span class="line">total 24</span><br><span class="line">-rw------- 1 root root       5 Nov 21 17:03 nohup.out</span><br><span class="line">-rw-r--r-- 1 root root      45 Nov 12 23:21 print.sh</span><br><span class="line">-rw-r--r-- 1 root root     196 Nov 21 17:08 show.log</span><br><span class="line">-rwxr--r-- 1 root root      58 Nov 21 17:07 show.sh</span><br><span class="line">drwxr-xr-x 2 root root    4096 Nov 17 10:24 size.log</span><br><span class="line">lrwxrwxrwx 1 root root       8 Nov 21 17:23 test -&gt; test.txt</span><br><span class="line">-rwxr-xr-- 1 root bigdata  198 Nov 18 11:47 test.txt</span><br><span class="line">[root@aliyun ~]#</span><br></pre></td></tr></table></figure><h3 id="使用场景："><a href="#使用场景：" class="headerlink" title="使用场景："></a>使用场景：</h3><ol><li><p>作为源文件的快捷方式存在，好处：升级源文件的时候只需要重新创建软连接，注意：环境变量中不能写源文件的路径，必须写软连接文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun~]# ll</span><br><span class="line">total 5</span><br><span class="line">lrwxrwxrwx 1 root root   8 Nov 21 17:23 mysql -&gt; mysql5.6</span><br><span class="line">drwxr-xr-x 2 root root   6 Nov 20 21:33 mysql5.6  #低版本部署</span><br><span class="line">drwxr-xr-x 2 root root   6 Nov 20 21:33 mysql5.7  #通过软连接来切换升级文件</span><br><span class="line">drwxr-xr-x 3 root root  44 Nov 17 23:13 ruozedata</span><br><span class="line">-rw-r--r-- 1 root root 846 Nov 17 23:12 ruozedata.zip</span><br></pre></td></tr></table></figure></li><li><p>作为数据盘在系统盘中日志写入目录的软连接，好处：日志写入和存储多个文件需要占用大量的磁盘，把日志的存储位置换到了数据盘中</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir /data01/log/  #创建数据盘下的日志目录</span><br><span class="line">mv  /var/log/hbase /data01/log/　　#移动系统盘的日志文件到数据盘</span><br><span class="line">ln -s /data01/log/hbase /var/log/hbase　　#数据盘的日志文件再软连接到系统盘</span><br></pre></td></tr></table></figure></li></ol><p><code>坑</code>：软连接文件创建后的文件和源为文件权限不同，必须注意和修改软连接文件和目标文件的权限</p><p><code>建议</code>：在创建软连接的时候，源文件路径和目标文件路径推介使用绝对路径</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> DataWarehouse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>熟练使用vim、系统命令和程序管理工具</title>
      <link href="/2018/09/20/linux/4/"/>
      <url>/2018/09/20/linux/4/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 vi</li><li>整理 进程 端口号</li><li>整理 连接拒绝 (权限受限)</li><li>整理 高危命令</li><li>常用的 wget yum rpm 压缩</li></ol><h3 id="vim中的常见用法-部分"><a href="#vim中的常见用法-部分" class="headerlink" title="vim中的常见用法(部分)"></a>vim中的常见用法(部分)</h3><table><thead><tr><th>复制</th><th>yy</th></tr></thead><tbody><tr><td>复制多行</td><td>nyy</td></tr><tr><td>当前行向下粘贴</td><td>p</td></tr><tr><td>当前行下上粘贴</td><td>P</td></tr><tr><td>当前位置插入</td><td>i(I)</td></tr><tr><td>当下位置的下一个位置插入</td><td>a(A)</td></tr><tr><td>当前行的下一行插入</td><td>o(O)</td></tr><tr><td>删除当前字符</td><td>x</td></tr><tr><td>删除当前位置到行尾</td><td>D</td></tr><tr><td>删除当前行</td><td>dd</td></tr><tr><td>删除当前行到未行</td><td>dG</td></tr><tr><td>删除n行</td><td>ndd</td></tr><tr><td>删除全部</td><td>gg + dG</td></tr><tr><td>跳转行尾</td><td>Shift + $</td></tr><tr><td>跳转行首</td><td>Shift + ^</td></tr><tr><td>跳转首行</td><td>gg</td></tr><tr><td>跳转未行</td><td>G</td></tr><tr><td>跳转到n行</td><td>:n或者是nG或者是ngg</td></tr><tr><td>撤回一次</td><td>u</td></tr><tr><td>撤回多次</td><td>U</td></tr></tbody></table><p><code>vim编辑中的坑：</code>编辑或者调优配置文件前，一定要备份</p><h3 id="系统命令"><a href="#系统命令" class="headerlink" title="系统命令"></a>系统命令</h3><h4 id="查看磁盘-（df-h）"><a href="#查看磁盘-（df-h）" class="headerlink" title="查看磁盘 （df -h）"></a>查看磁盘 （df -h）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# df -h</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/vda1        40G   11G   27G  29% /</span><br><span class="line">devtmpfs        911M     0  911M   0% /dev</span><br><span class="line">tmpfs           920M     0  920M   0% /dev/shm</span><br><span class="line">tmpfs           920M  332K  920M   1% /run</span><br><span class="line">tmpfs           920M     0  920M   0% /sys/fs/cgroup</span><br><span class="line">tmpfs           184M     0  184M   0% /run/user/0</span><br></pre></td></tr></table></figure><p>了解数据盘的格式：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/dev/vdb1        1T   10G    XXX   X% /data01 数据盘</span><br></pre></td></tr></table></figure><h4 id="查看内存（free-h）"><a href="#查看内存（free-h）" class="headerlink" title="查看内存（free -h）"></a>查看内存（free -h）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# free -h</span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           1.8G        853M         88M        336K        897M        792M</span><br><span class="line">Swap:            0B          0B          0B</span><br></pre></td></tr></table></figure><p>延伸：<a href="http://blog.itpub.net/30089851/viewspace-2131678/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-2131678/</a></p><h4 id="查看负载均衡（top）"><a href="#查看负载均衡（top）" class="headerlink" title="查看负载均衡（top）"></a>查看负载均衡（top）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# top</span><br><span class="line">top - 11:51:16 up 23 days, 12:56,  1 user,  load average: 0.00, 0.01, 0.05</span><br><span class="line">Tasks:  65 total,   1 running,  64 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu(s):  0.0 us,  0.3 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">KiB Mem :  1883724 total,    90012 free,   874596 used,   919116 buff/cache</span><br><span class="line">KiB Swap:        0 total,        0 free,        0 used.   810836 avail Mem </span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND                                       </span><br><span class="line">19516 root       0 -20  126596   9340   6508 S  0.3  0.5  98:38.33 AliYunDun                                     </span><br><span class="line">    1 root      20   0  125124   3344   2112 S  0.0  0.2   0:10.82 systemd                                       </span><br><span class="line">    2 root      20   0       0      0      0 S  0.0  0.0   0:00.00 kthreadd                                      </span><br><span class="line">    3 root      20   0       0      0      0 S  0.0  0.0   0:02.21 ksoftirqd/0                                   </span><br><span class="line">    5 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 kworker/0:0H</span><br></pre></td></tr></table></figure><p>注意：</p><ol><li>负载均衡的数值不能超过10</li><li>如果某服务长期占用cpu或者men，去检查这个进程是在做什么</li><li>如果cpu飙升3000%以上，夯住 ，代码级别，如果不是自己编写的代码，大概率硬件级别–&gt;内存条坏了</li></ol><h4 id="查看进程（ps-ef）"><a href="#查看进程（ps-ef）" class="headerlink" title="查看进程（ps -ef）"></a>查看进程（ps -ef）</h4><p>查看进程常常和 grep 配合使用</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ps -ef|grep ssh</span><br><span class="line">root      4295     1  0 Oct25 ?        00:00:00 /usr/sbin/sshd -D</span><br><span class="line">root     20282  4295  0 11:34 ?        00:00:00 sshd: root@pts/0</span><br><span class="line">root     20318 20284  0 11:56 pts/0    00:00:00 grep --color=auto ssh</span><br></pre></td></tr></table></figure><p>最后一条是自己的进程，可以加上 grep -v grep 去掉这条记录</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ps -ef|grep ssh | grep -v grep进程用户　进程的pid　父id　　　　　　　　　　　　　　　　进程用户的内容(进程所属的目录)</span><br><span class="line">root      4295     1  0 Oct25 ?        00:00:00 /usr/sbin/sshd -D</span><br><span class="line">root     20282  4295  0 11:34 ?        00:00:00 sshd: root@pts/0</span><br></pre></td></tr></table></figure><h4 id="查看端口号（netstat-nlp）"><a href="#查看端口号（netstat-nlp）" class="headerlink" title="查看端口号（netstat -nlp）"></a>查看端口号（netstat -nlp）</h4><p>最常配合查看进程得到的pid号查看端口号</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# netstat -nlp | grep 4295</span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      4295/sshd</span><br></pre></td></tr></table></figure><p><code>注意：</code> 如果查询出来的端口号前面的ip是127.0.0.1或者localhost，属于本地回环ip地址，需要修改相应的配置文件</p><p><code>场景：</code> 在centos部署大数据组件，发现一个错误 Connection refused</p><p>解决思路：</p><ol><li>ping ip 测试ip</li><li>telnet ip port 测试ip和端口号 </li><li>防火墙</li></ol><h4 id="telnet命令安装"><a href="#telnet命令安装" class="headerlink" title="telnet命令安装"></a>telnet命令安装</h4><h5 id="window："><a href="#window：" class="headerlink" title="window："></a>window：</h5><p><img src="https://img2018.cnblogs.com/i-beta/1657732/201911/1657732-20191118121808051-1147850773.png" alt="win10开启telnet命令"></p><h5 id="linux："><a href="#linux：" class="headerlink" title="linux："></a>linux：</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# yum install -y telnet</span><br><span class="line">[root@aliyun ~]# which telnet</span><br><span class="line">/usr/bin/telnet</span><br><span class="line">[root@aliyun ~]# telnet 121.196.220.143 22</span><br><span class="line">Trying 121.196.220.143...</span><br><span class="line">Connected to 121.196.220.143.</span><br><span class="line">Escape character is &apos;^]&apos;.</span><br><span class="line">SSH-2.0-OpenSSH_6.6.1</span><br></pre></td></tr></table></figure><h3 id="三个高危命令"><a href="#三个高危命令" class="headerlink" title="三个高危命令"></a>三个高危命令</h3><table><thead><tr><th>命令</th><th>作用</th></tr></thead><tbody><tr><td><code>rm -rf /</code></td><td>强制无提示删除</td></tr><tr><td><code>vim</code></td><td>编辑生产环境的配置文件不备份</td></tr><tr><td><code>kill -9 $(pgrep -f 匹配关键词)</code></td><td>杀死全部的进程</td></tr></tbody></table><p><code>杀进程之前，先ps 找到相关的进程，搞清楚，哪些是你要杀的，不然造成生产事故</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ps -ef | grep ssh</span><br><span class="line">root      4295     1  0 Oct25 ?        00:00:00 /usr/sbin/sshd -D</span><br><span class="line">root     20282  4295  0 11:34 ?        00:00:00 sshd: root@pts/0</span><br><span class="line">root     20329  4295  0 12:00 ?        00:00:00 sshd: root@pts/1</span><br><span class="line">root     20360  4295  0 12:01 ?        00:00:00 sshd: root@pts/2</span><br><span class="line">root     20380  4295  0 12:02 ?        00:00:00 sshd: root@pts/3</span><br><span class="line">root     20407  4295  0 12:12 ?        00:00:00 sshd: root@pts/4</span><br><span class="line">root     20474  4295  0 12:22 ?        00:00:00 sshd: root@pts/6</span><br><span class="line">root     20502 20476  0 12:30 pts/6    00:00:00 grep --color=auto ssh</span><br><span class="line">[root@aliyun ~]# kill -9 $(pgrep -f ssh)</span><br></pre></td></tr></table></figure><h3 id="常用的程序管理工具"><a href="#常用的程序管理工具" class="headerlink" title="常用的程序管理工具"></a>常用的程序管理工具</h3><h4 id="wget下载安装包"><a href="#wget下载安装包" class="headerlink" title="wget下载安装包"></a>wget下载安装包</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz</span><br></pre></td></tr></table></figure><h4 id="yum包管理"><a href="#yum包管理" class="headerlink" title="yum包管理"></a>yum包管理</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum search xxx</span><br><span class="line">yum install -y xxx</span><br><span class="line">yum remove xxx</span><br></pre></td></tr></table></figure><h4 id="rpm包管理"><a href="#rpm包管理" class="headerlink" title="rpm包管理"></a>rpm包管理</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun conf]# rpm -qa | grep http　　#查看</span><br><span class="line">httpd-2.4.6-90.el7.centos.x86_64</span><br><span class="line">httpd-tools-2.4.6-90.el7.centos.x86_64</span><br><span class="line">[root@aliyun conf]# rpm -e httpd-tools-2.4.6-90.el7.centos.x86_64　　#卸载失败，有依赖文件</span><br><span class="line">error: Failed dependencies:</span><br><span class="line">        httpd-tools = 2.4.6-90.el7.centos is needed by (installed) httpd-2.4.6-90.el7.centos.x86_64</span><br><span class="line">[root@aliyun conf]# rpm -e  --nodeps     httpd-tools-2.4.6-90.el7.centos.x86_64　　#强制跳过依赖检查</span><br></pre></td></tr></table></figure><h4 id="zip压缩解压"><a href="#zip压缩解压" class="headerlink" title="zip压缩解压"></a>zip压缩解压</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zip -r xxx.zip ./*  　　　　#在文件夹里面压缩文件 　　　　</span><br><span class="line">zip -r test.zip test/* 　　#在文件夹外卖压缩文件夹里面的文件unzip test.zip</span><br></pre></td></tr></table></figure><h4 id="tar压缩解压"><a href="#tar压缩解压" class="headerlink" title="tar压缩解压"></a>tar压缩解压</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -xzvf hadoop-2.6.0-cdh5.16.2.tar.gz　　　　#解压缩.ge的tar包</span><br><span class="line">tar -czvf hadoop-2.6.0-cdh5.16.2.tar.gz  hadoop-2.6.0-cdh5.16.2/*　　#压缩.ge的tar包</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> DataWarehouse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>熟悉Linux权限相关命令</title>
      <link href="/2018/09/20/linux/3/"/>
      <url>/2018/09/20/linux/3/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 用户用户组</li><li>整理 <code>sudo</code>命令</li><li>整理 用户无法登录 <code>passwd</code>文件</li><li>权限 <code>rwx------ chmod chown</code> 案例</li><li>其他命令 <code>- su find du</code>等</li></ol><h3 id="用户和用户组"><a href="#用户和用户组" class="headerlink" title="用户和用户组"></a>用户和用户组</h3><p>针对用户的相关文件在：<code>/usr/sbin/user*</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ll /usr/sbin/user*</span><br><span class="line">-rwxr-x---. 1 root root 118192 Nov  6  2016 /usr/sbin/useradd</span><br><span class="line">-rwxr-x---. 1 root root  80360 Nov  6  2016 /usr/sbin/userdel</span><br><span class="line">-rwxr-x---. 1 root root 113840 Nov  6  2016 /usr/sbin/usermod</span><br><span class="line">-rwsr-xr-x  1 root root  11296 Apr 13  2017 /usr/sbin/usernetctl</span><br></pre></td></tr></table></figure><p>针对用户组的相关文件在：<code>/usr/sbin/group*</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ll /usr/sbin/group*</span><br><span class="line">-rwxr-x---. 1 root root 65480 Nov  6  2016 /usr/sbin/groupadd</span><br><span class="line">-rwxr-x---. 1 root root 57016 Nov  6  2016 /usr/sbin/groupdel</span><br><span class="line">-rwxr-x---. 1 root root 57064 Nov  6  2016 /usr/sbin/groupmems</span><br><span class="line">-rwxr-x---. 1 root root 76424 Nov  6  2016 /usr/sbin/groupmod</span><br></pre></td></tr></table></figure><p>可以打印出<code>PATH</code>路径，就会发现<code>/user/sbin</code>已经被添加在了<code>PATH</code>环境中了，可以从主机的任意位置使用这些命令</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# echo $PATH</span><br><span class="line">/opt/module/jdk1.8.0_144/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin</span><br></pre></td></tr></table></figure><p>需求：</p><p>​        1. 添加<code>hadoop</code>用户</p><p>​        2. 删除<code>hadoop</code>用户</p><p>​        3. 重新创建<code>hadoop</code>用户，模拟用户丢失样式，并修正样式</p><p>​        4. 创建<code>bigdata</code>用户组，并把<code>hadoop</code>用户添加进这个用户组</p><p>​        5. 修改<code>bigdata</code>为<code>hadoop</code>的主组</p><ol><li><p>添加<code>hadoop</code>用户</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# useradd hadoop</span><br><span class="line">[root@aliyun ~]# id hadoop</span><br><span class="line">uid=1000(hadoop) gid=1000(hadoop) groups=1000(hadoop)</span><br></pre></td></tr></table></figure></li><li><p>删除<code>hadoop</code>用户</p><p>使用命令帮助查看 <code>userdel</code> 命令</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# userdel --help</span><br><span class="line">Usage: userdel [options] LOGIN</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  -f, --force                   force some actions that would fail otherwise</span><br><span class="line">                                e.g. removal of user still logged in</span><br><span class="line">                                or files, even if not owned by the user</span><br><span class="line">  -h, --help                    display this help message and exit</span><br><span class="line">  -r, --remove                  remove home directory and mail spool</span><br><span class="line">  -R, --root CHROOT_DIR         directory to chroot into</span><br><span class="line">  -Z, --selinux-user            remove any SELinux user mapping for the user</span><br></pre></td></tr></table></figure><p>会发现 <code>-r</code> 选项是删除家目录</p><p>在这里我们选择删除用户的时候不删除家目录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# userdel hadoop</span><br><span class="line">[root@aliyun ~]# id hadoop</span><br><span class="line">id: hadoop: no such user</span><br><span class="line">[root@aliyun home]# cat /etc/passwd | grep ruoze</span><br><span class="line">[root@aliyun home]# cat /etc/group | grep ruoze</span><br></pre></td></tr></table></figure><p>因为<code>hadoop</code>该组只有<code>hadoop</code>用户，当这个用户删除时，组会校验就他自己，会自动删除</p></li><li><p>重新创建<code>hadoop</code>用户，模拟用户丢失样式，并修正样式</p><p>创建<code>hadoop</code>用户</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# useradd hadoop</span><br><span class="line">useradd: warning: the home directory already exists.</span><br><span class="line">Not copying any file from skel directory into it.</span><br><span class="line">Creating mailbox file: File exists</span><br><span class="line">[root@aliyun ~]# id hadoop</span><br><span class="line">uid=1000(hadoop) gid=1000(hadoop) groups=1000(hadoop)</span><br></pre></td></tr></table></figure><p>模拟用户丢失样式</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ ll -a .bash*</span><br><span class="line">-rw-r--r-- 1 hadoop hadoop  18 Dec  7  2016 .bash_logout</span><br><span class="line">-rw-r--r-- 1 hadoop hadoop 193 Dec  7  2016 .bash_profile</span><br><span class="line">-rw-r--r-- 1 hadoop hadoop 231 Dec  7  2016 .bashrc</span><br><span class="line">[hadoop@aliyun ~]$ rm -rf .bash*</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop　　　　#切换用户</span><br><span class="line">Last login: Sun Nov 17 09:29:10 CST 2019 on pts/0</span><br><span class="line">-bash-4.2$ 　　　#用户样式丢失</span><br></pre></td></tr></table></figure><p>修正样式 (这里只有root权限才可以拷贝)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ll  -a /etc/skel/</span><br><span class="line">total 20</span><br><span class="line">drwxr-xr-x.  2 root root 4096 Aug 18  2017 .</span><br><span class="line">drwxr-xr-x. 81 root root 4096 Nov 17 09:27 ..</span><br><span class="line">-rw-r--r--   1 root root   18 Dec  7  2016 .bash_logout</span><br><span class="line">-rw-r--r--   1 root root  193 Dec  7  2016 .bash_profile</span><br><span class="line">-rw-r--r--   1 root root  231 Dec  7  2016 .bashrc</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cp /etc/skel/ .bash* /home/hadoop/</span><br><span class="line">cp: omitting directory ‘/etc/skel/’</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop　　#样式回来了</span><br><span class="line">Last login: Sun Nov 17 09:33:39 CST 2019 on pts/2</span><br><span class="line">[hadoop@aliyun ~]$</span><br></pre></td></tr></table></figure><p>创建<code>bigdata</code>用户组，并把<code>hadoop</code>用户添加进这个用户组</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# groupadd bigdata</span><br><span class="line">[root@aliyun ~]# usermod -a -G bigdata hadoop</span><br><span class="line">[root@aliyun ~]# id hadoop</span><br><span class="line">uid=1000(hadoop) gid=1000(hadoop) groups=1000(hadoop),1001(bigdata)</span><br></pre></td></tr></table></figure><p><em>20200309更新</em>：<code>mysqladmin</code>的属组一里加入<code>hadoop</code>用户</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">usermod -a -G hadoop mysqladmin</span><br></pre></td></tr></table></figure><p>修改<code>bigdata</code>为<code>hadoop</code>的属组</p><p>查看命令帮助发现有一条命令是改变用户的属组的</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-g, --gid GROUP               force use GROUP as new primary group</span><br><span class="line">[root@aliyun ~]# usermod -g bigdata hadoop　　#强制改变属组</span><br><span class="line">[root@aliyun ~]# id hadoop</span><br><span class="line">uid=1000(hadoop) gid=1001(bigdata) groups=1001(bigdata)</span><br></pre></td></tr></table></figure></li></ol><h3 id="sudo命令"><a href="#sudo命令" class="headerlink" title="sudo命令"></a>sudo命令</h3><p><code>sudo</code>命令是让普通用户具备<code>root</code>用户的权限</p><p>添加普通用户具备<code>root</code>权限的文件是：<code>/etc/sudoers</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">90 ## Allow root to run any commands anywhere </span><br><span class="line">91 root    ALL=(ALL)       ALL</span><br><span class="line">92 hadoop  ALL=(root)      NOPASSWD:ALL　　#新添加的内容</span><br></pre></td></tr></table></figure><h3 id="用户无法登录-修改passwd文件"><a href="#用户无法登录-修改passwd文件" class="headerlink" title="用户无法登录 修改passwd文件"></a>用户无法登录 修改<code>passwd</code>文件</h3><p>在模拟用户无法登陆之前，先说明管理用户信息的文件是：<code>/etc/passwd</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# tail -3  /etc/passwd</span><br><span class="line">redis:x:996:994:Redis Database Server:/var/lib/redis:/sbin/nologin</span><br><span class="line">mysqladmin:x:514:101::/usr/local/mysql:/bin/bash</span><br><span class="line">hadoop:x:1000:1001::/home/hadoop:/bin/bash</span><br></pre></td></tr></table></figure><p>需要注意的是最后一个冒号后面是用户的登陆权限</p><p>需求：</p><p>　　1. 模拟用户的登录权限是<code>/bin/false</code>，修改，并登录</p><p>　　2. 模拟用户的登录权限是<code>/sbin/nologin</code>，修改，并登录</p><p><strong>1. 模拟用户的登录权限是<code>/bin/false</code>，修改，并登录</strong></p><ol><li><p>模拟用户的登录权限是<code>/bin/false</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cat /etc/passwd | grep hadoop</span><br><span class="line">hadoop:x:1000:1001::/home/hadoop:/bin/false</span><br></pre></td></tr></table></figure></li><li><p>尝试登录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop</span><br><span class="line">Last login: Sun Nov 17 09:37:25 CST 2019 on pts/2</span><br><span class="line">[root@aliyun ~]# 　　#登录失败</span><br></pre></td></tr></table></figure></li><li><p>查看用户文件权限，并修改</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cat /etc/passwd | grep 'hadoop'</span><br><span class="line">hadoop:x:1000:1001::/home/hadoop:/bin/bash</span><br></pre></td></tr></table></figure></li><li><p>再次登录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop</span><br><span class="line">Last login: Sun Nov 17 09:56:43 CST 2019 on pts/1</span><br><span class="line">[hadoop@aliyun ~]$     #登录成功</span><br></pre></td></tr></table></figure></li></ol><p><strong>2. 模拟用户的登录权限是<code>/sbin/nologin</code>，修改，并登录</strong></p><ol><li><p>模拟用户的登录权限是<code>/sbin/nologin</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cat /etc/passwd | grep hadoop</span><br><span class="line">hadoop:x:1000:1001::/home/hadoop:/sbin/nologin</span><br></pre></td></tr></table></figure></li><li><p>尝试登录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop</span><br><span class="line">Last login: Sun Nov 17 09:59:35 CST 2019 on pts/1</span><br><span class="line">This account is currently not available.</span><br><span class="line">[root@aliyun ~]# 　　#登录失败</span><br></pre></td></tr></table></figure></li><li><p>查看用户文件权限，并修改</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cat /etc/passwd | grep 'hadoop'</span><br><span class="line">hadoop:x:1000:1001::/home/hadoop:/bin/bash</span><br></pre></td></tr></table></figure></li><li><p>再次登录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop</span><br><span class="line">Last login: Sun Nov 17 09:56:43 CST 2019 on pts/1</span><br><span class="line">[hadoop@aliyun ~]$     #登录成功</span><br></pre></td></tr></table></figure></li></ol><h3 id="rwx-chmod-chown-案例"><a href="#rwx-chmod-chown-案例" class="headerlink" title="rwx------ chmod chown 案例"></a><code>rwx------</code> <code>chmod</code> <code>chown</code> 案例</h3><p>查看文件或者目录的读写执行权限</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ll test.txt</span><br><span class="line">-rw-r--r-- 1 root root 12 Nov 12 23:36 test.txt</span><br><span class="line">r: read  4</span><br><span class="line">w: write 2 </span><br><span class="line">x: 执行  1</span><br><span class="line">-: 没权限 0</span><br></pre></td></tr></table></figure><p>　<code>rw-</code>第一组 6 代表文件或文件夹的用户<code>root</code>，读写<br>　<code>r--</code> 第二组 4 代表文件或文件夹的用户组<code>root</code>，读<br>　<code>r--</code> 第三组 4 代表其他组的所属用户对这个文件或文件夹的权限: 读</p><p><code>chmod</code> 命令用来修改文件或者目录的读写执行权限，加 <code>-R</code> 表示递归修改</p><p><code>chown</code> 命令用来修改文件或者目录的属主和属组，加 <code>-R</code> 表示递归修改</p><p>需求：</p><p>​        1. 修改 <code>test.tx</code>t 文件的属组为<code>bigdata</code></p><p>​        2.  <code>test.txt</code> 文件的权限为属主读写执行，属组读执行</p><ol><li><p>修改 <code>test.txt</code> 文件的属组为<code>bigdata</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# chown -R :bigdata test.txt </span><br><span class="line">[root@aliyun ~]# ll test.txt </span><br><span class="line">-rw-r--r-- 1 root bigdata 12 Nov 12 23:36 test.txt</span><br></pre></td></tr></table></figure></li><li><p>修改 <code>test.txt</code> 文件的权限为属主可读写执行，属组可读执行,其他可读</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# chmod -R 754 test.txt </span><br><span class="line">[root@aliyun ~]# ll test.txt </span><br><span class="line">-rwxr-xr-- 1 root bigdata 12 Nov 12 23:36 test.txt</span><br></pre></td></tr></table></figure><p>注意： <code>-R</code> 参数，目前可认为只有<code>chown</code>和<code>chmod</code>命令有，其他都为 <code>-r</code></p></li></ol><h3 id="其他命令-su-find-du"><a href="#其他命令-su-find-du" class="headerlink" title="其他命令 - su find du"></a>其他命令 - <code>su</code> <code>find</code> <code>du</code></h3><p><u><code>su</code>命令用来切换用户，使用<code>su -</code> 用户名的方式，切换的时候把环境也切换了</u></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop　　#su - 用户名</span><br><span class="line">Last login: Sun Nov 17 10:16:08 CST 2019 on pts/2</span><br><span class="line">[hadoop@aliyun ~]$ pwd</span><br><span class="line">/home/hadoop</span><br><span class="line">[root@aliyun ~]# su hadoop　　#su 用户名</span><br><span class="line">[hadoop@aliyun root]$ pwd</span><br><span class="line">/root</span><br></pre></td></tr></table></figure><p>需要注意 <code>.bash_profile</code> 和 <code>.bashrc</code> 两个文件中的环境生效的区别</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">.bash_profile文件 su ruoze不会执行，su - ruoze 都执行</span><br><span class="line">.bashrc文件       su ruoze执行   ，su - ruoze 都执行</span><br></pre></td></tr></table></figure><p><u><code>find</code>命令用来查找文件，在不确定文件名的情况下使用模糊匹配</u></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun root]$ find /home -name '*hadoop*'</span><br><span class="line">/home/hadoop</span><br></pre></td></tr></table></figure><p><u><code>du</code>命令用来查看文件或者目录大小</u></p><p>虽然 ls -l 也可以查看文件或者目录的大小，但是 <code>ls -l</code> 显示的目录大小并不准确</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ll -h  size.log/</span><br><span class="line">total 12K</span><br><span class="line">-rw-r--r-- 1 root root 286K Nov 17 10:24 lastlog</span><br></pre></td></tr></table></figure><p>再使用<code>du -sh</code> 查看一次</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# du -sh size.log/</span><br><span class="line">12K    size.log/</span><br></pre></td></tr></table></figure><p>最后进入<code>size.log</code>文件夹查看文件的大小</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun log]# du -sh lastlog</span><br><span class="line">12K    lastlog</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> DataWarehouse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux对环境变量的理解以及alias、rm、hostory的使用</title>
      <link href="/2018/09/19/linux/2/"/>
      <url>/2018/09/19/linux/2/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 全局环境变量 个人环境变量 <code>which</code>的理解</li><li>整理 别名</li><li>整理 删除</li><li>整理 <code>history</code></li></ol><h3 id="全局环境变量"><a href="#全局环境变量" class="headerlink" title="全局环境变量"></a>全局环境变量</h3><p>全局环境变量的配置文件是：<code>/etc/profile</code></p><p>全局环境变量中一般配置的是共用的程序环境 比如<code>java</code></p><p>下面以<code>java</code>为例子配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# vim /etc/profile</span><br></pre></td></tr></table></figure><p>java的安装路径在 <code>/usr/java</code>下，所以文件中如下配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_144</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br></pre></td></tr></table></figure><p>需要注意的是 <code>$PATH</code>接在<code>$JAVA_HOME</code>的后面，即把<code>$JAVA_HOME</code>放在<code>$PATH</code>的最前面</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/opt/module/jdk1.8.0_144/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/module/jdk1.8.0_144/bin:/root/bin</span><br></pre></td></tr></table></figure><h3 id="个人环境变量"><a href="#个人环境变量" class="headerlink" title="个人环境变量"></a>个人环境变量</h3><p>个人环境变量配置在 <code>~/.bashrc</code> 文件中，这里需要注意的是如果配置在 <code>~/.bash_profile</code>文件中，使用<code>ssh</code>远程连接的时候不会加载 <code>~/.bash_profile</code>，造成一些无法排查的<code>bug</code></p><p>个人环境配置一些独自使用的程序变量，如果配置在用户的个人环境中，其他用户无法访问，比如在<code>hadoop</code>用户下配置 <code>hadoop</code>的环境变量，只有<code>hadoop</code>一个用户能使用</p><h3 id="which的理解"><a href="#which的理解" class="headerlink" title="which的理解"></a>which的理解</h3><p>安装完程序或者配置完变量后，最好的习惯是使用<code>which</code>看一下，检查一下环境是否配置正确，否则可能遇到自以为正确的<code>bug</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# which java</span><br><span class="line">/opt/module/jdk1.8.0_144/bin/java</span><br></pre></td></tr></table></figure><h3 id="别名"><a href="#别名" class="headerlink" title="别名"></a>别名</h3><p>别名的使用可以简写冗长且难以记忆或者难以书写的命令</p><p>格式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alias [-p] [name[=value] ... ]</span><br></pre></td></tr></table></figure><p>例子：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# alias a='ll -a'</span><br><span class="line">[root@aliyun ~]# a</span><br><span class="line">total 80</span><br><span class="line">dr-xr-x---.  6 root root 4096 Nov 15 19:46 .</span><br><span class="line">dr-xr-xr-x. 19 root root 4096 Nov  3 17:29 ..</span><br><span class="line">-rw-------   1 root root 6453 Nov 14 11:16 .bash_history</span><br><span class="line">-rw-r--r--.  1 root root   18 Dec 29  2013 .bash_logout</span><br><span class="line">-rw-r--r--.  1 root root  176 Dec 29  2013 .bash_profile</span><br><span class="line">-rw-r--r--.  1 root root  176 Dec 29  2013 .bashrc</span><br><span class="line">drwx------   3 root root 4096 Aug 18  2017 .cache</span><br></pre></td></tr></table></figure><p>查询主机中已经存在的别名：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# alias</span><br><span class="line">alias cp='cp -i'</span><br><span class="line">alias egrep='egrep --color=auto'</span><br><span class="line">alias fgrep='fgrep --color=auto'</span><br><span class="line">alias grep='grep --color=auto'</span><br><span class="line">alias l.='ls -d .* --color=auto'</span><br><span class="line">alias ll='ls -l --color=auto'</span><br><span class="line">alias ls='ls --color=auto'</span><br><span class="line">alias mv='mv -i'</span><br><span class="line">alias rm='rm -i'</span><br><span class="line">alias which='alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde'</span><br></pre></td></tr></table></figure><p>永久配置别名：</p><p>配置在环境变量中 <code>/etc/profile</code> ，<code>~/.bashrc</code>，<code>~/.bash_profile</code>中，即永久配置别名</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> User specific aliases and <span class="built_in">functions</span></span></span><br><span class="line"></span><br><span class="line">alias rm='rm -i'</span><br><span class="line">alias cp='cp -i'</span><br><span class="line">alias mv='mv -i'</span><br></pre></td></tr></table></figure><h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><table><thead><tr><th>命令</th><th>作用</th></tr></thead><tbody><tr><td><code>mkdir</code></td><td>删除一个空文件</td></tr><tr><td><code>rm -f</code></td><td>直接删除文件</td></tr><tr><td><code>rm -rf</code></td><td>直接删除文件夹</td></tr><tr><td><code>rm -rf</code></td><td>是一个高危的命令</td></tr></tbody></table><p>场景：</p><p><code>shell</code>脚本中，定义变量<code>k = &quot;&quot;</code> 然后<code>rm -rf $k</code> 会默认指定根目录下</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">K=&quot;&quot;rm -rf $K 实际上是rm -rf /</span><br></pre></td></tr></table></figure><p>解决办法是先判断k是否为空</p><h3 id="history"><a href="#history" class="headerlink" title="history"></a>history</h3><p><code>history</code>命令用来查询历史记录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# history | head -10</span><br><span class="line">    1  passwd </span><br><span class="line">    2  ls</span><br><span class="line">    3  cat /etc/hosts</span><br><span class="line">    4  yum install redis</span><br><span class="line">    5  yum install epel-release</span><br><span class="line">    6  yum install redis</span><br><span class="line">    7  df -hT</span><br><span class="line">    8  service redis start</span><br><span class="line">    9  service redis stop</span><br><span class="line">   10  service redis status</span><br></pre></td></tr></table></figure><p>场景：</p><p>莫名其妙的发现主机中的数据没了，可以查看一下历史记录用了哪些命令</p><p>使用 <code>!n</code> 来快速使用一条历史命令</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# !7</span><br><span class="line">df -hT</span><br><span class="line">Filesystem     Type      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/vda1      ext4       40G   11G   27G  29% /</span><br><span class="line">devtmpfs       devtmpfs  911M     0  911M   0% /dev</span><br><span class="line">tmpfs          tmpfs     920M     0  920M   0% /dev/shm</span><br><span class="line">tmpfs          tmpfs     920M  332K  920M   1% /run</span><br><span class="line">tmpfs          tmpfs     920M     0  920M   0% /sys/fs/cgroup</span><br><span class="line">tmpfs          tmpfs     184M     0  184M   0% /run/user/0</span><br></pre></td></tr></table></figure><p><code>history -c</code> 命令可以清空当前窗口的历史输出命令。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# history -c</span><br><span class="line">[root@aliyun ~]# history</span><br><span class="line">1  history</span><br></pre></td></tr></table></figure><p>但是历史记录实际上保存在 <code>~/.bash_history</code>中的</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cat .bash_history | head -10</span><br><span class="line">passwd </span><br><span class="line">ls</span><br><span class="line">cat /etc/hosts</span><br><span class="line">yum install redis</span><br><span class="line">yum install epel-release</span><br><span class="line">yum install redis</span><br><span class="line">df -hT</span><br><span class="line">service redis start</span><br><span class="line">service redis stop</span><br><span class="line">service redis status</span><br></pre></td></tr></table></figure><p>彻底清空该文件夹的方式为：<code>cat /dell/null &gt; .bash_history</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cat /dev/null &gt; .bash_history </span><br><span class="line">[root@aliyun ~]# cat .bash_history</span><br><span class="line">[root@aliyun ~]#</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> DataWarehouse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux操作文件和定位错误</title>
      <link href="/2018/09/18/linux/1/"/>
      <url>/2018/09/18/linux/1/</url>
      
        <content type="html"><![CDATA[<h3 id="置空文件的一些坑"><a href="#置空文件的一些坑" class="headerlink" title="置空文件的一些坑"></a>置空文件的一些坑</h3><ol><li><p>最简单的是直接创建一个空文件:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# touch test.txt | ll test.txt</span><br><span class="line">-rw-r--r-- 1 root root 0 Nov 11 22:29 test.txt</span><br><span class="line">[root@aliyun var]#</span><br></pre></td></tr></table></figure></li><li><p>慎用 <code>echo &quot;&quot; &gt; test.txt</code> 这种方式置空文件</p><p>如果我们使用这种方法置空文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# echo "" &gt; test.txt</span><br><span class="line">[root@aliyun var]#  ll -h test.txt</span><br><span class="line">-rw-r--r-- 1 root root 1 Nov 11 22:31 test.txt</span><br></pre></td></tr></table></figure><p>它不是绝对意义上的为空，文件占有一个字节的大小</p></li><li><p>可以更换为<code>cat /dev/null &gt; test.txt</code> 这种方式置空文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# cat /dev/null &gt; test.txt | ll -h test.txt</span><br><span class="line">-rw-r--r-- 1 root root 0 Nov 11 22:34 test.txt</span><br></pre></td></tr></table></figure><p>真正意义上把文件置空为0个字节</p></li></ol><h3 id="如何定位ERROR"><a href="#如何定位ERROR" class="headerlink" title="如何定位ERROR"></a>如何定位ERROR</h3><ul><li><p>文件内容很小 几十兆<br>上传给<code>windows</code>，用<code>editplus</code>工具打开，在<code>editplus</code>中搜索，定位</p><p>上传下载  <code>yum install -y lrzsz</code></p></li><li><p>文件内容很大 至少几百兆</p><p> 直接定位到<code>ERROR</code>行</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# cat test.log | grep ERROR</span><br><span class="line">10 ERROR : 模拟错误</span><br></pre></td></tr></table></figure></li></ul><h3 id="使用grep定位REEOR上下文"><a href="#使用grep定位REEOR上下文" class="headerlink" title="使用grep定位REEOR上下文"></a>使用<code>grep</code>定位<code>REEOR</code>上下文</h3><ol><li><p>查看<code>ERROR</code>行的前十行（<code>before</code>）</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# cat test.log | grep -B 10 ERROR</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">9</span><br><span class="line">10 ERROR : 模拟错误</span><br></pre></td></tr></table></figure></li><li><p>查看<code>ERROR</code>行的后十行（<code>after</code>）</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# cat test.log | grep -A 10 ERROR</span><br><span class="line">10 ERROR : 模拟错误</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td></tr></table></figure></li><li><p>查看<code>ERROR</code>行的前后二十行</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# cat test.log | grep -C 10 ERROR</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">9</span><br><span class="line">10 ERROR : 模拟错误</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> DataWarehouse </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
