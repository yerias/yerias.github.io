<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Flink State管理&amp;Fault Tolerance</title>
      <link href="/2020/06/08/flink/4/"/>
      <url>/2020/06/08/flink/4/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>State的工作方式</li><li>重启策略</li><li>Checkpoint</li><li>Savepoint</li><li>State Backends</li></ol><h2 id="State的工作方式"><a href="#State的工作方式" class="headerlink" title="State的工作方式"></a>State的工作方式</h2><p>Flink 中有两种基本的状态：<code>Keyed State</code> 和 <code>Operator State</code>。</p><h3 id="Keyed-State"><a href="#Keyed-State" class="headerlink" title="Keyed State"></a>Keyed State</h3><p><em>Keyed State</em> 通常和 key 相关，仅可使用在 <code>KeyedStream</code> 的方法和算子中。</p><p>你可以把 Keyed State 看作分区或者共享的 Operator State, 而且每个 key 仅出现在一个分区内。 逻辑上每个 keyed-state 和唯一元组 &lt;parallel-operator-instance, key&gt; 绑定，由于每个 key 仅”属于” 算子的一个并发，因此简化为 &lt;operator, key&gt;。</p><p>Keyed State 会按照 <em>Key Group</em> 进行管理。Key Group 是 Flink 分发 Keyed State 的最小单元； Key Group 的数目等于作业的最大并发数。在执行过程中，每个 keyed operator 会对应到一个或多个 Key Group</p><h3 id="Operator-State"><a href="#Operator-State" class="headerlink" title="Operator State"></a>Operator State</h3><p>对于 <em>Operator State</em> (或者 <em>non-keyed state</em>) 来说，每个 operator state 和一个并发实例进行绑定。 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/connectors/kafka.html" target="_blank" rel="noopener">Kafka Connector</a> 是 Flink 中使用 operator state 的一个很好的示例。 每个 Kafka 消费者的并发在 Operator State 中维护一个 topic partition 到 offset 的映射关系。</p><p>Operator State 在 Flink 作业的并发改变后，会重新分发状态，分发的策略和 Keyed State 不一样。</p><h3 id="Raw-State-与-Managed-State"><a href="#Raw-State-与-Managed-State" class="headerlink" title="Raw State 与 Managed State"></a>Raw State 与 Managed State</h3><p><em>Keyed State</em> 和 <em>Operator State</em> 分别有两种存在形式：<em>managed</em> and <em>raw</em>.</p><p><em>Managed State</em> 由 Flink 运行时控制的数据结构表示，比如内部的 hash table 或者 RocksDB。 比如 “ValueState”, “ListState” 等。Flink runtime 会对这些状态进行编码并写入 checkpoint。</p><p><em>Raw State</em> 则保存在算子自己的数据结构中。checkpoint 的时候，Flink 并不知晓具体的内容，仅仅写入一串字节序列到 checkpoint。</p><p><code>注意:</code> 所有 datastream 的 function 都可以使用 managed state, 但是 raw state 则只能在实现算子的时候使用。 由于 Flink 可以在修改并发时更好的分发状态数据，并且能够更好的管理内存，因此<strong>建议使用 managed state</strong>（而不是 raw state）。</p><h3 id="使用-Managed-Keyed-State"><a href="#使用-Managed-Keyed-State" class="headerlink" title="使用 Managed Keyed State"></a>使用 Managed Keyed State</h3><p><em>managed keyed state</em> 接口提供不同类型状态的访问接口，这些状态都作用于当前输入数据的 key 下。换句话说，这些状态仅可在 <code>KeyedStream</code> 上使用，可以通过 <code>stream.keyBy(...)</code> 得到 <code>KeyedStream</code>.</p><ul><li><p><code>ValueState</code>: 保存一个可以更新和检索的值（如上所述，每个值都对应到当前的输入数据的 key，因此算子接收到的每个 key 都可能对应一个值）。 这个值可以通过 <code>update(T)</code> 进行更新，通过 <code>T value()</code> 进行检索。</p></li><li><p><code>MapState</code>: 维护了一个映射列表。 你可以添加键值对到状态中，也可以获得反映当前所有映射的迭代器。使用 <code>put(UK，UV)</code> 或者 <code>putAll(Map)</code> 添加映射。 使用 <code>get(UK)</code> 检索特定 key。 使用 <code>entries()</code>，<code>keys()</code> 和 <code>values()</code> 分别检索映射、键和值的可迭代视图。你还可以通过 <code>isEmpty()</code> 来判断是否包含任何键值对。</p></li><li><p><code>ListState</code>: 保存一个元素的列表。可以往这个列表中追加数据，并在当前的列表上进行检索。可以通过 <code>add(T)</code> 或者 <code>addAll(List)</code> 进行添加元素，通过 <code>Iterable get()</code> 获得整个列表。还可以通过 <code>update(List)</code> 覆盖当前的列表。</p></li><li><p><code>ReducingState</code>: 保存一个单值，表示添加到状态的所有值的聚合。接口与 <code>ListState</code> 类似，但使用 <code>add(T)</code> 增加元素，会使用提供的 <code>ReduceFunction</code> 进行聚合。</p></li><li><p><code>AggregatingState</code>: 保留一个单值，表示添加到状态的所有值的聚合。和 <code>ReducingState</code> 相反的是, 聚合类型可能与 添加到状态的元素的类型不同。 接口与 <code>ListState</code> 类似，但使用 <code>add(IN)</code> 添加的元素会用指定的 <code>AggregateFunction</code> 进行聚合。</p></li></ul><p>所有类型的状态还有一个<code>clear()</code> 方法，清除当前 key 下的状态数据，也就是当前输入元素的 key。请牢记，这些状态对象仅用于与状态交互。状态本身不一定存储在内存中，还可能在磁盘或其他位置。 另外需要牢记的是从状态中获取的值取决于输入元素所代表的 key。 因此，在不同 key 上调用同一个接口，可能得到不同的值。</p><p>你必须创建一个 <code>StateDescriptor</code>，才能得到对应的状态句柄。 这保存了状态名称（正如我们稍后将看到的，你可以创建多个状态，并且它们必须具有唯一的名称以便可以引用它们）， 状态所持有值的类型，并且可能包含用户指定的函数，例如<code>ReduceFunction</code>。 根据不同的状态类型，可以创建<code>ValueStateDescriptor</code>，<code>ListStateDescriptor</code>， <code>ReducingStateDescriptor</code>，<code>FoldingStateDescriptor</code> 或 <code>MapStateDescriptor</code>。</p><p>状态通过 <code>RuntimeContext</code> 进行访问，因此只能在 <em>rich functions</em> 中使用。请参阅<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/api_concepts.html#rich-functions" target="_blank" rel="noopener">这里</a>获取相关信息， 但是我们很快也会看到一个例子。<code>RichFunction</code> 中 <code>RuntimeContext</code> 提供如下方法：</p><ul><li><code>ValueState getState(ValueStateDescriptor)</code></li><li><code>ReducingState getReducingState(ReducingStateDescriptor)</code></li><li><code>ListState getListState(ListStateDescriptor)</code></li><li><code>AggregatingState getAggregatingState(AggregatingStateDescriptor)</code></li><li><code>FoldingState getFoldingState(FoldingStateDescriptor)</code></li><li><code>MapState getMapState(MapStateDescriptor)</code></li></ul><p>下面是一个 <code>ValueStat</code> 的例子，展示了如何将这些部分组合起来：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StateCustomApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据输入元素求平均数</span></span><br><span class="line"><span class="comment"> * 只要到达2个元素我们就开始算</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">averageKeyedValueState</span></span>(env: <span class="type">StreamExecutionEnvironment</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">env.fromCollection(<span class="type">List</span>(</span><br><span class="line">(<span class="number">1</span>, <span class="number">3</span>),</span><br><span class="line">(<span class="number">1</span>, <span class="number">5</span>),</span><br><span class="line">(<span class="number">1</span>, <span class="number">7</span>),</span><br><span class="line">(<span class="number">1</span>, <span class="number">4</span>),</span><br><span class="line">(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">)).keyBy(_._1)</span><br><span class="line">  .flatMap(<span class="keyword">new</span> <span class="type">CountWindowAverage</span>)</span><br><span class="line">  .print()</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">averageKeyedValueState(env)</span><br><span class="line">env.execute(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输入一个 kv  输出一个kv</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CountWindowAverage</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>[(<span class="type">Int</span>, <span class="type">Int</span>), (<span class="type">Int</span>, <span class="type">Int</span>)] </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义ValueState</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> sum: <span class="type">ValueState</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = _</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="comment">// 初始化state</span></span><br><span class="line">sum = getRuntimeContext.getState(<span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[(<span class="type">Int</span>, <span class="type">Int</span>)](<span class="string">"average"</span>, createTypeInformation[(<span class="type">Int</span>, <span class="type">Int</span>)]))</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>(value: (<span class="type">Int</span>, <span class="type">Int</span>), out: <span class="type">Collector</span>[(<span class="type">Int</span>, <span class="type">Int</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="comment">// 拿到state的值</span></span><br><span class="line"><span class="keyword">val</span> tmpState = sum.value()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 拿到当前state,看看它是否为null</span></span><br><span class="line"><span class="keyword">val</span> currentState = <span class="keyword">if</span> (<span class="literal">null</span> != tmpState) &#123;</span><br><span class="line">tmpState</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 拿到最新的state</span></span><br><span class="line"><span class="keyword">val</span> newState = (currentState._1 + <span class="number">1</span>, currentState._2 + value._2)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 更新state</span></span><br><span class="line">sum.update(newState)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (newState._1 &gt;= <span class="number">2</span>) &#123;</span><br><span class="line"><span class="comment">// 取平均值</span></span><br><span class="line">out.collect((value._1, newState._2 / newState._1))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 清空state</span></span><br><span class="line">sum.clear()</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面是一个 <code>MapState</code> 的例子，展示了如何将这些部分组合起来：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StateCustomApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">userBehaviorKeyedMapState</span></span>(env: <span class="type">StreamExecutionEnvironment</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">env.fromCollection(<span class="type">List</span>(</span><br><span class="line">(<span class="number">1</span>, <span class="string">"buy"</span>),</span><br><span class="line">(<span class="number">1</span>, <span class="string">"cart"</span>),</span><br><span class="line">(<span class="number">1</span>, <span class="string">"buy"</span>),</span><br><span class="line">(<span class="number">1</span>, <span class="string">"fav"</span>),</span><br><span class="line">(<span class="number">2</span>, <span class="string">"buy"</span>),</span><br><span class="line">(<span class="number">2</span>, <span class="string">"buy"</span>),</span><br><span class="line">(<span class="number">2</span>, <span class="string">"fav"</span>)</span><br><span class="line">)).keyBy(<span class="number">0</span>)</span><br><span class="line">  .flatMap(<span class="keyword">new</span> <span class="type">UserBehaviorCnt</span>)</span><br><span class="line">  .print()</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">userBehaviorKeyedMapState(env)</span><br><span class="line">env.execute(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 统计每个用户某种行为的操作次数</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserBehaviorCnt</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>[(<span class="type">Int</span>, <span class="type">String</span>), (<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>)] </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义MapState 维护一个kv类型的state</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> behaviorCntState: <span class="type">MapState</span>[<span class="type">String</span>, <span class="type">Int</span>] = _</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="comment">// 初始化MapState</span></span><br><span class="line">behaviorCntState = getRuntimeContext.getMapState(<span class="keyword">new</span> <span class="type">MapStateDescriptor</span>[<span class="type">String</span>, <span class="type">Int</span>](<span class="string">"userBehavior"</span>, classOf[<span class="type">String</span>], classOf[<span class="type">Int</span>]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>(value: (<span class="type">Int</span>, <span class="type">String</span>), out: <span class="type">Collector</span>[(<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="comment">// 定义一个behaviorCnt用来统计行为的次数，默认值为1</span></span><br><span class="line"><span class="keyword">var</span> behaviorCnt = <span class="number">1</span></span><br><span class="line"><span class="comment">// 第一次进来肯定不存在，则自动赋值为1，第二次进来存在则每次+1</span></span><br><span class="line"><span class="keyword">if</span> (behaviorCntState.contains(value._2)) &#123;</span><br><span class="line">behaviorCnt = behaviorCntState.get(value._2) + <span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 维护一个MapState来统计次数</span></span><br><span class="line">behaviorCntState.put(value._2, behaviorCnt)</span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">out.collect((value._1, value._2, behaviorCnt))</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="使用-Managed-Operator-State"><a href="#使用-Managed-Operator-State" class="headerlink" title="使用 Managed Operator State"></a>使用 Managed Operator State</h3><p>用户可以通过实现 <code>CheckpointedFunction</code> 或 <code>ListCheckpointed</code> 接口来使用 managed operator state。</p><p><code>ListCheckpointed</code> 接口是 <code>CheckpointedFunction</code> 的精简版，仅支持 even-split redistributuion 的 list state。同样需要实现两个方法：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">List&lt;T&gt; <span class="title">snapshotState</span><span class="params">(<span class="keyword">long</span> checkpointId, <span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">restoreState</span><span class="params">(List&lt;T&gt; state)</span> <span class="keyword">throws</span> Exception</span>;</span><br></pre></td></tr></table></figure><p><code>snapshotState()</code> 需要返回一个将写入到 checkpoint 的对象列表，<code>restoreState</code> 则需要处理恢复回来的对象列表。如果状态不可切分， 则可以在 <code>snapshotState()</code> 中返回 <code>Collections.singletonList(MY_STATE)</code>。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StateCustomApp</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用ListCheckpointed来实现buy的次数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">userBehaviorListCheckpointState</span></span>(env: <span class="type">StreamExecutionEnvironment</span>):<span class="type">Unit</span> = &#123;</span><br><span class="line">env.fromCollection(<span class="type">List</span>(</span><br><span class="line">(<span class="number">1</span>, <span class="string">"buy"</span>),</span><br><span class="line">(<span class="number">1</span>, <span class="string">"cart"</span>),</span><br><span class="line">(<span class="number">1</span>, <span class="string">"buy"</span>),</span><br><span class="line">(<span class="number">1</span>, <span class="string">"fav"</span>),</span><br><span class="line">(<span class="number">2</span>, <span class="string">"buy"</span>),</span><br><span class="line">(<span class="number">2</span>, <span class="string">"fav"</span>),</span><br><span class="line">(<span class="number">2</span>, <span class="string">"fav"</span>)</span><br><span class="line">)).keyBy(x =&gt; x._1)</span><br><span class="line">  .flatMap(<span class="keyword">new</span> <span class="type">RichFlatMapFunction</span>[(<span class="type">Int</span>,<span class="type">String</span>),(<span class="type">String</span>,<span class="type">Int</span>)] <span class="keyword">with</span> <span class="type">ListCheckpointed</span>[<span class="type">Integer</span>]&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义userBuyBehaviorCnt = 0</span></span><br><span class="line"><span class="keyword">var</span> userBuyBehaviorCnt = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 对每一行袁元素做操作</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>(value: (<span class="type">Int</span>, <span class="type">String</span>), out: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">Int</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="comment">// 判断用户行为 并做+1操作输出</span></span><br><span class="line"><span class="keyword">if</span> (value._2 == <span class="string">"buy"</span>)&#123;</span><br><span class="line">userBuyBehaviorCnt +=<span class="number">1</span></span><br><span class="line">out.collect(value._2,userBuyBehaviorCnt)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回一个包含userBuyBehaviorCnt的对象列表</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">snapshotState</span></span>(checkpointId: <span class="type">Long</span>, timestamp: <span class="type">Long</span>): util.<span class="type">List</span>[<span class="type">Integer</span>] = &#123;</span><br><span class="line"><span class="type">Collections</span>.singletonList(userBuyBehaviorCnt)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 处理恢复回来的对象列表</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConverters</span>._</span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">restoreState</span></span>(state: util.<span class="type">List</span>[<span class="type">Integer</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="comment">// 统计list中的个数而不是值</span></span><br><span class="line"><span class="keyword">for</span>(ele &lt;- state.asScala)&#123;</span><br><span class="line">userBuyBehaviorCnt+=<span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;).print()</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="重启策略"><a href="#重启策略" class="headerlink" title="重启策略"></a>重启策略</h2><p>当 Task 发生故障时，Flink 需要重启出错的 Task 以及其他受到影响的 Task ，以使得作业恢复到正常执行状态。</p><p>Flink 作业如果没有定义重启策略，则会遵循集群启动时加载的默认重启策略。 如果提交作业时设置了重启策略，该策略将覆盖掉集群的默认策略。</p><h3 id="Fixed-Delay-Restart-Strategy"><a href="#Fixed-Delay-Restart-Strategy" class="headerlink" title="Fixed Delay Restart Strategy"></a>Fixed Delay Restart Strategy</h3><p>固定延时重启策略按照给定的次数尝试重启作业。 如果尝试超过了给定的最大次数，作业将最终失败。 在连续的两次重启尝试之间，重启策略等待一段固定长度的时间。</p><p>通过在 <code>flink-conf.yaml</code> 中设置如下配置参数，默认启用此策略。</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">restart-strategy:</span> <span class="string">fixed-delay</span></span><br><span class="line"><span class="attr">restart-strategy.fixed-delay.attempts:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">restart-strategy.fixed-delay.delay:</span> <span class="number">10</span> <span class="string">s</span></span><br></pre></td></tr></table></figure><p>固定延迟重启策略也可以在程序中设置：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setRestartStrategy(RestartStrategies.fixedDelayRestart(</span><br><span class="line">  <span class="number">3</span>, <span class="comment">// 尝试重启的次数</span></span><br><span class="line">  Time.of(<span class="number">10</span>, TimeUnit.SECONDS) <span class="comment">// 延时</span></span><br><span class="line">));</span><br></pre></td></tr></table></figure><h2 id="Checkpoint"><a href="#Checkpoint" class="headerlink" title="Checkpoint"></a>Checkpoint</h2><p>Flink 中的每个方法或算子都能够是<strong>有状态的</strong>。 状态化的方法在处理单个 elements/events 的时候存储数据，让状态成为使各个类型的算子更加精细的重要部分。 为了让状态容错，Flink 需要为状态添加 <strong>checkpoint（检查点）</strong>。Checkpoint 使得 Flink 能够恢复状态和在流中的位置，从而向应用提供和无故障执行时一样的语义。</p><h3 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h3><ul><li>一个能够回放一段时间内数据的持久化数据源(Kafka/HDFS)</li><li>存放状态的持久化存储(HDFS)</li></ul><h3 id="开启与配置-Checkpoint"><a href="#开启与配置-Checkpoint" class="headerlink" title="开启与配置 Checkpoint"></a>开启与配置 Checkpoint</h3><p>默认情况下 checkpoint 是禁用的。通过调用 <code>StreamExecutionEnvironment</code> 的 <code>enableCheckpointing(n)</code> 来启用 checkpoint，里面的 <em>n</em> 是进行 checkpoint 的间隔，单位毫秒。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val env = StreamExecutionEnvironment.getExecutionEnvironment()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 每 1000ms 开始一次 checkpoint</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>)</span><br></pre></td></tr></table></figure><p>Checkpoint 其他的属性包括：</p><ul><li><em>精确一次（exactly-once）对比至少一次（at-least-once）</em>：你可以选择向 <code>enableCheckpointing(long interval, CheckpointingMode mode)</code> 方法中传入一个模式来选择使用两种保证等级中的哪一种。 对于大多数应用来说，精确一次是较好的选择。至少一次可能与某些延迟超低（始终只有几毫秒）的应用的关联较大。</li></ul><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/stream/state/checkpointing.html" target="_blank" rel="noopener">其他配置参数查看官网</a></p><h3 id="保存-Checkpoint"><a href="#保存-Checkpoint" class="headerlink" title="保存 Checkpoint"></a>保存 Checkpoint</h3><p>Checkpoint 在默认的情况下仅用于恢复失败的作业，并不保留，当程序取消时 checkpoint 就会被删除。当然，你可以通过配置来保留 checkpoint，这些被保留的 checkpoint 在作业失败或取消时不会被清除。这样，你就可以使用该 checkpoint 来恢复失败的作业。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">CheckpointConfig config = env.getCheckpointConfig();</span><br><span class="line">config.enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br></pre></td></tr></table></figure><p><code>ExternalizedCheckpointCleanup</code> 配置项定义了当作业取消时，对作业 checkpoint 的操作：</p><ul><li><strong><code>ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION</code></strong>：当作业取消时，保留作业的 checkpoint。注意，这种情况下，需要手动清除该作业保留的 checkpoint。</li><li><strong><code>ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION</code></strong>：当作业取消时，删除作业的 checkpoint。仅当作业失败时，作业的 checkpoint 才会被保留。</li></ul><h3 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h3><p>与 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/ops/state/savepoints.html" target="_blank" rel="noopener">savepoints</a> 相似，checkpoint 由元数据文件、数据文件（与 state backend 相关）组成。可通过配置文件中 “state.checkpoints.dir” 配置项来指定元数据文件和数据文件的存储路径，另外也可以在代码中针对单个作业特别指定该配置项。</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">/user-defined-checkpoint-dir</span></span><br><span class="line">    <span class="string">/&#123;job-id&#125;</span></span><br><span class="line">        <span class="string">|</span></span><br><span class="line">        <span class="string">+</span> <span class="string">--shared/</span></span><br><span class="line">        <span class="string">+</span> <span class="string">--taskowned/</span></span><br><span class="line">        <span class="string">+</span> <span class="string">--chk-1/</span></span><br><span class="line">        <span class="string">+</span> <span class="string">--chk-2/</span></span><br><span class="line">        <span class="string">+</span> <span class="string">--chk-3/</span></span><br><span class="line">        <span class="string">...</span></span><br></pre></td></tr></table></figure><p>其中 <strong>SHARED</strong> 目录保存了可能被多个 checkpoint 引用的文件，<strong>TASKOWNED</strong> 保存了不会被 JobManager 删除的文件，<strong>EXCLUSIVE</strong> 则保存那些仅被单个 checkpoint 引用的文件。</p><h4 id="通过配置文件全局配置"><a href="#通过配置文件全局配置" class="headerlink" title="通过配置文件全局配置"></a>通过配置文件全局配置</h4><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">state.checkpoints.dir</span>: <span class="string">hdfs:///checkpoints/</span></span><br></pre></td></tr></table></figure><h4 id="创建-state-backend-对单个作业进行配置"><a href="#创建-state-backend-对单个作业进行配置" class="headerlink" title="创建 state backend 对单个作业进行配置"></a>创建 state backend 对单个作业进行配置</h4><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">env.setStateBackend(new</span> <span class="string">RocksDBStateBackend("hdfs:///checkpoints-data/"));</span></span><br></pre></td></tr></table></figure><h2 id="Savepoint"><a href="#Savepoint" class="headerlink" title="Savepoint"></a>Savepoint</h2><p>Savepoint 是依据 Flink checkpointing 机制所创建的流作业执行状态的一致镜像。 你可以使用 Savepoint 进行 Flink 作业的停止与重启、fork 或者更新。 Savepoint 由两部分组成：稳定存储（列入 HDFS，S3，…) 上包含二进制文件的目录（通常很大），和元数据文件（相对较小）。 稳定存储上的文件表示作业执行状态的数据镜像。</p><p>Savepoint 的元数据文件以绝对路径的形式指向作为 Savepoint 的稳定存储上的所有文件的指针。</p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/ops/state/savepoints.html" target="_blank" rel="noopener">更多内容查看官网</a></p><h4 id="Savepoint和Checkpoint-区别"><a href="#Savepoint和Checkpoint-区别" class="headerlink" title="Savepoint和Checkpoint 区别"></a>Savepoint和Checkpoint 区别</h4><ul><li>Checkpoint 的主要目的是为意外失败的作业提供恢复机制。 Checkpoint 的生命周期由 Flink 管理，即 Flink 创建，管理和删除 Checkpoint - 无需用户交互。 作为一种恢复和定期触发的方法，Checkpoint 实现有两个设计目标：i）轻量级创建和 ii）尽可能快地恢复。 </li><li>Savepoint 由用户创建，拥有和删除。 他们的用例是计划的，手动备份和恢复。 例如，升级 Flink 版本，调整用户逻辑，改变并行度，以及进行红蓝部署等。Savepoint 更多地关注可移植性和对前面提到的作业更改的支持。</li></ul><h2 id="State-Backends"><a href="#State-Backends" class="headerlink" title="State Backends"></a>State Backends</h2><p>用 Data Stream API 编写的程序通常以各种形式保存状态：</p><ul><li>在 Window 触发之前要么收集元素、要么聚合</li><li>转换函数可以使用 key/value 格式的状态接口来存储状态</li><li>转换函数可以实现 <code>CheckpointedFunction</code> 接口，使其本地变量具有容错能力</li></ul><p>在启动 CheckPoint 机制时，状态会随着 CheckPoint 而持久化，以防止数据丢失、保障恢复时的一致性。 状态内部的存储格式、状态在 CheckPoint 时如何持久化以及持久化在哪里均取决于选择的 <strong>State Backend</strong>。</p><h3 id="可用的-State-Backends"><a href="#可用的-State-Backends" class="headerlink" title="可用的 State Backends"></a>可用的 State Backends</h3><p>Flink 内置了以下这些开箱即用的 state backends ：</p><ul><li><em>MemoryStateBackend</em></li><li><em>FsStateBackend</em></li><li><em>RocksDBStateBackend</em></li></ul><p>如果不设置，默认使用 MemoryStateBackend。</p><h3 id="MemoryStateBackend"><a href="#MemoryStateBackend" class="headerlink" title="MemoryStateBackend"></a>MemoryStateBackend</h3><p>在 <em>MemoryStateBackend</em> 内部，数据以 Java 对象的形式存储在堆中。 Key/value 形式的状态和窗口算子持有存储着状态值、触发器的 hash table。</p><p>在 CheckPoint 时，State Backend 对状态进行快照，并将快照信息作为 CheckPoint 应答消息的一部分发送给 JobManager(master)，同时 JobManager 也将快照信息存储在堆内存中。</p><p>MemoryStateBackend 能配置异步快照。强烈建议使用异步快照来防止数据流阻塞，注意，异步快照默认是开启的。 用户可以在实例化 <code>MemoryStateBackend</code> 的时候，将相应布尔类型的构造参数设置为 <code>false</code> 来关闭异步快照（仅在 debug 的时候使用），例如：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.setStateBackend(<span class="keyword">new</span> MemoryStateBackend(MAX_MEM_STATE_SIZE, <span class="keyword">false</span>));</span><br></pre></td></tr></table></figure><p>MemoryStateBackend 的限制：</p><ul><li>默认情况下，每个独立的状态大小限制是 5 MB。在 MemoryStateBackend 的构造器中可以增加其大小。</li><li>无论配置的最大状态内存大小（MAX_MEM_STATE_SIZE）有多大，都不能大于 akka frame 大小（看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/ops/config.html" target="_blank" rel="noopener">配置参数</a>）。</li><li>聚合后的状态必须能够放进 JobManager 的内存中。</li></ul><p>MemoryStateBackend 适用场景：</p><ul><li>本地开发和调试。</li><li>状态很小的 Job，例如：由每次只处理一条记录的函数（Map、FlatMap、Filter 等）构成的 Job。Kafka Consumer 仅仅需要非常小的状态。</li></ul><p>建议同时将 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/ops/memory/mem_setup.html#managed-memory" target="_blank" rel="noopener">managed memory</a> 设为0，以保证将最大限度的内存分配给 JVM 上的用户代码。</p><h3 id="FsStateBackend"><a href="#FsStateBackend" class="headerlink" title="FsStateBackend"></a>FsStateBackend</h3><p><em>FsStateBackend</em> 需要配置一个文件系统的 URL（类型、地址、路径），例如：”hdfs://namenode:40010/flink/checkpoints” 或 “file:///data/flink/checkpoints”。</p><p>FsStateBackend 将正在运行中的状态数据保存在 TaskManager 的内存中。CheckPoint 时，将状态快照写入到配置的文件系统目录中。 少量的元数据信息存储到 JobManager 的内存中（高可用模式下，将其写入到 CheckPoint 的元数据文件中）。</p><p>FsStateBackend 默认使用异步快照来防止 CheckPoint 写状态时对数据处理造成阻塞。 用户可以在实例化 <code>FsStateBackend</code> 的时候，将相应布尔类型的构造参数设置为 <code>false</code> 来关闭异步快照，例如：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.setStateBackend(<span class="keyword">new</span> FsStateBackend(<span class="string">"hdfs://hadoop:9000/flink/rocks_state_backend"</span>))</span><br></pre></td></tr></table></figure><p>FsStateBackend 适用场景:</p><ul><li>状态比较大、窗口比较长、key/value 状态比较大的 Job。</li><li>所有高可用的场景。</li></ul><p>建议同时将 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/ops/memory/mem_setup.html#managed-memory" target="_blank" rel="noopener">managed memory</a> 设为0，以保证将最大限度的内存分配给 JVM 上的用户代码。</p><h3 id="RocksDBStateBackend"><a href="#RocksDBStateBackend" class="headerlink" title="RocksDBStateBackend"></a>RocksDBStateBackend</h3><p><em>RocksDBStateBackend</em> 需要配置一个文件系统的 URL （类型、地址、路径），例如：”hdfs://namenode:40010/flink/checkpoints” 或 “file:///data/flink/checkpoints”。</p><p>RocksDBStateBackend 将正在运行中的状态数据保存在 <a href="http://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a> 数据库中，RocksDB 数据库默认将数据存储在 TaskManager 的数据目录。 CheckPoint 时，整个 RocksDB 数据库被 checkpoint 到配置的文件系统目录中。 少量的元数据信息存储到 JobManager 的内存中（高可用模式下，将其存储到 CheckPoint 的元数据文件中）。RocksDBStateBackend 只支持异步快照。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.setStateBackend(<span class="keyword">new</span> RocksDBStateBackend(filebackend, <span class="keyword">true</span>));</span><br></pre></td></tr></table></figure><p>RocksDBStateBackend 的限制：</p><ul><li>由于 RocksDB 的 JNI API 构建在 byte[] 数据结构之上, 所以每个 key 和 value 最大支持 2^31 字节。 重要信息: RocksDB 合并操作的状态（例如：ListState）累积数据量大小可以超过 2^31 字节，但是会在下一次获取数据时失败。这是当前 RocksDB JNI 的限制。</li></ul><p>RocksDBStateBackend 的适用场景：</p><ul><li>状态非常大、窗口非常长、key/value 状态非常大的 Job。</li><li>所有高可用的场景。</li></ul><p>注意，你可以保留的状态大小仅受磁盘空间的限制。与状态存储在内存中的 FsStateBackend 相比，RocksDBStateBackend 允许存储非常大的状态。 然而，这也意味着使用 RocksDBStateBackend 将会使应用程序的最大吞吐量降低。 所有的读写都必须序列化、反序列化操作，这个比基于堆内存的 state backend 的效率要低很多。</p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/ops/state/state_backends.html" target="_blank" rel="noopener">更多的进阶操作查看官网</a></p><h3 id="设置每个-Job-的-State-Backend"><a href="#设置每个-Job-的-State-Backend" class="headerlink" title="设置每个 Job 的 State Backend"></a>设置每个 Job 的 State Backend</h3><p><code>StreamExecutionEnvironment</code> 可以对每个 Job 的 State Backend 进行设置，如下所示：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val env = StreamExecutionEnvironment.getExecutionEnvironment()</span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> FsStateBackend(<span class="string">"hdfs://namenode:40010/flink/checkpoints"</span>))</span><br></pre></td></tr></table></figure><p>如果你想在 IDE 中使用 <code>RocksDBStateBackend</code>，或者需要在作业中通过编程方式动态配置 <code>RocksDBStateBackend</code>，必须添加以下依赖到 Flink 项目中。</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-statebackend-rocksdb_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="设置默认的（全局的）-State-Backend"><a href="#设置默认的（全局的）-State-Backend" class="headerlink" title="设置默认的（全局的） State Backend"></a>设置默认的（全局的） State Backend</h3><p>在 <code>flink-conf.yaml</code> 可以通过键 <code>state.backend</code> 设置默认的 State Backend。</p><p>可选值包括 <em>jobmanager</em> (MemoryStateBackend)、<em>filesystem</em> (FsStateBackend)、<em>rocksdb</em> (RocksDBStateBackend)， 或使用实现了 state backend 工厂 <a href="https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/state/StateBackendFactory.java" target="_blank" rel="noopener">StateBackendFactory</a> 的类的全限定类名， 例如： RocksDBStateBackend 对应为 <code>org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory</code>。</p><p><code>state.checkpoints.dir</code> 选项指定了所有 State Backend 写 CheckPoint 数据和写元数据文件的目录。 你能在这里找到关于 CheckPoint 目录结构的详细信息。</p><p>配置文件的部分示例如下所示：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># 用于存储 operator state 快照的 State Backend</span><br><span class="line">state.backend: filesystem</span><br><span class="line"></span><br><span class="line"># 存储快照的目录</span><br><span class="line">state.checkpoints.dir: hdfs:<span class="comment">//namenode:40010/flink/checkpoints</span></span><br></pre></td></tr></table></figure><h3 id="增量快照"><a href="#增量快照" class="headerlink" title="增量快照"></a>增量快照</h3><p>RocksDBStateBackend 支持<em>增量快照</em>。不同于产生一个包含所有数据的全量备份，增量快照中只包含自上一次快照完成之后被修改的记录，因此可以显著减少快照完成的耗时。</p><p>虽然状态数据量很大时我们推荐使用增量快照，但这并不是默认的快照机制，您需要通过下述配置手动开启该功能：</p><ul><li>在 <code>flink-conf.yaml</code> 中设置：<code>state.backend.incremental: true</code> 或者</li><li>在代码中按照右侧方式配置（来覆盖默认配置）：<code>RocksDBStateBackend backend = new RocksDBStateBackend(filebackend, true);</code></li></ul><h3 id="三种State-Backends对比"><a href="#三种State-Backends对比" class="headerlink" title="三种State Backends对比"></a>三种State Backends对比</h3><table><thead><tr><th>类别</th><th>State</th><th>Checkpoint</th><th>场景</th></tr></thead><tbody><tr><td>MemoryStateBackend</td><td>存在taskManager上的</td><td>jobmanager</td><td>测试</td></tr><tr><td>FsStateBackend</td><td>存在taskManager上的</td><td>fs</td><td>测试/部分生产</td></tr><tr><td>RocksDBStateBackend</td><td>存在RocksDB上</td><td>hdfs</td><td>生产</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客目录</title>
      <link href="/2020/05/20/contents/"/>
      <url>/2020/05/20/contents/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><h3 id="HADOOP"><a href="#HADOOP" class="headerlink" title="HADOOP"></a>HADOOP</h3><ul><li><p><strong><a href="https://yerias.github.io/2018/10/04/hadoop/1/">HDFS的伪分布式部署&amp;HADOOP的常用命令</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/10/05/hadoop/2/">yarn的伪分布式部署&amp;jps的原理&amp;oom-killer&amp;/tmp目录的clean机制</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/10/06/hadoop/3/">理解数据块、副本数、小文件的概念&amp;掌握HDFS架构&amp;掌握NN和SNN交互流程</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/10/07/hadoop/4/">HDFS的副本存放策略&amp;HDFS的读写流程&amp;Pid文件详解&amp;HDFS常用命令&amp;HDFS的回收站机制&amp;安全模式详解&amp;单、多节点的磁盘均衡策略</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/10/08/hadoop/5/">Hadoop2.7.6之前和Hadoop2.8.4之后的副本存放策略</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/10/09/hadoop/6/">Hadoop的Pid文件</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/10/10/hadoop/7/">MR的执行流程&amp;初探文件压缩&amp;初探文件格式&amp;分片数与任务数&amp;shuffle的执行流程&amp;WordCount的执行流程</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/10/11/hadoop/8/">YARN的调优&amp;YARN的三种调度器</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/10/12/hadoop/9/">Spark、IDEA和Maven的环境准备&amp;Hadoop的依赖以及常用API&amp;WordCount Debug流程&amp;map、reduce方法的参数类型和作用&amp;瘦包在服务器上的jar包依赖</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/10/13/hadoop/10/">InputFormat&amp;Partitioner&amp;Conbiner&amp;Sort&amp;OutputFormat</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/10/14/hadoop/11/">数据倾斜&amp;MRchain解决数据倾斜&amp;大小表Reduce Join&amp;大小表Map Join&amp;SQL的执行计划</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/10/15/hadoop/14/">MapReduce使用压缩以及在MR中的通用做法</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/10/15/hadoop/13/">HADOOP安装LZO压缩</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/10/16/hadoop/12/">MR调优之压缩</a></strong></p></li></ul><h3 id="HIVE"><a href="#HIVE" class="headerlink" title="HIVE"></a>HIVE</h3><ul><li><p><strong><a href="https://yerias.github.io/2018/11/01/hive/1/">Hive中的字符集编码若干问题</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/11/02/hive/2/">谷粒影音8道SQL题(各种Top N)</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/11/03/hive/3/">Hive的部署和初始化工作&amp;验证Hive部署成功</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/11/04/hive/4/">Hive数据类型&amp;DDL数据定义(增删查改)&amp;DML数据操作(导入导出)</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/11/05/hive/5/">HS2&amp;Hive的复杂数据结构&amp;行列互转&amp;常用函数&amp;静动态分区表&amp;桶表</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/11/06/hive/6/">Order By&amp;Sort By&amp;Distribute By&amp;Cluster By</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/11/07/hive/7/">创建伪表&amp;自定义UDF函数&amp;MR解决数据倾斜的问题&amp;行转列案例&amp;列转行案例&amp;使用hive实现wc&amp;修改hadoop的URI带来的hive数据库路径问题&amp;多文件多目录做wc或建表带来的问题</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/11/08/hive/8/">Windowing functions&amp;The OVER clause&amp;Analytics functions</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/11/09/hive/10/">大表Join大表&amp;大表Join小表&amp;group By解决数据倾斜</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/11/10/hive/11/">HIVE Skewed Table&amp;List Bucketing</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/11/11/hive/12/">HIVE调优之存储格式</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/11/12/hive/9/">HIVE调优之开发调优(1)</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/11/13/hive/13/">HIVE调优之开发调优(2)</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/11/14/hive/14/">Idea加载Hive源码，并且在控制台查询SQL</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/11/15/hive/15/">Hive元数据管理(1)</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/11/16/hive/16/">Hive元数据管理(2)</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/11/17/hive/17/">Hive元数据管理(3)</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/11/19/hive/18/">Hive-ORC文件存储格式</a></strong></p></li></ul><h3 id="FLUME"><a href="#FLUME" class="headerlink" title="FLUME"></a>FLUME</h3><ul><li><p><strong><a href="https://yerias.github.io/2018/12/01/flume/1/">Flume架构摸排</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/12/02/flume/2/">案例&amp;Flume单源单出口&amp;Flume单源多出口&amp;Flume多源单出口</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/12/03/flume/3/">Flume的Channel选择器&amp;Flume的Sink选择器&amp;Channel的两种类型</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/12/04/flume/4/">Flume源代码二次开发debug</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/12/05/flume/5/">Flume源代码二次开发Source&amp;Sink&amp;Interceptor&amp;Channel的事物保证</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/12/06/flume/6/">修改Flume源码使taildir source支持递归（可配置）</a></strong></p></li></ul><h3 id="SPARK"><a href="#SPARK" class="headerlink" title="SPARK"></a>SPARK</h3><ul><li><p><strong><a href="https://yerias.github.io/2019/10/01/spark/1/">编译Spark&amp;Idea配置Spark环境&amp;RDD五大特点&amp;Spark参数管理&amp;数据的读写</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/02/spark/2/">Spark之Transformations&amp;Action</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/03/spark/3/">Spark之WC产生多少个RDD</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/04/spark/4/">Spark之排序模块</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/07/spark/7/">Spark之监控模块</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/08/spark/8/">Spark之短信告警</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/10/spark/10/">Spark之分组TopN模块</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/09/spark/9/">经典案例&amp;多目录输出&amp;计数器&amp;持久化&amp;广播变量</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/05/spark/5/">Spark使用Yarn模式解决Jar乱飞情况</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/06/spark/6/">Spark术语&amp;Spark提交&amp;YARN上的提交模式&amp;窄依赖&amp;宽依赖</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/11/spark/11/">SparkSQL&amp;DataFrame的read和write&amp;SparkSQL做统计分析&amp;UDF函数&amp;存储格式的转换</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/12/spark/12/">Spark中的序列化</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/13/spark/13/">Spark源码之解读spark-shell脚本</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/15/spark/15/">RDD转换DadaFrame&amp;使用SQL操作数据源&amp;跨数据源join&amp;SQL与DF与DS的比较&amp;Spark元数据管理: catalog</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/16/spark/16/">从jdbc的角度解读外部数据源</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/17/spark/17/">数据列自动推导&amp;数据错误执行模式&amp;UDAF&amp;UDTF&amp;解读Spark SQL执行计划优化</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/18/spark/18/">自定义外部Text数据源</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/19/spark/19/">Idea加载Spark源码，并且在控制台查询SQL</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/20/spark/20/">Spark Streaming简介&amp;Spark Streaming的内部结构&amp;StreamingContext对象&amp;离散流（DStream）&amp;IDEA开发Spark Streaming</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/21/spark/21/">SS黑名单管理&amp;窗口&amp;闭包&amp;SS对接Kafka&amp;KafkaRDD</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/22/spark/22/">Kafka Offset管理</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/23/spark/23/">Spark 读写压缩文件的一次简单尝试</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/24/spark/24/">SS+Kafka提交服务器&amp;窗口函数&amp;SS调优</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/25/spark/25/">Spark各个版本特性</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/26/spark/26/">Spark性能优化指南</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/27/spark/14/">Spark性能优化指南之开发调优</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/28/spark/27/">Spark性能优化指南之资源参数调优</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/29/spark/28/">Spark性能优化指南之数据倾斜调优</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/30/spark/29/">Spark性能优化指南之Shuffle调优</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/10/31/spark/30/">Spark Broadcast实现动态更新作业配置</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/11/01/spark/31/">Spark内存管理</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/11/02/spark/32/">Spark自定义Hbase外部数据源(兼容稀疏存储)</a></strong></p></li></ul><h3 id="FLINK"><a href="#FLINK" class="headerlink" title="FLINK"></a>FLINK</h3><ul><li><strong><a href="https://yerias.github.io/2020/05/10/flink/1/">Flink整合CDH Hadoop编译&amp;简单测试</a></strong></li></ul><h3 id="AZKABAN"><a href="#AZKABAN" class="headerlink" title="AZKABAN"></a>AZKABAN</h3><ul><li><strong><a href="https://yerias.github.io/2020/02/20/azkaban/1/">Azkaban的安装&amp;使用&amp;坑</a></strong></li><li><strong><a href="https://yerias.github.io/2020/04/10/azkaban/2/">Azkaban配置Plugin实现Spark作业提交(非Shell)</a></strong></li></ul><h3 id="CDH"><a href="#CDH" class="headerlink" title="CDH"></a>CDH</h3><ul><li><strong><a href="https://yerias.github.io/2019/04/09/cdh/9/">CentOS7安装CDH 第九章：CDH中安装Kafka</a></strong></li></ul><h3 id="SCALA"><a href="#SCALA" class="headerlink" title="SCALA"></a>SCALA</h3><ul><li><p><strong><a href="https://yerias.github.io/2020/03/03/scala/1/">Scala之高阶函数</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2020/03/16/scala/2/">Scala之使用ScalikeJDBC操作MySQL</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2020/03/19/scala/3/">Scala之var和val的比较&amp;lazy懒加载</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2020/03/23/scala/4/">Scala之闭包&amp;柯里化</a></strong></p></li></ul><h3 id="JAVA"><a href="#JAVA" class="headerlink" title="JAVA"></a>JAVA</h3><ul><li><p><strong><a href="https://yerias.github.io/2019/01/03/java/3/">JAVA.UTIL包下的LINKEDLIST和迭代器快速失败的源码解析以及多线程下故障现象、导致原因、以及解决办法和优化建议</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/01/04/java/5/">HashMap1.8源码分析</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2019/01/04/java/4/">HashMap多线程问题</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2020/03/07/java/9/">FastJson的使用</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2020/03/11/java/10/">JAVA的String类源码解析</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2020/03/17/java/11/">JAVA的包装类解析</a></strong></p></li></ul><h3 id="ZOOKEEPER"><a href="#ZOOKEEPER" class="headerlink" title="ZOOKEEPER"></a>ZOOKEEPER</h3><ul><li><strong><a href="https://yerias.github.io/2020/03/02/zookeeper/1/">Zookeeper的安装&amp;使用</a></strong></li><li><strong><a href="https://yerias.github.io/2020/03/15/zookeeper/2/">Curator的介绍&amp;使用</a></strong></li></ul><h3 id="SQOOP"><a href="#SQOOP" class="headerlink" title="SQOOP"></a>SQOOP</h3><ul><li><strong><a href="https://yerias.github.io/2018/12/01/sqoop/1/">SQOOP安装&amp;RDBMS导入HDFS&amp;RDBMS导入HIVE&amp;HDFS导入RDBMS&amp;HIVE导入RDBMS&amp;SQOOP的ETL案例&amp;在SHELL中操作MYSQL</a></strong></li></ul><h3 id="REDIS"><a href="#REDIS" class="headerlink" title="REDIS"></a>REDIS</h3><ul><li><strong><a href="https://yerias.github.io/2020/04/23/redis/1/">单节点部署redis</a></strong></li></ul><h3 id="PRESTO"><a href="#PRESTO" class="headerlink" title="PRESTO"></a>PRESTO</h3><ul><li><p><strong><a href="https://yerias.github.io/2020/05/02/presto/1/">Presto扫盲</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2020/05/03/presto/2/">Presto部署</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2020/05/04/presto/3/">PrestoUDF开发</a></strong></p></li></ul><h3 id="OfflineDW"><a href="#OfflineDW" class="headerlink" title="OfflineDW"></a>OfflineDW</h3><ul><li><p><strong><a href="https://yerias.github.io/2020/02/14/offlinedw/1.项目开发的准备/">项目开发的准备</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2020/02/15/offlinedw/2.项目开发的流程/">项目开发的流程</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2020/02/16/offlinedw/3.项目数据的采集/">项目数据的采集</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2020/02/17/offlinedw/4.项目数据的ETL/">项目数据的ETL</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2020/02/18/offlinedw/5.数据仓库的分层/">数据仓库的分层</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2020/02/19/offlinedw/6.业务数据的抽取/">业务数据的抽取</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2020/02/20/offlinedw/7.结果数据的展示/">结果数据的展示</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2020/02/21/offlinedw/8.项目复盘/">项目复盘</a></strong></p></li></ul><h3 id="NGINX"><a href="#NGINX" class="headerlink" title="NGINX"></a>NGINX</h3><ul><li><strong><a href="https://yerias.github.io/2020/03/10/nginx/1/">Nginx的简介&amp;安装&amp;常用操作</a></strong></li></ul><h3 id="MYSQL"><a href="#MYSQL" class="headerlink" title="MYSQL"></a>MYSQL</h3><ul><li><p><strong><a href="https://yerias.github.io/2018/10/01/mysql/1/">MySQL二进制部署和DBeaver连接MySQL</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/10/02/mysql/2/">掌握MySQL的建表规范、DDL语句和权限操作</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/10/03/mysql/3/">掌握where、group、join语句和写SQL</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/10/07/mysql/数据重刷机制/">数据重刷机制(抛砖引玉)</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/10/08/mysql/5/">Linux下MySQL进程死掉的可能解决方案</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/10/08/mysql/4/">MySQL中的Top N</a></strong></p></li></ul><h3 id="KAFKA"><a href="#KAFKA" class="headerlink" title="KAFKA"></a>KAFKA</h3><ul><li><strong><a href="https://yerias.github.io/2020/04/22/kafka/3/">kafka eagle安装部署</a></strong></li><li><strong><a href="https://yerias.github.io/2020/05/18/kafka/2/">单节点部署三台kafka</a></strong></li></ul><h3 id="LINUX"><a href="#LINUX" class="headerlink" title="LINUX"></a>LINUX</h3><ul><li><strong><a href="https://yerias.github.io/2018/09/18/linux/1/">Linux操作文件和定位错误</a></strong></li><li><strong><a href="https://yerias.github.io/2018/09/19/linux/2/">Linux对环境变量的理解以及alias、rm、hostory的使用</a></strong></li><li><strong><a href="https://yerias.github.io/2018/09/20/linux/3/">熟悉Linux权限相关命令</a></strong></li><li><strong><a href="https://yerias.github.io/2018/09/20/linux/4/">熟练使用vim、系统命令和程序管理工具</a></strong></li><li><strong><a href="https://yerias.github.io/2018/09/21/linux/5/">后台执行、crontab调度和软连接的使用场景</a></strong></li></ul><h3 id="SHELL"><a href="#SHELL" class="headerlink" title="SHELL"></a>SHELL</h3><ul><li><p><strong><a href="https://yerias.github.io/2018/09/22/linux/shell/1/">Shell的变量</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/09/23/linux/shell/2/">Shell的字符串</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/09/24/linux/shell/3/">Shell的参数传递</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/09/25/linux/shell/4/">Shell的数组</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/09/26/linux/shell/5/">Shell的运算符</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/09/27/linux/shell/6/">Shell的echo命令</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/09/28/linux/shell/7/">Shell的printf命令</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/09/29/linux/shell/8/">Shell的流程控制</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2018/09/30/linux/shell/9/">Shell的test命令</a></strong></p></li></ul><h3 id="JVM"><a href="#JVM" class="headerlink" title="JVM"></a>JVM</h3><ul><li><p><strong><a href="https://yerias.github.io/2020/03/26/jvm/4/">JVM之内存模型</a></strong></p></li><li><p><strong><a href="https://yerias.github.io/2020/04/15/jvm/1/">JVM之运行时数据区</a></strong></p></li></ul><h3 id="HUE"><a href="#HUE" class="headerlink" title="HUE"></a>HUE</h3><ul><li><strong><a href="https://yerias.github.io/2020/03/09/hue/1/">HUE安装&amp;集成</a></strong></li></ul><h3 id="GIT"><a href="#GIT" class="headerlink" title="GIT"></a>GIT</h3><ul><li><strong><a href="https://yerias.github.io/2019/02/01/git/1/">GIT的常用操作&amp;GITHUB的常用操作&amp;在IDEA中使用GIT操作GITHUB</a></strong></li></ul><h3 id="错误记录"><a href="#错误记录" class="headerlink" title="错误记录"></a>错误记录</h3><ul><li><strong><a href="https://yerias.github.io/2020/03/19/error/1/">执行Saprk or Scala程序:找不到或者无法加载主类 xxx</a></strong></li><li><strong><a href="https://yerias.github.io/2020/03/25/error/2/">MapJoin，文件在HDFS上Idea报错：File does not exist: /xxx/yyy.txt#yyy.txt</a></strong></li><li><strong><a href="https://yerias.github.io/2020/03/25/error/3/">执行Hive SQL/MR 报错：Current usage: 77.8mb of 512.0mb physical memory used; 1.1gb of 1.0gb virtual memory used. Killing container.</a></strong></li><li><strong><a href="https://yerias.github.io/2020/03/26/error/4/">MR编程时，Driver传递的参数Mapper显示为NULL</a></strong></li><li><strong><a href="https://yerias.github.io/2020/03/30/error/5/">Spark疯狂踩坑系列</a></strong></li><li><strong><a href="https://yerias.github.io/2020/04/18/error/6/">Error: java.io.IOException: Invalid LZO header</a></strong></li><li><strong><a href="https://yerias.github.io/2020/04/27/error/7/">error: object hadoop is not a member of packee com</a></strong></li><li><strong><a href="https://yerias.github.io/2020/04/28/error/8/">FileNotFountException: file:/home/hadoop/lib/tunan-spark-core-1.0.jar!/ip2region.db</a></strong></li></ul><h3 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h3><ul><li><strong><a href="https://yerias.github.io/2020/04/26/翻译/1/">ORCFile in HDP 2: Better Compression, Better Performance</a></strong></li><li><strong><a href="https://yerias.github.io/2020/04/26/翻译/2/">Introducing Window Functions in Spark SQL</a></strong></li></ul><h3 id="生产案例"><a href="#生产案例" class="headerlink" title="生产案例"></a>生产案例</h3><ul><li><strong><a href="https://yerias.github.io/2018/10/09/PE/1/">DataNode OOM溢出</a></strong></li><li><strong><a href="https://yerias.github.io/2018/10/10/PE/2/">HDFS Block损坏恢复</a></strong></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink整合CDH Hadoop编译&amp;简单测试</title>
      <link href="/2020/05/10/flink/1/"/>
      <url>/2020/05/10/flink/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li><p>准备工作</p></li><li><p>下载源码包</p></li><li><p>准备操作</p><ol><li>配置支持CDH依赖</li><li>编译Flink-shaded</li><li>flink测试模块删减</li><li>配置支持maven-assembly-plugin插件</li><li>node、npm等依赖添加国内仓库</li><li>Kafka Schema Registry相关maven库配置</li></ol></li><li><p>执行Flink编译</p></li><li><p>提取编译后的安装包</p></li><li><p>单节点部署测试</p></li></ol><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><ul><li>maven3</li><li>jdk1.8</li></ul><h2 id="下载源码包"><a href="#下载源码包" class="headerlink" title="下载源码包"></a>下载源码包</h2><p>Flink官网: <a href="https://flink.apache.org/" target="_blank" rel="noopener">https://flink.apache.org/</a></p><p>下载地址: <a href="https://www.apache.org/dyn/closer.lua/flink/flink-1.10.0/flink-1.10.0-src.tgz" target="_blank" rel="noopener">Apache Flink 1.10.0 Source Release</a></p><h2 id="准备操作"><a href="#准备操作" class="headerlink" title="准备操作"></a>准备操作</h2><h3 id="配置支持CDH依赖"><a href="#配置支持CDH依赖" class="headerlink" title="配置支持CDH依赖"></a>配置支持CDH依赖</h3><p>maven默认不支持cdh的依赖下载，修改maven目录下conf中的settings.xml如下：（这里的cloudera-releases是flink源码中配置的id）</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mirrors</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>nexus-aliyun<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>*,!cloudera-releases,!cloudera<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>Nexus aliyun<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- hortonworks maven --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>nexus-hortonworks<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>*,!central<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>Nexus hortonworks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repo.hortonworks.com/content/groups/public/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>confluent<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>confluent<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>Nexus public mirror<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://packages.confluent.io/maven/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirrors</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>注意:</strong> 有时候hortonworks的仓库并不好使，如果发现不好使，注释掉即可</p><p>修改<code>flink-1.10.0/pom.xml</code>，添加：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--添加CDH的仓库--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="编译Flink-shaded"><a href="#编译Flink-shaded" class="headerlink" title="编译Flink-shaded"></a>编译Flink-shaded</h3><p>不同的 Flink 版本使用的 Flink-shaded不同，Flink 10.0 版本使用Flink-shaded 9.0<br>如果不编译的话会报错找不到：flink-shaded-hadoop-2:jar:2.6.0-cdh5.16.2-9.0</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">[ERROR]</span> <span class="string">Failed to execute goal on project flink-hadoop-fs: Could not resolve dependencies for project org.apache.flink:flink-hadoop-fs:jar:1.9.1: The following artifacts could not be resolved: org.apache.flink:flink-shaded-hadoop-2:jar:2.6.0-cdh5.14.2-7.0, org.apache.hadoop:hadoop-hdfs:jar:tests:2.6.0-cdh5.14.2, org.apache.hadoop:hadoop-common:jar:tests:2.6.0-cdh5.14.2: Could not find artifact org.apache.flink:flink-shaded-hadoop-2:jar:2.6.0-cdh5.14.2-7.0 in nexus-hortonworks (https://repo.hortonworks.com/content/groups/public/) -&gt; [Help 1]</span></span><br></pre></td></tr></table></figure><p>因此，这一步需要手动编译flink-shaded-hadoop-2，并将其打入到maven库。</p><ol><li><p>下载<a href="https://github.com/apache/flink-shaded/tree/release-10.0" target="_blank" rel="noopener">flink-shaded-10.0-src.tgz</a></p></li><li><p>修改项目pom.xml</p><p>在 <code>flink-shaded-7.0/pom.xml</code>文件中添加 cloudera 的maven库：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--添加CDH的仓库--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在<code>flink-shaded-7.0/flink-shaded-hadoop-2/pom.xml</code>文件中也添加：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--添加CDH的仓库--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在 <code>flink-shaded-7.0/flink-shaded-hadoop-2-uber/pom.xml</code> 中的 dependencyManagement 标签中添加如下依赖：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>commons-cli<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-cli<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>注意：这一步一定要添加，不然编译成功后，启动不了，并 .out 文件中抛出如下错误：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">Exception</span> <span class="string">in thread "main" java.lang.NoSuchMethodError: org.apache.commons.cli.Option.builder(Ljava/lang/String;)Lorg/apache/commons/cli/Option$Builder;</span></span><br></pre></td></tr></table></figure><p>原因是项目打包后，依赖的 commons-cli 是1.2版本的，build 方法在该版本中不存在。</p></li><li><p>开始编译：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">mvn</span> <span class="string">clean install -DskipTests -Dhadoop.version=2.6.0-cdh5.16.2</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="flink测试模块删减"><a href="#flink测试模块删减" class="headerlink" title="flink测试模块删减"></a>flink测试模块删减</h3><p>删除flink中的以下test模块，防止编译出错：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">module</span>&gt;</span>flink-tests<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">module</span>&gt;</span>flink-end-to-end-tests<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">module</span>&gt;</span>flink-yarn-tests<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">module</span>&gt;</span>flink-fs-tests<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置支持maven-assembly-plugin插件"><a href="#配置支持maven-assembly-plugin插件" class="headerlink" title="配置支持maven-assembly-plugin插件"></a>配置支持maven-assembly-plugin插件</h3><p>编辑<code>flink-1.10.0/flink-libraries/pom.xml</code>，新增maven-assembly-plugin插件，否则会报错。</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span><span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="node、npm等依赖添加国内仓库"><a href="#node、npm等依赖添加国内仓库" class="headerlink" title="node、npm等依赖添加国内仓库"></a>node、npm等依赖添加国内仓库</h3><p>为flink-runtime-web/添加国内仓库，编辑flink-1.10.0/flink-runtime-web/pom.xml。<br>Flink1.9.x的flink-runtime-web模块引入了frontend-maven-plugin依赖，并安装了node和部分依赖组件，添加国内仓库，否则会访问不到：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">nodeDownloadRoot</span>&gt;</span>https://registry.npm.taobao.org/dist/<span class="tag">&lt;/<span class="name">nodeDownloadRoot</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">npmDownloadRoot</span>&gt;</span>https://registry.npmjs.org/npm/-/<span class="tag">&lt;/<span class="name">npmDownloadRoot</span>&gt;</span></span><br></pre></td></tr></table></figure><p>注意frontend-maven-plugin原本就有，我们只是进行修改，修改后的完整配置是:</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.github.eirslett<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>frontend-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.6<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">id</span>&gt;</span>install node and npm<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goal</span>&gt;</span>install-node-and-npm<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">nodeDownloadRoot</span>&gt;</span>https://registry.npm.taobao.org/dist/<span class="tag">&lt;/<span class="name">nodeDownloadRoot</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">npmDownloadRoot</span>&gt;</span>https://registry.npmjs.org/npm/-/<span class="tag">&lt;/<span class="name">npmDownloadRoot</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">nodeVersion</span>&gt;</span>v10.9.0<span class="tag">&lt;/<span class="name">nodeVersion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">id</span>&gt;</span>npm install<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goal</span>&gt;</span>npm<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">arguments</span>&gt;</span>ci --cache-max=0 --no-save  --no-bin-links<span class="tag">&lt;/<span class="name">arguments</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">environmentVariables</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">HUSKY_SKIP_INSTALL</span>&gt;</span>true<span class="tag">&lt;/<span class="name">HUSKY_SKIP_INSTALL</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">environmentVariables</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">id</span>&gt;</span>npm run build<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goal</span>&gt;</span>npm<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">arguments</span>&gt;</span>run build<span class="tag">&lt;/<span class="name">arguments</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">workingDirectory</span>&gt;</span>web-dashboard<span class="tag">&lt;/<span class="name">workingDirectory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure><p>否则报错：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">[ERROR]</span> <span class="string">Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.6:npm (npm install) on project flink-runtime-web_2.11: Failed to run task: 'npm ci --cache-max=0 --no-save' failed. org.apache.commons.exec.ExecuteException: Process exited with an error: 1 (Exit value: 1) -&gt; [Help 1]</span></span><br></pre></td></tr></table></figure><h3 id="Kafka-Schema-Registry相关maven库配置"><a href="#Kafka-Schema-Registry相关maven库配置" class="headerlink" title="Kafka Schema Registry相关maven库配置"></a>Kafka Schema Registry相关maven库配置</h3><p>相关jar包在 Maven仓库中下载不到，所以需要在maven的settings文件中添加如下信息(在文章开头已经配置了)：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mirrors</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>confluent<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>confluent<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Nexus public mirror<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://packages.confluent.io/maven/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirrors</span>&gt;</span></span><br></pre></td></tr></table></figure><p>同时在flink项目的主pom里添加如下仓库配置</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>confluent<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://packages.confluent.io/maven/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">releases</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">releases</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>ICM<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.icm.edu.pl/artifactory/repo/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br></pre></td></tr></table></figure><p>否则会找不到io/confluent/kafka-schema-registry-client、kafka-schema-registry-parent、rest-utils-parent等依赖包。</p><h2 id="执行Flink编译"><a href="#执行Flink编译" class="headerlink" title="执行Flink编译"></a>执行Flink编译</h2><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">mvn</span> <span class="string">-T2C clean package -DskipTests -Pvendor-repos -Dhadoop.version=2.6.0-cdh5.16.2  -Dscala-2.12 -Drat.skip=true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数说明：</span></span><br><span class="line"><span class="comment"># -Pinclude-hadoop  # 将 hadoop的 jar包，打入到lib/中</span></span><br><span class="line"><span class="comment"># -Pvendor-repos# 激活 Maven 构建配置文件，其中包括 Cloudera，Hortonworks 或 MapR 等流行的 Hadoop 供应商的存储库。</span></span><br><span class="line"><span class="comment"># -Dhadoop.version=2.6.0-cdh5.14.2# 指定 hadoop 的版本</span></span><br><span class="line"><span class="comment"># -Dscala-2.11# Scala版本，默认是2.11，如果开发使用的Scala版本不同需要指定</span></span><br></pre></td></tr></table></figure><p>编译过程中的遇到比较多的问题都是跟依赖下载有关，一些问题的解决都在上面提到了，还有一些下不下来的包直接通过<a href="https://mvnrepository.com/" target="_blank" rel="noopener">maven库</a>或官方库下载到本地库</p><p>如果编译所需依赖都已下载，时间大概在半小时左右，视情况而定。<br>编译成功效果：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Reactor Summary:</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] force-shading ...................................... SUCCESS [  <span class="number">2.031</span> s]</span><br><span class="line">[INFO] flink .............................................. SUCCESS [  <span class="number">1.561</span> s]</span><br><span class="line">[INFO] flink-annotations .................................. SUCCESS [  <span class="number">4.092</span> s]</span><br><span class="line">[INFO] flink-shaded-curator ............................... SUCCESS [  <span class="number">5.122</span> s]</span><br><span class="line">[INFO] flink-metrics ...................................... SUCCESS [  <span class="number">0.663</span> s]</span><br><span class="line">[INFO] flink-metrics-core ................................. SUCCESS [  <span class="number">4.475</span> s]</span><br><span class="line">[INFO] flink-test-utils-parent ............................ SUCCESS [  <span class="number">0.676</span> s]</span><br><span class="line">[INFO] flink-test-utils-junit ............................. SUCCESS [  <span class="number">3.405</span> s]</span><br><span class="line">[INFO] flink-core ......................................... SUCCESS [ <span class="number">19.344</span> s]</span><br><span class="line">[INFO] flink-java ......................................... SUCCESS [  <span class="number">7.728</span> s]</span><br><span class="line">[INFO] flink-queryable-state .............................. SUCCESS [  <span class="number">0.193</span> s]</span><br><span class="line">[INFO] flink-queryable-state-client-java .................. SUCCESS [  <span class="number">1.361</span> s]</span><br><span class="line">[INFO] flink-filesystems .................................. SUCCESS [  <span class="number">1.046</span> s]</span><br><span class="line">[INFO] flink-hadoop-fs .................................... SUCCESS [  <span class="number">3.766</span> s]</span><br><span class="line">[INFO] flink-runtime ...................................... SUCCESS [<span class="number">01</span>:<span class="number">07</span> min]</span><br><span class="line">[INFO] flink-scala ........................................ SUCCESS [<span class="number">01</span>:<span class="number">35</span> min]</span><br><span class="line">[INFO] flink-mapr-fs ...................................... SUCCESS [  <span class="number">3.541</span> s]</span><br><span class="line">[INFO] flink-filesystems :: flink-fs-hadoop-shaded ........ SUCCESS [  <span class="number">8.875</span> s]</span><br><span class="line">[INFO] flink-s3-fs-base ................................... SUCCESS [ <span class="number">28.538</span> s]</span><br><span class="line">[INFO] flink-s3-fs-hadoop ................................. SUCCESS [ <span class="number">22.938</span> s]</span><br><span class="line">[INFO] flink-s3-fs-presto ................................. SUCCESS [ <span class="number">30.871</span> s]</span><br><span class="line">[INFO] flink-swift-fs-hadoop .............................. SUCCESS [ <span class="number">49.025</span> s]</span><br><span class="line">[INFO] flink-oss-fs-hadoop ................................ SUCCESS [ <span class="number">25.729</span> s]</span><br><span class="line">[INFO] flink-azure-fs-hadoop .............................. SUCCESS [ <span class="number">29.693</span> s]</span><br><span class="line">[INFO] flink-optimizer .................................... SUCCESS [  <span class="number">3.194</span> s]</span><br><span class="line">[INFO] flink-clients ...................................... SUCCESS [  <span class="number">1.632</span> s]</span><br><span class="line">[INFO] flink-streaming-java ............................... SUCCESS [ <span class="number">10.892</span> s]</span><br><span class="line">[INFO] flink-test-utils ................................... SUCCESS [  <span class="number">8.855</span> s]</span><br><span class="line">[INFO] flink-runtime-web .................................. SUCCESS [<span class="number">05</span>:<span class="number">56</span> min]</span><br><span class="line">[INFO] flink-streaming-scala .............................. SUCCESS [<span class="number">01</span>:<span class="number">37</span> min]</span><br><span class="line">[INFO] flink-table ........................................ SUCCESS [  <span class="number">0.818</span> s]</span><br><span class="line">[INFO] flink-table-common ................................. SUCCESS [  <span class="number">4.308</span> s]</span><br><span class="line">[INFO] flink-table-api-java ............................... SUCCESS [  <span class="number">5.168</span> s]</span><br><span class="line">[INFO] flink-table-api-java-bridge ........................ SUCCESS [  <span class="number">3.254</span> s]</span><br><span class="line">[INFO] flink-table-api-scala .............................. SUCCESS [ <span class="number">19.390</span> s]</span><br><span class="line">[INFO] flink-table-api-scala-bridge ....................... SUCCESS [ <span class="number">22.887</span> s]</span><br><span class="line">[INFO] flink-sql-parser ................................... SUCCESS [ <span class="number">13.172</span> s]</span><br><span class="line">[INFO] flink-state-backends ............................... SUCCESS [  <span class="number">0.712</span> s]</span><br><span class="line">[INFO] flink-statebackend-rocksdb ......................... SUCCESS [  <span class="number">4.609</span> s]</span><br><span class="line">[INFO] flink-libraries .................................... SUCCESS [  <span class="number">1.007</span> s]</span><br><span class="line">[INFO] flink-cep .......................................... SUCCESS [ <span class="number">11.703</span> s]</span><br><span class="line">[INFO] flink-table-planner ................................ SUCCESS [<span class="number">04</span>:<span class="number">36</span> min]</span><br><span class="line">[INFO] flink-connectors ................................... SUCCESS [  <span class="number">1.105</span> s]</span><br><span class="line">[INFO] flink-orc .......................................... SUCCESS [ <span class="number">10.208</span> s]</span><br><span class="line">[INFO] flink-jdbc ......................................... SUCCESS [ <span class="number">10.759</span> s]</span><br><span class="line">[INFO] flink-hadoop-compatibility ......................... SUCCESS [ <span class="number">18.036</span> s]</span><br><span class="line">[INFO] flink-table-runtime-blink .......................... SUCCESS [ <span class="number">12.010</span> s]</span><br><span class="line">[INFO] flink-table-planner-blink .......................... SUCCESS [<span class="number">05</span>:<span class="number">52</span> min]</span><br><span class="line">[INFO] flink-hbase ........................................ SUCCESS [<span class="number">07</span>:<span class="number">06</span> min]</span><br><span class="line">[INFO] flink-hcatalog ..................................... SUCCESS [ <span class="number">22.569</span> s]</span><br><span class="line">[INFO] flink-metrics-jmx .................................. SUCCESS [  <span class="number">2.176</span> s]</span><br><span class="line">[INFO] flink-connector-kafka-base ......................... SUCCESS [ <span class="number">19.286</span> s]</span><br><span class="line">[INFO] flink-connector-kafka-<span class="number">0.9</span> .......................... SUCCESS [  <span class="number">7.931</span> s]</span><br><span class="line">[INFO] flink-connector-kafka-<span class="number">0.10</span> ......................... SUCCESS [  <span class="number">3.107</span> s]</span><br><span class="line">[INFO] flink-connector-kafka-<span class="number">0.11</span> ......................... SUCCESS [  <span class="number">7.556</span> s]</span><br><span class="line">[INFO] flink-formats ...................................... SUCCESS [  <span class="number">0.361</span> s]</span><br><span class="line">[INFO] flink-json ......................................... SUCCESS [  <span class="number">8.512</span> s]</span><br><span class="line">[INFO] flink-connector-elasticsearch-base ................. SUCCESS [ <span class="number">15.884</span> s]</span><br><span class="line">[INFO] flink-connector-elasticsearch2 ..................... SUCCESS [ <span class="number">38.263</span> s]</span><br><span class="line">[INFO] flink-connector-elasticsearch5 ..................... SUCCESS [ <span class="number">40.943</span> s]</span><br><span class="line">[INFO] flink-connector-elasticsearch6 ..................... SUCCESS [ <span class="number">15.306</span> s]</span><br><span class="line">[INFO] flink-csv .......................................... SUCCESS [  <span class="number">2.397</span> s]</span><br><span class="line">[INFO] flink-connector-hive ............................... SUCCESS [<span class="number">01</span>:<span class="number">39</span> min]</span><br><span class="line">[INFO] flink-connector-rabbitmq ........................... SUCCESS [  <span class="number">1.635</span> s]</span><br><span class="line">[INFO] flink-connector-twitter ............................ SUCCESS [  <span class="number">6.631</span> s]</span><br><span class="line">[INFO] flink-connector-nifi ............................... SUCCESS [  <span class="number">3.832</span> s]</span><br><span class="line">[INFO] flink-connector-cassandra .......................... SUCCESS [ <span class="number">31.134</span> s]</span><br><span class="line">[INFO] flink-avro ......................................... SUCCESS [ <span class="number">15.516</span> s]</span><br><span class="line">[INFO] flink-connector-filesystem ......................... SUCCESS [  <span class="number">6.685</span> s]</span><br><span class="line">[INFO] flink-connector-kafka .............................. SUCCESS [  <span class="number">7.954</span> s]</span><br><span class="line">[INFO] flink-connector-gcp-pubsub ......................... SUCCESS [  <span class="number">4.723</span> s]</span><br><span class="line">[INFO] flink-sql-connector-elasticsearch6 ................. SUCCESS [ <span class="number">16.608</span> s]</span><br><span class="line">[INFO] flink-sql-connector-kafka-<span class="number">0.9</span> ...................... SUCCESS [  <span class="number">2.115</span> s]</span><br><span class="line">[INFO] flink-sql-connector-kafka-<span class="number">0.10</span> ..................... SUCCESS [  <span class="number">4.332</span> s]</span><br><span class="line">[INFO] flink-sql-connector-kafka-<span class="number">0.11</span> ..................... SUCCESS [  <span class="number">1.628</span> s]</span><br><span class="line">[INFO] flink-sql-connector-kafka .......................... SUCCESS [  <span class="number">7.450</span> s]</span><br><span class="line">[INFO] flink-connector-kafka-<span class="number">0.8</span> .......................... SUCCESS [  <span class="number">4.267</span> s]</span><br><span class="line">[INFO] flink-avro-confluent-registry ...................... SUCCESS [ <span class="number">11.695</span> s]</span><br><span class="line">[INFO] flink-parquet ...................................... SUCCESS [  <span class="number">8.175</span> s]</span><br><span class="line">[INFO] flink-sequence-file ................................ SUCCESS [  <span class="number">1.294</span> s]</span><br><span class="line">[INFO] flink-examples ..................................... SUCCESS [  <span class="number">0.457</span> s]</span><br><span class="line">[INFO] flink-examples-batch ............................... SUCCESS [ <span class="number">48.271</span> s]</span><br><span class="line">[INFO] flink-examples-streaming ........................... SUCCESS [ <span class="number">32.402</span> s]</span><br><span class="line">[INFO] flink-examples-table ............................... SUCCESS [ <span class="number">31.118</span> s]</span><br><span class="line">[INFO] flink-examples-build-helper ........................ SUCCESS [  <span class="number">0.274</span> s]</span><br><span class="line">[INFO] flink-examples-streaming-twitter ................... SUCCESS [  <span class="number">1.147</span> s]</span><br><span class="line">[INFO] flink-examples-streaming-state-machine ............. SUCCESS [  <span class="number">0.807</span> s]</span><br><span class="line">[INFO] flink-examples-streaming-gcp-pubsub ................ SUCCESS [  <span class="number">6.252</span> s]</span><br><span class="line">[INFO] flink-container .................................... SUCCESS [  <span class="number">1.893</span> s]</span><br><span class="line">[INFO] flink-queryable-state-runtime ...................... SUCCESS [  <span class="number">3.343</span> s]</span><br><span class="line">[INFO] flink-gelly ........................................ SUCCESS [ <span class="number">10.752</span> s]</span><br><span class="line">[INFO] flink-gelly-scala .................................. SUCCESS [<span class="number">01</span>:<span class="number">01</span> min]</span><br><span class="line">[INFO] flink-gelly-examples ............................... SUCCESS [ <span class="number">49.843</span> s]</span><br><span class="line">[INFO] flink-cep-scala .................................... SUCCESS [ <span class="number">28.524</span> s]</span><br><span class="line">[INFO] flink-state-processor-api .......................... SUCCESS [  <span class="number">5.763</span> s]</span><br><span class="line">[INFO] flink-table-uber ................................... SUCCESS [ <span class="number">15.688</span> s]</span><br><span class="line">[INFO] flink-table-uber-blink ............................. SUCCESS [  <span class="number">3.355</span> s]</span><br><span class="line">[INFO] flink-sql-client ................................... SUCCESS [ <span class="number">23.316</span> s]</span><br><span class="line">[INFO] flink-quickstart ................................... SUCCESS [  <span class="number">2.348</span> s]</span><br><span class="line">[INFO] flink-quickstart-java .............................. SUCCESS [  <span class="number">2.894</span> s]</span><br><span class="line">[INFO] flink-quickstart-scala ............................. SUCCESS [  <span class="number">3.156</span> s]</span><br><span class="line">[INFO] flink-contrib ...................................... SUCCESS [  <span class="number">0.203</span> s]</span><br><span class="line">[INFO] flink-connector-wikiedits .......................... SUCCESS [  <span class="number">2.453</span> s]</span><br><span class="line">[INFO] flink-mesos ........................................ SUCCESS [<span class="number">01</span>:<span class="number">05</span> min]</span><br><span class="line">[INFO] flink-yarn ......................................... SUCCESS [  <span class="number">6.167</span> s]</span><br><span class="line">[INFO] flink-metrics-dropwizard ........................... SUCCESS [  <span class="number">0.927</span> s]</span><br><span class="line">[INFO] flink-metrics-graphite ............................. SUCCESS [  <span class="number">0.302</span> s]</span><br><span class="line">[INFO] flink-metrics-influxdb ............................. SUCCESS [  <span class="number">1.825</span> s]</span><br><span class="line">[INFO] flink-metrics-prometheus ........................... SUCCESS [  <span class="number">1.091</span> s]</span><br><span class="line">[INFO] flink-metrics-statsd ............................... SUCCESS [  <span class="number">0.853</span> s]</span><br><span class="line">[INFO] flink-metrics-datadog .............................. SUCCESS [  <span class="number">1.633</span> s]</span><br><span class="line">[INFO] flink-metrics-slf4j ................................ SUCCESS [  <span class="number">0.880</span> s]</span><br><span class="line">[INFO] flink-python ....................................... SUCCESS [  <span class="number">4.793</span> s]</span><br><span class="line">[INFO] flink-scala-shell .................................. SUCCESS [ <span class="number">43.526</span> s]</span><br><span class="line">[INFO] flink-dist ......................................... SUCCESS [ <span class="number">18.899</span> s]</span><br><span class="line">[INFO] flink-docs ......................................... SUCCESS [  <span class="number">5.133</span> s]</span><br><span class="line">[INFO] flink-ml-parent .................................... SUCCESS [  <span class="number">0.399</span> s]</span><br><span class="line">[INFO] flink-ml-api ....................................... SUCCESS [  <span class="number">4.031</span> s]</span><br><span class="line">[INFO] flink-ml-lib ....................................... SUCCESS [  <span class="number">1.550</span> s]</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: <span class="number">18</span>:<span class="number">19</span> min (Wall Clock)</span><br><span class="line">[INFO] Finished at: <span class="number">2020</span>-<span class="number">01</span>-<span class="number">02</span>T23:<span class="number">01</span>:<span class="number">28</span>+<span class="number">08</span>:<span class="number">00</span></span><br><span class="line">[INFO] Final Memory: <span class="number">334</span>M/<span class="number">847</span>M</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure><h2 id="提取编译后的安装包"><a href="#提取编译后的安装包" class="headerlink" title="提取编译后的安装包"></a>提取编译后的安装包</h2><p>编译成功之后的位置在flink-1.10.0/flink-dist/target/flink-1.10.0-bin中，将文件目录压缩，上传到服务器上就可以配置使用了。</p><h2 id="单节点部署测试"><a href="#单节点部署测试" class="headerlink" title="单节点部署测试"></a>单节点部署测试</h2><ol><li><p>修改flink-conf.yaml</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">jobmanager.rpc.address</span>: <span class="string">hadoop</span></span><br><span class="line"><span class="meta">taskmanager.numberOfTaskSlots</span>: <span class="string">4</span></span><br><span class="line"><span class="meta">rest.port</span>: <span class="string">18081</span></span><br></pre></td></tr></table></figure></li><li><p>修改slaves</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">hadoop</span></span><br></pre></td></tr></table></figure></li><li><p>启动</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">start-cluster.sh</span></span><br></pre></td></tr></table></figure></li><li><p>UI启动</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">hadoop</span>:<span class="string">18081</span></span><br></pre></td></tr></table></figure></li><li><p>在windows快速生成Flink项目</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">mvn</span> <span class="string">archetype:generate \</span></span><br><span class="line">  <span class="meta">-DarchetypeGroupId</span>=<span class="string">org.apache.flink \</span></span><br><span class="line">  <span class="meta">-DarchetypeArtifactId</span>=<span class="string">flink-quickstart-scala \</span></span><br><span class="line">  <span class="meta">-DarchetypeVersion</span>=<span class="string">$&#123;1:-1.10.0&#125; \</span></span><br><span class="line">  <span class="meta">-DgroupId</span>=<span class="string">com.tunan.flink \</span></span><br><span class="line">  <span class="meta">-DartifactId</span>=<span class="string">tunan-flink \</span></span><br><span class="line">  <span class="meta">-Dversion</span>=<span class="string">1.0.0 \</span></span><br><span class="line">  <span class="meta">-Dpackage</span>=<span class="string">com.tunan.quickstart \</span></span><br><span class="line">  <span class="meta">-DinteractiveMode</span>=<span class="string">false</span></span><br></pre></td></tr></table></figure></li></ol><hr><p>参考:</p><p><a href="https://ci.apache.org/projects/flink/flink-docs-stable/flinkDev/building.html" target="_blank" rel="noopener">Apache Flink官方编译手册</a></p><p><a href="https://blog.csdn.net/qq_38976805/article/details/103067833" target="_blank" rel="noopener">Flink1.9.1源码编译支持hadoop-2.6.0-cdh5.16.2</a></p><p><a href="https://blog.csdn.net/qq_38976805/article/details/103067833" target="_blank" rel="noopener">Flink1.9.1源码编译支持hadoop-2.6.0-cdh5.16.2</a></p><p><a href="https://www.jianshu.com/p/d5ed58d7aa65" target="_blank" rel="noopener">Maven添加Kafka Schema Registry的pom依赖</a></p><p><a href="https://blog.csdn.net/u010886217/article/details/84350465" target="_blank" rel="noopener">Maven-CDH版本hadoop添加pom的依赖</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PrestoUDF开发</title>
      <link href="/2020/05/04/presto/3/"/>
      <url>/2020/05/04/presto/3/</url>
      
        <content type="html"><![CDATA[<h2 id="Presto函数"><a href="#Presto函数" class="headerlink" title="Presto函数"></a>Presto函数</h2><p>在 Presto 中，函数大体分为三种：scalar，aggregation 和 window 类型。分别如下：</p><p>1）scalar标量函数，简单来说就是 Java 中的一个静态方法，本身没有任何状态。</p><p>2）aggregation累积状态的函数，或聚集函数，如count，avg。如果只是单节点，单机状态可以直接用一个变量存储即可，但是presto是分布式计算引擎，状态数据会在多个节点之间传输，因此状态数据需要被序列化成 Presto 的内部格式才可以被传输。</p><p>3）window 窗口函数，如同sparkSQL中的窗口函数类似</p><p>官网地址：<a href="https://prestodb.github.io/docs/current/develop/functions.html" target="_blank" rel="noopener">https://prestodb.github.io/docs/current/develop/functions.html</a></p><h2 id="自定义Scalar函数的实现"><a href="#自定义Scalar函数的实现" class="headerlink" title="自定义Scalar函数的实现"></a>自定义Scalar函数的实现</h2><h3 id="定义一个java类"><a href="#定义一个java类" class="headerlink" title="定义一个java类"></a>定义一个java类</h3><ol><li><p>用 @ScalarFunction 的 Annotation 标记实现业务逻辑的静态方法。</p></li><li><p>用 @Description 描述函数的作用，这里的内容会在 SHOW FUNCTIONS 中显示。</p></li><li><p>用@SqlType 标记函数的返回值类型，如返回字符串，因此是 StandardTypes.VARCHAR。</p></li><li><p>Java 方法的返回值必须使用 Presto 内部的序列化方式，因此字符串类型必须返回 Slice， 使用 Slices.utf8Slice 方法可以方便的将 String 类型转换成 Slice 类型</p></li></ol><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunna.spark.presto.udf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.function.Description;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.function.ScalarFunction;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.function.SqlType;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.type.StandardTypes;</span><br><span class="line"><span class="keyword">import</span> io.airlift.slice.Slice;</span><br><span class="line"><span class="keyword">import</span> io.airlift.slice.Slices;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PrefixUDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Description</span>(<span class="string">"输入的数据加上前缀"</span>)      <span class="comment">//描述</span></span><br><span class="line">    <span class="meta">@ScalarFunction</span>(<span class="string">"tunan_prefix"</span>)       <span class="comment">//方法名称</span></span><br><span class="line">    <span class="meta">@SqlType</span>(StandardTypes.VARCHAR)       <span class="comment">//返回类型</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Slice <span class="title">prefix</span><span class="params">(@SqlType(StandardTypes.VARCHAR)</span>Slice input)</span>&#123;</span><br><span class="line">        <span class="comment">// Slices.utf8Slice 方法可以方便的将 String 类型转换成 Slice 类型</span></span><br><span class="line">        <span class="keyword">return</span> Slices.utf8Slice(<span class="string">"tunan_prefix_"</span>+input.toStringUtf8());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Presto插件机制"><a href="#Presto插件机制" class="headerlink" title="Presto插件机制"></a>Presto插件机制</h3><p>presto不能像hive那样配置自定义的udf，要采用这种插件机制实现。Presto 的插件(Plugin)机制，是 Presto 能够整合多种数据源的核心。通过实现不同的 Plugin，Presto 允许用户在不同类型的数据源之间进行 JOIN 等计算。Presto 内部的所有数据源都是通过插件机制实现， 例如 MySQL、Hive、HBase等。Presto 插件机制不仅通过加载 Connector 来实现不同数据源的访问，还通过加载 FunctionFactory 来实现 UDF 的加载。 Presto 的 Plugin 遵循 Java 中的 ServiceLoader 规范， 实现非常简单。</p><p>实现一个plugin接口如：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunna.spark.presto.udf;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.Plugin;</span><br><span class="line"><span class="keyword">import</span> com.google.common.collect.ImmutableSet;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Set;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProstoUDFPlugin</span> <span class="keyword">implements</span> <span class="title">Plugin</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Set&lt;Class&lt;?&gt;&gt; getFunctions() &#123;</span><br><span class="line">        <span class="keyword">return</span> ImmutableSet.&lt;Class&lt;?&gt;&gt;builder()</span><br><span class="line">                .add(PrefixUDF<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">                .<span class="title">build</span>()</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="注册函数"><a href="#注册函数" class="headerlink" title="注册函数"></a>注册函数</h3><p>在resources下创建META-INF/services目录，创建文件com.facebook.presto.spi.Plugin，拷贝Plugin的全限定名</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">com.tunna.spark.presto.udf.ProstoUDFPlugin</span><br></pre></td></tr></table></figure><p>最后在presto的plugin目录下创建我们自己的目录，并且打包上传到我们自己的目录下，需要重启presto才能将jar中的自定义函数加载进去，如果有多个依赖，都需要放在我们创建的目录下</p><h2 id="自定义Aggregation函数的实现"><a href="#自定义Aggregation函数的实现" class="headerlink" title="自定义Aggregation函数的实现"></a>自定义Aggregation函数的实现</h2><h3 id="实现原理步骤"><a href="#实现原理步骤" class="headerlink" title="实现原理步骤"></a>实现原理步骤</h3><p>Presto 把 Aggregation 函数分解成三个步骤执行：</p><ol><li><p>input(state, data): 针对每条数据，执行 input 函数。这个过程是并行执行的，因此在每个有数据的节点都会执行，最终得到多个累积的状态数据。</p></li><li><p>combine(state1, state2)：将所有节点的状态数据聚合起来，多次执行，直至所有状态数据被聚合成一个最终状态，也就是 Aggregation 函数的输出结果。</p></li><li><p>output(final_state, out)：最终输出结果到一个 BlockBuilder。</p></li></ol><h3 id="具体代码实现过程"><a href="#具体代码实现过程" class="headerlink" title="具体代码实现过程"></a>具体代码实现过程</h3><ol><li>一个继承AccumulatorState的State接口，自定义get和set方法</li><li>定义一个 Java 类，使用 @AggregationFunction 标记为 Aggregation 函数</li><li>使用 @InputFunction、 @CombineFunction、@OutputFunction 分别标记计算函数、合并结果函数和最终输出函数在 Plugin 处注册 Aggregation 函数</li></ol><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunna.spark.presto.aggudf;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.block.BlockBuilder;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.function.*;</span><br><span class="line"><span class="keyword">import</span> com.facebook.presto.spi.type.StandardTypes;</span><br><span class="line"><span class="keyword">import</span> io.airlift.slice.Slice;</span><br><span class="line"><span class="keyword">import</span> io.airlift.slice.Slices;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> com.facebook.presto.spi.type.VarcharType.VARCHAR;</span><br><span class="line"></span><br><span class="line"><span class="meta">@AggregationFunction</span>(<span class="string">"tunan_concat"</span>)    <span class="comment">// Agg方法名</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TunanAggregationUDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@InputFunction</span>  <span class="comment">//输入函数</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">input</span><span class="params">(StringValueState state,@SqlType(StandardTypes.VARCHAR)</span> Slice value)</span>&#123;</span><br><span class="line"></span><br><span class="line">        state.setStringValue(Slices.utf8Slice(isNull(state.getStringValue())+<span class="string">"|"</span>+value.toStringUtf8()));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@CombineFunction</span>    <span class="comment">//合并函数</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">combine</span><span class="params">(StringValueState state1,StringValueState state2)</span></span>&#123;</span><br><span class="line">        state1.setStringValue(Slices.utf8Slice(isNull(state1.getStringValue())+<span class="string">"|"</span>+isNull(state2.getStringValue())));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@OutputFunction</span>(StandardTypes.VARCHAR)  <span class="comment">//输出函数</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">output</span><span class="params">(StringValueState state, BlockBuilder builder)</span></span>&#123;</span><br><span class="line">        VARCHAR.writeSlice(builder,state.getStringValue());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断null值</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">isNull</span><span class="params">(Slice slice)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> slice ==<span class="keyword">null</span>?<span class="string">""</span>:slice.toStringUtf8();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="注册函数-1"><a href="#注册函数-1" class="headerlink" title="注册函数"></a>注册函数</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProstoUDFPlugin</span> <span class="keyword">implements</span> <span class="title">Plugin</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Set&lt;Class&lt;?&gt;&gt; getFunctions() &#123;</span><br><span class="line">        <span class="keyword">return</span> ImmutableSet.&lt;Class&lt;?&gt;&gt;builder()</span><br><span class="line">                .add(PrefixUDF<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">                .<span class="title">add</span>(<span class="title">TunanAggregationUDF</span>.<span class="title">class</span>) // 新加的</span></span><br><span class="line"><span class="class">                .<span class="title">build</span>()</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://yerias.github.io/presto/udf.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Presto </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Presto </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Presto部署</title>
      <link href="/2020/05/03/presto/2/"/>
      <url>/2020/05/03/presto/2/</url>
      
        <content type="html"><![CDATA[<h2 id="安装Presto"><a href="#安装Presto" class="headerlink" title="安装Presto"></a>安装Presto</h2><p>1.下载</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">wget</span> <span class="string">https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.200/presto-server-0.200.tar.gz</span></span><br></pre></td></tr></table></figure><p>官网下载最新版本: <a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-server" target="_blank" rel="noopener">点我进入官网下载</a> ，注意选择presto-server</p><p>2.解压</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">tar</span> <span class="string">-zxvf presto-server-0.200.tar.gz -C /usr/local/</span></span><br></pre></td></tr></table></figure><p>/usr/local/presto-server-0.200则为安装目录，另外Presto还需要数据目录，数据目录最好不要在安装目录里面，方便后面Presto的版本升级。</p><h2 id="配置Presto"><a href="#配置Presto" class="headerlink" title="配置Presto"></a>配置Presto</h2><p>在安装目录里创建etc目录。这目录会有以下配置：</p><ul><li>结点属性（Node Properties）：每个结点的环境配置</li><li>JVM配置（JVM Config）：Java虚拟机的命令行选项</li><li>配置属性（Config Properties）：Persto server的配置</li><li>Catelog属性（Catalog Properties）：配置Connector（数据源）</li></ul><h3 id="结点属性（Node-Properties）"><a href="#结点属性（Node-Properties）" class="headerlink" title="结点属性（Node Properties）"></a>结点属性（Node Properties）</h3><p>结点属性文件etc/node.properties，包含每个结点的配置。一个结点是一个Presto实例。这文件一般是在Presto第一次安装时创建的。以下是最小配置：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">node.environment</span>=<span class="string">production</span></span><br><span class="line"><span class="meta">node.id</span>=<span class="string">ffffffff-ffff-ffff-ffff-ffffffffffff</span></span><br><span class="line"><span class="meta">node.data-dir</span>=<span class="string">/var/presto/data</span></span><br></pre></td></tr></table></figure><p><code>node.environment</code>: 环境名字，Presto集群中的结点的环境名字都必须是一样的。<br><code>node.id</code>: 唯一标识，每个结点的标识都必须是为一的。就算重启或升级Presto都必须还保持原来的标识。<br><code>node.data-dir</code>: 数据目录，Presto用它来保存log和其他数据，<strong>建议放在自己定义的目录下</strong>。</p><h3 id="JVM配置（JVM-Config）"><a href="#JVM配置（JVM-Config）" class="headerlink" title="JVM配置（JVM Config）"></a>JVM配置（JVM Config）</h3><p>JVM配置文件etc/jvm.config，包含启动Java虚拟机时的命令行选项。格式是每一行是一个命令行选项。此文件数据是由shell解析，所以选项中包含空格或特殊字符会被忽略。</p><p>以下是参考配置：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">-server</span></span><br><span class="line"><span class="attr">-Xmx16G</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+UseG1GC</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">G1HeapRegionSize=32M</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+UseGCOverheadLimit</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+ExplicitGCInvokesConcurrent</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+HeapDumpOnOutOfMemoryError</span></span><br><span class="line"><span class="meta">-XX</span>:<span class="string">+ExitOnOutOfMemoryError</span></span><br></pre></td></tr></table></figure><p>注意：如果<code>ExitOnOutOfMemoryError</code>报错，注释即可</p><p>因为<code>OutOfMemoryError</code>会导致JVM存在不一致状态，所以用heap dump来debug，来找出进程为什么崩溃的原因。</p><h3 id="配置属性（Config-Properties）"><a href="#配置属性（Config-Properties）" class="headerlink" title="配置属性（Config Properties）"></a>配置属性（Config Properties）</h3><p>配置属性文件etc/config.properties，包含Presto server的配置。Presto server可以同时为coordinator和worker，但一个大集群里最好就是只指定一台机器为coordinator。<br>以下是coordinator的最小配置：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">coordinator</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">node-scheduler.include-coordinator</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">http-server.http.port</span>=<span class="string">8080</span></span><br><span class="line"><span class="meta">query.max-memory</span>=<span class="string">50GB</span></span><br><span class="line"><span class="meta">query.max-memory-per-node</span>=<span class="string">1GB</span></span><br><span class="line"><span class="meta">discovery-server.enabled</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">discovery.uri</span>=<span class="string">http://example.net:8080</span></span><br></pre></td></tr></table></figure><p>以下是worker的最小配置：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">coordinator</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">http-server.http.port</span>=<span class="string">8080</span></span><br><span class="line"><span class="meta">query.max-memory</span>=<span class="string">50GB</span></span><br><span class="line"><span class="meta">query.max-memory-per-node</span>=<span class="string">1GB</span></span><br><span class="line"><span class="meta">discovery.uri</span>=<span class="string">http://example.net:8080</span></span><br></pre></td></tr></table></figure><p>如果适用于测试目的，需要将一台机器同时配置为coordinator和worker，则使用以下配置：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">coordinator</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">node-scheduler.include-coordinator</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">http-server.http.port</span>=<span class="string">8080</span></span><br><span class="line"><span class="meta">query.max-memory</span>=<span class="string">2GB</span></span><br><span class="line"><span class="meta">query.max-memory-per-node</span>=<span class="string">512MB</span></span><br><span class="line"><span class="meta">discovery-server.enabled</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">discovery.uri</span>=<span class="string">http://example.net:8080</span></span><br></pre></td></tr></table></figure><p><code>coordinator</code>： 是否运行该实例为coordinator（接受client的查询和管理查询执行）。<br><code>node-scheduler.include-coordinator:coordinator</code>: 是否也作为work。对于大型集群来说，在coordinator里做worker的工作会影响查询性能。<br><code>http-server.http.port</code>:指定HTTP端口。Presto使用HTTP来与外部和内部进行交流。<br><code>query.max-memory</code>: 查询能用到的最大总内存<br><code>query.max-memory-per-node</code>: 查询能用到的最大单结点内存<br><code>discovery-server.enabled</code>: Presto使用Discovery服务去找到集群中的所有结点。每个Presto实例在启动时都会在Discovery服务里注册。这样可以简化部署，不需要额外的服务，Presto的coordinator内置一个Discovery服务。也是使用HTTP端口。<br><code>discovery.uri</code>: Discovery服务的URI。将example.net:8080替换为coordinator的host和端口。<strong>这个URI不能以斜杠结尾，这个错误需特别注意，不然会报404错误</strong>。</p><p>另外还有以下属性：<br><code>jmx.rmiregistry.port</code>: 指定JMX RMI的注册。JMX client可以连接此端口<br><code>jmx.rmiserver.port</code>: 指定JXM RMI的服务器。可通过JMX监听。</p><p>详情请查看<a href="https://prestodb.io/docs/current/admin/resource-groups.html" target="_blank" rel="noopener">Resource Groups</a></p><h3 id="Catelog属性（Catalog-Properties）"><a href="#Catelog属性（Catalog-Properties）" class="headerlink" title="Catelog属性（Catalog Properties）"></a>Catelog属性（Catalog Properties）</h3><p>Presto通过connector访问数据。而connector是挂载（mount）在catelog中。connector支持catelog里所有的schema和table。举个例子，Hive connector映射每个Hive数据库到schema，因此Hive connector挂载在hive catelog（所以可以把catelog理解为目录，挂载），而且Hive包含table clicks在数据库web，所以这个table在Presto是hive.web.clicks。<br>Catalog的注册是通过etc/catalog目录下的catalog属性文件。例如，创建etc/catalog/jmx.properties，将jmxconnector挂载在jmx catelog：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">connector.name</span>=<span class="string">jmx</span></span><br></pre></td></tr></table></figure><p>查看<a href="https://prestodb.io/docs/current/connector.html" target="_blank" rel="noopener">Connectors</a>查看更多信息。</p><p>启动命令：</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">bin/launcher</span> <span class="string">start</span></span><br></pre></td></tr></table></figure><p>日志在val/log目录下：<br><code>launcher.log</code>: 记录服务初始化情况和一些JVM的诊断。<br><code>server.log: Presto</code>: 的主要日志文件。会自动被压缩。<br><code>http-request.log</code>: 记录HTTP请求。会自动被压缩。</p><h2 id="配置MySQL-Connector"><a href="#配置MySQL-Connector" class="headerlink" title="配置MySQL Connector"></a>配置<a href="https://prestodb.io/docs/current/connector/mysql.html" target="_blank" rel="noopener">MySQL Connector</a></h2><p>创建<code>etc/catalog/mysql.properties</code></p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">connector.name</span>=<span class="string">mysql</span></span><br><span class="line"><span class="meta">connection-url</span>=<span class="string">jdbc:mysql://example.net:3306</span></span><br><span class="line"><span class="meta">connection-user</span>=<span class="string">root</span></span><br><span class="line"><span class="meta">connection-password</span>=<span class="string">secret</span></span><br></pre></td></tr></table></figure><h2 id="配置Hive-Connector"><a href="#配置Hive-Connector" class="headerlink" title="配置Hive Connector"></a>配置<a href="https://prestodb.io/docs/current/connector/hive.html" target="_blank" rel="noopener">Hive Connector</a></h2><p>创建<code>etc/catalog/hive.properties</code></p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">connector.name</span>=<span class="string">hive-hadoop2</span></span><br><span class="line"><span class="meta">hive.metastore.uri</span>=<span class="string">thrift://example.net:9083</span></span><br><span class="line"><span class="meta">hive.config.resources</span>=<span class="string">/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml</span></span><br></pre></td></tr></table></figure><p>还可以在<code>jvm.config</code>中Hadoop的代理用户</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">-DHADOOP_USER_NAME</span>=<span class="string">hdfs_user</span></span><br></pre></td></tr></table></figure><h2 id="运行Presto命令行界面"><a href="#运行Presto命令行界面" class="headerlink" title="运行Presto命令行界面"></a>运行Presto命令行界面</h2><p>1.下载 presto-cli-0.200-executable.jar(<a href="https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/" target="_blank" rel="noopener">下载最新版</a>),<br>2.修改名字 presto-cli-0.200-executable.jar为 presto<br>3.修改执行权限chmod +x<br>4.运行</p><p>指定catelog</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">./presto</span> <span class="string">--server localhost:8080 --catalog hive --schema default</span></span><br></pre></td></tr></table></figure><p>不指定catelog，可在命令行使用多个catalog</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">./presto</span> <span class="string">--server localhost:8080</span></span><br></pre></td></tr></table></figure><p>注意: JDK必须大于<code>jdk-8u151</code></p><p><strong>hive join mysql</strong></p><p><code>select * from hive.default.emp a join mysql.tunan.dept b on a.deptno = b.deptno;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"> empno | ename  |    job    | jno  |    date    |  sal   | prize  | deptno | deptno |   dname    | level </span><br><span class="line"><span class="comment">-------+--------+-----------+------+------------+--------+--------+--------+--------+------------+-------</span></span><br><span class="line"> 7369  | SMITH  | CLERK     | 7902 | 1980-12-17 |  800.0 | NULL   | 20     | 20     | RESEARCH   |  1800 </span><br><span class="line"> 7499  | ALLEN  | SALESMAN  | 7698 | 1981-2-20  | 1600.0 |  300.0 | 30     | 30     | SALES      |  1900 </span><br><span class="line"> 7521  | WARD   | SALESMAN  | 7698 | 1981-2-22  | 1250.0 |  500.0 | 10     | 10     | ACCOUNTING |  1700 </span><br><span class="line"> 7566  | JONES  | MANAGER   | 7839 | 1981-4-2   | 2975.0 | NULL   | 10     | 10     | ACCOUNTING |  1700 </span><br><span class="line"> 7654  | MARTIN | SALESMAN  | 7698 | 1981-9-28  | 1250.0 | 1400.0 | 30     | 30     | SALES      |  1900</span><br></pre></td></tr></table></figure><p>命令帮助: help</p>]]></content>
      
      
      <categories>
          
          <category> Presto </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Presto </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Presto扫盲</title>
      <link href="/2020/05/02/presto/1/"/>
      <url>/2020/05/02/presto/1/</url>
      
        <content type="html"><![CDATA[<h2 id="Presto简介"><a href="#Presto简介" class="headerlink" title="Presto简介"></a>Presto简介</h2><h3 id="不是什么"><a href="#不是什么" class="headerlink" title="不是什么"></a>不是什么</h3><p>虽然Presto可以解析SQL，但它不是一个标准的数据库。不是MySQL、PostgreSQL或者Oracle的代替品，也不能用来处理在线事务（OLTP）</p><h3 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h3><p>Presto通过使用分布式查询，可以快速高效的完成海量数据的查询。作为Hive和Pig的补充，Presto不仅能访问HDFS，也能访问不同的数据源，包括：RDBMS和其他数据源（如Cassandra）。</p><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><img src="https://yerias.github.io/presto/20200502161939.jpg" alt=""></p><p>图中各个组件的概念及作用会在下文讲述。</p><h3 id="Presto中SQL运行过程：MapReduce-vs-Presto"><a href="#Presto中SQL运行过程：MapReduce-vs-Presto" class="headerlink" title="Presto中SQL运行过程：MapReduce vs Presto"></a>Presto中SQL运行过程：MapReduce vs Presto</h3><p><img src="https://yerias.github.io/presto/3280890894-5af69697e1249_articlex.png" alt=""></p><p>使用内存计算，减少与硬盘交互。</p><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol><li>Presto与hive对比，都能够处理PB级别的海量数据分析，但Presto是基于内存运算，减少没必要的硬盘IO，所以更快。</li><li>能够连接多个数据源，跨数据源连表查，如从hive查询大量网站访问记录，然后从mysql中匹配出设备信息。</li><li>部署也比hive简单，因为hive是基于HDFS的，需要先部署HDFS。</li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li>虽然能够处理PB级别的海量数据分析，但不是代表Presto把PB级别都放在内存中计算的。而是根据场景，如count，avg等聚合运算，是边读数据边计算，再清内存，再读数据再计算，这种耗的内存并不高。但是连表查，就可能产生大量的临时数据，因此速度会变慢，反而hive此时会更擅长。</li><li>为了达到实时查询，可能会想到用它直连MySQL来操作查询，这效率并不会提升，瓶颈依然在MySQL，此时还引入网络瓶颈，所以会比原本直接操作数据库要慢。</li></ol><h2 id="Presto概念"><a href="#Presto概念" class="headerlink" title="Presto概念"></a>Presto概念</h2><h3 id="服务器类型（Server-Types）"><a href="#服务器类型（Server-Types）" class="headerlink" title="服务器类型（Server Types）"></a>服务器类型（Server Types）</h3><p>Presto有两类服务器：coordinator和worker。</p><h4 id="Coordinator"><a href="#Coordinator" class="headerlink" title="Coordinator"></a>Coordinator</h4><p>Coordinator服务器是用来解析语句，执行计划分析和管理Presto的worker结点。Presto安装必须有一个Coordinator和多个worker。如果用于开发环境和测试，则一个Presto实例可以同时担任这两个角色。</p><p>Coordinator跟踪每个work的活动情况并协调查询语句的执行。 Coordinator为每个查询建立模型，模型包含多个stage，每个stage再转为task分发到不同的worker上执行。</p><p>Coordinator与Worker、client通信是通过REST API。</p><h4 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h4><p>Worker是负责执行任务和处理数据。Worker从connector获取数据。Worker之间会交换中间数据。Coordinator是负责从Worker获取结果并返回最终结果给client。</p><p>当Worker启动时，会广播自己去发现 Coordinator，并告知 Coordinator它是可用，随时可以接受task。</p><p>Worker与Coordinator、Worker通信是通过REST API。</p><h3 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h3><p>贯穿全文，你会看到一些术语：connector、catelog、schema和table。这些是Presto特定的数据源</p><h4 id="Connector"><a href="#Connector" class="headerlink" title="Connector"></a>Connector</h4><p>Connector是适配器，用于Presto和数据源（如Hive、RDBMS）的连接。你可以认为类似JDBC那样，但却是Presto的SPI的实现，使用标准的API来与不同的数据源交互。</p><p>Presto有几个内建Connector：JMX的Connector、System Connector（用于访问内建的System table）、Hive的Connector、TPCH（用于TPC-H基准数据）。还有很多第三方的Connector，所以Presto可以访问不同数据源的数据。</p><p>每个catalog都有一个特定的Connector。如果你使用catelog配置文件，你会发现每个文件都必须包含connector.name属性，用于指定catelog管理器（创建特定的Connector使用）。一个或多个catelog用同样的connector是访问同样的数据库。例如，你有两个Hive集群。你可以在一个Presto集群上配置两个catelog，两个catelog都是用Hive Connector，从而达到可以查询两个Hive集群。</p><h4 id="Catelog"><a href="#Catelog" class="headerlink" title="Catelog"></a>Catelog</h4><p>一个Catelog包含Schema和Connector。例如，你配置JMX的catelog，通过JXM Connector访问JXM信息。当你执行一条SQL语句时，可以同时运行在多个catelog。</p><p>Presto处理table时，是通过表的完全限定（fully-qualified）名来找到catelog。例如，一个表的权限定名是hive.test_data.test，则test是表名，test_data是schema，hive是catelog。</p><p>Catelog的定义文件是在Presto的配置目录中。</p><h4 id="Schema"><a href="#Schema" class="headerlink" title="Schema"></a>Schema</h4><p>Schema是用于组织table。把catelog好schema结合在一起来包含一组的表。当通过Presto访问hive或Mysq时，一个schema会同时转为hive和mysql的同等概念。</p><h4 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h4><p>Table跟关系型的表定义一样，但数据和表的映射是交给Connector。</p><h3 id="执行查询的模型（Query-Execution-Model）"><a href="#执行查询的模型（Query-Execution-Model）" class="headerlink" title="执行查询的模型（Query Execution Model）"></a>执行查询的模型（Query Execution Model）</h3><h4 id="语句（Statement）"><a href="#语句（Statement）" class="headerlink" title="语句（Statement）"></a>语句（Statement）</h4><p>Presto执行ANSI兼容的SQL语句。当Presto提起语句时，指的就是ANSI标准的SQL语句，包含着列名、表达式和谓词。</p><p>之所以要把语句和查询分开说，是因为Presto里，语句只是简单的文本SQL语句。而当语句执行时，Presto则会创建查询和分布式查询计划并在Worker上运行。</p><h4 id="查询（Query）"><a href="#查询（Query）" class="headerlink" title="查询（Query）"></a>查询（Query）</h4><p>当Presto解析一个语句时，它将其转换为一个查询，并创建一个分布式查询计划（多个互信连接的stage，运行在Worker上）。如果想获取Presto的查询情况，则获取每个组件（正在执行这语句的结点）的快照。</p><p>查询和语句的区别是，语句是存SQL文本，而查询是配置和实例化的组件。一个查询包含：stage、task、split、connector、其他组件和数据源。</p><h4 id="Stage"><a href="#Stage" class="headerlink" title="Stage"></a>Stage</h4><p>当Presto执行查询时，会将执行拆分为有层次结构的stage。例如，从hive中的10亿行数据中聚合数据，此时会创建一个用于聚合的根stage，用于聚合其他stage的数据。</p><p>层次结构的stage类似一棵树。每个查询都由一个根stage，用于聚合其他stage的数据。stage是Coordinator的分布式查询计划（distributed query plan）的模型，stage不是在worker上运行。</p><h4 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h4><p>由于stage不是在worker上运行。stage又会被分为多个task，在不同的work上执行。<br>Task是Presto结构里是“work horse”。一个分布式查询计划会被拆分为多个stage，并再转为task，然后task就运行或处理split。Task有输入和输出，一个stage可以分为多个并行执行的task，一个task可以分为多个并行执行的driver。</p><h4 id="Split"><a href="#Split" class="headerlink" title="Split"></a>Split</h4><p>Task运行在split上。split是一个大数据集合中的一块。分布式查询计划最底层的stage是通过split从connector上获取数据，分布式查询计划中间层或顶层则是从它们下层的stage获取数据。</p><p>Presto调度查询，coordinator跟踪每个机器运行什么任务，那些split正在被处理。</p><h4 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h4><p>Task包含一个或多个并行的driver。Driver在数据上处理，并生成输出，然后由Task聚合，最后传送给stage的其他task。一个driver是Operator的序列。driver是Presto最最低层的并行机制。一个driver有一个输出和一个输入。</p><h4 id="Operator"><a href="#Operator" class="headerlink" title="Operator"></a>Operator</h4><p>Operator用来消费，传送和生产数据。如一个Operator从connector中扫表获取数据，然后生产数据给其他Operator消费。一个过滤Operator消费数据，并应用谓词，最后生产出子集数据。</p><h4 id="Exchange"><a href="#Exchange" class="headerlink" title="Exchange"></a>Exchange</h4><p>Exchange在Presto结点的不同stage之间传送数据。Task生产和消费数据是通过Exchange客户端。</p><hr><p>参考：<a href="https://prestodb.io/docs/current/overview/concepts.html" target="_blank" rel="noopener">https://prestodb.io/docs/current/overview/concepts.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Presto </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Presto </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FileNotFountException: file:/home/hadoop/lib/tunan-spark-core-1.0.jar!/ip2region.db</title>
      <link href="/2020/04/28/error/8/"/>
      <url>/2020/04/28/error/8/</url>
      
        <content type="html"><![CDATA[<p>以后看到标题这种Error，先别管其他的，首先看看代码中有没有把Master注释掉，不然jar包中的文件永远到不了服务器的环境中去，就算把文件在服务器上的路径写死都没用。</p><p>由于Spark不会自动清理–files和–jars传到服务器中的文件，因此只要我们传上去的jar包运行通一次，后面不管代码中有没有指定Master，都能找到服务器中的文件。</p><p>报错图示1：</p><p><img src="https://yerias.github.io/error/notfount1.png" alt="notfount1"></p><p>报错图示2：</p><p><img src="https://yerias.github.io/error/notfount2.png" alt="notfount2"></p>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>error: object hadoop is not a member of packee com</title>
      <link href="/2020/04/27/error/7/"/>
      <url>/2020/04/27/error/7/</url>
      
        <content type="html"><![CDATA[<p>这个问题是在Spark读取Lzo压缩文件的时候碰见的，Spark读取Lzo压缩文件的时候，就算文件添加了索引，也不能分片，原因是要在获取文件的时候使用newAPIHadoopFile算子读取文件获取rdd</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.newAPIHadoopFile(in, classOf[<span class="type">LzoTextInputFormat</span>], classOf[<span class="type">LongWritable</span>],</span><br><span class="line">                              classOf[<span class="type">Text</span>]).map(x =&gt; x._2.toString)</span><br></pre></td></tr></table></figure><p>而这里最重要的就是LzoTextInputFormat类，这个类是Twitter的，但是添加了Twitter<a href="https://maven.twttr.com/" target="_blank" rel="noopener">仓库</a>后，能进仓库，但是不能下载，最后向朋友要了jar包，关键在于只给了我hadoop-lzo-0.4.20.jar，我通过添加外部依赖的方式，加到了项目里，运行的时候就报了error: object hadoop is not a member of packee com的错误</p><p><img src="E:%5Chexo%5Cyeriasblog%5Cthemes%5Cmelody%5Csource%5Cerror%5Chadoop-lzo.png" alt="hadoop-lzo"></p><p>其原因是只有jar包，没有pom文件，最后将jar包和pom文件一起放入maven仓库中，解决问题</p><hr><p><strong>终极解决办法是在github上下载源码，通过编译maven install到本地仓库</strong></p>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introducing Window Functions in Spark SQL</title>
      <link href="/2020/04/26/%E7%BF%BB%E8%AF%91/2/"/>
      <url>/2020/04/26/%E7%BF%BB%E8%AF%91/2/</url>
      
        <content type="html"><![CDATA[<p>原文：<a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="noopener">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a></p><hr><p>在这篇博客文章中，我们将介绍Apache Spark 1.4中添加的新窗口函数特性。窗口函数允许Spark SQL的用户计算结果，比如给定行的秩或输入行的范围内的移动平均值。它们显著提高了Spark的SQL和DataFrame api的表达能力。本博客将首先介绍窗口函数的概念，然后讨论如何与Spark SQL和Spark的DataFrame API一起使用它们。</p><h2 id="什么是窗口函数"><a href="#什么是窗口函数" class="headerlink" title="什么是窗口函数?"></a>什么是窗口函数?</h2><p>在1.4之前，Spark SQL支持两种函数，可用于计算单个返回值。内置函数或udf(如substr或round)将单个行中的值作为输入，并为每个输入行生成单个返回值。聚合函数(如SUM或MAX)对一组行进行操作，并为每个组计算单个返回值。</p><p>虽然这两种方法在实践中都非常有用，但是仍然有大量的操作不能单独使用这些类型的函数来表示。具体来说，无法同时对一组行进行操作，同时仍然为每个输入行返回一个值。这种限制使得执行各种数据处理任务(如计算移动平均值、计算累计和或访问当前行之前的行值)变得非常困难。幸运的是，对于Spark SQL的用户来说，窗口函数填补了这一空白。</p><p>在其核心，一个窗口函数根据一组行(称为Frame)为表的每个输入行计算一个返回值。每个输入行都可以有一个与之关联的唯一frame。窗口函数的这种特性使它们比其他函数更强大，并允许用户以简洁的方式表达各种数据处理任务，而这些任务如果没有窗口函数是很难(如果不是不可能)表达的。现在，让我们看两个例子。</p><p>假设我们有一个如下所示的productRevenue表。</p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/1-1.png" alt="1-1"></p><p>我们想回答两个问题:</p><ol><li><p>每一类中最畅销和第二畅销的产品是什么(<strong>分组top n</strong>)?</p></li><li><p>每种产品的收入和同类产品中最畅销的产品的收入有什么不同(<strong>最大值 - 当前值</strong>)?</p></li></ol><p>回答第一个问题“在每个类别中，最畅销和第二畅销的产品是什么?”，我们需要根据产品的收入对其进行分类，并根据排名选择最畅销和第二畅销的产品。下面是通过使用窗口函数dense_rank来回答这个问题的SQL查询(我们将在下一节中解释使用窗口函数的语法)。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  product,</span><br><span class="line">  <span class="keyword">category</span>,</span><br><span class="line">  revenue</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">        product,</span><br><span class="line">        <span class="keyword">category</span>,</span><br><span class="line">        revenue,</span><br><span class="line">        <span class="keyword">dense_rank</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">category</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> revenue <span class="keyword">DESC</span>) <span class="keyword">as</span> <span class="keyword">rank</span></span><br><span class="line">    <span class="keyword">FROM</span> productRevenue) tmp</span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">  <span class="keyword">rank</span> &lt;= <span class="number">2</span></span><br></pre></td></tr></table></figure><p>这个查询的结果如下所示。如果不使用窗口函数，就很难用SQL表达查询，即使可以表达SQL查询，底层引擎也很难有效地评估查询。</p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/1-2.png" alt="1-2"></p><p>对于第二个问题，“每种产品的收入与同类产品中最畅销的产品的收入有什么不同?”，要计算一个产品的收入差异，我们需要找到每个产品在相同类别下的最高收入价值。下面是一个用于回答这个问题的Python DataFrame程序(<strong>python代码不重要，看思路</strong>)。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.window <span class="keyword">import</span> Window</span><br><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> func</span><br><span class="line">windowSpec = \</span><br><span class="line">  Window <span class="comment"># 定义开窗函数</span></span><br><span class="line">    .partitionBy(df[<span class="string">'category'</span>]) \   <span class="comment"># 分组</span></span><br><span class="line">    .orderBy(df[<span class="string">'revenue'</span>].desc()) \<span class="comment"># 排序</span></span><br><span class="line">    .rangeBetween(-sys.maxsize, sys.maxsize)  <span class="comment"># 拿到最大值</span></span><br><span class="line">dataFrame = sqlContext.table(<span class="string">"productRevenue"</span>) <span class="comment"># 拿到表</span></span><br><span class="line">revenue_difference = \</span><br><span class="line">  (func.max(dataFrame[<span class="string">'revenue'</span>]).over(windowSpec) - dataFrame[<span class="string">'revenue'</span>]) <span class="comment"># 使用开窗函数</span></span><br><span class="line">dataFrame.select(     <span class="comment"># 查询</span></span><br><span class="line">  dataFrame[<span class="string">'product'</span>],</span><br><span class="line">  dataFrame[<span class="string">'category'</span>],</span><br><span class="line">  dataFrame[<span class="string">'revenue'</span>],</span><br><span class="line">  revenue_difference.alias(<span class="string">"revenue_difference"</span>))   <span class="comment"># 给个别名</span></span><br></pre></td></tr></table></figure><p>这个程序的结果如下所示。在不使用窗口函数的情况下，用户必须找到所有类别的所有最高收入值，然后将这个派生的数据集与原始的productRevenue表连接起来，以计算收入差异。</p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/1-3.png" alt="1-3"></p><h2 id="使用窗口函数"><a href="#使用窗口函数" class="headerlink" title="使用窗口函数"></a>使用窗口函数</h2><p>Spark SQL支持三种窗口函数:排序函数、分析函数和聚合函数。可用的排序函数和分析函数总结如下表所示。对于聚合函数，用户可以使用任何现有的聚合函数作为窗口函数。</p><table><thead><tr><th></th><th><strong>SQL</strong></th><th><strong>DataFrame API</strong></th></tr></thead><tbody><tr><td><strong>Ranking functions</strong></td><td>rank</td><td>rank</td></tr><tr><td></td><td>dense_rank</td><td>denseRank</td></tr><tr><td></td><td>percent_rank</td><td>percentRank</td></tr><tr><td></td><td>ntile</td><td>ntile</td></tr><tr><td></td><td>row_number</td><td>rowNumber</td></tr><tr><td><strong>Analytic functions</strong></td><td>cume_dist</td><td>cumeDist</td></tr><tr><td></td><td>first_value</td><td>firstValue</td></tr><tr><td></td><td>last_value</td><td>lastValue</td></tr><tr><td></td><td>lag</td><td>lag</td></tr><tr><td></td><td>lead</td><td>lead</td></tr></tbody></table><p>要使用窗口函数，用户需要标记一个函数被任意一方用作窗口函数</p><ol><li>在SQL中支持的函数后添加OVER子句，例如<code>avg(revenue) OVER (...)</code>; </li><li>调用DataFrame API中支持的函数上的over方法，例如<code>rank().over(...)</code></li></ol><p>一旦一个函数被标记为一个窗口函数，下一个关键步骤就是定义与这个函数相关的窗口规范。窗口规范定义在与给定输入行关联的frame中包含哪些行。一个窗口规范包括三个部分:</p><ul><li><p>分区规范:控制哪些行将与给定行位于同一分区中。此外，在订购和计算frame之前，用户可能希望确保将category列具有相同值的所有行收集到相同的机器上。如果没有给出分区规范，那么所有数据必须收集到一台机器上。</p></li><li><p>排序规范:控制分区中的行排序的方式，确定给定行在其分区中的位置。</p></li><li><p>Frame规范:根据当前输入行的相对位置，声明当前输入行的frame中包含哪些行。例如，“当前行之前的三行到当前行”描述了一个frame，其中包括当前输入行和出现在当前行之前的三行。</p></li></ul><p>在SQL中， <code>PARTITION BY</code> 和 <code>ORDER BY</code> 关键字分别用于为分区规范指定分区表达式和为排序规范指定排序表达式。SQL语法如下所示。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">OVER (PARTITION BY ... ORDER BY ...)</span><br></pre></td></tr></table></figure><p>在DataFrame API中，我们提供了实用程序函数来定义窗口规范。以Python为例，用户可以按如下方式指定分区表达式和排序表达式。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.window <span class="keyword">import</span> Window</span><br><span class="line">windowSpec = \</span><br><span class="line">  Window \<span class="comment"># 定义窗口</span></span><br><span class="line">    .partitionBy(...) \  <span class="comment"># 分区</span></span><br><span class="line">    .orderBy(...)  <span class="comment"># 排序</span></span><br></pre></td></tr></table></figure><p>除了排序和分区之外，用户还需要定义frame的开始边界、frame的结束边界和frame的类型，这是frame规范的三个组成部分。</p><p>边界有<code>UNBOUNDED PRECEDING</code>, <code>UNBOUNDED FOLLOWING</code>, <code>CURRENT ROW</code>, <code>PRECEDING</code>,和<code>&lt;value&gt; FOLLOWING</code>五种类型。 <code>&lt;value&gt; FOLLOWING</code>. <code>UNBOUNDED PRECEDING</code> and <code>UNBOUNDED FOLLOWING</code>分别表示分区的第一行和最后一行。对于其他三种类型的边界，它们指定当前输入行的位置偏移量，并根据框架的类型定义它们的特定含义。有两种类型的frame，<em>ROW</em> frame 和<em>RANGE</em> frame.</p><h3 id="ROW-frame"><a href="#ROW-frame" class="headerlink" title="ROW frame"></a>ROW frame</h3><p>ROW frame是基于当前输入行位置的物理偏移量，也就是说<code>CURRENT ROW</code>, <code>&lt;value&gt; PRECEDING</code>, or <code>&lt;value&gt; FOLLOWING</code>指定物理偏移.如果使用<code>CURRENT ROW</code>作为边界，它表示当前输入行。 <code>PRECEDING</code> and <code>FOLLOWING</code>分别描述当前输入行之前和之后出现的行数。</p><p>下图演示了一个行frame， <code>1 PRECEDING</code>作为开始边界， <code>1 FOLLOWING</code> 作为结束边界(在SQL中表现为<code>ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING</code>)</p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/2-1-1024x338.png" alt="2-1-1024x338"></p><h3 id="RANGE-frame"><a href="#RANGE-frame" class="headerlink" title="RANGE frame"></a>RANGE frame</h3><p>RANGE frames基于当前输入行位置的逻辑偏移量，语法与ROW frame类似。逻辑偏移量是当前输入行的排序表达式的值与frame的边界行相同表达式的值之间的差。由于这个定义，当使用RANGE frame时，只允许一个排序表达式。此外，对于RANGE frame，对于边界计算而言，具有与当前输入行相同的排序表达式值的所有行都被认为是相同的行。</p><p>现在，让我们看一个例子。在本例中，排序表达式是revenue;开始边界是 <code>2000 PRECEDING</code>;结束边界是<code>1000 FOLLOWING</code>，(这个frame被在SQL中被定义为<code>RANGE BETWEEN 2000 PRECEDING AND 1000 FOLLOWING</code>)，下面的五幅图说明了如何使用当前输入行的更新来更新frame。基本上，对于每一个当前的输入行，基于收入值，我们计算收入范围<code>[current revenue value - 2000, current revenue value + 1000]</code>。收入值在此范围内的所有行都位于当前输入行的frame中。</p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/2-2-1024x369.png" alt="2-2-1024x369"></p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/2-3-1024x263.png" alt="2-3-1024x263"></p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/2-4-1024x263.png" alt="2-4-1024x263"></p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/2-5-1024x263.png" alt="2-5-1024x263"></p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/2-6-1024x263.png" alt="2-6-1024x263"></p><p>总之，要定义窗口规范，用户可以在SQL中使用以下语法。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">OVER (PARTITION BY ... ORDER BY ... frame_type BETWEEN <span class="keyword">start</span> <span class="keyword">AND</span> <span class="keyword">end</span>)</span><br></pre></td></tr></table></figure><p>在这里，<code>frame_type</code>可以是行(对于ROW frame)或范围(对于 RANGE frame);</p><p>都可以使用 <code>UNBOUNDED PRECEDING</code>, <code>CURRENT ROW</code>, <code>&lt;value&gt; PRECEDING</code>, and <code>&lt;value&gt; FOLLOWING</code>其中的任意一个作为开始;  <code>UNBOUNDED FOLLOWING</code>, <code>CURRENT ROW</code>, <code>&lt;value&gt; PRECEDING</code>, and<code>&lt;value&gt; FOLLOWING</code>其中的任意一个作为结束.</p><p>在Python DataFrame API中，用户可以定义如下的窗口规范。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.window <span class="keyword">import</span> Window</span><br><span class="line"><span class="comment"># Defines partitioning specification and ordering specification.</span></span><br><span class="line">windowSpec = \</span><br><span class="line">  Window \</span><br><span class="line">    .partitionBy(...) \</span><br><span class="line">    .orderBy(...)</span><br><span class="line"><span class="comment"># Defines a Window Specification with a ROW frame.</span></span><br><span class="line">windowSpec.rowsBetween(start, end)</span><br><span class="line"><span class="comment"># Defines a Window Specification with a RANGE frame.</span></span><br><span class="line">windowSpec.rangeBetween(start, end)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 原文翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 原文翻译 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ORCFile in HDP 2: Better Compression, Better Performance</title>
      <link href="/2020/04/26/%E7%BF%BB%E8%AF%91/1/"/>
      <url>/2020/04/26/%E7%BF%BB%E8%AF%91/1/</url>
      
        <content type="html"><![CDATA[<p>原文：<a href="https://blog.cloudera.com/orcfile-in-hdp-2-better-compression-better-performance/" target="_blank" rel="noopener">https://blog.cloudera.com/orcfile-in-hdp-2-better-compression-better-performance/</a></p><hr><p>即将发布的Hive 0.12将在存储层带来一些新的重大改进，包括更高的压缩和更好的查询性能。</p><h2 id="高压缩"><a href="#高压缩" class="headerlink" title="高压缩"></a>高压缩</h2><p>ORCFile是在Hive 0.11中引入的，提供了很好的压缩，通过一些技术来实现，包括运行长度编码、字符串字典编码和位图编码。</p><p>这种对效率的关注导致了一些令人印象深刻的压缩比。这张图片显示了TPC-DS数据集在不同编码下的规模为500。该数据集包含随机生成的数据，包括字符串、浮点数和整数数据。</p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/ORCFile.png" alt="ORCFile"></p><p>我们已经看到，有些客户的集群已经从存储的角度扩展到了ORCFile，这是一种释放空间的方法，同时与现有的作业100%兼容。</p><p>存储在ORCFile中的数据可以通过HCatalog读取或写入，因此任何Pig或Map/Reduce进程都可以无缝地运行。Hive 12 建立在这些令人印象深刻的压缩比上，并在Hive和执行层提供深度集成来加速查询，从处理更大的数据集和更低的延迟的角度来看都是如此。</p><h2 id="谓词下推"><a href="#谓词下推" class="headerlink" title="谓词下推"></a>谓词下推</h2><p>SQL查询通常会有一些WHERE条件，可以用来方便地排除需要考虑的行。在旧版本的Hive中，在稍后通过SQL处理消除之前，将行从存储层中读取出来。有很多浪费的开销，Hive 12通过允许将谓词下推并在存储层本身进行计算来优化这一点。它是由环境控制的<code>hive.optimize.ppd=true</code>.</p><p>这需要读者具有足够的智慧来理解谓词。幸运的是，ORC已经进行了相应的改进，允许将谓词推入其中，并利用其内联索引提供性能优势。</p><p>例如，如果你有一个SQL查询:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(*) <span class="keyword">FROM</span> CUSTOMER <span class="keyword">WHERE</span> CUSTOMER.state = ‘CA’;</span><br></pre></td></tr></table></figure><p>ORCFile只返回与WHERE谓词实际匹配的行，而跳过驻留在任何其他状态的客户。从表中读取的列越多，要避免的数据封送处理就越多，速度也就越快。</p><h2 id="ORCFile内联索引"><a href="#ORCFile内联索引" class="headerlink" title="ORCFile内联索引"></a>ORCFile内联索引</h2><p>在进入下一节之前，我们需要花一点时间讨论ORCFile如何将行分解为行组，并在这些行组中应用列式压缩和索引。</p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/orc2.png" alt="orc2"></p><p>ORC的谓词下推将参考内联索引，以尝试确定何时可以一次性跳过整个块。有时，您的数据集自然会促进这一点。例如，如果您的数据是一个时间序列，并且时间戳是单调递增的，那么当您在这个时间戳上加上where条件时，ORC将能够跳过许多行组。</p><p>在其他情况下，可能需要对数据进行排序。如果对一列进行排序，相关记录将被限制在磁盘上的一个区域，其他部分将很快被跳过。</p><p>对于数字类型和字符串类型，跳过可以工作。在这两个实例中，都是通过在内联索引中记录一个最小值和一个最大值并确定查找值是否落在这个范围之外来完成的。</p><p>排序可以导致非常好的加速。这里有一个权衡，您需要提前决定对哪些列排序。决策过程在某种程度上类似于决定在传统SQL系统中索引哪些列。最好的回报是您拥有一个经常使用的列，并且在非常特定的条件下访问它，并且在很多查询中使用它。请记住，您可以在创建表时设置hive.enforce.sorting=true并使用SORT BY关键字强制Hive对列进行排序。</p><p>ORCFile是我们的一个重要的刺激举措，以提高Hive性能100倍。为了显示影响，我们使用修改后的数据模式运行修改后的TPC-DS查询27。查询27在一个大型事实表上执行星型模式连接，访问4个独立的维度表。在修改后的模式中，销售的状态被反规范化为事实表，结果表按状态排序。通过这种方式，当查询扫描事实表时，它可以跳过整个行块，因为查询根据状态进行筛选。这导致了一些增量加速，正如您可以从下面的图表中看到的。</p><p><img src="https://yerias.github.io/%E7%BF%BB%E8%AF%91/orc1.png" alt="orc1"></p><p>这个功能给你最好的性价比时:</p><ol><li><p>在具有中等到较大基数的列上，您经常以一种精确的方式过滤大型事实表。</p></li><li><p>您可以选择大量的列，或者宽列。您保存的数据封送处理越多，您的加速就越大。</p></li></ol><h2 id="使用ORCFile"><a href="#使用ORCFile" class="headerlink" title="使用ORCFile"></a>使用ORCFile</h2><p>使用ORCFile或将现有数据转换为ORCFile非常简单。要使用它，只需添加存储为orc到您的create table语句的结尾，如下所示:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> mytable (</span><br><span class="line">...</span><br><span class="line">) <span class="keyword">STORED</span> <span class="keyword">AS</span> orc;</span><br></pre></td></tr></table></figure><p>若要将现有数据转换为ORCFile，请创建一个与源表具有相同模式并存储为orc的表，然后可以使用如下查询:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> orctable <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> oldtable;</span><br></pre></td></tr></table></figure><p>Hive将处理所有细节的ORCFile转换，你可以自由删除旧表，以释放大量的空间。</p><p>创建ORC表时，可以使用许多表属性进一步优化ORC的工作方式。</p><table><thead><tr><th><strong>Key</strong></th><th><strong>Default</strong></th><th><strong>Notes</strong></th></tr></thead><tbody><tr><td><code>orc.compress</code></td><td><code>ZLIB</code></td><td>压缩除使用列式压缩外，还可以使用NONE, ZLIB, SNAPPY</td></tr><tr><td><code>orc.compress.size</code></td><td><code>262,144 (= 256 KiB)</code></td><td>每个压缩块中的字节数</td></tr><tr><td>orc.stripe.size</td><td><code>268,435,456 (= 256 MiB)</code></td><td>每条数据中的字节数</td></tr><tr><td><code>orc.row.index.stride</code></td><td><code>10,000</code></td><td>索引项之间的行数 (必须 &gt;= 1,000)</td></tr><tr><td><code>orc.create.index</code></td><td><code>true</code></td><td>是否创建内联索引</td></tr></tbody></table><p>例如，假设您想使用snappy压缩而不是zlib压缩。方法如下:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> mytable (</span><br><span class="line">...</span><br><span class="line">) <span class="keyword">STORED</span> <span class="keyword">AS</span> orc tblproperties (<span class="string">"orc.compress"</span>=<span class="string">"SNAPPY"</span>);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 原文翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 原文翻译 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka batch相关&amp;partition数据迁移&amp;删除topic后遗症</title>
      <link href="/2020/04/25/kafka/5/"/>
      <url>/2020/04/25/kafka/5/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>Kafka batch相关</li><li>partition数据迁移</li><li>删除topic后遗症</li></ol><h2 id="Kafka-batch相关"><a href="#Kafka-batch相关" class="headerlink" title="Kafka batch相关"></a>Kafka batch相关</h2><p>在创建topic的时候需要设置两个值，分别是partitions数量和replication-factor副本数量，这两个参数分别代表kafka的吞吐量和设定副本数维护Kafka的可靠性。</p><p>Partitions的数量根据公司的能力设定，最少3个。</p><p>场景:</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">1250 * 8 =1W #ss的速率 * 分区</span><br><span class="line">2500 * 8 =2w#调整后的ss的速率 * 分区</span><br></pre></td></tr></table></figure><p>Kafka的消费速率往往通过两个角度来调整的，一个是设置的ss消费速率，一个是设置kafka的分区数。</p><p>kafka的分区数和ss分区数 1:1 对应，需要确定的是调整后的速率不能产生处理时间的延迟，这个延迟跟ss的消费时间有关，ss的固定时间内能处理多少数据量是有限的。</p><p>案例:</p><p>如何每个batch时间原先为3s，增加速率后变为5s，产生了2s延迟。下一个批次继续延迟2s，下下个批次继续延迟2s，最终产生雪崩的效应。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="number">3</span>s : <span class="number">3</span> * <span class="number">1250</span> * <span class="number">8</span> = <span class="number">3</span>W  经验值 稳定值  经历过长时间验证的 </span><br><span class="line"><span class="number">3</span>s : <span class="number">3</span> * <span class="number">2500</span> * <span class="number">8</span> = <span class="number">6</span>w  夯住了</span><br></pre></td></tr></table></figure><h2 id="Partition数据迁移"><a href="#Partition数据迁移" class="headerlink" title="Partition数据迁移"></a>Partition数据迁移</h2><p><a href="http://kafka.apache.org/documentation/#basic_ops_automigrate" target="_blank" rel="noopener">官网地址</a>，切记: 在创建topic的时候，好好想想设置多少个Partition。</p><p>分区重新分配工具可用于将某些主题从当前代理集移到新添加的代理。这在扩展现有集群时通常很有用，因为与一次移动一个分区相比，将整个主题移至新的代理集更容易。用于执行此操作时，用户应提供应移至新代理集的主题列表和新代理的目标列表。然后，该工具将给定主题列表中的所有分区均匀分布在新的一组代理中。在此过程中，主题的复制因子保持不变。有效地，将输入主题列表的所有分区的副本从旧的代理集移至新添加的代理。</p><p>例如，以下示例将主题foo1，foo2的所有分区移动到新的代理人5,6的集合。在此步骤结束时，主题foo1和foo2的所有分区仅存在于代理5,6上。</p><p>由于该工具将主题的输入列表作为json文件接受，因此您首先需要确定要移动的主题并按以下方式创建json文件：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&gt; ``cat` `topics-to-move.json``&#123;``"topics"``: [&#123;``"topic"``: ``"foo1"``&#125;,``      ``&#123;``"topic"``: ``"foo2"``&#125;],``"version"``:1``&#125;</span><br></pre></td></tr></table></figure><p>JSON文件准备好后，请使用分区重新分配工具生成候选分配：</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&gt; bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --topics-to-move-json-file topics-to-move.json --broker-list "5,6" --generate</span><br><span class="line">Current partition replica assignment</span><br><span class="line"> </span><br><span class="line">&#123;<span class="attr">"version"</span>:<span class="number">1</span>,</span><br><span class="line"><span class="attr">"partitions"</span>:[&#123;<span class="attr">"topic"</span>:<span class="string">"foo1"</span>,<span class="attr">"partition"</span>:<span class="number">2</span>,<span class="attr">"replicas"</span>:[<span class="number">1</span>,<span class="number">2</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo1"</span>,<span class="attr">"partition"</span>:<span class="number">0</span>,<span class="attr">"replicas"</span>:[<span class="number">3</span>,<span class="number">4</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo2"</span>,<span class="attr">"partition"</span>:<span class="number">2</span>,<span class="attr">"replicas"</span>:[<span class="number">1</span>,<span class="number">2</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo2"</span>,<span class="attr">"partition"</span>:<span class="number">0</span>,<span class="attr">"replicas"</span>:[<span class="number">3</span>,<span class="number">4</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo1"</span>,<span class="attr">"partition"</span>:<span class="number">1</span>,<span class="attr">"replicas"</span>:[<span class="number">2</span>,<span class="number">3</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo2"</span>,<span class="attr">"partition"</span>:<span class="number">1</span>,<span class="attr">"replicas"</span>:[<span class="number">2</span>,<span class="number">3</span>]&#125;]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line"> </span><br><span class="line">&#123;<span class="attr">"version"</span>:<span class="number">1</span>,</span><br><span class="line"><span class="attr">"partitions"</span>:[&#123;<span class="attr">"topic"</span>:<span class="string">"foo1"</span>,<span class="attr">"partition"</span>:<span class="number">2</span>,<span class="attr">"replicas"</span>:[<span class="number">5</span>,<span class="number">6</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo1"</span>,<span class="attr">"partition"</span>:<span class="number">0</span>,<span class="attr">"replicas"</span>:[<span class="number">5</span>,<span class="number">6</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo2"</span>,<span class="attr">"partition"</span>:<span class="number">2</span>,<span class="attr">"replicas"</span>:[<span class="number">5</span>,<span class="number">6</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo2"</span>,<span class="attr">"partition"</span>:<span class="number">0</span>,<span class="attr">"replicas"</span>:[<span class="number">5</span>,<span class="number">6</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo1"</span>,<span class="attr">"partition"</span>:<span class="number">1</span>,<span class="attr">"replicas"</span>:[<span class="number">5</span>,<span class="number">6</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo2"</span>,<span class="attr">"partition"</span>:<span class="number">1</span>,<span class="attr">"replicas"</span>:[<span class="number">5</span>,<span class="number">6</span>]&#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该工具生成候选分配，该分配会将所有分区从主题foo1，foo2移至代理5,6。但是请注意，此时分区移动尚未开始，它仅告诉您当前分配和建议的新分配。如果您要回滚到当前分配，则应将其保存。新的赋值应保存在json文件（例如，expand-cluster-reassignment.json）中，然后使用–execute选项输入到工具中，如下所示：</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&gt; bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json --execute</span><br><span class="line">Current partition replica assignment</span><br><span class="line"> </span><br><span class="line">&#123;<span class="attr">"version"</span>:<span class="number">1</span>,</span><br><span class="line"><span class="attr">"partitions"</span>:[&#123;<span class="attr">"topic"</span>:<span class="string">"foo1"</span>,<span class="attr">"partition"</span>:<span class="number">2</span>,<span class="attr">"replicas"</span>:[<span class="number">1</span>,<span class="number">2</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo1"</span>,<span class="attr">"partition"</span>:<span class="number">0</span>,<span class="attr">"replicas"</span>:[<span class="number">3</span>,<span class="number">4</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo2"</span>,<span class="attr">"partition"</span>:<span class="number">2</span>,<span class="attr">"replicas"</span>:[<span class="number">1</span>,<span class="number">2</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo2"</span>,<span class="attr">"partition"</span>:<span class="number">0</span>,<span class="attr">"replicas"</span>:[<span class="number">3</span>,<span class="number">4</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo1"</span>,<span class="attr">"partition"</span>:<span class="number">1</span>,<span class="attr">"replicas"</span>:[<span class="number">2</span>,<span class="number">3</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo2"</span>,<span class="attr">"partition"</span>:<span class="number">1</span>,<span class="attr">"replicas"</span>:[<span class="number">2</span>,<span class="number">3</span>]&#125;]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions</span><br><span class="line">&#123;<span class="attr">"version"</span>:<span class="number">1</span>,</span><br><span class="line"><span class="attr">"partitions"</span>:[&#123;<span class="attr">"topic"</span>:<span class="string">"foo1"</span>,<span class="attr">"partition"</span>:<span class="number">2</span>,<span class="attr">"replicas"</span>:[<span class="number">5</span>,<span class="number">6</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo1"</span>,<span class="attr">"partition"</span>:<span class="number">0</span>,<span class="attr">"replicas"</span>:[<span class="number">5</span>,<span class="number">6</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo2"</span>,<span class="attr">"partition"</span>:<span class="number">2</span>,<span class="attr">"replicas"</span>:[<span class="number">5</span>,<span class="number">6</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo2"</span>,<span class="attr">"partition"</span>:<span class="number">0</span>,<span class="attr">"replicas"</span>:[<span class="number">5</span>,<span class="number">6</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo1"</span>,<span class="attr">"partition"</span>:<span class="number">1</span>,<span class="attr">"replicas"</span>:[<span class="number">5</span>,<span class="number">6</span>]&#125;,</span><br><span class="line">              &#123;<span class="attr">"topic"</span>:<span class="string">"foo2"</span>,<span class="attr">"partition"</span>:<span class="number">1</span>,<span class="attr">"replicas"</span>:[<span class="number">5</span>,<span class="number">6</span>]&#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后，–verify选项可与该工具一起使用，以检查分区重新分配的状态。请注意，应将相同的expand-cluster-reassignment.json（与–execute选项一起使用）与–verify选项一起使用：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&gt; bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json --verify</span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition [foo1,0] completed successfully</span><br><span class="line">Reassignment of partition [foo1,1] is in progress</span><br><span class="line">Reassignment of partition [foo1,2] is in progress</span><br><span class="line">Reassignment of partition [foo2,0] completed successfully</span><br><span class="line">Reassignment of partition [foo2,1] completed successfully</span><br><span class="line">Reassignment of partition [foo2,2] completed successfully</span><br></pre></td></tr></table></figure><h2 id="删除topic后遗症"><a href="#删除topic后遗症" class="headerlink" title="删除topic后遗症"></a>删除topic后遗症</h2><p>创建topic需要注意几个关键点</p><ol><li>Kafka topic名称规范: 英文字母小写 </li><li>在建topic之前，名字真的想好了再建</li><li>不要轻易删除topic，除非生产上这个topic不用了，数据量较大也没关系7天后自定删除</li><li>不要有强迫症 </li><li>删除有风险 </li></ol><p>当不成功的时候，暴力删除</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># 删除文件目录</span><br><span class="line">rm -rf /home/hadoop/tmp/kafka-logs/jj*</span><br><span class="line"></span><br><span class="line"># 删除zookeeper目录</span><br><span class="line">rmr /kafka/brokers/topics/jj</span><br><span class="line">rmr /kafka/config/topics/jj</span><br><span class="line">rmr /kafka/admin/delete_topics/jj</span><br></pre></td></tr></table></figure><p>再建一个相同名称的topic，重启kafka集群 ，但是重启有风险可能起不来</p><p>解决办法:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">kill -<span class="number">9</span> $(pgrep -f kafka)</span><br><span class="line">喘息的机会(如果没有完全kill，kafka起不来)</span><br><span class="line">/home/hadoop/app/kafka/bin/kafka-server-start.sh -daemon /home/hadoop/app/kafka/config/server.properties</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Kakfa </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kakfa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka消费时保证消息的顺序性</title>
      <link href="/2020/04/24/kafka/4/"/>
      <url>/2020/04/24/kafka/4/</url>
      
        <content type="html"><![CDATA[<p>Kafka架构通过多个Partition提高并发，Producer生产数据的时候默认使用Hash发送数据到每个Partition，这样就造成了消费数据的时候只能保证分区内有序，而分区间无序(每个partition是一个有序的队列)。</p><h3 id="现象"><a href="#现象" class="headerlink" title="现象"></a>现象</h3><p>最直接的现象就是kafka消费乱序，造成日志处理的先后顺序发生了变化，比如我的业务逻辑如下:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="number">100</span>,<span class="number">100</span></span><br><span class="line"><span class="keyword">update</span>  <span class="number">100</span>,<span class="number">1</span></span><br><span class="line"><span class="keyword">update</span>  <span class="number">100</span>,<span class="number">2</span></span><br><span class="line"><span class="keyword">update</span>  <span class="number">100</span>,<span class="number">3</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">100</span></span><br></pre></td></tr></table></figure><p>采集到Kafka再使用Spark Streaming处理的时候的顺序却变成了</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span>  <span class="number">100</span>,<span class="number">1</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">100</span></span><br><span class="line"><span class="keyword">update</span>  <span class="number">100</span>,<span class="number">2</span></span><br><span class="line"><span class="keyword">update</span>  <span class="number">100</span>,<span class="number">3</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="number">100</span>,<span class="number">100</span></span><br></pre></td></tr></table></figure><p>因为处理的顺序乱了，所以会造成丢数、多数、数据数量是一样但内容不一样等问题。</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><ol><li><p>设置一个分区，但是这样虽然保证了数据有序，但是吞吐量直线下降。</p></li><li><p>设置一个符合特征数据的key，如<code>db-table-id</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public <span class="type">ProducerRecord</span>(<span class="type">String</span> topic, <span class="type">Integer</span> partition, <span class="type">K</span> key, <span class="type">V</span> value) &#123;</span><br><span class="line">    <span class="keyword">this</span>(topic, partition, <span class="literal">null</span>, key, value, <span class="literal">null</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如上代码，在Producer发送数据时候，可以指定一个key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。</p><p>在maxwell -&gt; kafka的时候，需要设置<code>producer_partition_by=primary_key</code></p><p>参数可以设置为:</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">PARTITION_BY</span>: [ database | table | primary_key | transaction_id | column | random ]</span><br></pre></td></tr></table></figure></li></ol><p>   接着，我们在消费者里可能会搞多个线程来并发处理消息。因为如果消费者是单线程消费处理，而处理比较耗时的话，比如处理一条消息耗时几十 ms，那么 1 秒钟只能处理几十条消息，这吞吐量太低了。而多个线程并发跑的话，顺序可能就乱掉了。</p><p>   这时候引入另外一个Producer的参数<code>max.in.flight.requests.per.connection=1</code>，限制客户端在单个连接上能够发送的未响应请求的个数。设置此值是1表示kafka broker在响应请求之前client不能再向同一个broker发送请求。设置此参数是为了避免消息乱序。</p><p>   flume 发到 kafka也是类似的思路(未测试)，及时没有提供key的设置，还可以修改源码。</p><p>其他参考链接: <a href="https://www.cnblogs.com/huxi2b/p/6056364.html" target="_blank" rel="noopener">https://www.cnblogs.com/huxi2b/p/6056364.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Kakfa </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kakfa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单节点部署redis</title>
      <link href="/2020/04/23/redis/1/"/>
      <url>/2020/04/23/redis/1/</url>
      
        <content type="html"><![CDATA[<h3 id="第一步：下载redis安装包-整个安装流程建议在root用户下完成"><a href="#第一步：下载redis安装包-整个安装流程建议在root用户下完成" class="headerlink" title="第一步：下载redis安装包(整个安装流程建议在root用户下完成)"></a>第一步：下载redis安装包(整个安装流程建议在root用户下完成)</h3><p>wget <a href="http://download.redis.io/releases/redis-5.0.5.tar.gz" target="_blank" rel="noopener">http://download.redis.io/releases/redis-5.0.5.tar.gz</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop local]# wget http://download.redis.io/releases/redis-5.0.5.tar.gz</span><br><span class="line">--2017-12-13 12:35:12--  http://download.redis.io/releases/redis-5.0.5.tar.gz</span><br><span class="line">Resolving download.redis.io (download.redis.io)... 109.74.203.151</span><br><span class="line">Connecting to download.redis.io (download.redis.io)|109.74.203.151|:80... connected.</span><br><span class="line">HTTP request sent, awaiting response... 200 OK</span><br><span class="line">Length: 1723533 (1.6M) [application/x-gzip]</span><br><span class="line">Saving to: ‘redis-5.0.5.tar.gz’</span><br><span class="line"></span><br><span class="line"><span class="meta">100%</span><span class="bash">[==========================================================================================================&gt;] 1,723,533    608KB/s   <span class="keyword">in</span> 2.8s   </span></span><br><span class="line"></span><br><span class="line">2017-12-13 12:35:15 (608 KB/s) - ‘redis-5.0.5.tar.gz’ saved [1723533/1723533]</span><br></pre></td></tr></table></figure><h3 id="第二步：解压压缩包"><a href="#第二步：解压压缩包" class="headerlink" title="第二步：解压压缩包"></a>第二步：解压压缩包</h3><p>tar -zxvf redis-5.0.5.tar.gz</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop local]# tar -zxvf redis-5.0.5.tar.gz</span><br></pre></td></tr></table></figure><h3 id="第三步：yum安装gcc依赖"><a href="#第三步：yum安装gcc依赖" class="headerlink" title="第三步：yum安装gcc依赖"></a>第三步：yum安装gcc依赖</h3><p>yum install gcc</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop local]# yum install gcc</span><br></pre></td></tr></table></figure><p>遇到选择,输入y即可</p><h3 id="第四步：跳转到redis解压目录下"><a href="#第四步：跳转到redis解压目录下" class="headerlink" title="第四步：跳转到redis解压目录下"></a>第四步：跳转到redis解压目录下</h3><p>cd redis-5.0.5</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop local]# cd redis-5.0.5</span><br></pre></td></tr></table></figure><h3 id="第五步：编译安装"><a href="#第五步：编译安装" class="headerlink" title="第五步：编译安装"></a>第五步：编译安装</h3><p>make MALLOC=libc　　</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop redis-5.0.5]# make MALLOC=libc</span><br></pre></td></tr></table></figure><p>cd src &amp;&amp; make install</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop redis-5.0.5]# cd src &amp;&amp; make install</span><br><span class="line">    CC Makefile.dep</span><br><span class="line"></span><br><span class="line">Hint: It's a good idea to run 'make test' ;)</span><br><span class="line"></span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br></pre></td></tr></table></figure><h3 id="第六步：修改配置文件"><a href="#第六步：修改配置文件" class="headerlink" title="第六步：修改配置文件"></a>第六步：修改配置文件</h3><p>修改redis.conf</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 关闭保护模式</span></span><br><span class="line">protected-mode no</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置端口号</span></span><br><span class="line">prot 16379</span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许后台运行</span></span><br><span class="line">daemonize yes</span><br><span class="line"><span class="meta">#</span><span class="bash"> 日志文件保存位置</span></span><br><span class="line">logfile /home/hadoop/tmp/redis.log</span><br></pre></td></tr></table></figure><h3 id="第七步：设置redis服务"><a href="#第七步：设置redis服务" class="headerlink" title="第七步：设置redis服务"></a>第七步：设置redis服务</h3><p>1、在/etc目录下新建redis目录</p><p>mkdir redis</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop etc]# mkdir redis</span><br></pre></td></tr></table></figure><p>2、将/home/hadoop/app/redis-5.0.5/redis.conf 文件复制一份到/etc/redis目录下，并命名为6379.conf　　</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop redis]# cp /home/hadoop/app/redis-5.0.5/redis.conf /etc/redis/6379.conf</span><br></pre></td></tr></table></figure><p>3、将redis的启动脚本复制一份放到/etc/init.d目录下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop init.d]# cp /home/hadoop/app/redis-5.0.5/utils/redis_init_script /etc/init.d/redisd</span><br></pre></td></tr></table></figure><p>4、设置redis开机自启动</p><p>先切换到/etc/init.d目录下</p><p>然后执行自启命令</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop init.d]# chkconfig redisd on</span><br><span class="line">service redisd does not support chkconfig</span><br></pre></td></tr></table></figure><p>看结果是redisd不支持chkconfig</p><p>解决方法：</p><p>使用vim编辑redisd文件，在第一行加入如下两行注释，保存退出</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> chkconfig:   2345 90 10</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> description:  Redis is a persistent key-value database</span></span><br></pre></td></tr></table></figure><p>注释的意思是，redis服务必须在运行级2，3，4，5下被启动或关闭，启动的优先级是90，关闭的优先级是10。</p><p>再次执行开机自启命令，成功</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop init.d]# chkconfig redisd on</span><br></pre></td></tr></table></figure><p>现在可以直接已服务的形式启动和关闭redis了</p><p>启动：</p><p>service redisd start　</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop ~]# service redisd start</span><br><span class="line">Starting Redis server...</span><br><span class="line">2288:C 13 Dec 13:51:38.087 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo</span><br><span class="line">2288:C 13 Dec 13:51:38.087 # Redis version=4.0.6, bits=64, commit=00000000, modified=0, pid=2288, just started</span><br><span class="line">2288:C 13 Dec 13:51:38.087 # Configuration loaded</span><br></pre></td></tr></table></figure><p>关闭：</p><p>方法1：service redisd stop</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop ~]# service redisd stop</span><br><span class="line">Stopping ...</span><br><span class="line">Redis stopped</span><br></pre></td></tr></table></figure><p>方法2：redis-cli SHUTDOWN</p><p>如果出现如下问题：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop ~]# service redisd start</span><br><span class="line">/var/run/redis_6379.pid exists, process is already running or crashed</span><br></pre></td></tr></table></figure><p>可参考资料：<a href="http://blog.csdn.net/luozhonghua2014/article/details/54649295" target="_blank" rel="noopener">http://blog.csdn.net/luozhonghua2014/article/details/54649295</a></p><h3 id="开启客户端"><a href="#开启客户端" class="headerlink" title="开启客户端"></a>开启客户端</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 本机启动</span></span><br><span class="line">./redis-cli</span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定端口和主机启动</span></span><br><span class="line">./redis-cli -p 16379 -h hadoop</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka eagle安装部署</title>
      <link href="/2020/04/23/kafka/3/"/>
      <url>/2020/04/23/kafka/3/</url>
      
        <content type="html"><![CDATA[<h5 id="1-kafka-zookeeper准备"><a href="#1-kafka-zookeeper准备" class="headerlink" title="1.kafka+zookeeper准备"></a>1.kafka+zookeeper准备</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">这里假设你已经把kafka+zookeeper安装完成，但是需要注意的几点是：</span><br><span class="line">1.kafka需要开启JMX端口</span><br><span class="line">    找到kafka安装路径，进入到bin文件夹，修改下面的地方。</span><br><span class="line">    vi kafka-server-start.sh</span><br><span class="line">...</span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$KAFKA_HEAP_OPTS</span>"</span> = <span class="string">"x"</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">"-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70"</span></span><br><span class="line">    <span class="built_in">export</span> JMX_PORT=<span class="string">"9999"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">...</span><br><span class="line">    参考链接：[kafka添加jmx端口]:https://ke.smartloli.org/3.Manuals/11.Metrics.html</span><br><span class="line"></span><br><span class="line">2.了解kafka在zookeeper配置</span><br><span class="line">    需要查看kafka的server.properties配置</span><br><span class="line">    找到zookeeper.connect此项配置，这个是要配置到eagle里面的</span><br><span class="line">    此处假设zookeeper.connect=192.168.18.11:2181,192.168.18.12:2181,192.168.18.13:2181</span><br><span class="line">    ！！！PS:此处踩了坑，如果说这里的zookeeper地址后面加了其他路径，在kafka-eagle里面也要配置，</span><br><span class="line">    否则在kafka-eagle的Dashboard中无法读取到kafka的信息。比如我们有人安装的kafka集群里面就有    </span><br><span class="line">    192.168.18.11:2181/kafka1或者192.168.18.11:2181/kafka2这种地址。</span><br><span class="line">    如果你在安装kafka的时候没有配置多余路径，这样是最好的，如果有一定要加上。</span><br><span class="line">3.连通性测试</span><br><span class="line">  安装kafka-eagle的服务器，一定要提前测试是否能连接kafka注册的zookeeper端口</span><br><span class="line">  telnet 端口进行测试</span><br></pre></td></tr></table></figure><h5 id="2-JDK环境准备"><a href="#2-JDK环境准备" class="headerlink" title="2.JDK环境准备"></a>2.JDK环境准备</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">此处就忽略不说了，kafka既然会安装，也是依赖JDK环境的。版本没要求，但是最好是1.7以上。</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$JAVA_HOME</span></span><br><span class="line">测试一下JDK环境是否安装成功</span><br></pre></td></tr></table></figure><h5 id="3-开始安装kafka-eagle"><a href="#3-开始安装kafka-eagle" class="headerlink" title="3.开始安装kafka-eagle"></a>3.开始安装kafka-eagle</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">1.下载安装包</span><br><span class="line">软件安装目录建议按照自己的规范来，以后好找</span><br><span class="line"><span class="built_in">cd</span> /tmp</span><br><span class="line">wget https://github.com/smartloli/kafka-eagle-bin/archive/v1.2.2.tar.gz</span><br><span class="line">tar zxf v1.2.2.tar.gz</span><br><span class="line"><span class="built_in">cd</span> kafka-eagle-bin-1.2.2</span><br><span class="line">tar zxf kafka-eagle-web-1.2.2-bin.tar.gz -C /data/app/</span><br><span class="line"><span class="built_in">cd</span> /data/app</span><br><span class="line">mv kafka-eagle-web-1.2.2 kafka-eagle</span><br><span class="line"></span><br><span class="line">2.环境配置</span><br><span class="line">vi /etc/profile</span><br><span class="line"><span class="built_in">export</span> KE_HOME=/data/app/kafka-eagle</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$KE_HOME</span>/bin</span><br><span class="line">ps:此处的KE_HOME按照自己实际安装的目录来，我安装在/data/app/kafka-eagle下面</span><br><span class="line">如果你是安装的其他目录，别忘了修改。</span><br><span class="line"></span><br><span class="line">3.配置修改</span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;KE_HOME&#125;</span>/conf</span><br><span class="line">vi system-config.properties</span><br><span class="line"></span><br><span class="line"><span class="comment"># multi zookeeper&amp;kafka cluster list -- The client connection address of the Zookeeper cluster is set here</span></span><br><span class="line"><span class="comment">#如果只有一个集群的话，就写一个cluster1就行了</span></span><br><span class="line">kafka.eagle.zk.cluster.alias=cluster1,cluster2   </span><br><span class="line"><span class="comment">#这里填上刚才上准备工作中的zookeeper.connect地址</span></span><br><span class="line">cluster1.zk.list=192.168.18.11:2181,192.168.18.12:2181,192.168.18.13:2181</span><br><span class="line"><span class="comment">#如果多个集群，继续写，如果没有注释掉</span></span><br><span class="line">cluster2.zk.list=192.168.18.21:2181,192.168.18.22:2181,192.168.18.23:2181/kafka </span><br><span class="line"></span><br><span class="line"><span class="comment"># zk limit -- Zookeeper cluster allows the number of clients to connect to</span></span><br><span class="line">kafka.zk.limit.size=25</span><br><span class="line"></span><br><span class="line"><span class="comment"># kafka eagel webui port -- WebConsole port access address</span></span><br><span class="line">kafka.eagle.webui.port=8048     <span class="comment">###web界面地址端口</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kafka offset storage -- Offset stored in a Kafka cluster, if stored in the zookeeper, you can not use this option</span></span><br><span class="line">kafka.eagle.offset.storage=kafka</span><br><span class="line"></span><br><span class="line"><span class="comment"># delete kafka topic token -- Set to delete the topic token, so that administrators can have the right to delete</span></span><br><span class="line">kafka.eagle.topic.token=keadmin</span><br><span class="line"></span><br><span class="line"><span class="comment"># kafka sasl authenticate, current support SASL_PLAINTEXT</span></span><br><span class="line"><span class="comment">#如果kafka开启了sasl认证，需要在这个地方配置sasl认证文件</span></span><br><span class="line">kafka.eagle.sasl.enable=<span class="literal">false</span></span><br><span class="line">kafka.eagle.sasl.protocol=SASL_PLAINTEXT</span><br><span class="line">kafka.eagle.sasl.mechanism=PLAIN</span><br><span class="line">kafka.eagle.sasl.client=/data/kafka-eagle/conf/kafka_client_jaas.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">#下面两项是配置数据库的，默认使用sqlite，如果量大，建议使用mysql，这里我使用的是sqlit</span></span><br><span class="line"><span class="comment">#如果想使用mysql建议在文末查看官方文档</span></span><br><span class="line"><span class="comment"># Default use sqlite to store data</span></span><br><span class="line">kafka.eagle.driver=org.sqlite.JDBC</span><br><span class="line"><span class="comment"># It is important to note that the '/hadoop/kafka-eagle/db' path must exist.</span></span><br><span class="line">kafka.eagle.url=jdbc:sqlite:/data/app/kafka-eagle/db/ke.db   <span class="comment">#这个地址，按照安装目录进行配置</span></span><br><span class="line">kafka.eagle.username=root</span><br><span class="line">kafka.eagle.password=smartloli</span><br><span class="line"></span><br><span class="line"><span class="comment"># &lt;Optional&gt; set mysql address</span></span><br><span class="line"><span class="comment">#kafka.eagle.driver=com.mysql.jdbc.Driver</span></span><br><span class="line"><span class="comment">#kafka.eagle.url=jdbc:mysql://127.0.0.1:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span></span><br><span class="line"><span class="comment">#kafka.eagle.username=root</span></span><br><span class="line"><span class="comment">#kafka.eagle.password=smartloli</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">如果开启了sasl认证，需要自己去修改kafka-eagle目录下的conf/kafka_client_jaas.conf</span><br><span class="line">此处不多说</span><br></pre></td></tr></table></figure><h5 id="4-启动kafka-eagle"><a href="#4-启动kafka-eagle" class="headerlink" title="4.启动kafka-eagle"></a>4.启动kafka-eagle</h5><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">cd $&#123;KE_HOME&#125;/bin</span><br><span class="line">chmod +x ke.sh</span><br><span class="line">./ke.sh start</span><br><span class="line">查看日志是否出问题</span><br><span class="line">tailf ../<span class="built_in">log</span>/<span class="built_in">log</span>.<span class="built_in">log</span></span><br><span class="line">如果没问题，则直接登录</span><br><span class="line">http:<span class="comment">//host:8048/ke</span></span><br><span class="line">默认用户名:admin</span><br><span class="line">默认密码:<span class="number">12345</span></span><br><span class="line">如果进入到一下界面，就说明你安装成功了！</span><br></pre></td></tr></table></figure><p><img src="https://yerias.github.io/kafka/eagle.jpg" alt="eagle"></p><h5 id="5-问题汇总"><a href="#5-问题汇总" class="headerlink" title="5.问题汇总"></a>5.问题汇总</h5><ol><li><p>ZKPoolUtils.localhost-startStop-1 - ERROR - Unable to connect to zookeeper server within timeout: 100000</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">这个是网络问题，在kafka-eagle服务器上自己测试一下能否能telnet通配置的zk地址。</span><br><span class="line">cat system-config.properties|grep cluster1.zk.<span class="built_in">list</span></span><br><span class="line">测试这个配置的端口</span><br></pre></td></tr></table></figure></li><li><p>ERROR - Get kafka consumer has error,msg is No resolvable bootstrap urls given in bootstrap.servers</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">这个问题是配置的zk地址有问题，看看kafka配置的zk地址</span><br><span class="line">跟自己在eagle上配置的地址是否相同，有没有少目录或者端口配置。</span><br><span class="line">可以看看文章开头，kafka+zookeeper准备-了解kafka在zookeeper配置</span><br></pre></td></tr></table></figure></li><li><p>zookeeper state changed (AuthFailed)</p><figure class="highlight css"><table><tr><td class="code"><pre><span class="line">这个问题有两种情况</span><br><span class="line">1.认证的问题，确认你配置的认证文件是否正确</span><br><span class="line">2<span class="selector-class">.zk</span>地址问题，看看<span class="selector-tag">kafka</span>配置的<span class="selector-tag">zk</span>地址，跟自己在<span class="selector-tag">eagle</span>上配置的地址是否相同</span><br><span class="line">可以看看文章开头，<span class="selector-tag">kafka</span>+<span class="selector-tag">zookeeper</span>准备<span class="selector-tag">-</span>了解<span class="selector-tag">kafka</span>在<span class="selector-tag">zookeeper</span>配置</span><br></pre></td></tr></table></figure></li></ol><h5 id="6-kafka-eagle使用"><a href="#6-kafka-eagle使用" class="headerlink" title="6.kafka-eagle使用"></a>6.kafka-eagle使用</h5><p>kafka-eagle官方文档: <a href="https://ke.smartloli.org/2.Install/2.Installing.html" target="_blank" rel="noopener">https://ke.smartloli.org/2.Install/2.Installing.html</a></p><p>kafka-eagle下载地址: <a href="http://download.smartloli.org/" target="_blank" rel="noopener">http://download.smartloli.org/</a></p><p>kafka-eagle git地址: <a href="https://github.com/smartloli/kafka-eagle" target="_blank" rel="noopener">https://github.com/smartloli/kafka-eagle</a></p>]]></content>
      
      
      <categories>
          
          <category> Kakfa </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kakfa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单节点部署三台kafka</title>
      <link href="/2020/04/22/kafka/2/"/>
      <url>/2020/04/22/kafka/2/</url>
      
        <content type="html"><![CDATA[<ol><li><p>下载地址：<a href="http://archive.cloudera.com/kafka/kafka/4/kafka-2.2.1-kafka4.1.0.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/kafka/kafka/4/kafka-2.2.1-kafka4.1.0.tar.gz</a></p></li><li><p>解压</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf kafka-2.2.1-kafka4.1.0.tar.gz -C ../app</span><br></pre></td></tr></table></figure></li><li><p>在部署kafka之前 ，检测zookeeper是ok的</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] ls /</span><br><span class="line">[zktest, tunan, zookeeper, kafka]</span><br></pre></td></tr></table></figure></li><li><p>编辑config/server.properties文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">broker.id=0</span></span><br><span class="line"><span class="meta">#</span><span class="bash">log.dirs=/tmp/kafka-logs</span></span><br><span class="line"></span><br><span class="line">broker.id=0</span><br><span class="line">host.name=hadoop</span><br><span class="line">port=9090</span><br><span class="line">log.dirs=/home/hadoop/tmp/kafka-logs00</span><br><span class="line">zookeeper.connect=hadoop:2181/kafka</span><br></pre></td></tr></table></figure></li><li><p>复制Kafka文件夹为三份</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp -R kafka_2.11-2.2.1-kafka-4.1.0/ kafka01</span><br><span class="line">cp -R kafka_2.11-2.2.1-kafka-4.1.0/ kafka02</span><br><span class="line">mv  kafka_2.11-2.2.1-kafka-4.1.0/ kafka03</span><br></pre></td></tr></table></figure></li><li><p>修改kafka02的config/server.properties文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">broker.id=1</span><br><span class="line">host.name=hadoop</span><br><span class="line">port=9091</span><br><span class="line">log.dirs=/home/hadoop/tmp/kafka-logs01</span><br><span class="line">zookeeper.connect=hadoop:2181/kafka</span><br></pre></td></tr></table></figure></li><li><p>修改kafka03的config/server.properties文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">broker.id=2</span><br><span class="line">host.name=hadoop</span><br><span class="line">port=9092</span><br><span class="line">log.dirs=/home/hadoop/tmp/kafka-logs02</span><br><span class="line">zookeeper.connect=hadoop:2181/kafka</span><br></pre></td></tr></table></figure></li><li><p>启动(三台都需要输入命令)</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin/kafka-server-start.sh -daemon config/server.properties</span><br></pre></td></tr></table></figure></li><li><p>创建topic</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./kafka-topics.sh \</span><br><span class="line">--create \</span><br><span class="line">--zookeeper hadoop:2181/kafka \</span><br><span class="line">--partitions 3 \</span><br><span class="line">--replication-factor 2 \</span><br><span class="line">--topic test</span><br></pre></td></tr></table></figure></li><li><p>查看topic</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./kafka-topics.sh \</span><br><span class="line">--list \</span><br><span class="line">--zookeeper hadoop:2181/kafka</span><br></pre></td></tr></table></figure></li><li><p>查看指定topic的状况</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./kafka-topics.sh \</span><br><span class="line">--describe \</span><br><span class="line">--zookeeper hadoop:2181/kafka \</span><br><span class="line">--topic test</span><br></pre></td></tr></table></figure></li><li><p>测试</p><p>启动生产者</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./kafka-console-producer.sh \</span><br><span class="line">--broker-list hadoop:9090,hadoop:9091,hadoop:9092 \</span><br><span class="line">--topic test</span><br></pre></td></tr></table></figure><p>启动消费者</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh \</span><br><span class="line">--bootstrap-server hadoop:9090,hadoop:9091,hadoop:9092 \</span><br><span class="line">--from-beginning \</span><br><span class="line">--topic test</span><br></pre></td></tr></table></figure><p>发送数据</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">a</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">a</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>接收数据</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a</span><br><span class="line">a</span><br><span class="line">...</span><br></pre></td></tr></table></figure></li><li><p>关闭kafka集群(每台都要执行)</p><p>修改kafka-server-stop.sh</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">PIDS=$(ps ax | grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '&#123;print $1&#125;')</span><br></pre></td></tr></table></figure><p>修改为</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">PIDS=$(jps -lm | grep -i 'kafka'| awk '&#123;print $1&#125;')</span><br></pre></td></tr></table></figure><p><strong>命令详解：</strong>使用jps -lm命令列出所有的java进程，然后通过管道，利用grep -i ‘kafka.Kafka’命令将kafka进程筛出来，最后再接一管道命令，利用awk将进程号取出来。</p><p>分别执行kafka-server-stop.sh</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Kakfa </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kakfa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Error: java.io.IOException: Invalid LZO header</title>
      <link href="/2020/04/18/error/6/"/>
      <url>/2020/04/18/error/6/</url>
      
        <content type="html"><![CDATA[<p>在使用Flume传输数据的时候，需要注意几个字段</p><p>我们这里使用的是flume传输到hdfs</p><p>参数：hdfs.fileType 指定的数据传输类型，默认SequenceFile，如果直接传输本文本数据，则会乱码。在传输文本数据的时候它的值要修改为DataStream</p><p>而现在根据我们的错误提示就知道我们使用了Lzo压缩，所以需要把它的值修改为CompressedStream，即可解决问题。</p><table><thead><tr><th>Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td>hdfs.fileType</td><td>SequenceFile</td><td>File format: currently <code>SequenceFile</code>, <code>DataStream</code> or <code>CompressedStream</code> (1)DataStream will not compress output file and please don’t set codeC (2)CompressedStream requires set hdfs.codeC with an available codeC</td></tr></tbody></table><hr><p>还需要注意的一个参数是：hdfs.codeC ，在使用flume时，可以将数据压缩输出，它的值可选为gzip, bzip2, lzo, lzop, snappy</p><p>lzop的后缀是lzo</p><p>lzo的后缀是lzp.default</p><hr><p>还是要熟悉一下flume的文档。。。<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#hdfs-sink" target="_blank" rel="noopener">hdfs</a></p>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM之运行时数据区</title>
      <link href="/2020/04/15/jvm/1/"/>
      <url>/2020/04/15/jvm/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>jvm命令</li><li>jvm的运行时数据区</li><li>jvm会发生哪些ERROR</li><li>从一个class出发理解数据区</li></ol><h2 id="jvm命令"><a href="#jvm命令" class="headerlink" title="jvm命令"></a>jvm命令</h2><h3 id="JVM参数类型"><a href="#JVM参数类型" class="headerlink" title="JVM参数类型"></a>JVM参数类型</h3><ol><li>标准: 稳定的，长期没有变化</li><li>X: 相对变化较少的</li><li>XX: 变化较大，JVM调优重点</li></ol><p>设置参数时，idea指定在VM options里面，命令行直接加在java命令后</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">java -Xss10m -XX:+PrintGCDetails JVMParams</span><br></pre></td></tr></table></figure><h3 id="常见的XX类型的参数"><a href="#常见的XX类型的参数" class="headerlink" title="常见的XX类型的参数"></a>常见的XX类型的参数</h3><ol><li><p>-XX:+PrintGCDetails: 打印GC日志</p></li><li><p>-XX:+PrintFlagsInitial: 打印所有初始的参数信息</p></li><li><p>-XX:+PrintFlagsFinal: 打印所有最终的参数信息</p></li><li><p>-Xms设置堆的最小空间大小。</p></li><li><p>-Xmx设置堆的最大空间大小。</p></li><li><p>-XX:NewSize设置新生代最小空间大小。</p></li><li><p>-XX:MaxNewSize设置新生代最大空间大小。</p></li><li><p>-XX:PermSize设置永久代最小空间大小。</p></li><li><p>-XX:MaxPermSize设置永久代最大空间大小。</p></li><li><p>-Xss设置每个线程的堆栈大小。</p></li><li><p>没有直接设置老年代的参数，但是可以设置堆空间大小和新生代空间大小两个参数来间接控制</p><p> <strong>老年代空间大小=堆空间大小-年轻代大空间大小</strong></p></li></ol><p>例如：</p><p>java -XX:+PrintFlagsFinal -version </p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">uintx MaxHeapSize             := <span class="number">2048917504</span>     &#123;product&#125;</span><br><span class="line">intx MaxInlineLevel            = <span class="number">9</span>              &#123;product&#125;</span><br><span class="line">intx MaxInlineSize       = <span class="number">35</span>             &#123;product&#125;</span><br><span class="line">bool ParGCTrimOverflow      = <span class="keyword">true</span>           &#123;product&#125;</span><br><span class="line">bool ParGCUseLocalOverflow     = <span class="keyword">false</span>          &#123;product&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>上面只显示部分参数，但是能够说明我们需要理解的内容，即 ‘=’ 表示默认值，‘:=’ 表示被修改过的值。同时还有数值类型和布尔类型。</p><h3 id="几个特殊的XX类型参数"><a href="#几个特殊的XX类型参数" class="headerlink" title="几个特殊的XX类型参数"></a>几个特殊的XX类型参数</h3><p>-Xms、-Xmx、-Xss 实际上是XX类型的缩写</p><p>-Xms ==&gt; -XX:InitialHeapSize: 表示为: -Xms10m<br>-Xmx ==&gt; -XX:MaxHeapSize: 表示为: -Xmx10m<br>-Xss ==&gt; -XX:ThreadStackSize</p><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><ol><li><p>查看java进程：jps</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[hadoop<span class="meta">@hadoop</span> ~]$ jps</span><br><span class="line"><span class="number">13612</span> JVMParams</span><br><span class="line"><span class="number">13644</span> Jps</span><br></pre></td></tr></table></figure></li><li><p>查看java进程的参数信息<br>jinfo -flag name pid </p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[hadoop<span class="meta">@hadoop</span> ~]$ jinfo -flag MaxHeapSize <span class="number">13612</span></span><br><span class="line">-XX:MaxHeapSize=<span class="number">2048917504</span></span><br><span class="line">[hadoop<span class="meta">@hadoop</span> ~]$ jinfo -flag InitialHeapSize <span class="number">13612</span></span><br><span class="line">-XX:InitialHeapSize=<span class="number">130023424</span></span><br><span class="line">[hadoop<span class="meta">@hadoop</span> ~]$ jinfo -flag ThreadStackSize <span class="number">13612</span></span><br><span class="line">-XX:ThreadStackSize=<span class="number">1024</span></span><br></pre></td></tr></table></figure><p>怎么理解-XX:MaxHeapSize=2048917504，-XX:InitialHeapSize=130023424 ?</p><p>分析：</p><p>我主机的物理内存为8G，2048917504k = 1.9G，130023424  = 124M</p><p>理论上heap的最大值为物理内存的1/4，最小值为物理内存的1/64</p><p>但是一般情况下，我们会把MaxHeapSize和InitialHeapSize设置相同的值，防止内存抖动</p></li><li><p>查看java进程的默认和设置的参数</p><p>jinfo -flags pid </p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[hadoop<span class="meta">@hadoop</span> ~]$ jinfo -flags  <span class="number">13612</span></span><br><span class="line">Non-<span class="keyword">default</span> VM flags: -XX:CICompilerCount=<span class="number">2</span> ...</span><br><span class="line">Command line:  -Xss10m -XX:+PrintGCDetails ...</span><br></pre></td></tr></table></figure></li></ol><h2 id="jvm的运行时数据区"><a href="#jvm的运行时数据区" class="headerlink" title="jvm的运行时数据区"></a>jvm的运行时数据区</h2><p><img src="https://yerias.github.io/java_img/%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA.jpg" alt="运行时数据区"></p><h3 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h3><p>程序计数器是每个线程私有的</p><p>程序计数器是一块较小的内存空间，它可以看做是当前线程的行号指示器，这在多线程环境下非常有用。使得线程切换后能够恢复到正确的执行位置。</p><p>在java虚拟机规范中，这是唯一一个没有规定任何OutOfMemoryError的地方</p><h3 id="Java虚拟机栈"><a href="#Java虚拟机栈" class="headerlink" title="Java虚拟机栈"></a>Java虚拟机栈</h3><p>java虚拟机栈也是每个线程私有的，它的生命周期和线程相同</p><p>虚拟机栈描述的是java方法执行的线程内存模型: 每个方法被执行的时候，java虚拟机栈都会同步创建一个Frame(栈帧)用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法被调用直到执行完毕的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。</p><p>最常用的就是局部变量表，局部变量表存放了编译期可知的各种java虚拟机的基本数据类型(boolean、byte、char、sort、int、long、float、double)、对象引用(reference类型)和returnAddress类型，这部分在后面讲有详细的解释。</p><p>在java虚拟机规范中，这个区域可能存在两种异常，如果线程请求的栈深度大于虚拟机所允许的深度，会抛出StackOverflowError异常，常见的有循环调用方法名；如果java虚拟机栈容量可以动态扩展，当栈无法申请到足够的内存时就会抛出OutOfMemoryError异常。</p><h3 id="本地方法栈"><a href="#本地方法栈" class="headerlink" title="本地方法栈"></a>本地方法栈</h3><p>本地方法栈是线程私有的</p><p>本地方法栈与虚拟机栈所发挥的作用是非常类似的，其区别只是虚拟机栈为虚拟机执行java方法服务，而本地方法栈则为本地(native)方法服务</p><p>常见的本地方法有getClass、hashCode、clone、notify、notifyAll、wait、sleep等</p><p>与java虚拟机栈一样，本地方法栈也会在栈深度溢出的时候或者栈扩展失败的时候抛出StackOutflowError和OutOfMemoryError异常。</p><h3 id="java堆"><a href="#java堆" class="headerlink" title="java堆"></a>java堆</h3><p>java堆是所有线程共享的，是虚拟机所管理的内存中最大的一块，在虚拟机启动时创建。</p><p>此内存区域的唯一目的是存放对象实例，对象实例包括对象和数组。</p><p>如果在java堆中没有内存完成实例分配，并且堆也无法扩展，java虚拟机将会抛出OutOfMemoryError异常。</p><h3 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h3><p>方法区是所有线程共享的</p><p>它用于存储已被虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据，简单点说就是Class。</p><p>方法区只是java虚拟机的规范，它属于堆的一个逻辑部分，为了和堆区分开，也叫非堆。在jdk8之前，方法区的具体实现叫做永久代，</p><ol><li>由于类及方法的信息大小很难确定，所以内存设置小了会发生OOM，设置大了又浪费</li><li>GC复杂度高，回收效率低</li><li>合并 HotSpot 与 JRockit </li></ol><p>所以在jdk8完全用元空间替换了永久代，元空间直接使用的系统内存。</p><p>在java虚拟机规范中，如果方法区无法满足内存分配需求时，会抛出OouOfMemoryError</p><h3 id="直接内存"><a href="#直接内存" class="headerlink" title="直接内存"></a>直接内存</h3><p>直接内存不是java虚拟机规范中定义的内存区域，但是这部分也会被频繁使用，所以也可能会抛出OutOfMemoryError。</p><h2 id="jvm会发生哪些ERROR"><a href="#jvm会发生哪些ERROR" class="headerlink" title="jvm会发生哪些ERROR"></a>jvm会发生哪些ERROR</h2><h3 id="java堆内存OOM异常测试"><a href="#java堆内存OOM异常测试" class="headerlink" title="java堆内存OOM异常测试"></a>java堆内存OOM异常测试</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">-Xmx10m -Xms10m -XX:+HeapDumpOnOutOfMemoryError</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HeapOOM</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">OOMObject</span></span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        ArrayList&lt;OOMObject&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">            list.add(<span class="keyword">new</span> OOMObject());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">Dumping heap to java_pid184.hprof ...</span><br><span class="line">Heap dump file created [<span class="number">10209413</span> bytes in <span class="number">0.080</span> secs]</span><br></pre></td></tr></table></figure><h3 id="java虚拟机栈和本地方法栈SOF测试"><a href="#java虚拟机栈和本地方法栈SOF测试" class="headerlink" title="java虚拟机栈和本地方法栈SOF测试"></a>java虚拟机栈和本地方法栈SOF测试</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">-Xss2M</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JavaVMStackSOF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> stackLength=<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">stackLeak</span><span class="params">()</span></span>&#123;</span><br><span class="line">        stackLength++;</span><br><span class="line">        stackLeak();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        JavaVMStackSOF stackSOF = <span class="keyword">new</span> JavaVMStackSOF();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            stackSOF.stackLeak();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">            System.out.println(<span class="string">"栈深度："</span>+stackSOF.stackLength);</span><br><span class="line">            <span class="keyword">throw</span> e;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">栈深度：<span class="number">41075</span></span><br><span class="line">Exception in thread <span class="string">"main"</span> java.lang.StackOverflowError</span><br></pre></td></tr></table></figure><h3 id="java虚拟机栈和本地方法栈OOM测试"><a href="#java虚拟机栈和本地方法栈OOM测试" class="headerlink" title="java虚拟机栈和本地方法栈OOM测试"></a>java虚拟机栈和本地方法栈OOM测试</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">-Xss4m</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JavaVMStackOOM</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">dontStop</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">stackleakByThread</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">            <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">                    dontStop();</span><br><span class="line">            &#125;).start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        JavaVMStackOOM oom = <span class="keyword">new</span> JavaVMStackOOM();</span><br><span class="line">        oom.stackleakByThread();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果:</p><p>死机</p><h3 id="方法区和运行时常量池OOM"><a href="#方法区和运行时常量池OOM" class="headerlink" title="方法区和运行时常量池OOM"></a>方法区和运行时常量池OOM</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">-XX:PerSize=<span class="number">6</span>m -XX:MaxPermSize=<span class="number">6</span>M</span><br><span class="line"><span class="keyword">import</span> java.util.HashSet;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RuntimeConstantPoolOOM</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        HashSet&lt;String&gt; set = <span class="keyword">new</span> HashSet&lt;String&gt;();</span><br><span class="line">        <span class="keyword">short</span> i = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">            set.add(String.valueOf(i++).intern());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果:</p><p>jdk8没有测试出来</p><h2 id="从一个class出发理解数据区"><a href="#从一个class出发理解数据区" class="headerlink" title="从一个class出发理解数据区"></a>从一个class出发理解数据区</h2><p><img src="https://yerias.github.io/java_img/class%E7%B1%BB%E5%AF%B9%E6%AF%94%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA.jpg" alt="class类对比运行时数据区"></p><p>如图所示，很容易的理解各区分别保存java代码中的哪些部分</p><p>堆区: 保存的People对象</p><p>栈区: 保存的栈帧，栈帧中保存了引用name和age和引用people</p><p>方法区: 保存的People.class相关的，包括类型信息、常量、静态变量sss</p><p>运行时常量池: 保存name和age字符串内容</p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban配置Plugin实现Spark作业提交(非Shell)</title>
      <link href="/2020/04/10/azkaban/2/"/>
      <url>/2020/04/10/azkaban/2/</url>
      
        <content type="html"><![CDATA[<p>第一步，我们要打开azkaban的<a href="https://github.com/azkaban/azkaban/tree/master/az-hadoop-jobtype-plugin/src/jobtypes" target="_blank" rel="noopener">官网</a>，配置一些文件和参数，如图所示</p><p><img src="https://yerias.github.io/azkaban_img/az%E9%85%8D%E7%BD%AESpark%E6%8F%90%E4%BA%A4.jpg" alt="az配置Spark提交"></p><p>将<code>spark</code>、<code>common.properties</code>、<code>commonprivate.properties</code>拷贝到服务器中对应的目录，最终的文件展示如下</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">[hadoop<span class="meta">@hadoop</span> jobtypes]$ tree</span><br><span class="line">.</span><br><span class="line">├── commonprivate.properties</span><br><span class="line">├── common.properties</span><br><span class="line">└── spark</span><br><span class="line">    ├── plugin.properties</span><br><span class="line">    └── <span class="keyword">private</span>.properties</span><br></pre></td></tr></table></figure><ol><li><p>配置commonprivate.properties中hadoop.home和spark.home指定的家目录</p></li><li><p>配置common.properties中hadoop.home和spark.home指定的家目录</p></li><li><p>修改private.properties文件中的参数(临时方案，可行)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">jobtype.classpath=$&#123;hadoop.classpath&#125;:$&#123;spark.home&#125;/conf:$&#123;spark.home&#125;/lib<span class="comment">/*</span></span><br><span class="line"><span class="comment">===&gt;</span></span><br><span class="line"><span class="comment">jobtype.classpath=hadoop.classpath:$&#123;spark.home&#125;/conf:$&#123;spark.home&#125;/lib/*</span></span><br></pre></td></tr></table></figure><p>这么做的原因是我们以上的文件中没有配置hadoop.classpath，官方也没有说明hadoop.classpath应该配置什么参数，目前修改掉引用不影响程序的使用。</p></li></ol><p>第二步，在conf/azkaban.properties文件下增加一个配置，主机名:端口号(随意修改)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">azkaban.webserver.url=https:<span class="comment">//hadoop:8666</span></span><br></pre></td></tr></table></figure><p>第三步，提交作业，所配的参数需要参考<a href="https://github.com/azkaban/azkaban/blob/master/az-hadoop-jobtype-plugin/src/main/java/azkaban/jobtype/SparkJobArg.java" target="_blank" rel="noopener">官网</a></p><p>测试案例:</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">config:</span><br><span class="line">    user.to.proxy: hadoop</span><br><span class="line">nodes:</span><br><span class="line">  - name: sparkwc</span><br><span class="line">    <span class="class"><span class="keyword">type</span></span>: spark</span><br><span class="line">    config:</span><br><span class="line">      <span class="class"><span class="keyword">class</span></span>: com.data.spark.wc.<span class="type">SparkWC</span></span><br><span class="line">      master: yarn</span><br><span class="line">      deploy-mode: client</span><br><span class="line">      executor-memory: <span class="number">512</span>M</span><br><span class="line">      driver-memory: <span class="number">512</span>M</span><br><span class="line">      conf.spark.testing.memory: <span class="number">471859200</span></span><br><span class="line">      execution-jar: tunan-spark-utils<span class="number">-1.0</span>.jar</span><br><span class="line">      jars: tunan-spark-core<span class="number">-1.0</span>.jar</span><br><span class="line">      params: hdfs:<span class="comment">//hadoop:9000/input/wc.txt hdfs://hadoop:9000/out</span></span><br></pre></td></tr></table></figure><p>注意配置文件中的 jar 没有写路径，这么提交<strong>需要把 jar 包和配置文件一起打成zip包</strong>，提交到AZ的Web界面</p><p>第四步，查看结果</p><p><em>20200413更新：</em>  数据下标越界问题：hadoop下的/share/hadoop/common/lib/paranamer-2.3.jar过时，使用–jars传spark下的/jars/paranamer-2.8.jar    </p><p><em>20200423更新：</em> 所有参数都可以使用conf传递，</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">execution_jar  对应的是appjar</span><br><span class="line">jars 对应的是 依赖jar</span><br><span class="line">params: $&#123;execution_jar&#125; 参数<span class="number">1</span> 参数<span class="number">2</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark疯狂踩坑系列</title>
      <link href="/2020/03/30/error/5/"/>
      <url>/2020/03/30/error/5/</url>
      
        <content type="html"><![CDATA[<p>如果WEB UI界面或者程序日志里面看不到错误，使用以下方式查看日志</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn logs -applicationId application_1585536649766_xxxx</span><br></pre></td></tr></table></figure><p>错误1</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Error: Could not find or load main class org.apache.spark.deploy.yarn.ApplicationMaster</span><br></pre></td></tr></table></figure><p>解决办法：</p><p>检查spark-defaults.conf中的配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark.yarn.jars=hdfs://hadoop:9000/spark-yarn/jars/*.jar</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark.yarn.archive  hdfs://hadoop000:8020/tmp/spark-archive/spark2.4.5.zip</span><br></pre></td></tr></table></figure><p>以上两种配置方式不可以错乱</p><p>错误2</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">java.lang.NoClassDefFoundError: org/lionsoul/ip2region/DbConfig</span><br></pre></td></tr></table></figure><p>解决办法</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--jars /home/hadoop/lib/ip2region-1.7.2.jar</span><br></pre></td></tr></table></figure><p>错误3</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">java.lang.NullPointerException</span><br><span class="line">at com.tunan.spark.utils.IpParseUtil.IpParse(IpParseUtil.java:19)</span><br></pre></td></tr></table></figure><p>解决办法</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--files /home/hadoop/lib/ip2region.db</span><br></pre></td></tr></table></figure><hr><p>代码中拿出文件有两种方式</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String dbPath = GetIPRegion.class.getResource("/ip2region.db").getPath();</span><br><span class="line">String dbPath = SparkFiles.get(<span class="string">"/ip2region.db"</span>);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MR编程时，Driver传递的参数Mapper显示为NULL</title>
      <link href="/2020/03/26/error/4/"/>
      <url>/2020/03/26/error/4/</url>
      
        <content type="html"><![CDATA[<p>在进行MR编程时，除了需要拿到HDFS上面的数据，有时候还需要Driver和Mapper或者Reducer之间进行参数传递</p><p>先看看我碰到的问题</p><p><img src="https://yerias.github.io/java_img/14.png" alt=""></p><p><img src="https://yerias.github.io/java_img/15.png" alt=""></p><p>在Driver中配置向Conf中配置了参数，在Mapper中从Context中拿出来的却是null值，<strong>问题出现在Job.getInstance() 中没有把Conf传递进去。</strong></p><p>所以以后要注意，在创建Job的时候要把Conf也放进去。</p><p>还需要注意的是Conf中的设值必须在Job.getInstance()上面完成</p>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM之内存模型</title>
      <link href="/2020/03/26/jvm/4/"/>
      <url>/2020/03/26/jvm/4/</url>
      
        <content type="html"><![CDATA[<p>Java内存模型其实就是围绕着在并发过程中如果解决原子性、有序性和可见性的通信规则</p><p><img src="https://yerias.github.io/java_img/1.jpg" alt="线程、主内存、工作内存三者之间的关系"></p><h2 id="主内存与工作内存"><a href="#主内存与工作内存" class="headerlink" title="主内存与工作内存"></a>主内存与工作内存</h2><p>Java内存模型的主要目的就是定义程序中各种变量的访问规则，即关注在虚拟机中把变量存储到内存和从内存中取出变量这样的底层细节。</p><p>此处的变量指的是包括了实例字段，静态字段和构成数组对象的元素。但不包括局部变量和方法参数，因为后者是线程私有的。</p><ol><li><p>主内存和工作内存的关系？</p><p>Java内存模型规定了所有的变量都存储在主内存中，每条线程还有自己的工作内存，线程的工作内存保存了被该线程使用的变量的主内存副本。线程对变量的所有操作(读取、赋值等)都必须在工作内存中进行，而不能直接读写主内存中的数据。不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递需要通过主内存来完成。</p></li><li><p>主内存和工作内存如何交互？</p><p>主内存和工作内存如何交互？即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步回主内存。Java内存模型定义了8种操作来完成。分别是</p><ul><li>lock(锁定)</li><li>unlock(解锁)</li><li>read(读取)</li><li>load(载入)</li><li>use(使用)</li><li>assign(赋值)</li><li>store(存储)</li><li>write(写入)</li></ul><p>他们每一种操作都是原子的、不可再分的(double和long类型，32位主机会拆分成两次执行)</p><p>他们规定了八种执行规则，但这不是我们关心的重点，作为开发者更需要了解的是先行发生原则–用来确定一个操作在并发环境下是否安全</p></li></ol><h2 id="Volatile变量的特殊规则"><a href="#Volatile变量的特殊规则" class="headerlink" title="Volatile变量的特殊规则"></a>Volatile变量的特殊规则</h2><p>Volatile是Java虚拟机提供的最轻量级的<strong>同步机制</strong></p><p>为什么说他是最轻量级的同步机制？因为它只能保证可见性、有序性，而不能保证原子性。虽然应用场景有限，但是快，能保证其他线程的立即可见性。</p><p>当一个变量被定义成volatile之后，它具备两项特性：</p><ol><li><p><strong>保证此变量对所有线程的可见性，</strong>这里的可见性是指一条线程修改了这个变量的值，新值对于其他线程来说是立即得知的。由于volatile不能保证原子性(比如运算是分两步来做的)，只能在以下两条规则的场景中进行运算。</p><ul><li>运算结果不依赖变量的当前值。</li><li>变量不需要与其他的状态变量共同参与不变约束。</li></ul><p>简单点说，就是自己玩自己的，典型的引用场景就是作位状态标志的修饰符。</p></li><li><p><strong>通过加入内存屏障禁止指令重排序</strong>，指令重排序优化是编辑器做的一种代码执行顺序的优化，只关注结果，不关注过程，但是这么做可能让程序逻辑混乱，比如本来应该在后面执行的代码，跑到前面来执行了，这时候就要使用volatile禁止指令重排序，从而保证代码的执行顺序和程序的顺序相同。</p></li></ol><h2 id="可见性、有序性、原子性"><a href="#可见性、有序性、原子性" class="headerlink" title="可见性、有序性、原子性"></a>可见性、有序性、原子性</h2><ol><li><p>原子性</p><p>Java内存模型直接保证的原子性变量操作包括read、load、assign、use、store、write这六个，我们大致可以认为，对基本数据类型的访问、读写都是具备原子性的。</p><p>对于其他场景的原子性保证，Java内存模型提供了lock和unlock操作满足这种需求，反应到代码中就是synchronized关键字，因此<strong>synchronized是具备原子性的</strong>。</p></li><li><p>可见性</p><p>可见性就是指当一个线程修改了共享变量的值时，其他线程能够立即得知这个修改。</p><p><strong>volatile</strong>的特殊规则保证了新值能够立即从工作内存同步到主内存，以及每次使用前从主内存刷新。</p><p><strong>synchronized</strong>是通过”对一个变量执行unlock操作之前，必须先把此变量同步回主内存中”实现的。</p><p><strong>final</strong>关键字修饰的字段在构造器中初始化完成，并且构造器没有把”this”的引用逃逸出去，那么在其他线程中就能看见final修饰字段的值。</p></li><li><p>有序性</p><p>java程序中天然的有序性可以总结为一句话: 如果在本线程内观察，所有的操作都是有序的；如果在一个线程中观察另一个线程，所有的操作都是无序的。前半句是指 “线程内变量为串行语义”，后半句是指 “指令重排序” 现象和 “工作内存和主内存同步延迟” 现象。</p><p><strong>volatile</strong>关键字本身就包含了禁止指令重排序的语义。</p><p><strong>synchronized</strong>是由 “一个变量在同一时刻只允许一条线程对其进行lock操作” 这条规则获得的。 </p></li></ol><h2 id="先行发生原则"><a href="#先行发生原则" class="headerlink" title="先行发生原则"></a>先行发生原则</h2><p>先行发生是Java内存模型定义的两项操作之间的顺序关系，如果说操作A先行发生于操作B，其实就是说在发生操作B之前，操作A产生的影响被操作B观察到，”影响” 包括修改了内存中共享变量的值、发生了消息、调用了方法等。</p><p>Java内存模型中 “天然的” 先行发生关系包括以下八种，如果两个操作之间的关系不在此列，并且无法从下列规则推导出来，则他们就没有<strong>顺序型保障</strong>，虚拟机可以对它们进行任意的<strong>重排序</strong>。 </p><ol><li>程序次序规则</li><li>管程锁定规则</li><li>volatile变量规则</li><li>线程启动规则</li><li>线程终止规则</li><li>线程中断规则</li><li>对象终结规则</li><li>传递性</li></ol><ul><li>“时间上的先后顺序” 与 “先行发生” 之间有什么不同?</li></ul><p>时间先后顺序与先行发生原则之间基本没有因果关系，所以我们衡量并发安全问题的时候不要受时间顺序的干扰，一切必须以先行发生原则为准。</p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>执行Hive SQL/MR 报错：Current usage: 77.8mb of 512.0mb physical memory used; 1.1gb of 1.0gb virtual memory used. Killing container.</title>
      <link href="/2020/03/25/error/3/"/>
      <url>/2020/03/25/error/3/</url>
      
        <content type="html"><![CDATA[<p>从错误消息中，可以看到使用的虚拟内存超过了当前1.0gb的限制。这可以通过两种方式解决：</p><p><strong>禁用虚拟内存限制检查</strong></p><p>YARN只会忽略该限制；为此，请将其添加到您的<code>yarn-site.xml</code>：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether virtual memory limits will be enforced for containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>此设置的默认值为<code>true</code>。</p><p><strong>增加虚拟内存与物理内存的比率</strong></p><p>在<code>yarn-site.xml</code>更改中，此值将高于当前设置</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Ratio between virtual memory to physical memory when setting memory limits for containers. Container allocations are expressed in terms of physical memory, and virtual memory usage is allowed to exceed this allocation by this ratio.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>默认是 <code>2.1</code></p><p>还可以增加分配给容器的物理内存量。</p><p>注意：更改配置后不要忘记重新启动yarn。</p><hr><p>如果不能修改集群配置，我们可以参考这么做：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">-Dmapreduce.map.memory.mb=<span class="number">4096</span></span><br></pre></td></tr></table></figure><hr><p>在运行MR的作业中我们需要关心一下几个参数：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">yarn.nodemanager.resource.memory-mb</span><br><span class="line">yarn.scheduler.minimum-allocation-mb</span><br><span class="line">yarn.scheduler.maximum-allocation-mb</span><br><span class="line">yarn.nodemanager.vmem-check-enabled</span><br><span class="line">yarn.nodemanager.vmem-pmem-ratio</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapJoin，文件在HDFS上Idea报错：File does not exist: /xxx/yyy.txt#yyy.txt</title>
      <link href="/2020/03/25/error/2/"/>
      <url>/2020/03/25/error/2/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.io.FileNotFoundException: File does not exist: /data/dept.txt#dept.txt</span><br></pre></td></tr></table></figure><p>先去HDFS上确定文件是否存在，文件不存在，put文件上去，再次运行</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://192.168.91.10:9000/data/emp.txt</span><br></pre></td></tr></table></figure><p>有是Path找不到，再去HDFS上检查这个文件是否存在，文件不存在，再次put上去，然后运行</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INFO DFSClient: Could not obtain BP-1292531802-192.168.181.10-1583457649867:blk_1073746058_5236 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.91.10:50010,DS-f8088840-5065-4320-8b51-4563ccec125a,DISK] Dead nodes:  DatanodeInfoWithStorage[192.168.91.10:50010,DS-f8088840-5065-4320-8b51-4563ccec125a,DISK]. Will get new block locations from namenode and retry...</span><br><span class="line">WARN DFSClient: DFS chooseDataNode: got # 2 IOException, will wait for 3263.5374223592803 msec.</span><br><span class="line">WARN BlockReaderFactory: I/O error constructing remote block reader.</span><br><span class="line">java.net.ConnectException: Connection timed out: no further information</span><br></pre></td></tr></table></figure><p>继续报错，连接不到DataNode的块，去命令行输入jps发现进程没挂，然后我怀疑是因为我的host没有配置hadoop的ip映射关系，于是配上后，继续运行</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java.lang.Exception: java.io.FileNotFoundException: dept.txt (系统找不到指定的文件。)</span><br><span class="line">Caused by: java.io.FileNotFoundException: dept.txt (系统找不到指定的文件。)</span><br></pre></td></tr></table></figure><p>不出所望的再次报错，这次的日志多，上下文分析一下</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">WARN FileUtil: Command &apos;G:\hadoop-2.7.2\bin\winutils.exe symlink E:\Java\hadoop\hadoop-client\dept.txt \tmp\hadoop-Tunan\mapred\local\1585106809398\dept.txt&apos; failed 1 with: CreateSymbolicLink error (1314): ???????????</span><br><span class="line">WARN LocalDistributedCacheManager: Failed to create symlink: \tmp\hadoop-Tunan\mapred\local\1585106809398\dept.txt &lt;- E:\Java\hadoop\hadoop-client/dept.txt</span><br></pre></td></tr></table></figure><p>由于我们给hdfs上面文件的路径创建了一个链接，也就是symlink，我们发现这个两个警告是symlink创建失败，于是使用管理员的身份启动Idea，解决</p><hr><p>需要注意是除了管理员身份启动Idea，还需要在在程序的main方法下开启symlink的功能，才能在<code>new FileInputStream(new File(&quot;dept.txt&quot;))</code>中使用</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">FileSystem.enableSymlinks();</span><br></pre></td></tr></table></figure><p>还需要注意的是本地操作HDFS的一些配置</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"hadoop"</span>);</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://192.168.91.10:9000"</span>);</span><br><span class="line">conf.set(<span class="string">"dfs.client.use.datanode.hostname"</span>, <span class="string">"true"</span>);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala之闭包&amp;柯里化</title>
      <link href="/2020/03/23/scala/4/"/>
      <url>/2020/03/23/scala/4/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>闭包</li><li>方法与函数的区别</li><li>柯里化</li></ol><h2 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h2><p>说到柯里化必先说起闭包，我们先不关心闭包和柯里化是什么，而是看一个transformation</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> init:<span class="type">Int</span> = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> i = list.foldLeft[<span class="type">Int</span>](init)((x,y) =&gt; &#123;</span><br><span class="line">    println(<span class="string">s"init = <span class="subst">$init</span> | x = <span class="subst">$x</span> | y = <span class="subst">$y</span>"</span> )</span><br><span class="line">    x+y</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">println(i)</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">init = <span class="number">10</span> | x = <span class="number">10</span> | y = <span class="number">1</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">11</span> | y = <span class="number">2</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">13</span> | y = <span class="number">3</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">16</span> | y = <span class="number">4</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">20</span> | y = <span class="number">5</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">25</span> | y = <span class="number">6</span></span><br><span class="line"><span class="number">31</span></span><br></pre></td></tr></table></figure><p>从结果来看 <code>foldLeft</code> 需要三个参数，<code>init</code>初始值，变量x，变量y，然后将他们累加(不考虑其他运算规则)</p><p>现在我们来看看闭包的解释：</p><ol><li><p>闭包是一个函数，返回值依赖于声明在函数外部的一个或多个变量。</p></li><li><p>闭包通常来讲可以简单的认为是可以访问一个函数里面局部变量的另外一个函数。</p></li></ol><p><code>val i = list.foldLeft[Int](init)((x,y))</code> 是一个闭包，返回值依赖声明在函数外的<code>init</code></p><p>如果我们把<code>foldLeft[Int](init)((x,y))</code> 拆分成两个函数的话，一个是有明确参数引用的 <code>foldLeft[Int](init)</code>， 另外一个是匿名函数<code>((x,y))</code>，如果我们观察仔细的话，会发现上面的代码中，第一次输出的结果 <code>init = x</code>  我们就可以简单的认为匿名函数<code>((x,y))</code>访问了<code>flodLeft</code>中的局部变量<code>init</code> ,事实上，<code>x</code>第一次的值就是拿的<code>init</code> 。</p><p>我们现在可以总结闭包就是在函数外面声明了一个变量，在函数中引用了这个变量，就称之为闭包，其他函数可以依赖闭包(函数)中的这个变量。由于闭包是把外部变量包了进来，所以这个变量的生命周期和闭包的生命周期一致。</p><p>最后在看一个案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> factor = <span class="number">3</span></span><br><span class="line"><span class="keyword">val</span> multiplier = (i:<span class="type">Int</span>) =&gt; i * factor</span><br><span class="line">println(multiplier(<span class="number">3</span>))</span><br></pre></td></tr></table></figure><p>没错，<strong>函数的返回值依赖于声明在函数外部的一个或多个变量就是闭包。</strong></p><h2 id="方法与函数的区别"><a href="#方法与函数的区别" class="headerlink" title="方法与函数的区别"></a>方法与函数的区别</h2><p>还需要讲一下方法与函数区别，因为柯里化指的是将原来接受两个参数的方法变成新的接受一个参数的函数的过程。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">method</span> </span>= (x:<span class="type">Int</span>,y:<span class="type">Int</span>) =&gt; x+y</span><br><span class="line">method: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span><span class="comment">//方法</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> func = (x:<span class="type">Int</span>,y:<span class="type">Int</span>) =&gt; x+y</span><br><span class="line">func: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1307</span>/<span class="number">1309775952</span>@<span class="number">32</span>a1aabf<span class="comment">//函数</span></span><br></pre></td></tr></table></figure><ol><li>首先应该要知道=号右边的内容 <code>(x: Int, y: Int) =&gt; x + y</code>是一个函数体。</li><li>方法只能用def修饰，函数可以用def修饰，也可以用val修饰。</li><li>当函数用def来接收之后，不再显示为function，转换为方法。</li><li>方法可以省略参数，函数不可以。</li><li>函数可以作为方法的参数。</li></ol><h2 id="柯里化"><a href="#柯里化" class="headerlink" title="柯里化"></a>柯里化</h2><p>理解闭包后，我们看一个复杂的闭包案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">addBase</span></span>(x:<span class="type">Int</span>) = (y:<span class="type">Int</span>) =&gt; x+y</span><br><span class="line">addBase: (x: <span class="type">Int</span>)<span class="type">Int</span> =&gt; <span class="type">Int</span></span><br></pre></td></tr></table></figure><p>首先我们看到<code>addBase</code>是一个方法，<code>(y:Int) =&gt; x+y</code>是一个函数体，下面我们试试执行<code>addBase(x:Int)</code></p><p>会返回什么?</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; addBase(<span class="number">3</span>)</span><br><span class="line">res27: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1308</span>/<span class="number">761477414</span>@<span class="number">553</span>f7b1e</span><br></pre></td></tr></table></figure><p>返回了一个函数，现在我们明白了，<code>addBase(x:Int)</code>是一个<strong>方法</strong>，在传入具体的值后，返回了一个具体的<strong>函数</strong></p><p>到现在 “=” 号后面的 <code>(y:Int) =&gt; x+y</code> 这段还没有使用到，但是我们知道 “=” 号后面是一个函数体，给函数中传入y 返回 x+y? 我们试试。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> addThree = addBase(<span class="number">3</span>)<span class="comment">//使用个变量接收函数</span></span><br><span class="line">addThree: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1308</span>/<span class="number">761477414</span>@<span class="number">65e1</span>c98c</span><br><span class="line">scala&gt; addThree(<span class="number">4</span>)<span class="comment">//传入4</span></span><br><span class="line">res29: <span class="type">Int</span> = <span class="number">7</span><span class="comment">//输出7</span></span><br></pre></td></tr></table></figure><p>等等。。。这就是一个闭包啊，x=3是外部传入的给函数<code>addBase</code>的，这时的<code>addBase</code>就是函数，它的内部维护了一个变量x=3，这时另外一个函数<code>addThree</code> 在输出x+y前，访问了<code>addBase</code>中的x=3。</p><p>完全符合闭包的定义规则，<strong>函数的返回值依赖于声明在函数外部的一个或多个变量就是闭包</strong>。</p><p>上面的<code>addBase</code>方法使我们一次传入一个3一次传入一个4，那么有没有办法一次传入呢？</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; addBase(<span class="number">3</span>)(<span class="number">4</span>)</span><br><span class="line">res34: <span class="type">Int</span> = <span class="number">7</span></span><br></pre></td></tr></table></figure><p>没错，这就是柯里化，<strong>柯里化指的是将原来接受两个参数的方法变成新的接受一个参数的函数的过程。并且新的函数返回一个以原有第二个参数作为参数的函数。</strong></p><p>我们发现了上面闭包的代码其实就是柯里化的过程</p><p>现在我们总结下柯里化是什么?</p><ol><li><p>柯里化指的是将原来接受两个参数的方法变成新的接受一个参数的函数的过程。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">sum1</span></span>(x:<span class="type">Int</span>): <span class="type">Int</span> =&gt; <span class="type">Int</span> = (y:<span class="type">Int</span>)=&gt;x+y</span><br><span class="line">sum1: (x: <span class="type">Int</span>)<span class="type">Int</span> =&gt; <span class="type">Int</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> res1 = sum1(<span class="number">3</span>)</span><br><span class="line">res1: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1321</span>/<span class="number">962341885</span>@<span class="number">41</span>d283de</span><br><span class="line"></span><br><span class="line">scala&gt; res1(<span class="number">4</span>)</span><br><span class="line">res38: <span class="type">Int</span> = <span class="number">7</span></span><br></pre></td></tr></table></figure><p>==&gt;上面是接受两个参数，下面是接受一个参数</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">sum2</span></span>(x:<span class="type">Int</span>)(y:<span class="type">Int</span>): <span class="type">Int</span> = x+y</span><br><span class="line">sum2: (x: <span class="type">Int</span>)(y: <span class="type">Int</span>)<span class="type">Int</span></span><br><span class="line"></span><br><span class="line">scala&gt; sum2(<span class="number">2</span>)(<span class="number">3</span>)</span><br><span class="line">res39: <span class="type">Int</span> = <span class="number">5</span></span><br></pre></td></tr></table></figure></li><li><p>新的函数返回一个以原有第二个参数作为参数的函数。</p><p>意思就是原来要分两步做的事情，现在分一步就做好了，底层自动调用和返回。在这个过程中，使用了闭包。</p></li></ol><p>懵逼之余。。。返回最开始的<code>foldLeft</code>案例，看<code>foldLeft</code> 方法源码：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foldLeft</span></span>[<span class="type">B</span>](z: <span class="type">B</span>)(<span class="meta">@deprecatedName</span>(<span class="symbol">'f</span>) op: (<span class="type">B</span>, <span class="type">A</span>) =&gt; <span class="type">B</span>): <span class="type">B</span></span><br></pre></td></tr></table></figure><p>这其实就是一个柯里化应用</p><ol><li><p>折叠操作是一个递归的过程，将上一次的计算结果代入到函数中 </p></li><li><p>作为结果的参数在<code>foldLeft</code>是第一个参数，下个参数在<code>foldRight</code>是第二个参数</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> i = list.foldLeft[<span class="type">Int</span>](init)(_+_)</span><br></pre></td></tr></table></figure><p>==&gt;<code>init</code> = 10 作为初始值只用一次，然后(_+_)累加，累加的结果放在第一个参数位置</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">x = <span class="number">10</span> | y = <span class="number">1</span></span><br><span class="line">x = <span class="number">11</span> | y = <span class="number">2</span></span><br><span class="line">x = <span class="number">13</span> | y = <span class="number">3</span></span><br><span class="line">x = <span class="number">16</span> | y = <span class="number">4</span></span><br><span class="line">x = <span class="number">20</span> | y = <span class="number">5</span></span><br><span class="line">x = <span class="number">25</span> | y = <span class="number">6</span></span><br></pre></td></tr></table></figure></li></ol><p><strong>总结</strong></p><ol><li><p>柯里化技术在提高适用性还是在延迟执行或者固定易变因素等方面有着重要重要的作用，加上scala语言本身就是推崇简洁编码，使得同样功能的函数在定义与转换的时候会更加灵活多样。另外在Spark的源码中有大量运用scala柯里化技术的情况，需要掌握好该技术才能看得懂相关的源代码。</p></li><li><p>在scala柯里化中，闭包也发挥着重要的作用。所谓的闭包就是变量出了函数的定义域外在其他代码块还能其作用，这样的情况称之为闭包。就上述讨论的案例而言，如果没有闭包作用，那么转换后函数其实返回的匿名函数是无法在与第一个参数x相关结合的，自然也就无法保证其所实现的功能是跟原来一致的。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala之var和val的比较&amp;lazy懒加载</title>
      <link href="/2020/03/19/scala/3/"/>
      <url>/2020/03/19/scala/3/</url>
      
        <content type="html"><![CDATA[<h3 id="1：内容是否可变：val修饰的是不可变的，var修饰是可变的"><a href="#1：内容是否可变：val修饰的是不可变的，var修饰是可变的" class="headerlink" title="1：内容是否可变：val修饰的是不可变的，var修饰是可变的"></a>1：内容是否可变：val修饰的是不可变的，var修饰是可变的</h3><p>下面看一段代码，你猜是否有错误</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ValAndVar</span> </span>&#123;</span><br><span class="line">    <span class="comment">//val 修饰由于不可变性必须初始化</span></span><br><span class="line">    <span class="keyword">val</span> <span class="type">LOVE</span>:<span class="type">String</span> = _</span><br><span class="line">    <span class="keyword">var</span> <span class="type">SEX</span>:<span class="type">String</span>  = _</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> name = <span class="string">"tunan"</span></span><br><span class="line">        <span class="keyword">var</span> age = <span class="number">18</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//val 修饰由于不可变性不能重新赋值</span></span><br><span class="line">        name = <span class="string">"zhangsan"</span></span><br><span class="line">        age = <span class="number">19</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>真实的结果:</p><p><img src="https://yerias.github.io/scala_img/val%E5%92%8Cvar%E7%9A%84%E5%8F%AF%E5%8F%98%E6%80%A7%E6%AF%94%E8%BE%83.jpg" alt="val和var的可变性比较"></p><ol><li>val是不可变的，所以修饰的变量必须初始化</li><li>val是不可变的，所以修饰的变量不能重新赋值</li><li>val是不可变的，所以是多线程安全的</li><li>val是不可变的，不用担心会改变它修饰的对象的状态</li><li>val是不可变的，增强了代码的可读性，不用担心它的内容发生变化</li><li>var是可变的，可以增强代码的灵活性，和val互补</li></ol><h3 id="2：val修饰的变量在编译后类似于java中的中的变量被final修饰"><a href="#2：val修饰的变量在编译后类似于java中的中的变量被final修饰" class="headerlink" title="2：val修饰的变量在编译后类似于java中的中的变量被final修饰"></a>2：val修饰的变量在编译后类似于java中的中的变量被final修饰</h3><ol><li><p>先看源代码</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ValAndVar</span> </span>&#123;</span><br><span class="line">    <span class="keyword">val</span> <span class="type">LOVE</span>:<span class="type">String</span> = <span class="string">"篮球"</span></span><br><span class="line">    <span class="keyword">var</span> <span class="type">SEX</span>:<span class="type">String</span>  = _</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> name:<span class="type">String</span> = <span class="string">"tunan"</span></span><br><span class="line">        <span class="keyword">var</span> age:<span class="type">Int</span> = <span class="number">18</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>再看反编译后的代码(只保留了我们想要的部分)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ValAndVar$</span> </span>&#123;</span><br><span class="line">  public static <span class="type">ValAndVar</span>$ <span class="type">MODULE</span>$;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">String</span> <span class="type">LOVE</span>;</span><br><span class="line">    </span><br><span class="line">  public void main(<span class="type">String</span>[] args) &#123;</span><br><span class="line">    <span class="type">String</span> name = <span class="string">"tunan"</span>;</span><br><span class="line">    int age = <span class="number">18</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们发现这段代码很诡异，scala中的类变量，在字节码层面转换成了 parivate final ，而main方法中的变量却没有添加final修饰，这是否证明编译器有问题？</p><p>答案是否定的，对于val或者final都只是给编译器用的，编译器如果发现你给此变量重新赋值会抛出错误。同时字节码(bytecode)不具备表达一个局部变量是不可变(immutable)的能力。</p><p>所以就有了现在结果。</p></li></ol><h3 id="3：lazy修饰符可以修饰变量，但是这个变量必须是val修饰的"><a href="#3：lazy修饰符可以修饰变量，但是这个变量必须是val修饰的" class="headerlink" title="3：lazy修饰符可以修饰变量，但是这个变量必须是val修饰的"></a>3：lazy修饰符可以修饰变量，但是这个变量必须是val修饰的</h3><ol><li><p>在证明lazy修饰的变量必须是val之前，我们先看看lazy是什么？</p><p>Scala中使用关键字lazy来定义惰性变量，实现延迟加载(懒加载)。<br>惰性变量只能是不可变变量，并且只有在调用惰性变量时，才会去实例化这个变量。</p><p>在Java中，一般使用get和set实现延迟加载(懒加载)，而在Scala中对延迟加载这一特性提供了语法级别的支持:</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> name = initName()</span><br></pre></td></tr></table></figure><p>使用lazy关键字修饰变量后，只有在使用该变量时，才会调用其实例化方法。也就是说在定义name=initName()时并不会调用initName()方法，只有在后面的代码中使用变量name时才会调用initName()方法。</p><p>如果<strong>不使用lazy关键字对变量修饰</strong>，那么变量name是立即实例化的，下面将通过一组案例对比认识：</p><p><code>不使用lazy修饰的方法：</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LazyDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initName</span></span>:<span class="type">String</span>=&#123;</span><br><span class="line">        println(<span class="string">"初始化initName"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"返回intName"</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="comment">//        lazy val name = initName</span></span><br><span class="line">        <span class="keyword">val</span> name = initName<span class="comment">//程序走到这里，就打印了initName的输出语句</span></span><br><span class="line">        println(<span class="string">"hello，欢迎来到图南之家"</span>)</span><br><span class="line">        println(name)<span class="comment">//程序走到这里，打印initName的返回值</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的name没有使用lazy关键字进行修饰，所以name是立即实例化的。</p><p>结果：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">初始化initName</span><br><span class="line">hello，欢迎来到图南之家</span><br><span class="line">返回intName</span><br></pre></td></tr></table></figure><p><code>使用lazy修饰后的方法：</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LazyDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initName</span></span>:<span class="type">String</span>=&#123;</span><br><span class="line">        println(<span class="string">"初始化initName"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"返回intName"</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">lazy</span> <span class="keyword">val</span> name = initName<span class="comment">//不调用initName方法，即不打印initName中的输出语句</span></span><br><span class="line"><span class="comment">//        val name = initName</span></span><br><span class="line">        println(<span class="string">"hello，欢迎来到图南之家"</span>)<span class="comment">//打印main方法中的输出语句</span></span><br><span class="line">        println(name)<span class="comment">//打印initName的输出语句，打印返回值</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在声明name时，并没有立即调用实例化方法initName(),而是在使用name时，才会调用实例化方法,并且无论调用多少次，实例化方法只会执行一次。</p><p>结果：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">hello，欢迎来到图南之家</span><br><span class="line">初始化initName</span><br><span class="line">返回intName</span><br></pre></td></tr></table></figure></li><li><p>证明lazy只能修饰的变量只能使用val</p><p>我们发现name都是使用val修饰的，如果我们使用var修饰会怎么样呢？</p><p><img src="https://yerias.github.io/scala_img/lazy%E6%87%92%E5%8A%A0%E8%BD%BD.jpg" alt="lazy懒加载"></p><p>我们发现报错：<code>&#39;lazy&#39; modifier allowed only with value definitions</code></p><p>实际上就是认为<code>lazy</code>修饰的变量只能<code>val</code>修饰</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>执行Saprk or Scala程序:找不到或者无法加载主类 xxx</title>
      <link href="/2020/03/19/error/1/"/>
      <url>/2020/03/19/error/1/</url>
      
        <content type="html"><![CDATA[<p>使用百度和谷歌，测试了广大程序员给出的各种解决办法，包括更换jdk版本(之前也是jdk8，小版本不同)，更换scala 的版本(2.12大版本内更换小版本，因为我的spark2.4.5需要的scala版本是2.12)，更换idea的输出路径，重构代码，清理idea的缓存，删除依赖重新下载，重建项目，导入其他同学的项目，皆出现<code>找不到或者无法加载主类 xxx</code>异常</p><p>最后重装idea，解决！</p>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA的包装类解析</title>
      <link href="/2020/03/17/java/11/"/>
      <url>/2020/03/17/java/11/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>什么是包装类</li><li>自动装箱和自动拆箱</li><li>包装类可以为null，而基本类型不可以</li><li>包装类型可用于泛型，而基本类型不可以</li><li>基本类型比包装类型更高效</li><li>两个包装类型的值可以相同，但却可以不相等</li></ol><h2 id="什么是包装类"><a href="#什么是包装类" class="headerlink" title="什么是包装类"></a>什么是包装类</h2><p>Java的每个基本类型都有对应的包装类型，比如说int的包装类型是Integer，double的包装类型是Double。</p><h2 id="自动装箱和自动拆箱"><a href="#自动装箱和自动拆箱" class="headerlink" title="自动装箱和自动拆箱"></a>自动装箱和自动拆箱</h2><p>既然有了基本类型和包装类型，肯定有些是要在他们之间进行转换。把基本类型转换成包装类型的过程叫做装箱。反之，把包装类型转换成基本类型的过程叫做拆箱。</p><p>在Java5之前，开发人员要进行手动拆装箱</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Integer integer2 = <span class="keyword">new</span> Integer(<span class="number">10</span>);</span><br><span class="line"><span class="keyword">int</span> j = integer2.intValue();</span><br></pre></td></tr></table></figure><p>java5引入了自动拆装箱的功能，减少了开发人员的工作</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Integer chenmo  = <span class="number">10</span>;  <span class="comment">// 自动装箱</span></span><br><span class="line"><span class="keyword">int</span> wanger = chenmo;     <span class="comment">// 自动拆箱</span></span><br></pre></td></tr></table></figure><p>使用反编译工作编译后的结果如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Integer integer3 = Integer.valueOf(<span class="number">10</span>);</span><br><span class="line"><span class="keyword">int</span> k = integer3.intValue();</span><br></pre></td></tr></table></figure><p>也就说自动装箱是调用了<code>Integer.valueOf()</code>完成的，自动拆箱是通过调用<code>integer.intValue()</code>完成的。</p><p>理解了自动拆装箱的原理后，我们来看一道面试题</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">int</span> b = <span class="number">100</span>;</span><br><span class="line">System.out.println(a ==b);</span><br><span class="line"></span><br><span class="line">Integer c = <span class="number">100</span>;</span><br><span class="line">Integer d = <span class="number">100</span>;</span><br><span class="line">System.out.println(c == d);</span><br><span class="line"></span><br><span class="line">Integer e = <span class="number">200</span>;</span><br><span class="line">Integer f = <span class="number">200</span>;</span><br><span class="line">System.out.println(e == f);</span><br><span class="line"></span><br><span class="line">System.out.println( a == c);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> h = <span class="number">200</span>;</span><br><span class="line">System.out.println( e == h);</span><br></pre></td></tr></table></figure><p>在看这段代码之前，我们要明白的是 == 号，基础类型比较的是值，引用类型比较的是内存地址</p><p>第一段代码，很好理解，基础类型比较的是值</p><p>第二段代码是包装类，这里需要引入一个缓冲池(IntegerCache )的概念，JVM把-128到127的数值存到了内存中，需要的时候直接从内存拿，而不是重新创建一个对象</p><p>第三段代码也很容易理解，如果-128到127是从缓冲池中拿，那么超过这个范围的，自然就是堆中创建了</p><p>第四段是基本类型和包装类型做比较，这时候包装类型会先转成基本类型，然后再比较</p><p>第五段代码同上，没有在堆中创建的对象这一步</p><p>结果即是：<code>true、true、false、true、true</code></p><p>之前我们就已经知道了自动装箱是Integer.valueOf()方法，我们现在看看它的源码，如果做到-128到127是缓冲池中拿，而超过了则需要在堆中创建。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Integer <span class="title">valueOf</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//如果需要包装的数值大于IntegerCache.low 并且小于IntegerCache.high，就在IntegerCache.cache中拿，否则就new一个Integer，从这里看cache应该是个数组，保存了-128到127的数值</span></span><br><span class="line">    <span class="keyword">if</span> (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high)</span><br><span class="line">       <span class="comment">// 数据中的位置是0 - 255 ==&gt; -127 - 128 ==&gt; -IntegerCache.low的位置的值是0</span></span><br><span class="line">        <span class="keyword">return</span> IntegerCache.cache[i + (-IntegerCache.low)];</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Integer(i);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">IntegerCache</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> low = -<span class="number">128</span>;<span class="comment">//最小值-128</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> high;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> Integer cache[];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//静态代码块，初始化时就加载好了</span></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="comment">// 最大值可以通过配置文件设置</span></span><br><span class="line">        <span class="keyword">int</span> h = <span class="number">127</span>;<span class="comment">//最大值127</span></span><br><span class="line">        <span class="comment">//从jvm的配置中拿到自定义缓冲池最大值的参数</span></span><br><span class="line">        String integerCacheHighPropValue =</span><br><span class="line">            sun.misc.VM.getSavedProperty(<span class="string">"java.lang.Integer.IntegerCache.high"</span>);</span><br><span class="line">        <span class="comment">//integerCacheHighPropValue的值默认是null</span></span><br><span class="line">        <span class="keyword">if</span> (integerCacheHighPropValue != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">//拿到传入的最大值</span></span><br><span class="line">                <span class="keyword">int</span> i = parseInt(integerCacheHighPropValue);</span><br><span class="line">                i = Math.max(i, <span class="number">127</span>);</span><br><span class="line">                <span class="comment">// 最大值不能超过Integer定义的最大值</span></span><br><span class="line">                h = Math.min(i, Integer.MAX_VALUE - (-low) -<span class="number">1</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span>( NumberFormatException nfe) &#123;</span><br><span class="line">                <span class="comment">// 如果属性不能被解析成int型，就忽略它。</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        high = h;<span class="comment">//赋值给high</span></span><br><span class="line"><span class="comment">//(high - low) + 1 = 256</span></span><br><span class="line">        cache = <span class="keyword">new</span> Integer[(high - low) + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span> j = low;<span class="comment">// 遍历 0 - 256</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; cache.length; k++)</span><br><span class="line">            <span class="comment">// -127开始累加，并且都放到cache中，注意k是从0开始的</span></span><br><span class="line">            cache[k] = <span class="keyword">new</span> Integer(j++);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// range [-128, 127] must be interned (JLS7 5.1.7)</span></span><br><span class="line">        <span class="keyword">assert</span> IntegerCache.high &gt;= <span class="number">127</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">IntegerCache</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>看到这里也就明白了，-128-127是如何实现的，超过这个范围为什么是在堆中创建了，在这个源码中，还有一个地方没有解释，那就是参数<code>java.lang.Integer.IntegerCache.high</code>，这个参数可以加载到手动传入的值，从而扩大或者缩小缓冲池的最大值</p><p>如果我们设置这个值的大小为200：<code>-Djava.lang.Integer.IntegerCache.high=200</code>，那么我们就能看到下面的代码的结果是true</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Integer e = <span class="number">200</span>;</span><br><span class="line">Integer f = <span class="number">200</span>;</span><br><span class="line">System.out.println(e == f);</span><br></pre></td></tr></table></figure><p>看完上面的分析之后，我希望大家记住一点：<strong>当需要进行自动装箱时，如果数字在 -128 至 127 之间时，会直接使用缓存中的对象，而不是重新创建一个对象</strong>。</p><p>注意：缓冲池只有Integer类型有</p><h2 id="包装类可以为null，而基本类型不可以"><a href="#包装类可以为null，而基本类型不可以" class="headerlink" title="包装类可以为null，而基本类型不可以"></a>包装类可以为null，而基本类型不可以</h2><p>别小看这一点区别，它使得包装类型可以应用于 POJO 中，而基本类型则不行。</p><p>那为什么 POJO 的属性必须要用包装类型呢？</p><p>对于基本数据类型，数据库的查询结果可能是 null，如果使用基本类型的话，因为要自动拆箱（将包装类型转为基本类型，比如说把 Integer 对象转换成 int 值），就会抛出 <code>NullPointerException</code> 的异常。因为基础类型的值只能是数值。</p><h2 id="包装类型可用于泛型，而基本类型不可以"><a href="#包装类型可用于泛型，而基本类型不可以" class="headerlink" title="包装类型可用于泛型，而基本类型不可以"></a>包装类型可用于泛型，而基本类型不可以</h2><p>我们先尝试定义一个List</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;<span class="keyword">int</span>&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br></pre></td></tr></table></figure><p>报错：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Syntax error, insert <span class="string">"Dimensions"</span> to complete ReferenceTypeList&lt;Integer&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br></pre></td></tr></table></figure><p>原因是泛型只能使用Object 类及其子类，所以包装类型可用于泛型，而基本类型不可以</p><h2 id="基本类型比包装类型更高效"><a href="#基本类型比包装类型更高效" class="headerlink" title="基本类型比包装类型更高效"></a>基本类型比包装类型更高效</h2><p>基本数据类型在栈中直接存储具体的数值，而包装类型则存储在堆中，栈中存放的是引用</p><p><img src="https://yerias.github.io/java_img/13.png" alt=""></p><p>很显然，相对于基本数据类型而言，包装类型需要占用更多的内存空间，假如没有基本数据类型的话，对于数值这类经常能用到的数据来说，每次都要通过new来创建包装类型就显得非常笨重。</p><h2 id="两个包装类型的值可以相同，但却可以不相等"><a href="#两个包装类型的值可以相同，但却可以不相等" class="headerlink" title="两个包装类型的值可以相同，但却可以不相等"></a>两个包装类型的值可以相同，但却可以不相等</h2><p>两个包装类型的值可以相同，但却不相等</p><p>不相等是因为两个包装类型在使用“==”进行判断的时候，判断的是其指向的内存地址是否相等。包装类的值如果是在堆中创建出的话，因为内存地址不同，所以返回的是false。</p><p>而值相同是因为包装类型在做<code>equals</code>比较的时候，都先拆箱成了基础类型，然后再做比较，即比较的是内容，所以为true。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (obj <span class="keyword">instanceof</span> Integer) &#123;</span><br><span class="line">        <span class="keyword">return</span> value == ((Integer)obj).intValue();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala之使用ScalikeJDBC操作MySQL</title>
      <link href="/2020/03/16/scala/2/"/>
      <url>/2020/03/16/scala/2/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>简介</li><li>配置</li><li>操作数据库</li></ol><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>ScalikeJDBC是一款给Scala开发者使用的简介访问类库，它是基于SQL的，使用者只需要关注SQL逻辑的编写，所有的数据库操作都交给ScalikeJDBC。这个类库内置包含了JDBCAPI，并且给用户提供了简单易用并且非常灵活的API。并且，QueryDSl（通用查询查询框架）使你的代码类型安全，半年过去可重复使用。我们可以在生产环境大胆地使用这款DB访问类库。</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><ol><li><p>解决依赖</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.11.8<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scalikejdbc.version</span>&gt;</span>3.3.2<span class="tag">&lt;/<span class="name">scalikejdbc.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mysql.jdbc.version</span>&gt;</span>5.1.38<span class="tag">&lt;/<span class="name">mysql.jdbc.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--Scala相关依赖--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--scalikejdbc相关依赖--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scalikejdbc<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scalikejdbc_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scalikejdbc.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scalikejdbc<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scalikejdbc-config_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scalikejdbc.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;mysql.jdbc.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>解决配置</p><p>在<code>src</code>的<code>main</code>目录下配置一个<code>resource</code>文件夹，文件夹下再创建一个<code>application.conf</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">db.default.driver="com.mysql.jdbc.Driver"</span><br><span class="line">db.default.url="jdbc:mysql://hadoop001/ruoze_d6?characterEncoding=utf-8"</span><br><span class="line">db.default.user="root"</span><br><span class="line">db.default.password="123456"</span><br><span class="line"></span><br><span class="line"><span class="comment"># Connection Pool settings</span></span><br><span class="line">db.default.poolInitialSize=10</span><br><span class="line">db.default.poolMaxSize=20</span><br><span class="line">db.default.connectionTimeoutMillis=1000</span><br></pre></td></tr></table></figure></li></ol><h2 id="操作数据库"><a href="#操作数据库" class="headerlink" title="操作数据库"></a>操作数据库</h2><ol><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> Employer(</span><br><span class="line">    <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">10</span>),</span><br><span class="line">    age <span class="built_in">varchar</span>(<span class="number">4</span>),</span><br><span class="line">    salary  <span class="built_in">varchar</span>(<span class="number">10</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure></li><li><p>scala编程实现增删改查操作</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.wsk.bigdata.scala.scalikejdbc</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scalikejdbc._</span><br><span class="line"><span class="keyword">import</span> scalikejdbc.config._</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义样例类获取数据</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Employer</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span>, salary: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">JdbcTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">DBs</span>.setupAll()<span class="comment">//初始化配置</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//数据</span></span><br><span class="line">    <span class="keyword">val</span> employers = <span class="type">List</span>(<span class="type">Employer</span>(<span class="string">"zhangsan"</span>, <span class="number">20</span>, <span class="number">18000</span>), <span class="type">Employer</span>(<span class="string">"zhangliu"</span>, <span class="number">50</span>, <span class="number">300000</span>), <span class="type">Employer</span>(<span class="string">"lisi"</span>, <span class="number">22</span>, <span class="number">22000</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//批量插入</span></span><br><span class="line">    insert(employers)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//查询出结果</span></span><br><span class="line">    <span class="keyword">val</span> results = select()</span><br><span class="line">    <span class="keyword">for</span> (employer &lt;- results) &#123;</span><br><span class="line">      println(employer.name, employer.age, employer.salary)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//修改</span></span><br><span class="line">    update(<span class="number">1000</span>, <span class="string">"zhangsan"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//根据条件删除</span></span><br><span class="line">    deleteByname(<span class="string">"zhangliu"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//删除所有</span></span><br><span class="line">    deleteAll()</span><br><span class="line"></span><br><span class="line">   <span class="comment">//关闭资源</span></span><br><span class="line">    <span class="type">DBs</span>.closeAll()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">//插入数据</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(employers: <span class="type">List</span>[<span class="type">Employer</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//事物插入</span></span><br><span class="line">    <span class="type">DB</span>.localTx &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      <span class="keyword">for</span> (employer &lt;- employers) &#123;</span><br><span class="line">        <span class="type">SQL</span>(<span class="string">"insert into wsktest(name,age,salary) values(?,?,?)"</span>)</span><br><span class="line">          .bind(employer.name, employer.age, employer.salary)</span><br><span class="line">          .update()<span class="comment">//更新操作</span></span><br><span class="line">                .apply()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  <span class="comment">//查询操作</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">select</span></span>(): <span class="type">List</span>[<span class="type">Employer</span>] = &#123;</span><br><span class="line">    <span class="type">DB</span>.readOnly &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      <span class="type">SQL</span>(<span class="string">"select * from wsktest"</span>)</span><br><span class="line">        .map(rs =&gt; <span class="type">Employer</span>(rs.string(<span class="string">"name"</span>), rs.int(<span class="string">"age"</span>), rs.long(<span class="string">"salary"</span>)))</span><br><span class="line">        .list()  <span class="comment">//结果转换成list</span></span><br><span class="line">        .apply() </span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//更新操作</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(age: <span class="type">Int</span>, name: <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="type">DB</span>.autoCommit &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      <span class="type">SQL</span>(<span class="string">"update wsktest set age = ? where name = ?"</span>)</span><br><span class="line">        .bind(age, name)</span><br><span class="line">        .update()<span class="comment">//更新操作</span></span><br><span class="line">        .apply()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//根据条件删除</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteByname</span></span>(name: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">DB</span>.autoCommit &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      <span class="type">SQL</span>(<span class="string">"delete from wsktest where name = ?"</span>)</span><br><span class="line">        .bind(name)<span class="comment">//更新操作</span></span><br><span class="line">        .update()</span><br><span class="line">        .apply()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//删除所有</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteAll</span></span>(): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="type">DB</span>.autoCommit &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      <span class="type">SQL</span>(<span class="string">"delete from wsktest "</span>)</span><br><span class="line">        .update()<span class="comment">//更新操作</span></span><br><span class="line">        .apply()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Curator的介绍&amp;使用</title>
      <link href="/2020/03/15/zookeeper/2/"/>
      <url>/2020/03/15/zookeeper/2/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>简介</li><li>基于Curator的Zookeeper基本用法</li><li>监听器</li><li>分布式锁</li><li>Leader选举</li></ol><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Apache Curator是一个比较完善的ZooKeeper客户端框架，通过封装的一套高级API 简化了ZooKeeper的操作。通过查看官方文档，可以发现Curator主要解决了三类问题：</p><ul><li>封装ZooKeeper client与ZooKeeper server之间的连接处理</li><li>提供了一套Fluent风格的操作API</li><li>提供ZooKeeper各种应用场景(recipe， 比如：分布式锁服务、集群领导选举、共享计数器、缓存机制、分布式队列等)的抽象封装</li></ul><h3 id="Curator主要从以下几个方面降低了zk使用的复杂性："><a href="#Curator主要从以下几个方面降低了zk使用的复杂性：" class="headerlink" title="Curator主要从以下几个方面降低了zk使用的复杂性："></a>Curator主要从以下几个方面降低了zk使用的复杂性：</h3><ul><li>重试机制:提供可插拔的重试机制, 它将给捕获所有可恢复的异常配置一个重试策略，并且内部也提供了几种标准的重试策略(比如指数补偿)</li><li>连接状态监控: Curator初始化之后会一直对zk连接进行监听，一旦发现连接状态发生变化将会作出相应的处理</li><li>zk客户端实例管理:Curator会对zk客户端到server集群的连接进行管理，并在需要的时候重建zk实例，保证与zk集群连接的可靠性</li><li>各种使用场景支持:Curator实现了zk支持的大部分使用场景（甚至包括zk自身不支持的场景），这些实现都遵循了zk的最佳实践，并考虑了各种极端情况</li></ul><h2 id="基于Curator的Zookeeper基本用法"><a href="#基于Curator的Zookeeper基本用法" class="headerlink" title="基于Curator的Zookeeper基本用法"></a>基于Curator的Zookeeper基本用法</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CuratorBase</span> </span>&#123;</span><br><span class="line">    <span class="comment">//会话超时时间</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> SESSION_TIMEOUT = <span class="number">30</span> * <span class="number">1000</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//连接超时时间</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> CONNECTION_TIMEOUT = <span class="number">3</span> * <span class="number">1000</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//ZooKeeper服务地址</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String CONNECT_ADDR = <span class="string">"192.168.1.1:2100,192.168.1.1:2101,192.168.1.:2102"</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//创建连接实例</span></span><br><span class="line">    <span class="keyword">private</span> CuratorFramework client = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;  </span><br><span class="line">        <span class="comment">//1 重试策略：初试时间为1s 重试10次</span></span><br><span class="line">        RetryPolicy retryPolicy = <span class="keyword">new</span> ExponentialBackoffRetry(<span class="number">1000</span>, <span class="number">10</span>);</span><br><span class="line">        <span class="comment">//2 通过工厂创建连接</span></span><br><span class="line">        CuratorFramework client = CuratorFrameworkFactory.builder()</span><br><span class="line">                    .connectString(CONNECT_ADDR).connectionTimeoutMs(CONNECTION_TIMEOUT)</span><br><span class="line">                    .sessionTimeoutMs(SESSION_TIMEOUT)</span><br><span class="line">                    .retryPolicy(retryPolicy)</span><br><span class="line"><span class="comment">//命名空间           .namespace("super")</span></span><br><span class="line">                    .build();</span><br><span class="line">        <span class="comment">//3 开启连接</span></span><br><span class="line">        cf.start();</span><br><span class="line">        </span><br><span class="line">        System.out.println(States.CONNECTED);</span><br><span class="line">        System.out.println(cf.getState());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建永久节点</span></span><br><span class="line">        client.create().forPath(<span class="string">"/curator"</span>,<span class="string">"/curator data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//创建永久有序节点</span></span><br><span class="line">        client.create().withMode(CreateMode.PERSISTENT_SEQUENTIAL).forPath(<span class="string">"/curator_sequential"</span>,<span class="string">"/curator_sequential data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//创建临时节点</span></span><br><span class="line">        client.create().withMode(CreateMode.EPHEMERAL)</span><br><span class="line">            .forPath(<span class="string">"/curator/ephemeral"</span>,<span class="string">"/curator/ephemeral data"</span>.getBytes());</span><br><span class="line"> </span><br><span class="line">        <span class="comment">//创建临时有序节点</span></span><br><span class="line">        client.create().withMode(CreateMode.EPHEMERAL_SEQUENTIAL) .forPath(<span class="string">"/curator/ephemeral_path1"</span>,<span class="string">"/curator/ephemeral_path1 data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        client.create().withProtection().withMode(CreateMode.EPHEMERAL_SEQUENTIAL).forPath(<span class="string">"/curator/ephemeral_path2"</span>,<span class="string">"/curator/ephemeral_path2 data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//测试检查某个节点是否存在</span></span><br><span class="line">        Stat stat1 = client.checkExists().forPath(<span class="string">"/curator"</span>);</span><br><span class="line">        Stat stat2 = client.checkExists().forPath(<span class="string">"/curator2"</span>);</span><br><span class="line">        </span><br><span class="line">        System.out.println(<span class="string">"'/curator'是否存在： "</span> + (stat1 != <span class="keyword">null</span> ? <span class="keyword">true</span> : <span class="keyword">false</span>));</span><br><span class="line">        System.out.println(<span class="string">"'/curator2'是否存在： "</span> + (stat2 != <span class="keyword">null</span> ? <span class="keyword">true</span> : <span class="keyword">false</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取某个节点的所有子节点</span></span><br><span class="line">        System.out.println(client.getChildren().forPath(<span class="string">"/"</span>));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//获取某个节点数据</span></span><br><span class="line">        System.out.println(<span class="keyword">new</span> String(client.getData().forPath(<span class="string">"/curator"</span>)));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//设置某个节点数据</span></span><br><span class="line">        client.setData().forPath(<span class="string">"/curator"</span>,<span class="string">"/curator modified data"</span>.getBytes());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建测试节点</span></span><br><span class="line">        client.create().orSetData().creatingParentContainersIfNeeded()</span><br><span class="line">            .forPath(<span class="string">"/curator/del_key1"</span>,<span class="string">"/curator/del_key1 data"</span>.getBytes());</span><br><span class="line"> </span><br><span class="line">        client.create().orSetData().creatingParentContainersIfNeeded()</span><br><span class="line">        .forPath(<span class="string">"/curator/del_key2"</span>,<span class="string">"/curator/del_key2 data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        client.create().forPath(<span class="string">"/curator/del_key2/test_key"</span>,<span class="string">"test_key data"</span>.getBytes());</span><br><span class="line">              </span><br><span class="line">        <span class="comment">//删除该节点</span></span><br><span class="line">        client.delete().forPath(<span class="string">"/curator/del_key1"</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//级联删除子节点</span></span><br><span class="line">        client.delete().guaranteed().deletingChildrenIfNeeded().forPath(<span class="string">"/curator/del_key2"</span>);</span><br><span class="line">    ｝ </span><br><span class="line">｝</span><br></pre></td></tr></table></figure><ul><li><code>orSetData()</code>方法：如果节点存在则Curator将会使用给出的数据设置这个节点的值，相当于 setData() 方法</li><li><code>creatingParentContainersIfNeeded()</code>方法：如果指定节点的父节点不存在，则Curator将会自动级联创建父节点</li><li><code>guaranteed()</code>方法：如果服务端可能删除成功，但是client没有接收到删除成功的提示，Curator将会在后台持续尝试删除该节点</li><li><code>deletingChildrenIfNeeded()</code>方法：如果待删除节点存在子节点，则Curator将会级联删除该节点的子节点</li></ul><p>事务管理：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">* 事务管理：碰到异常，事务会回滚</span><br><span class="line">     * <span class="meta">@throws</span> Exception</span><br><span class="line">     */</span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testTransaction</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">//定义几个基本操作</span></span><br><span class="line">        CuratorOp createOp = client.transactionOp().create()</span><br><span class="line">                .forPath(<span class="string">"/curator/one_path"</span>,<span class="string">"some data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        CuratorOp setDataOp = client.transactionOp().setData()</span><br><span class="line">                .forPath(<span class="string">"/curator"</span>,<span class="string">"other data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        CuratorOp deleteOp = client.transactionOp().delete()</span><br><span class="line">                .forPath(<span class="string">"/curator"</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//事务执行结果</span></span><br><span class="line">        List&lt;CuratorTransactionResult&gt; results = client.transaction()</span><br><span class="line">                .forOperations(createOp,setDataOp,deleteOp);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//遍历输出结果</span></span><br><span class="line">        <span class="keyword">for</span>(CuratorTransactionResult result : results)&#123;</span><br><span class="line">            System.out.println(<span class="string">"执行结果是： "</span> + result.getForPath() + <span class="string">"--"</span> + result.getType());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//因为节点“/curator”存在子节点，所以在删除的时候将会报错，事务回滚</span></span><br></pre></td></tr></table></figure><h2 id="监听器"><a href="#监听器" class="headerlink" title="监听器"></a>监听器</h2><p>Curator提供了三种Watcher(Cache)来监听结点的变化：</p><ul><li><strong>Path Cache</strong>：监视一个路径下1）孩子结点的创建、2）删除，3）以及结点数据的更新。产生的事件会传递给注册的PathChildrenCacheListener。</li><li><strong>Node Cache</strong>：监视一个结点的创建、更新、删除，并将结点的数据缓存在本地。</li><li><strong>Tree Cache</strong>：Path Cache和Node Cache的“合体”，监视路径下的创建、更新、删除事件，并缓存路径下所有孩子结点的数据。</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 在注册监听器的时候，如果传入此参数，当事件触发时，逻辑由线程池处理</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        ExecutorService pool = Executors.newFixedThreadPool(<span class="number">2</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 监听数据节点的变化情况</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">final</span> NodeCache nodeCache = <span class="keyword">new</span> NodeCache(client, <span class="string">"/zk-huey/cnode"</span>, <span class="keyword">false</span>);</span><br><span class="line">        nodeCache.start(<span class="keyword">true</span>);</span><br><span class="line">        nodeCache.getListenable().addListener(</span><br><span class="line">            <span class="keyword">new</span> NodeCacheListener() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">nodeChanged</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    System.out.println(<span class="string">"Node data is changed, new data: "</span> + </span><br><span class="line">                        <span class="keyword">new</span> String(nodeCache.getCurrentData().getData()));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;, </span><br><span class="line">            pool</span><br><span class="line">        );</span><br><span class="line">        </span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 监听子节点的变化情况</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">final</span> PathChildrenCache childrenCache = <span class="keyword">new</span> PathChildrenCache(client, <span class="string">"/zk-huey"</span>, <span class="keyword">true</span>);</span><br><span class="line">        childrenCache.start(StartMode.POST_INITIALIZED_EVENT);</span><br><span class="line">        childrenCache.getListenable().addListener(</span><br><span class="line">            <span class="keyword">new</span> PathChildrenCacheListener() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">childEvent</span><span class="params">(CuratorFramework client, PathChildrenCacheEvent event)</span></span></span><br><span class="line"><span class="function">                        <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">switch</span> (event.getType()) &#123;</span><br><span class="line">                        <span class="keyword">case</span> CHILD_ADDED:</span><br><span class="line">                            System.out.println(<span class="string">"CHILD_ADDED: "</span> + event.getData().getPath());</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        <span class="keyword">case</span> CHILD_REMOVED:</span><br><span class="line">                            System.out.println(<span class="string">"CHILD_REMOVED: "</span> + event.getData().getPath());</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        <span class="keyword">case</span> CHILD_UPDATED:</span><br><span class="line">                            System.out.println(<span class="string">"CHILD_UPDATED: "</span> + event.getData().getPath());</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        <span class="keyword">default</span>:</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            pool</span><br><span class="line">        );</span><br><span class="line">        </span><br><span class="line">        client.setData().forPath(<span class="string">"/zk-huey/cnode"</span>, <span class="string">"world"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        Thread.sleep(<span class="number">10</span> * <span class="number">1000</span>);</span><br><span class="line">        pool.shutdown();</span><br><span class="line">        client.close();</span><br></pre></td></tr></table></figure><h2 id="分布式锁"><a href="#分布式锁" class="headerlink" title="分布式锁"></a>分布式锁</h2><p>分布式编程时，比如最容易碰到的情况就是应用程序在线上多机部署，于是当多个应用同时访问某一资源时，就需要某种机制去协调它们。例如，现在一台应用正在rebuild缓存内容，要临时锁住某个区域暂时不让访问；又比如调度程序每次只想一个任务被一台应用执行等等。</p><p>下面的程序会启动两个线程t1和t2去争夺锁，拿到锁的线程会占用5秒。运行多次可以观察到，有时是t1先拿到锁而t2等待，有时又会反过来。Curator会用我们提供的lock路径的结点作为全局锁，这个结点的数据类似这种格式：[<em>c</em>64e0811f-9475-44ca-aa36-c1db65ae5350-lock-0000000005]，每次获得锁时会生成这种串，释放锁时清空数据。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.curator.framework.CuratorFramework;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.CuratorFrameworkFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.recipes.locks.InterProcessMutex;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.retry.RetryNTimes;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Curator framework's distributed lock test.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CuratorDistrLockTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Zookeeper info */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ZK_ADDRESS = <span class="string">"192.168.1.100:2181"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ZK_LOCK_PATH = <span class="string">"/zktest"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 1.Connect to zk</span></span><br><span class="line">        CuratorFramework client = CuratorFrameworkFactory.newClient(</span><br><span class="line">                ZK_ADDRESS,</span><br><span class="line">                <span class="keyword">new</span> RetryNTimes(<span class="number">10</span>, <span class="number">5000</span>)</span><br><span class="line">        );</span><br><span class="line">        client.start();</span><br><span class="line">        System.out.println(<span class="string">"zk client start successfully!"</span>);</span><br><span class="line"></span><br><span class="line">        Thread t1 = <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">            doWithLock(client);</span><br><span class="line">        &#125;, <span class="string">"t1"</span>);</span><br><span class="line">        Thread t2 = <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">            doWithLock(client);</span><br><span class="line">        &#125;, <span class="string">"t2"</span>);</span><br><span class="line"></span><br><span class="line">        t1.start();</span><br><span class="line">        t2.start();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">doWithLock</span><span class="params">(CuratorFramework client)</span> </span>&#123;</span><br><span class="line">        InterProcessMutex lock = <span class="keyword">new</span> InterProcessMutex(client, ZK_LOCK_PATH);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (lock.acquire(<span class="number">10</span> * <span class="number">1000</span>, TimeUnit.SECONDS)) &#123;</span><br><span class="line">                System.out.println(Thread.currentThread().getName() + <span class="string">" hold lock"</span>);</span><br><span class="line">                Thread.sleep(<span class="number">5000L</span>);</span><br><span class="line">                System.out.println(Thread.currentThread().getName() + <span class="string">" release lock"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                lock.release();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Leader选举"><a href="#Leader选举" class="headerlink" title="Leader选举"></a>Leader选举</h2><p>当集群里的某个服务down机时，我们可能要从slave结点里选出一个作为新的master，这时就需要一套能在分布式环境中自动协调的Leader选举方法。Curator提供了LeaderSelector监听器实现Leader选举功能。同一时刻，只有一个Listener会进入takeLeadership()方法，说明它是当前的Leader。注意：<strong>当Listener从takeLeadership()退出时就说明它放弃了“Leader身份”</strong>，这时Curator会利用Zookeeper再从剩余的Listener中选出一个新的Leader。autoRequeue()方法使放弃Leadership的Listener有机会重新获得Leadership，如果不设置的话放弃了的Listener是不会再变成Leader的。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.curator.framework.CuratorFramework;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.CuratorFrameworkFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.recipes.leader.LeaderSelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.recipes.leader.LeaderSelectorListener;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.state.ConnectionState;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.retry.RetryNTimes;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.utils.EnsurePath;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Curator framework's leader election test.</span></span><br><span class="line"><span class="comment"> * Output:</span></span><br><span class="line"><span class="comment"> *  LeaderSelector-2 take leadership!</span></span><br><span class="line"><span class="comment"> *  LeaderSelector-2 relinquish leadership!</span></span><br><span class="line"><span class="comment"> *  LeaderSelector-1 take leadership!</span></span><br><span class="line"><span class="comment"> *  LeaderSelector-1 relinquish leadership!</span></span><br><span class="line"><span class="comment"> *  LeaderSelector-0 take leadership!</span></span><br><span class="line"><span class="comment"> *  LeaderSelector-0 relinquish leadership! </span></span><br><span class="line"><span class="comment"> *      ...</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CuratorLeaderTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Zookeeper info */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ZK_ADDRESS = <span class="string">"192.168.1.100:2181"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ZK_PATH = <span class="string">"/zktest"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        LeaderSelectorListener listener = <span class="keyword">new</span> LeaderSelectorListener() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">takeLeadership</span><span class="params">(CuratorFramework client)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                System.out.println(Thread.currentThread().getName() + <span class="string">" take leadership!"</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// takeLeadership() method should only return when leadership is being relinquished.</span></span><br><span class="line">                Thread.sleep(<span class="number">5000L</span>);</span><br><span class="line"></span><br><span class="line">                System.out.println(Thread.currentThread().getName() + <span class="string">" relinquish leadership!"</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">stateChanged</span><span class="params">(CuratorFramework client, ConnectionState state)</span> </span>&#123;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">            registerListener(listener);</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">            registerListener(listener);</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">            registerListener(listener);</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        Thread.sleep(Integer.MAX_VALUE);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">registerListener</span><span class="params">(LeaderSelectorListener listener)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 1.Connect to zk</span></span><br><span class="line">        CuratorFramework client = CuratorFrameworkFactory.newClient(</span><br><span class="line">                ZK_ADDRESS,</span><br><span class="line">                <span class="keyword">new</span> RetryNTimes(<span class="number">10</span>, <span class="number">5000</span>)</span><br><span class="line">        );</span><br><span class="line">        client.start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.Ensure path</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">new</span> EnsurePath(ZK_PATH).ensure(client.getZookeeperClient());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.Register listener</span></span><br><span class="line">        LeaderSelector selector = <span class="keyword">new</span> LeaderSelector(client, ZK_PATH, listener);</span><br><span class="line">        selector.autoRequeue();</span><br><span class="line">        selector.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><p>转载：<a href="https://www.cnblogs.com/erbing/p/9799098.html" target="_blank" rel="noopener">https://www.cnblogs.com/erbing/p/9799098.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Zookeeper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Zookeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA的String类源码解析</title>
      <link href="/2020/03/11/java/10/"/>
      <url>/2020/03/11/java/10/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>源码的角度解析String不可变</li><li>String Pool 的角度解析String不可变</li><li>String对象不可变性的优缺点</li><li>String对象是否真的不可变</li><li>从源码的角度解析StringBuilder的可变</li><li>从源码的角度解析StringBuffer和StringBuilder的异同</li><li>编译器对String做出了哪些优化</li></ol><h2 id="从源码的角度解析String不可变"><a href="#从源码的角度解析String不可变" class="headerlink" title="从源码的角度解析String不可变"></a>从源码的角度解析String不可变</h2><p>所谓的不可变类是指这个类的实例一旦创建完成后，就不能改变其成员变量值。</p><p>String类中每一个看起来会修改String值的方法，实际上都是创建了一个全新的String对象</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">upcase</span><span class="params">(String s)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> s.toUpperCase();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    String name = <span class="string">"tunan"</span>;</span><br><span class="line">    System.out.println(name);<span class="comment">//tunan</span></span><br><span class="line"></span><br><span class="line">    String name2 = upcase(name);</span><br><span class="line">    System.out.println(name2);<span class="comment">//TUNAN</span></span><br><span class="line"></span><br><span class="line">    System.out.println(name);<span class="comment">//tunan</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当把name传给upcase()方法的时候，实际上传递的是一个引用的拷贝。而该引用所指的对象其实一直待在单一的物理位置上，从未动过。</p><p>回到upcase()的定义，传入其中的引用有了名字s，只有upcase()运行的时候，局部引用s才存在。一旦upcase()运行结束，s就消失了。当然了，upcase()的返回值，其实只是最终结果的引用。这足已说明，upcase()返回的引用已经指向了一个新的对象name2，而原本的name则还在原地。</p><p>既然String类型的变量name没有变过，我们从源码的角度去看为什么没有改变</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">String</span></span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span>, <span class="title">Comparable</span>&lt;<span class="title">String</span>&gt;, <span class="title">CharSequence</span> </span>&#123;</span><br><span class="line">    <span class="comment">//使用字节数组存储字符串</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">char</span> value[];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//存储hash值</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> hash; <span class="comment">// Default to 0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以清楚的看到String类是一个final类，但这并不是String不可变的真正原因，继续看String实现了CharSequence接口</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">CharSequence</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">length</span><span class="params">()</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">char</span> <span class="title">charAt</span><span class="params">(<span class="keyword">int</span> index)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="function">CharSequence <span class="title">subSequence</span><span class="params">(<span class="keyword">int</span> start, <span class="keyword">int</span> end)</span></span>;</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>CharSequence是一个接口，它只包括length(), charAt(int index), subSequence(int start, int end)这几个API接口。同时除了String实现了CharSequence之外，StringBuffer和StringBuilder也实现了CharSequence接口。 </p><p>也就是说，CharSequence其实也就是定义了字符串操作的接口，其他具体的实现是由String、StringBuilder、StringBuffer完成的，String、StringBuilder、StringBuffer都可以转化为CharSequence类型。</p><p>继续看String类，<code>private final char value[];</code>这个final类型的字符型变量才是真正存储字符串的容器，也正是因为这个变量是final的，才真正决定了字符串不可变，也许你不相信，你可以说Stirng类也是final修饰的，也是不可变的，那么如果StringBuilder和StringBuffer也是final修饰的呢？</p><h2 id="String-Pool-的角度解析String不可变"><a href="#String-Pool-的角度解析String不可变" class="headerlink" title="String Pool 的角度解析String不可变"></a>String Pool 的角度解析String不可变</h2><p>JVM为了提升性能和减少内存开销，避免字符串的重复创建，其维护了一块特殊的内存空间，这就是我们今天要讨论的核心，即字符串池（String Pool）。</p><p>我们知道，在Java中有两种创建字符串对象的方式：</p><ol><li><p>采用字面值的方式赋值 </p></li><li><p>采用new关键字新建一个字符串对象。</p><p>这两种方式在性能和内存占用方面存在着差别。</p></li></ol><p>方式一：采用字面值的方式赋值，例如：</p><p><img src="https://yerias.github.io/java_img/1.png" alt=""></p><p>采用字面值的方式创建一个字符串时，JVM首先会去字符串池中查找是否存在”aaa”这个对象，如果不存在，则在字符串池中创建”aaa”这个对象，然后将池中”aaa”这个对象的引用地址返回给字符串常量str，这样str会指向池中”aaa”这个字符串对象；如果存在，则不创建任何对象，直接将池中”aaa”这个对象的地址返回，赋给字符串常量。</p><p><img src="https://yerias.github.io/java_img/3.jpg" alt=""></p><p> 在本例中，执行：str == str2 ，会得到以下结果：</p><p><img src="https://yerias.github.io/java_img/2.png" alt=""></p><p>这是因为，创建字符串对象str2时，字符串池中已经存在”aaa”这个对象，直接把对象”aaa”的引用地址返回给str2，这样str2指向了池中”aaa”这个对象，也就是说str和str2指向了同一个对象，因此语句System.out.println(str == str2)输出：true。</p><p>方式二：采用new关键字新建一个字符串对象，例如：</p><p><img src="https://yerias.github.io/java_img/4.png" alt=""></p><p>采用new关键字新建一个字符串对象时，JVM首先在字符串池中查找有没有”aaa”这个字符串对象，如果有，则不在池中再去创建”aaa”这个对象了，直接在堆中创建一个”aaa”字符串对象，然后将堆中的这个”aaa”对象的地址返回赋给引用str3，这样，str3就指向了堆中创建的这个”aaa”字符串对象；如果没有，则首先在字符串池中创建一个”aaa”字符串对象，然后再在堆中创建一个”aaa”字符串对象，然后将堆中这个”aaa”字符串对象的地址返回赋给str3引用，这样，str3指向了堆中创建的这个”aaa”字符串对象。</p><p><img src="https://yerias.github.io/java_img/6.jpg" alt=""></p><p>在这个例子中，执行：str3 == str4，得到以下结果：</p><p><img src="https://yerias.github.io/java_img/5.png" alt=""></p><p>因为，采用new关键字创建对象时，每次new出来的都是一个新的对象，也即是说引用str3和str4指向的是两个不同的对象，因此语句System.out.println(str3 == str4)输出：false。</p><p>字符串池的实现有一个前提条件：String对象是不可变的。因为这样可以保证多个引用可以同时指向字符串池中的同一个对象。如果字符串是可变的，那么一个引用操作改变了对象的值，对其他引用会有影响，这样显然是不合理的。</p><p><strong>Java语言规范（Java Language Specification）</strong>中对字符串做出了如下说明：每一个字符串常量都是指向一个字符串类实例的引用。字符串对象有一个固定值。字符串常量，或者一般的说，常量表达式中的字符串都被使用方法 String.intern进行保留来共享唯一的实例。</p><p>以上是Java语言规范中的原文，比较官方，用更通俗易懂的语言翻译过来主要说明了三点：</p><ol><li>每一个字符串常量都指向字符串池中或者堆内存中的一个字符串实例。</li><li>字符串对象值是固定的，一旦创建就不能再修改。</li><li>字符串常量或者常量表达式中的字符串都被方法String.intern()在字符串池中保留了唯一的实例。</li></ol><p><img src="https://yerias.github.io/java_img/7.jpg" alt=""></p><p>​        其他包</p><p>​            <img src="https://yerias.github.io/java_img/8.png" alt=""></p><p>​        结果</p><p>​                <img src="https://yerias.github.io/java_img/9.png" alt=""></p><p>结论：</p><ul><li>同一个包下同一个类中的字符串常量的引用指向同一个字符串对象；</li><li>同一个包下不同的类中的字符串常量的引用指向同一个字符串对象；</li><li>不同的包下不同的类中的字符串常量的引用仍然指向同一个字符串对象；</li><li>由常量表达式计算出的字符串是在编译时进行计算,然后被当作常量；</li><li>在运行时通过连接计算出的字符串是新创建的，因此是不同的；</li><li>通过计算生成的字符串显示调用intern方法后产生的结果与原来存在的同样内容的字符串常量是一样的。</li></ul><p>从上面的例子可以看出，字符串常量在<strong>编译时</strong>计算和在<strong>运行时</strong>计算，其执行过程是不同的，得到的结果也是不同的。我们来看看下面这段代码：</p><p><img src="https://yerias.github.io/java_img/10.png" alt=""></p><p> 代码输出如下：</p><p><img src="https://yerias.github.io/java_img/11.png" alt=""></p><p>为什么出现上面的结果呢？这是因为，字符串字面量拼接操作是在Java编译器编译期间就执行了，也就是说编译器编译时，直接把”java”、”language”和”specification”这三个字面量进行”+”操作得到一个”javalanguagespecification” 常量，并且直接将这个常量放入字符串池中，这样做实际上是一种优化，将3个字面量合成一个，避免了创建多余的字符串对象。而字符串引用的”+”运算是在Java运行期间执行的，即str + str2 + str3在程序执行期间才会进行计算，它会在堆内存中重新创建一个拼接后的字符串对象。总结来说就是：字面量”+”拼接是在编译期间进行的，拼接后的字符串存放在字符串池中；而字符串引用的”+”拼接运算实在运行时进行的，新创建的字符串存放在堆中。</p><p><strong>到这里我们也能理解了什么是字符串的不可变性</strong>，其本质是在字符串池中开辟了一块空间，字符串的地址不变，字符串变量重新赋值感觉是字符串变了，其实是在字符串池中开辟了另外一块空间，并且字符串的引用重新指向新的空间地址，而原来的字符串内容和内存地址在字符串池中没有改变过。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String name = <span class="string">"aaa"</span>;</span><br><span class="line">name = <span class="string">"bbb"</span>;</span><br><span class="line">System.out.println(name);</span><br></pre></td></tr></table></figure><p><img src="https://yerias.github.io/java_img/12.jpg" alt=""></p><p>字符串池的位置是在堆中，那么GC的时候<strong>字符串如何保证不被GC？</strong><br>为了优化空间，运行时实例创建的全局字符串常量池中有一个表，总是为池中每个唯一的字符串对象维护一个引用。这就意味着它们一直引用着字符串常量池中的对象，所以，在常量池中的这些字符串不会被垃圾收集器回收。</p><p>总结：字符串是常量，字符串池中的每个字符串对象只有唯一的一份，可以被多个引用所指向，避免了重复创建内容相同的字符串；通过字面值赋值创建的字符串对象存放在字符串池中，通过关键字new出来的字符串对象存放在堆中。</p><h2 id="String对象不可变性的优缺点"><a href="#String对象不可变性的优缺点" class="headerlink" title="String对象不可变性的优缺点"></a>String对象不可变性的优缺点</h2><p><strong>1.字符串常量池的需要</strong>.<br>字符串常量池可以将一些字符常量放在常量池中重复使用，避免每次都重新创建相同的对象、节省存储空间。但如果字符串是可变的，此时相同内容的String还指向常量池的同一个内存空间，当某个变量改变了该内存的值时，其他遍历的值也会发生改变。所以不符合常量池设计的初衷。</p><p><strong>2. 线程安全考虑</strong>。<br>同一个字符串实例可以被多个线程共享。这样便不用因为线程安全问题而使用同步。字符串自己便是线程安全的。</p><p><strong>3. 类加载器要用到字符串，不可变性提供了安全性，以便正确的类被加载</strong>。譬如你想加载java.sql.Connection类，而这个值被改成了myhacked.Connection，那么会对你的数据库造成不可知的破坏。</p><p><strong>4. 支持hash映射和缓存。</strong><br>因为字符串是不可变的，所以在它创建的时候hashcode就被缓存了，不需要重新计算。这就使得字符串很适合作为Map中的键，字符串的处理速度要快过其它的键对象。这就是HashMap中的键往往都使用字符串。</p><p>缺点：</p><ol><li>如果有对String对象值改变的需求，那么会创建大量的String对象(使用StringBuffer或者StringBuilder替代)。</li></ol><h2 id="String对象是否真的不可变"><a href="#String对象是否真的不可变" class="headerlink" title="String对象是否真的不可变"></a>String对象是否真的不可变</h2><p>String对象的不可变，其根本是内存地址的不可变，这在字符串池中有解析</p><p>虽然String对象将value设置为final，并且还通过各种机制保证其成员变量不可改变。但是还是可以通过反射机制的手段改变其值。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建字符串"Hello World"， 并赋给引用s</span></span><br><span class="line">String s = <span class="string">"hello world"</span>;</span><br><span class="line">System.out.println(<span class="string">"s = "</span> +s);</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取String类中的value字段(private final char value[];)</span></span><br><span class="line">Field valueFieldOfString  = String.class.getDeclaredField("value");</span><br><span class="line"></span><br><span class="line"><span class="comment">//改变value属性的访问权限</span></span><br><span class="line">valueFieldOfString .setAccessible(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取s对象上的value属性的值</span></span><br><span class="line"><span class="keyword">char</span>[] value = (<span class="keyword">char</span>[]) valueFieldOfString.get(s);</span><br><span class="line"></span><br><span class="line"><span class="comment">//改变value所引用的数组中的第5个字符</span></span><br><span class="line">value[<span class="number">5</span>] =<span class="string">'_'</span>;</span><br><span class="line">System.out.println(<span class="string">"s = "</span> +s);</span><br></pre></td></tr></table></figure><p>打印结果为：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">s = Hello World</span><br><span class="line">s = Hello_World</span><br></pre></td></tr></table></figure><p>发现String的值已经发生了改变。也就是说，通过反射是可以修改所谓的“不可变”对象的</p><h2 id="从源码的角度解析StringBuilder的可变"><a href="#从源码的角度解析StringBuilder的可变" class="headerlink" title="从源码的角度解析StringBuilder的可变"></a>从源码的角度解析StringBuilder的可变</h2><p>StringBuilder可以动态构造字符串，并且是线程不安全的，我们从源码的角度解析StringBuilder为什么可以动态构造字符串。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">StringBuilder</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">AbstractStringBuilder</span></span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span>, <span class="title">CharSequence</span></span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">    <span class="comment">//默认char容量16</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">StringBuilder</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(<span class="number">16</span>);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//指定了则使用父类的构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">StringBuilder</span><span class="params">(<span class="keyword">int</span> capacity)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(capacity);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>我们首先看到StringBuilder也是final修饰的， 和String一样，不仅如此StringBuffer也是final修饰的，下面将不再解释，它继承了AbstractStringBuilder，并且和String、StringBuffer一样，都实现了CharSequence接口</p><p>看构造方法默认容量是16，指定了容量则使用父类的构造方法，我们现在去看下父类中如何实现的</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">AbstractStringBuilder</span> <span class="keyword">implements</span> <span class="title">Appendable</span>, <span class="title">CharSequence</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 使用字节数组存储字符串</span></span><br><span class="line">    <span class="keyword">char</span>[] value;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 记录存储的字符串的长度</span></span><br><span class="line">    <span class="keyword">int</span> count;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 空构造方法</span></span><br><span class="line">    AbstractStringBuilder() &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 传入初始值的构造方法</span></span><br><span class="line">    AbstractStringBuilder(<span class="keyword">int</span> capacity) &#123;</span><br><span class="line">        value = <span class="keyword">new</span> <span class="keyword">char</span>[capacity];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>父类的构造方法中是new了一个指定长度的char字节数组，这说明StringBuilder底层也是使用字符数组保存字符串的，需要注意的是value的定义，和String类中的实现不同，这里没有private和final修饰，正是因为这点，所以StringBuilder是可变的，StringBuilder的value字节数组可以动态的改变大小。</p><p>我们已经知道了StringBuilder为什么可变，还需要注意的是它的append方法，该方法直接决定了StringBuilder如何追加字符串。也是和StringBuffer唯一不同的地方</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StringBuilder <span class="title">append</span><span class="params">(String str)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>.append(str);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>直接重写的父类方法</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> AbstractStringBuilder <span class="title">append</span><span class="params">(String str)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (str == <span class="keyword">null</span>)<span class="comment">//检查是否空</span></span><br><span class="line">        <span class="keyword">return</span> appendNull();</span><br><span class="line">    <span class="keyword">int</span> len = str.length();<span class="comment">//获得字符串长度</span></span><br><span class="line">    ensureCapacityInternal(count + len);<span class="comment">//检查容量/库容</span></span><br><span class="line">    str.getChars(<span class="number">0</span>, len, value, count);<span class="comment">//拷贝内容</span></span><br><span class="line">    count += len;<span class="comment">//增加长度</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;<span class="comment">//返回</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 扩容</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">newCapacity</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// overflow-conscious code</span></span><br><span class="line">    <span class="keyword">int</span> newCapacity = (value.length &lt;&lt; <span class="number">1</span>) + <span class="number">2</span>;<span class="comment">//扩容为原字节数组长度的两倍+2，注意不是count</span></span><br><span class="line">    <span class="keyword">if</span> (newCapacity - minCapacity &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        newCapacity = minCapacity;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (newCapacity &lt;= <span class="number">0</span> || MAX_ARRAY_SIZE - newCapacity &lt; <span class="number">0</span>)</span><br><span class="line">        ? hugeCapacity(minCapacity)</span><br><span class="line">        : newCapacity;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们发现append方法的底层是对字符数组内容的复制，并且容量不够时，是扩容为原字节数组长度的两倍+2，是字节数组，不是容量</p><h2 id="从源码的角度解析StringBuffer和StringBuilder的异同"><a href="#从源码的角度解析StringBuffer和StringBuilder的异同" class="headerlink" title="从源码的角度解析StringBuffer和StringBuilder的异同"></a>从源码的角度解析StringBuffer和StringBuilder的异同</h2><p>StringBuffer和StringBuilder的所有实现一模一样，包括继承的父类，实现的接口，扩容机制，value的定义，正是这些特性让他们两很像，同时也都支持动态构造字符串。</p><p>我们知道StringBuffer和StringBuilder最大的不同是线程安全性的问题，StringBuffer在所有以StringBuilder为基础的代码上，在重写父类的方法的同时加了synchronized修饰，保证了线程的安全</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//下面只是节选一些StringBuffer中的函数</span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(<span class="keyword">char</span> ch)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(<span class="keyword">char</span>[] chars)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(<span class="keyword">char</span>[] chars, <span class="keyword">int</span> start, <span class="keyword">int</span> length)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(Object obj)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(String string)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(StringBuffer sb)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(CharSequence s)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(CharSequence s, <span class="keyword">int</span> start, <span class="keyword">int</span> end)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">insert</span><span class="params">(<span class="keyword">int</span> index, <span class="keyword">char</span> ch)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">insert</span><span class="params">(<span class="keyword">int</span> index, <span class="keyword">char</span>[] chars)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">insert</span><span class="params">(<span class="keyword">int</span> index, <span class="keyword">char</span>[] chars, <span class="keyword">int</span> start, <span class="keyword">int</span> length)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">insert</span><span class="params">(<span class="keyword">int</span> index, String string)</span></span></span><br></pre></td></tr></table></figure><h2 id="编译器对String做出了哪些优化"><a href="#编译器对String做出了哪些优化" class="headerlink" title="编译器对String做出了哪些优化"></a>编译器对String做出了哪些优化</h2><p>String的不可变性会带来一定的效率问题。为String对象重载的 “+” 操作符就是一个例子。重载的意思是，一个操作符在应用于特定的类时，被赋予了特殊的意义。</p><p>我们用一段代码来验证 “+” 用来拼接String</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String mongo = <span class="string">"mongo"</span>;</span><br><span class="line">String s = <span class="string">"abc"</span> + mongo + <span class="string">"def"</span> +<span class="number">47</span>;</span><br><span class="line">System.out.println(s);<span class="comment">//abcmongodef47</span></span><br></pre></td></tr></table></figure><p>我们猜想一下字符串s的工作方式，它可能有一个append方法，首先s的内容是abc，然后新建一个字符串的内容是abcmongo，继续新建内容是abcmongodef的字符串，最后新建abcmongodef47的字符串，也许你会说为什么不是 “abc” + mongo + “def” +47 一起生成一个字符串然后赋值给s，但是我们不要忘记字符串String，它是一个类。</p><p>这种设计方法可以行的通，但是为了最终生成的String，产生了一大堆的需要GC的中间对象。这样的性能是非常糟糕的。</p><p>那么String是如何做优化的？我们使用JDK自带的工具javap来反编译以上代码，-c表示生成JVM字节码，删除没用的部分，剩下的内容如下</p><p><code>javap -c StringTest</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Compiled from <span class="string">"StringTest.java"</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">string</span>.<span class="title">StringTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(java.lang.String[])</span></span>;</span><br><span class="line">    Code:</span><br><span class="line">       0: ldc           #2                  // String mongo</span><br><span class="line">       <span class="number">2</span>: astore_1<span class="comment">// store mongo</span></span><br><span class="line">       3: new           #3                  // StringBuilder</span><br><span class="line">       <span class="number">6</span>: dup</span><br><span class="line">       7: invokespecial #4                  // StringBuilder."&lt;init&gt;":()V</span><br><span class="line">      10: ldc           #5                  // String abc</span><br><span class="line">      12: invokevirtual #6                  // StringBuilder.append：abc</span><br><span class="line">      <span class="number">15</span>: aload_1<span class="comment">// load mongo</span></span><br><span class="line">      16: invokevirtual #6                  // StringBuilder.append：mongo</span><br><span class="line">      19: ldc           #7                  // String def</span><br><span class="line">      21: invokevirtual #6                  // StringBuilder.append：def</span><br><span class="line">      <span class="number">24</span>: bipush        <span class="number">47</span></span><br><span class="line">      26: invokevirtual #8                  // StringBuilder.append：47</span><br><span class="line">      29: invokevirtual #9                  // StringBuilder.toString：abcmongodef47</span><br><span class="line">      <span class="number">32</span>: astore_2<span class="comment">// store s = abcmongodef47</span></span><br><span class="line">      33: getstatic     #10                 // Field System.out:Ljava/io/PrintStream;</span><br><span class="line">      <span class="number">36</span>: aload_2</span><br><span class="line">      37: invokevirtual #11                 // PrintStream.println:(Ljava/lang/String;)V</span><br><span class="line">      <span class="number">40</span>: <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>即使看不懂编译语句也不重要，我们需要注意的重点是：编译器自动引入了java.lang.StringBuilder类，虽然我们在源码中并没有使用StringBuilder类，但是编译器却自动使用了它，因为它更加高效。</p><p>现在也许你会觉得可以随意的使用String对象，反正编译器会自动优化性能，<strong>可是我们千万要记住一点，在循环的内部拼接字符串，并不会起到优化的效果。</strong></p><p>下面的程序采用两种方式生成String：方法一使用多个String对象；方法二中使用了StringBuilder。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WitherStringBuilder</span> </span>&#123;</span><br><span class="line"><span class="comment">//方法一</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">implicit</span><span class="params">(String[] fields)</span></span>&#123;</span><br><span class="line">        String result=<span class="string">""</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; fields.length; i++) &#123;</span><br><span class="line">            result += fields[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//方法二</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">explicit</span><span class="params">(String[] fields)</span></span>&#123;</span><br><span class="line">        StringBuilder result = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; fields.length; i++) &#123;</span><br><span class="line">            result.append(fields[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行javap -c WitherStringBuilder，可以看到两个方法对应的(简化过的)字节码，首先是implicit()方法：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> java.lang.<span class="function">String <span class="title">implicit</span><span class="params">(java.lang.String[])</span></span>;</span><br><span class="line">  Code:</span><br><span class="line">     0: ldc           #2                  // String</span><br><span class="line">     <span class="number">2</span>: astore_2</span><br><span class="line">     <span class="number">3</span>: iconst_0</span><br><span class="line">     <span class="number">4</span>: istore_3</span><br><span class="line">     <span class="number">5</span>: iload_3</span><br><span class="line">     <span class="number">6</span>: aload_1</span><br><span class="line">     <span class="number">7</span>: arraylength</span><br><span class="line">     <span class="number">8</span>: if_icmpge     <span class="number">38</span></span><br><span class="line">    11: new           #3                  // StringBuilder</span><br><span class="line">    <span class="number">14</span>: dup</span><br><span class="line">    15: invokespecial #4                  // StringBuilder."&lt;init&gt;":()</span><br><span class="line">    <span class="number">18</span>: aload_2</span><br><span class="line">    19: invokevirtual #5                  // StringBuilder.append:()</span><br><span class="line">    <span class="number">22</span>: aload_1</span><br><span class="line">    <span class="number">23</span>: iload_3</span><br><span class="line">    <span class="number">24</span>: aaload</span><br><span class="line">    25: invokevirtual #5                  //StringBuilder.append:()</span><br><span class="line">    28: invokevirtual #6                  // StringBuilder.toString:()</span><br><span class="line">    <span class="number">31</span>: astore_2</span><br><span class="line">    <span class="number">32</span>: iinc          <span class="number">3</span>, <span class="number">1</span></span><br><span class="line">    <span class="number">35</span>: goto          <span class="number">5</span></span><br><span class="line">    <span class="number">38</span>: aload_2</span><br><span class="line">    <span class="number">39</span>: areturn</span><br></pre></td></tr></table></figure><p>注意从第8行到第35行构成一个循环体。</p><p>第8行：对堆栈中的操作数进行 “大于或等于的整数比较运算”，循环结束时跳到第38行。</p><p>第35行：返回循环体的起始点(第5行)。</p><p>要注意的重点是：StringBuilder是在循环之内构造的，这意味着每经过循环一次，就会创建一个新的StringBuilder对象。这样的操作没有任何优化可言。</p><p>下面是explicit()方法对应的字节码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> java.lang.<span class="function">String <span class="title">explicit</span><span class="params">(java.lang.String[])</span></span>;</span><br><span class="line">  Code:</span><br><span class="line">     0: new           #3                  // StringBuilder</span><br><span class="line">     <span class="number">3</span>: dup</span><br><span class="line">     4: invokespecial #4                  // StringBuilder."&lt;init&gt;":()</span><br><span class="line">     <span class="number">7</span>: astore_2</span><br><span class="line">     <span class="number">8</span>: iconst_0</span><br><span class="line">     <span class="number">9</span>: istore_3</span><br><span class="line">    <span class="number">10</span>: iload_3</span><br><span class="line">    <span class="number">11</span>: aload_1</span><br><span class="line">    <span class="number">12</span>: arraylength</span><br><span class="line">    <span class="number">13</span>: if_icmpge     <span class="number">30</span></span><br><span class="line">    <span class="number">16</span>: aload_2</span><br><span class="line">    <span class="number">17</span>: aload_1</span><br><span class="line">    <span class="number">18</span>: iload_3</span><br><span class="line">    <span class="number">19</span>: aaload</span><br><span class="line">    20: invokevirtual #5                  // StringBuilder.append:()</span><br><span class="line">    <span class="number">23</span>: pop</span><br><span class="line">    <span class="number">24</span>: iinc          <span class="number">3</span>, <span class="number">1</span></span><br><span class="line">    <span class="number">27</span>: goto          <span class="number">10</span></span><br><span class="line">    <span class="number">30</span>: aload_2</span><br><span class="line">    31: invokevirtual #6                  // StringBuilder.toString:()</span><br><span class="line">    <span class="number">34</span>: areturn</span><br></pre></td></tr></table></figure><p>可以看到，不仅循环部分的代码更加简短，而且它只生成了一个StringBuilder对象。所以遇到循环内拼接字符串时在循环体的外部定义StringBuilder()可以大大提升程序的性能。当然，如果字符串操作简单的话，那么就可以信赖编译器的优化。</p><p>而且显示地创建StringBuilder还允许你预先为其指定大小。如果你已经知道最终的字符串大概多长，那预先指定StringBuilder的大小还可以避免多次重新分配缓冲。</p><hr><p>参考书籍：《Java编程思想(第4版)》</p><p>参考文章：</p><p><a href="https://www.cnblogs.com/kissazi2/p/3648671.html" target="_blank" rel="noopener">https://www.cnblogs.com/kissazi2/p/3648671.html</a></p><p><a href="https://www.cnblogs.com/xudong-bupt/p/3961159.html" target="_blank" rel="noopener">https://www.cnblogs.com/xudong-bupt/p/3961159.html</a></p><p><a href="https://www.cnblogs.com/fangfuhai/p/5500065.html" target="_blank" rel="noopener">https://www.cnblogs.com/fangfuhai/p/5500065.html</a></p><p><a href="https://www.cnblogs.com/jaylon/p/5721571.html" target="_blank" rel="noopener">https://www.cnblogs.com/jaylon/p/5721571.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nginx的简介&amp;安装&amp;常用操作</title>
      <link href="/2020/03/10/nginx/1/"/>
      <url>/2020/03/10/nginx/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>Nginx的功能介绍</li><li>Nginx的安装</li><li>Nginx常用操作</li></ol><h2 id="Nginx的功能介绍"><a href="#Nginx的功能介绍" class="headerlink" title="Nginx的功能介绍"></a>Nginx的功能介绍</h2><p>Nginx是一个轻量级、高性能、稳定性高、并发性好的HTTP和反向代理服务器。也是由于其的特性，其应用非常广。</p><h3 id="反向代理"><a href="#反向代理" class="headerlink" title="反向代理"></a>反向代理</h3><p>在说反向代理之前说一下正向代理：某些情况下，代理服务器代理我们用户去访问服务器，需要用户手动的设置代理服务器的ip和端口号。正向代理的最大特点是代理服务器在客户端</p><p>反向代理是用来代理服务器的，代理服务器代理我们要访问的目标服务器。代理服务器接受请求，然后将请求转发给内部网络的服务器(集群化)，并将从服务器上得到的结果返回给客户端，此时代理服务器对外就表现为一个服务器。反向代理的最大特点是代理服务器在服务器端</p><p><img src="https://yerias.github.io/nginx_img/1.jpg" alt=""></p><h3 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h3><p>负载均衡是在高并发情况下需要使用。其原理就是将数据流量分摊到多个服务器执行，减轻每台服务器的压力，多台服务器(集群)共同完成工作任务，从而提高了数据的吞吐量。</p><p>Nginx可使用的负载均衡策略有：轮询（默认）、权重、ip_hash、url_hash(第三方)、fair(第三方)</p><p><img src="https://yerias.github.io/nginx_img/2.jpg" alt=""></p><p>Ip hash算法，对客户端请求的ip进行hash操作，然后根据hash结果将同一个客户端ip的请求分发给同一台服务器进行处理，可以解决session不共享的问题。 </p><p><img src="https://yerias.github.io/nginx_img/3.jpg" alt=""></p><h3 id="动静分离"><a href="#动静分离" class="headerlink" title="动静分离"></a>动静分离</h3><p>Nginx提供的动静分离是指把动态请求和静态请求分离开，合适的服务器处理相应的请求，使整个服务器系统的性能、效率更高。</p><p>Nginx可以根据配置对不同的请求做不同转发，这是动态分离的基础。静态请求对应的静态资源可以直接放在Nginx上做缓冲，更好的做法是放在相应的缓冲服务器上。动态请求由相应的后端服务器处理。</p><p>常见的静态文件有js、css、html，常见的动态文件有jsp、servlet</p><h2 id="Nginx的安装"><a href="#Nginx的安装" class="headerlink" title="Nginx的安装"></a>Nginx的安装</h2><h3 id="安装所需环境"><a href="#安装所需环境" class="headerlink" title="安装所需环境"></a>安装所需环境</h3><p><strong>一. gcc 安装</strong><br>安装 nginx 需要先将官网下载的源码进行编译，编译依赖 gcc 环境，如果没有 gcc 环境，则需要安装：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install gcc-c++</span><br></pre></td></tr></table></figure><p><strong>二. PCRE pcre-devel 安装</strong><br>PCRE(Perl Compatible Regular Expressions) 是一个Perl库，包括 perl 兼容的正则表达式库。nginx 的 http 模块使用 pcre 来解析正则表达式，所以需要在 linux 上安装 pcre 库，pcre-devel 是使用 pcre 开发的一个二次开发库。nginx也需要此库。命令：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install -y pcre pcre-devel</span><br></pre></td></tr></table></figure><p><strong>三. zlib 安装</strong><br>zlib 库提供了很多种压缩和解压缩的方式， nginx 使用 zlib 对 http 包的内容进行 gzip ，所以需要在 Centos 上安装 zlib 库。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install -y zlib zlib-devel</span><br></pre></td></tr></table></figure><p><strong>四. OpenSSL 安装</strong><br>OpenSSL 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及 SSL 协议，并提供丰富的应用程序供测试或其它目的使用。<br>nginx 不仅支持 http 协议，还支持 https（即在ssl协议上传输http），所以需要在 Centos 安装 OpenSSL 库。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install -y openssl openssl-devel</span><br></pre></td></tr></table></figure><p><strong>五. 综合操作下载命令</strong></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install gcc-c++ pcre pcre-devel zlib zlib-devel openssl openssl-devel -y</span><br></pre></td></tr></table></figure><h3 id="下载安装Nginx"><a href="#下载安装Nginx" class="headerlink" title="下载安装Nginx"></a>下载安装Nginx</h3><ol><li><p>官网下载<code>.tar.gz</code>安装包，地址：<a href="https://nginx.org/en/download.html" target="_blank" rel="noopener">https://nginx.org/en/download.html</a></p></li><li><p>使用<code>wget</code>命令下载（推荐）。确保系统已经安装了wget，如果没有安装，执行 yum install wget 安装。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget -c https://nginx.org/download/nginx-1.12.0.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>解压依然是直接命令：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf nginx-1.12.0.tar.gz -C ../app</span><br><span class="line">cd nginx-1.12.0</span><br></pre></td></tr></table></figure></li><li><p>默认配置(默认是安装在/usr/local/nginx，不推介指定安装目录)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./configure</span><br></pre></td></tr></table></figure></li><li><p>编译安装(root用户)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure></li><li><p>查找安装路径</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">whereis nginx</span><br></pre></td></tr></table></figure><p>结果：/usr/local/nginx</p></li></ol><h2 id="Nginx常用操作"><a href="#Nginx常用操作" class="headerlink" title="Nginx常用操作"></a>Nginx常用操作</h2><ol><li><p>启动、停止nginx</p><p>cd /usr/local/nginx/sbin/</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./nginx #启动</span><br><span class="line">./nginx -s stop#停止</span><br><span class="line">./nginx -s quit#退出</span><br><span class="line">./nginx -s reload#重新加载配置文件</span><br></pre></td></tr></table></figure><p>启动时报80端口被占用:nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)</p><p>解决办法：安装net-tool 包：<code>yum install net-tools</code></p><p><strong>命令详解：</strong></p><p><code>./nginx -s quit</code>:此方式停止步骤是待nginx进程处理任务完毕进行停止。<br><code>./nginx -s stop</code>:此方式相当于先查出nginx进程id再使用kill命令强制杀掉进程。</p></li><li><p>重启Nginx</p><p>对 nginx 进行重启相当于先停止再启动，即先执行停止命令再执行启动命令。如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./nginx -s quit</span><br><span class="line">./nginx</span><br></pre></td></tr></table></figure></li><li><p>重新加载配置文件</p><p>当 ngin x的配置文件 nginx.conf 修改后，要想让配置生效需要重启 nginx，使用<code>-s reload</code>不用先停止 ngin x再启动 nginx 即可将配置信息在 nginx 中生效，如下:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./nginx -s reload</span><br></pre></td></tr></table></figure><p>启动成功后，可打开浏览器访问，默认端口80</p></li><li><p>配置开机自启</p><p>即在<code>rc.local</code>增加启动代码就可以了。</p><p>vi /etc/rc.local</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/usr/local/nginx/sbin/nginx</span><br></pre></td></tr></table></figure><p>设置执行权限：<code>chmod 755 rc.local</code></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HUE安装&amp;集成</title>
      <link href="/2020/03/09/hue/1/"/>
      <url>/2020/03/09/hue/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li><p>hue简介</p></li><li><p>安装maven</p></li><li><p>安装ant</p></li><li><p>安装hue</p></li><li><p>hue集成hdfs</p></li><li><p>hue集成yarn</p></li><li><p>hue集成hive</p></li><li><p>hue集成mysql</p></li><li><p>hue集成zookeeper</p></li><li><p>hue集成hbase</p></li><li><p>hue集成oozie</p></li><li><p>Shell脚本</p></li></ol><h2 id="hue简介"><a href="#hue简介" class="headerlink" title="hue简介"></a>hue简介</h2><p>HUE=Hadoop User Experience</p><p>Hue是一个开源的Apache Hadoop UI系统，由Cloudera Desktop演化而来，最后Cloudera公司将其贡献给Apache基金会的Hadoop社区，它是基于Python Web框架Django实现的。</p><p>通过使用Hue，可以在浏览器端的Web控制台上与Hadoop集群进行交互，来分析处理数据，例如操作HDFS上的数据，运行MapReduce Job，执行Hive的SQL语句，浏览HBase数据库等等。（说人话就是支持提供各种Web图形化界面的）</p><h2 id="安装maven"><a href="#安装maven" class="headerlink" title="安装maven"></a>安装maven</h2><ol><li>上传解压apache-maven-3.6.3-bin.tar到~/app目录下</li><li>配置环境变量</li><li>输入<code>mvn -version</code>测试是否安装成功</li></ol><h2 id="安装ant"><a href="#安装ant" class="headerlink" title="安装ant"></a>安装ant</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install ant -y</span><br></pre></td></tr></table></figure><h2 id="安装hue"><a href="#安装hue" class="headerlink" title="安装hue"></a>安装hue</h2><ol><li><p>hue相关网站</p><p>hue官网：<a href="http://gethue.com/" target="_blank" rel="noopener">http://gethue.com/</a></p><p>配置文档：<a href="http://archive.cloudera.com/cdh5/cdh/5/hue-3.9.0-cdh5.16.2" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hue-3.9.0-cdh5.16.2</a></p><p>源码：<a href="https://github.com/cloudera/hue" target="_blank" rel="noopener">https://github.com/cloudera/hue</a></p><p>这里我们直接用下载hue：<a href="http://archive.cloudera.com/cdh5/cdh/5/hue-3.9.0-cdh5.16.2.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hue-3.9.0-cdh5.16.2.tar.gz</a></p></li><li><p>安装hue所需要的依赖包</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install asciidoc cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-plain gcc gcc-c++ krb5-devel libffi-devel libtidy libxml2-devel libxslt-devel make mysql mysql-devel openldap-devel python-devel sqlite-devel openssl-devel gmp-devel -y</span><br></pre></td></tr></table></figure></li><li><p>解压安装Hue的tar包</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd ~/software</span><br><span class="line">tar -zxvf hue-3.7.0-cdh5.3.6.tar -C ~/app</span><br></pre></td></tr></table></figure></li><li><p>编译</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd hue-3.7.0-cdh5.3.6</span><br><span class="line">make apps</span><br></pre></td></tr></table></figure></li><li><p>修改hue.ini配置文件</p><p>进入到desktop/conf目录下,找到hue.ini文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">secret_key=jFE93j;2[290-eiw.KEiwN2s3['d;/.q[eIW^y#e=+Iei*@Mn&lt;qW5o</span><br><span class="line">http_host=hadoop</span><br><span class="line">http_port=8888</span><br><span class="line">time_zone=Asia/Shanghai</span><br></pre></td></tr></table></figure></li><li><p>启动</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">build/env/bin/supervisor</span><br></pre></td></tr></table></figure></li></ol><h2 id="hue集成hdfs"><a href="#hue集成hdfs" class="headerlink" title="hue集成hdfs"></a>hue集成hdfs</h2><ol><li><p>配置hdfs.site.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>配置core-site.xml</p><p>设置代理用户</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hadoop.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hadoop.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果你的Hadoop配置了<code>高可用</code>，则必须通过httpfs来访问，需要添加如下属性，反则则不必须。（如果HUE服务与Hadoop服务不在同一节点，则必须配置）</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.httpfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.httpfs.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置httpfs-site.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>httpfs.proxyuser.hue.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>httpfs.proxyuser.hue.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>以上两个属性主要用于HUE服务与Hadoop服务不在同一台节点上所必须的配置。</p></li><li><p>配置hue.ini</p><p>找到<code>[[hdfs_clusters]]</code>标签</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS服务器地址</span><br><span class="line">fs_defaultfs=hdfs://hadoop:9000</span><br><span class="line"></span><br><span class="line">如果开启了高可用，需要配置如下</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># logical_name=mycluster</span></span></span><br><span class="line"></span><br><span class="line">向HDFS发送命令的请求地址</span><br><span class="line">webhdfs_url=http://hadoop:50070/webhdfs/v1</span><br><span class="line"></span><br><span class="line">HADOOP的一些配置</span><br><span class="line">hadoop_conf_dir=/home/hadoop/app/hadoop/etc/hadoop</span><br><span class="line">hadoop_hdfs_home=/home/hadoop/app/hadoop</span><br><span class="line">hadoop_bin=/home/hadoop/app/hadoop/bin</span><br></pre></td></tr></table></figure></li><li><p>如果配置了高可用则启动hue前需要先启动httpfs服务</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/home/hadoop/app/hadoop/sbin/httpfs.sh start</span><br></pre></td></tr></table></figure></li></ol><h2 id="hue集成yarn"><a href="#hue集成yarn" class="headerlink" title="hue集成yarn"></a>hue集成yarn</h2><p>如果没有在hadoop中配置历史节点，需要先配置历史节点</p><p>mapred-site.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>启动历史服务器</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver</span></span><br></pre></td></tr></table></figure><p>找到<code>[[yarn_clusters]]</code>标签，涉及修改配置如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn服务的配置</span><br><span class="line">resourcemanager_host=192.168.56.86</span><br><span class="line">resourcemanager_port=8032</span><br><span class="line"></span><br><span class="line">是否将作业提交到此群集，并监控作业执行情况</span><br><span class="line">submit_to=True</span><br><span class="line"><span class="meta">#</span><span class="bash">logical_name=cluster-yarn1(如果开高可用的话)</span></span><br><span class="line"></span><br><span class="line">配置yarn资源管理的访问入口</span><br><span class="line">resourcemanager_api_url=http://hadoop:8088</span><br><span class="line">proxy_api_url=http://hadoop:8088</span><br><span class="line"></span><br><span class="line">历史服务器管理的入口，查看作业的历史运行情况</span><br><span class="line">history_server_api_url=http://hadoop:19888</span><br></pre></td></tr></table></figure><p><code>注意</code>：hue界面的超级用户名字需要和提交到yarn的用户名字相同</p><h2 id="hue集成hive"><a href="#hue集成hive" class="headerlink" title="hue集成hive"></a>hue集成hive</h2><p>配置hive-site.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>TCP绑定的主机<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.long.polling.timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>5000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>HiveServer2在响应使用长轮询的异步调用之前等待的时间（毫秒）<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://hadoop:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>指向的是运行metastore服务的主机<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置hue.ini，找到[beeswax]属性标签，涉及修改如下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[beeswax]</span><br><span class="line">hive_server_host=hadoop</span><br><span class="line">hive_server_port=10000</span><br><span class="line">hive_conf_dir=/home/hadoop/app/hive/conf</span><br></pre></td></tr></table></figure><p>启动</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/hive --service metastore &amp;</span><br><span class="line">bin/hive --service hiveserver2 1&gt;/dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p><code>注意：</code>如果设置了uris，在今后使用Hive时，那么必须启动如上两个命令，否则Hive无法正常启动。</p><h2 id="hue集成mysql"><a href="#hue集成mysql" class="headerlink" title="hue集成mysql"></a>hue集成mysql</h2><p>配置hue.ini，找到[[[mysql]]]标签，涉及修改如下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[[[mysql]]]</span><br><span class="line">nice_name="mysql"</span><br><span class="line">engine=mysql</span><br><span class="line">host=hadoop</span><br><span class="line">port=3306</span><br><span class="line">user=root</span><br><span class="line">password=root</span><br></pre></td></tr></table></figure><h2 id="hue集成Zookeeper"><a href="#hue集成Zookeeper" class="headerlink" title="hue集成Zookeeper"></a>hue集成Zookeeper</h2><p>配置hue.ini</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zookeeper]</span><br><span class="line">[[clusters]]</span><br><span class="line">[[[default]]]</span><br><span class="line">host_ports=hadoop:2181</span><br></pre></td></tr></table></figure><p>启动zookeeper</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zkServer.sh start</span><br><span class="line">zkServer.sh status</span><br></pre></td></tr></table></figure><h2 id="hue集成hbase"><a href="#hue集成hbase" class="headerlink" title="hue集成hbase"></a>hue集成hbase</h2><h2 id="hue集成oozie"><a href="#hue集成oozie" class="headerlink" title="hue集成oozie"></a>hue集成oozie</h2><h2 id="Shell脚本"><a href="#Shell脚本" class="headerlink" title="Shell脚本"></a>Shell脚本</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line">echo "启动httpfs服务"</span><br><span class="line">/home/hadoop/app/hadoop/sbin/httpfs.sh start</span><br><span class="line"></span><br><span class="line">echo "启动历史服务"</span><br><span class="line">/home/hadoop/app/hadoop/sbin/mr-jobhistory-daemon.sh start historyserver</span><br><span class="line"></span><br><span class="line">echo "启动hive元数据"</span><br><span class="line"><span class="meta">$</span><span class="bash">HIVE_HOME/bin/hive --service metastore &amp;</span></span><br><span class="line"></span><br><span class="line">echo "启动hive服务端"</span><br><span class="line"><span class="meta">$</span><span class="bash">HIVE_HOME/bin/hive --service hiveserver2 1 &gt; /dev/null 2&gt;&amp;1 &amp;</span></span><br><span class="line"></span><br><span class="line">echo "启动hue"</span><br><span class="line"><span class="meta">$</span><span class="bash">HUE_HOME/build/env/bin/supervisor 1 &gt; /dev/null 2&gt;&amp;1 &amp;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hue </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FastJson的使用</title>
      <link href="/2020/03/07/java/9/"/>
      <url>/2020/03/07/java/9/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近在开发过程中使用了大量的<code>json</code>作为前后端数据交换的方式,由于之前没有对<code>json</code>做过系统的学习,所有在使用过程中查阅了大量的文档与资料,这里主要记录了我在开发后对<code>json</code>以及<code>FastJson</code>使用的总结</p><h2 id="Json介绍"><a href="#Json介绍" class="headerlink" title="Json介绍"></a>Json介绍</h2><p><code>JSON</code>(javaScript Object Notation)是一种轻量级的数据交换格式。主要采用键值对(<code>{&quot;name&quot;: &quot;json&quot;}</code>)的方式来保存和表示数据。<code>JSON</code>是<code>JS</code>对象的字符串表示法，它使用文本表示一个<code>JS</code>对象的信息，本质上是一个字符串。更多简介见<a href="http://www.json.org/json-zh.html" target="_blank" rel="noopener">介绍JSON</a>。</p><h2 id="FastJson-简介"><a href="#FastJson-简介" class="headerlink" title="FastJson  简介"></a>FastJson  简介</h2><p>在日志解析,前后端数据传输交互中,经常会遇到字符串(String)与<code>json</code>,<code>XML</code>等格式相互转换与解析，其中<code>json</code>以跨语言，跨前后端的优点在开发中被频繁使用，基本上可以说是标准的数据交换格式。<a href="https://github.com/alibaba/FastJson" target="_blank" rel="noopener">fastjson</a>是一个java语言编写的高性能且功能完善的JSON库，它采用一种“假定有序快速匹配”的算法，把<code>JSON Parse</code> 的性能提升到了极致。它的接口简单易用，已经被广泛使用在缓存序列化，协议交互，Web输出等各种应用场景中。</p><h2 id="FastJson-常用-API"><a href="#FastJson-常用-API" class="headerlink" title="FastJson  常用 API"></a>FastJson  常用 API</h2><p>导入Jar包</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;com.alibaba&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;fastjson&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;1.2.47&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>FastJson API 入口类是<code>com.alibaba.FastJson.JSON</code>,常用的序列化操作都可以在<code>JSON</code>类上的静态方法直接完成。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Object <span class="title">parse</span><span class="params">(String text)</span></span>; <span class="comment">// 把JSON文本parse为JSONObject或者JSONArray </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> JSONObject <span class="title">parseObject</span><span class="params">(String text)</span>； <span class="comment">// 把JSON文本parse成JSONObject    </span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> &lt;T&gt; T <span class="title">parseObject</span><span class="params">(String text, Class&lt;T&gt; clazz)</span></span>; <span class="comment">// 把JSON文本parse为JavaBean </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> JSONArray <span class="title">parseArray</span><span class="params">(String text)</span></span>; <span class="comment">// 把JSON文本parse成JSONArray </span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> &lt;T&gt; <span class="function">List&lt;T&gt; <span class="title">parseArray</span><span class="params">(String text, Class&lt;T&gt; clazz)</span></span>; <span class="comment">//把JSON文本parse成JavaBean集合 </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String <span class="title">toJSONString</span><span class="params">(Object object)</span></span>; <span class="comment">// 将JavaBean序列化为JSON文本 </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String <span class="title">toJSONString</span><span class="params">(Object object, <span class="keyword">boolean</span> prettyFormat)</span></span>; <span class="comment">// 将JavaBean序列化为带格式的JSON文本 </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Object <span class="title">toJSON</span><span class="params">(Object javaObject)</span></span>; <span class="comment">//将JavaBean转换为JSONObject或者JSONArray。</span></span><br></pre></td></tr></table></figure><h3 id="使用方法举例"><a href="#使用方法举例" class="headerlink" title="使用方法举例"></a>使用方法举例</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//将JSON文本转换为java对象</span></span><br><span class="line"><span class="keyword">import</span> com.alibaba.FastJson.JSON;</span><br><span class="line">Model model = JSON.parseObject(jsonStr, Model<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure><h3 id="有关类库的一些说明"><a href="#有关类库的一些说明" class="headerlink" title="有关类库的一些说明"></a>有关类库的一些说明</h3><ul><li>JSONArray : 相当于List</li><li>JSONObject: 相当于Map&lt;String,Object&gt;</li></ul><h2 id="FastJson-使用实例"><a href="#FastJson-使用实例" class="headerlink" title="FastJson  使用实例"></a>FastJson  使用实例</h2><h3 id="Java对象与Json字符串的互转"><a href="#Java对象与Json字符串的互转" class="headerlink" title="Java对象与Json字符串的互转"></a>Java对象与Json字符串的互转</h3><p><code>User</code>测试类</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.json.user;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-03-07 09:54</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String username;</span><br><span class="line">    <span class="keyword">private</span> String password;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">User</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">User</span><span class="params">(String username, String password)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.username = username;</span><br><span class="line">        <span class="keyword">this</span>.password = password;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getUsername</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> username;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUsername</span><span class="params">(String username)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.username = username;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPassword</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> password;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPassword</span><span class="params">(String password)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.password = password;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>UserGroup</code>测试类</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.json.user;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-03-07 09:55</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserGroup</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> List&lt;User&gt; users= <span class="keyword">new</span> ArrayList&lt;User&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">UserGroup</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">UserGroup</span><span class="params">(String name, List&lt;User&gt; users)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.users = users;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;User&gt; <span class="title">getUsers</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> users;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUsers</span><span class="params">(List&lt;User&gt; users)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.users = users;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>fastJson</code>测试类</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.json.user;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-03-07 09:58</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestFastJosn</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        objectToJson();</span><br><span class="line">        JsonToObject();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//java对象转 json字符串</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">objectToJson</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="comment">//简单java类转json</span></span><br><span class="line">        User user = <span class="keyword">new</span> User(<span class="string">"tunan"</span>,<span class="string">"123456"</span>);</span><br><span class="line">        String userJson = JSON.toJSONString(user);</span><br><span class="line">        System.out.println(<span class="string">"简单java类转字符串： "</span>+userJson);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//List&lt;Object&gt;转Json字符串</span></span><br><span class="line">        User user1 = <span class="keyword">new</span> User(<span class="string">"xiaoqi"</span>, <span class="string">"961228"</span>);</span><br><span class="line">        User user2 = <span class="keyword">new</span> User(<span class="string">"tunan"</span>, <span class="string">"123456"</span>);</span><br><span class="line">        List&lt;User&gt; users = <span class="keyword">new</span> ArrayList&lt;User&gt;();</span><br><span class="line">        users.add(user1);</span><br><span class="line">        users.add(user2);</span><br><span class="line">        String listJson = JSON.toJSONString(users);</span><br><span class="line">        System.out.println(<span class="string">"List&lt;Object&gt;转Json字符串： "</span>+listJson);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//复杂java转json字符串</span></span><br><span class="line">        UserGroup userGroup = <span class="keyword">new</span> UserGroup(<span class="string">"student"</span>, users);</span><br><span class="line">        String userGroupJson = JSON.toJSONString(userGroup);</span><br><span class="line">        System.out.println(<span class="string">"复杂java转json字符串： "</span>+userGroupJson);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Json字符串 转Java类</span></span><br><span class="line">    <span class="comment">//注：字符串中使用双引号需要转义 (" --&gt; \"),这里使用的是单引号</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">JsonToObject</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * json字符串转简单java对象</span></span><br><span class="line"><span class="comment">         * 字符串：&#123;"password":"123456","username":"dmego"&#125;</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        String userJson = <span class="string">"&#123;'password':'123456','username':'dmego'&#125;"</span>;</span><br><span class="line">        User user = JSON.parseObject(userJson, User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        System.out.println(<span class="string">"json字符串转简单java对象: "</span>+user.toString());</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * json字符串转List&lt;Object&gt;对象</span></span><br><span class="line"><span class="comment">         * 字符串：[&#123;"password":"123123","username":"zhangsan"&#125;,&#123;"password":"321321","username":"lisi"&#125;]</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        String listUserJson = <span class="string">"[&#123;'password':'123123','username':'zhangsan'&#125;,&#123;'password':'321321','username':'lisi'&#125;]"</span>;</span><br><span class="line">        List&lt;User&gt; userList = JSON.parseArray(listUserJson, User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        System.out.println(<span class="string">"json字符串转List&lt;Object&gt;对象: "</span>+userList.toString());</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*json字符串转复杂java对象</span></span><br><span class="line"><span class="comment">         * 字符串：&#123;"name":"userGroup","users":[&#123;"password":"123123","username":"zhangsan"&#125;,&#123;"password":"321321","username":"lisi"&#125;]&#125;</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        String userGroupJson = <span class="string">"&#123;'name':'userGroup','users':[&#123;'password':'123123','username':'zhangsan'&#125;,&#123;'password':'321321','username':'lisi'&#125;]&#125;"</span>;</span><br><span class="line">        UserGroup userGroup = JSON.parseObject(userGroupJson, UserGroup<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        System.out.println(<span class="string">"json字符串转复杂java对象: "</span>+userGroup.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">简单java类转字符串： &#123;<span class="string">"password"</span>:<span class="string">"123456"</span>,<span class="string">"username"</span>:<span class="string">"tunan"</span>&#125;</span><br><span class="line">List&lt;Object&gt;转Json字符串： [&#123;<span class="string">"password"</span>:<span class="string">"961228"</span>,<span class="string">"username"</span>:<span class="string">"xiaoqi"</span>&#125;,&#123;<span class="string">"password"</span>:<span class="string">"123456"</span>,<span class="string">"username"</span>:<span class="string">"tunan"</span>&#125;]</span><br><span class="line">复杂java转json字符串： &#123;<span class="string">"name"</span>:<span class="string">"student"</span>,<span class="string">"users"</span>:[&#123;<span class="string">"password"</span>:<span class="string">"961228"</span>,<span class="string">"username"</span>:<span class="string">"xiaoqi"</span>&#125;,&#123;<span class="string">"password"</span>:<span class="string">"123456"</span>,<span class="string">"username"</span>:<span class="string">"tunan"</span>&#125;]&#125;</span><br><span class="line"></span><br><span class="line">json字符串转简单java对象: User&#123;username=<span class="string">'dmego'</span>, password=<span class="string">'123456'</span>&#125;</span><br><span class="line">json字符串转List&lt;Object&gt;对象: [User&#123;username=<span class="string">'zhangsan'</span>, password=<span class="string">'123123'</span>&#125;, User&#123;username=<span class="string">'lisi'</span>, password=<span class="string">'321321'</span>&#125;]</span><br><span class="line">json字符串转复杂java对象: UserGroup&#123;name=<span class="string">'userGroup'</span>, users=[User&#123;username=<span class="string">'zhangsan'</span>, password=<span class="string">'123123'</span>&#125;, User&#123;username=<span class="string">'lisi'</span>, password=<span class="string">'321321'</span>&#125;]&#125;</span><br></pre></td></tr></table></figure><p>通过结果看代码:</p><p>我们可以看到，在<code>JavaBean</code>转<code>Json</code>的时候，使用了<code>JSON.toJSONString()</code>方法</p><ol><li>该方法中传入的是对象，即生成对象的<code>Json</code>字符串</li><li>如果传入的是<code>List</code>，生成的就是对象的<code>Json</code>字符串数组</li><li>如果传入的是复杂类型既有对象，又有对象数组，那么生成的就是个<code>Json</code>对象，这个对象中包括对象的<code>Json</code>字符串和对象的<code>Json</code>字符串数组，<code>Json</code>的字符串数组中又包含对象的<code>Json</code>字符串。</li></ol><p>在<code>Json</code>转<code>JavaBean</code>的时候，会根据调用<code>JSON.parseObject()</code>和<code>JSON.parseArray()</code>的不同，输出不同的结果</p><ol><li>传入对象的<code>Json</code>字符串和需要生成的<code>Java</code>对象，需要调用<code>JSON.parseObject()</code>方法，输出的就是一个普通的对象</li><li>传入对象的<code>Json</code>字符串数组和要生成的<code>Java</code>对象，需要调用<code>JSON.parseArray()</code>方法，输出是一个<code>Java</code>对象数组</li><li>传入的即包含对象的<code>Json</code>字符串，又包含对象的<code>Json</code>字符串数组，那么需要调用一个<code>JSON.parseObject()</code>方法，该方法会输出一个复杂的Java对象，该对象既包括对象又包括对象数组。</li></ol><h2 id="FastJson解析复杂嵌套Json字符串"><a href="#FastJson解析复杂嵌套Json字符串" class="headerlink" title="FastJson解析复杂嵌套Json字符串"></a>FastJson解析复杂嵌套Json字符串</h2><p>这个实例是我在开发中用到的，先给出要解析的Json字符串</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">"id"</span>: <span class="string">"user_list"</span>,</span><br><span class="line">        <span class="string">"key"</span>: <span class="string">"id"</span>,</span><br><span class="line">        <span class="string">"tableName"</span>: <span class="string">"用户列表"</span>,</span><br><span class="line">        <span class="string">"className"</span>: <span class="string">"cn.dmego.domain.User"</span>,</span><br><span class="line">        <span class="string">"column"</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"key"</span>: <span class="string">"rowIndex"</span>,</span><br><span class="line">                <span class="string">"header"</span>: <span class="string">"序号"</span>,</span><br><span class="line">                <span class="string">"width"</span>: <span class="string">"50"</span>,</span><br><span class="line">                <span class="string">"allowSort"</span>: <span class="string">"false"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"key"</span>: <span class="string">"id"</span>,</span><br><span class="line">                <span class="string">"header"</span>: <span class="string">"id"</span>,</span><br><span class="line">                <span class="string">"hidden"</span>: <span class="string">"true"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"key"</span>: <span class="string">"name"</span>,</span><br><span class="line">                <span class="string">"header"</span>: <span class="string">"姓名"</span>,</span><br><span class="line">                <span class="string">"width"</span>: <span class="string">"100"</span>,</span><br><span class="line">                <span class="string">"allowSort"</span>: <span class="string">"true"</span></span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">"id"</span>: <span class="string">"role_list"</span>,</span><br><span class="line">        <span class="string">"key"</span>: <span class="string">"id"</span>,</span><br><span class="line">        <span class="string">"tableName"</span>: <span class="string">"角色列表"</span>,</span><br><span class="line">        <span class="string">"className"</span>: <span class="string">"cn.dmego.domain.Role"</span>,</span><br><span class="line">        <span class="string">"column"</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"key"</span>: <span class="string">"rowIndex"</span>,</span><br><span class="line">                <span class="string">"header"</span>: <span class="string">"序号"</span>,</span><br><span class="line">                <span class="string">"width"</span>: <span class="string">"50"</span>,</span><br><span class="line">                <span class="string">"allowSort"</span>: <span class="string">"false"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"key"</span>: <span class="string">"id"</span>,</span><br><span class="line">                <span class="string">"header"</span>: <span class="string">"id"</span>,</span><br><span class="line">                <span class="string">"hidden"</span>: <span class="string">"true"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"key"</span>: <span class="string">"name"</span>,</span><br><span class="line">                <span class="string">"header"</span>: <span class="string">"名称"</span>,</span><br><span class="line">                <span class="string">"width"</span>: <span class="string">"100"</span>,</span><br><span class="line">                <span class="string">"allowSort"</span>: <span class="string">"true"</span></span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>要想解析这种复杂的字符串，首先得先定义好与之相符的Java POJO 对象，经过观察，我们发现，这个是一个Json对象数组，每一个对象里包含了许多属性，其中还有一个属性的类型也是对象数组。所有，我们从里到外，先定义最里面的对象：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Column</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String key;</span><br><span class="line">    <span class="keyword">private</span> String header;</span><br><span class="line">    <span class="keyword">private</span> String width;</span><br><span class="line">    <span class="keyword">private</span> String allowSort;</span><br><span class="line">    <span class="keyword">private</span> String hidden;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Column&#123;"</span> +</span><br><span class="line">                <span class="string">"key='"</span> + key + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", header='"</span> + header + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", width='"</span> + width + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", allowSort='"</span> + allowSort + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", hidden='"</span> + hidden + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//这里省略部分getter与setter方法 </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>再定义外层的对象：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Query</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String id;</span><br><span class="line">    <span class="keyword">private</span> String key;</span><br><span class="line">    <span class="keyword">private</span> String tableName;</span><br><span class="line">    <span class="keyword">private</span> String className;</span><br><span class="line">    <span class="keyword">private</span> List&lt;Column&gt; column;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Query&#123;"</span> +</span><br><span class="line">                <span class="string">"id='"</span> + id + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", key='"</span> + key + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", tableName='"</span> + tableName + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", className='"</span> + className + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", column="</span> + column +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">     <span class="comment">//这里省略部分getter与setter方法 </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我的这个Json文件放置在类路径下，最后想将这个Json字符串转化为List对象，并且将column 对象数组转化为query对象里的List属性</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>: 将这个json字符串转化为List对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-03-07 10:47</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RoleTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        RoleTest roleTest = <span class="keyword">new</span> RoleTest();</span><br><span class="line">        List&lt;Query&gt; queries = roleTest.JsonToObject();</span><br><span class="line">        <span class="comment">//生成的是个List，需要循环输出</span></span><br><span class="line">        queries.forEach(System.out::println);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span>  List&lt;Query&gt; <span class="title">JsonToObject</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        ClassLoader loader = <span class="keyword">this</span>.getClass().getClassLoader();</span><br><span class="line">        InputStream in = loader.getResourceAsStream(<span class="string">"query.json"</span>);</span><br><span class="line">        <span class="comment">//这里是一次全部读出来了，大数据处理时需要按行读取</span></span><br><span class="line">        String jsonTxet = IOUtils.toString(in, <span class="string">"utf8"</span>);</span><br><span class="line">        List&lt;Query&gt; query = JSON.parseArray(jsonTxet, Query<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="keyword">return</span> query;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Query&#123;id=<span class="string">'user_list'</span>, key=<span class="string">'id'</span>, tableName=<span class="string">'用户列表'</span>, className=<span class="string">'cn.dmego.domain.User'</span>, column=[Column&#123;key=<span class="string">'rowIndex'</span>, header=<span class="string">'序号'</span>, width=<span class="string">'50'</span>, allowSort=<span class="string">'false'</span>, hidden=<span class="string">'null'</span>&#125;, Column&#123;key=<span class="string">'id'</span>, header=<span class="string">'id'</span>, width=<span class="string">'null'</span>, allowSort=<span class="string">'null'</span>, hidden=<span class="string">'true'</span>&#125;, Column&#123;key=<span class="string">'name'</span>, header=<span class="string">'姓名'</span>, width=<span class="string">'100'</span>, allowSort=<span class="string">'true'</span>, hidden=<span class="string">'null'</span>&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Query&#123;id=<span class="string">'role_list'</span>, key=<span class="string">'id'</span>, tableName=<span class="string">'角色列表'</span>, className=<span class="string">'cn.dmego.domain.Role'</span>, column=[Column&#123;key=<span class="string">'rowIndex'</span>, header=<span class="string">'序号'</span>, width=<span class="string">'50'</span>, allowSort=<span class="string">'false'</span>, hidden=<span class="string">'null'</span>&#125;, Column&#123;key=<span class="string">'id'</span>, header=<span class="string">'id'</span>, width=<span class="string">'null'</span>, allowSort=<span class="string">'null'</span>, hidden=<span class="string">'true'</span>&#125;, Column&#123;key=<span class="string">'name'</span>, header=<span class="string">'名称'</span>, width=<span class="string">'100'</span>, allowSort=<span class="string">'true'</span>, hidden=<span class="string">'null'</span>&#125;]&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala之高阶函数</title>
      <link href="/2020/03/03/scala/1/"/>
      <url>/2020/03/03/scala/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>sorted</li><li>sortBy</li><li>sortWith</li><li>flatten</li><li>map</li><li>flatMap</li><li>filter</li><li>groupBy</li><li>flod</li><li>reduce</li><li>wc</li></ol><h2 id="sorted"><a href="#sorted" class="headerlink" title="sorted"></a>sorted</h2><p>排序，默认升序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> array = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">    array.sorted</span><br></pre></td></tr></table></figure><h2 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h2><p>排序，指定排序规则</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> arrMap = <span class="type">Array</span>(('a', <span class="number">1</span>), ('b', <span class="number">4</span>), ('d', <span class="number">3</span>), ('c', <span class="number">2</span>))</span><br><span class="line">    arrMap.sortBy(_._1)</span><br><span class="line">    arrMap.sortBy(_._2)</span><br></pre></td></tr></table></figure><h2 id="sortWith"><a href="#sortWith" class="headerlink" title="sortWith"></a>sortWith</h2><p>排序，多个字段联合排序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> arrMap2 = <span class="type">Array</span>((<span class="string">"a"</span>, <span class="number">2</span>), (<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">2</span>), (<span class="string">"d"</span>, <span class="number">5</span>), (<span class="string">"c"</span>, <span class="number">4</span>))</span><br><span class="line">    arrMap2.sortWith((t1,t2)=&gt;&#123;</span><br><span class="line">      <span class="keyword">if</span> (!t1._1.equalsIgnoreCase(t2._1))&#123;</span><br><span class="line">        t1._1.compareTo(t2._1)&lt;<span class="number">0</span></span><br><span class="line">      &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        t1._2.compareTo(t2._2)&lt;<span class="number">0</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure><h2 id="flatten"><a href="#flatten" class="headerlink" title="flatten"></a>flatten</h2><p>扁平化</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> flatten = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), <span class="type">Array</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>))</span><br><span class="line">    flatten.flatten</span><br></pre></td></tr></table></figure><h2 id="map"><a href="#map" class="headerlink" title="map"></a>map</h2><p>映射</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> map = <span class="type">Array</span>(<span class="string">"this is a demo,hello world"</span>)</span><br><span class="line">    map.map(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    map.map(_.split(<span class="string">" "</span>)).flatten.map((_,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><h2 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h2><p>flatMap ==&gt; flatten + map</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> flatMap = <span class="type">Array</span>(<span class="string">"this is a demo"</span>, <span class="string">"hello word"</span>)</span><br><span class="line">    flatMap.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    flatMap.flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    flatMap.flatMap(line =&gt; (<span class="keyword">for</span> (i&lt;- line.split(<span class="string">" "</span>)) <span class="keyword">yield</span> (i,<span class="number">1</span>)))</span><br></pre></td></tr></table></figure><h2 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h2><p>过滤</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> filter = <span class="type">Array</span>(<span class="string">"this is a demo"</span>, <span class="string">"hello word"</span>)</span><br><span class="line">    filter.filter(_.contains(<span class="string">"hello"</span>))</span><br><span class="line">    filter.filterNot(_.contains(<span class="string">"hello"</span>))</span><br></pre></td></tr></table></figure><h2 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h2><p>分组</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> groupBy = <span class="type">Array</span>(<span class="string">"good"</span>, <span class="string">"good"</span>, <span class="string">"study"</span>)</span><br><span class="line">    groupBy.groupBy(x=&gt;x)</span><br><span class="line">    groupBy.groupBy(x=&gt;x).map(t=&gt;(t._1,t._2.size))</span><br></pre></td></tr></table></figure><h2 id="flod"><a href="#flod" class="headerlink" title="flod"></a>flod</h2><p>聚合，需要指定默认值</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> flod = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">4</span>)</span><br><span class="line">    flod.fold(<span class="number">10</span>)((a,b)=&gt;&#123;</span><br><span class="line">      println(a+<span class="string">"  :  "</span>+b)</span><br><span class="line">      a-b</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure><h2 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h2><p>聚合</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> reduce = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">4</span>)</span><br><span class="line">    reduce.reduce(_+_)</span><br><span class="line">    reduce.reduce(_-_)</span><br><span class="line">    reduce.reduceLeft(_+_)</span><br><span class="line">    reduce.reduceLeft(_-_)</span><br><span class="line">    reduce.reduceRight(_+_)</span><br><span class="line">    reduce.reduceRight(_-_)</span><br></pre></td></tr></table></figure><h2 id="wc"><a href="#wc" class="headerlink" title="wc"></a>wc</h2><p>方案1</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> wc = <span class="type">Array</span>(<span class="string">"this is demo"</span>, <span class="string">"good good study"</span>, <span class="string">"day day up"</span>)</span><br><span class="line">    wc.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .groupBy(word=&gt;word)</span><br><span class="line">      .toList</span><br><span class="line">      .map(t=&gt;(t._1,t._2.size))</span><br><span class="line">      .sortBy(_._1)</span><br><span class="line">      .mkString(<span class="string">","</span>)</span><br></pre></td></tr></table></figure><p>方案2</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">wc.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">     .map((_,<span class="number">1</span>))</span><br><span class="line">     .groupBy(_._1)</span><br><span class="line">     .toList</span><br><span class="line">     .map(t=&gt;(t._1,t._2.size))</span><br><span class="line">     .sortBy(_._1)</span><br><span class="line">     .mkString(<span class="string">"\t"</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zookeeper的安装&amp;使用</title>
      <link href="/2020/03/02/zookeeper/1/"/>
      <url>/2020/03/02/zookeeper/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>什么是分布式</li><li>为什么选择Zookeeper</li><li>Zookeeper的设计目标</li><li>Zookeeper的数据模型</li><li>安装Zookeeper</li><li>单节点配置Zookeeper</li><li>多节点配置Zookeeper</li><li>Zookeeper的常用命令</li><li>Zookeeper的监听</li><li>Zookeeper四字命令</li></ol><h2 id="什么是分布式"><a href="#什么是分布式" class="headerlink" title="什么是分布式"></a>什么是分布式</h2><p>分布式系统</p><p>分布式系统是由一组通过网络进行通信、为了完成共同的任务而协调工作的计算机节点组成的系统。分布式系统的出现是为了用廉价的、普通的机器完成单个计算机无法完成的计算、存储任务。其目的是<strong>利用更多的机器，处理更多的数据</strong>。</p><p>分布式协调技术</p><p>分布式系统的出现带来了分布式协调的问题，也就是我们如何对分布式系统中的进程进行调度，假设我们在某一台机器上面挂载了一个资源，然后其他物理分布的进程都要竞争这个资源，但是我们又不希望他们同时访问，这时候就需要一个协调器，让他们有序的来访问在这个资源。这个协调器就是我们经常提到的那个<strong>锁</strong>，比如说”进程-1”在使用该资源的时候，会先去获得锁，”进程1”获得锁以后会对该资源保持<strong>独占</strong>，这样其他进程就无法访问该资源，”进程1”用完该资源以后就将锁释放掉，让其他进程来获得锁，那么通过这个锁机制，我们就能保证了分布式系统中多个进程能够有序的访问该临界资源。那么我们把这个分布式环境下的这个锁叫作<strong>分布式锁</strong>。分布式锁也就是我们<strong>分布式协调技术</strong>实现的核心内容。</p><h2 id="为什么选择zookeeper"><a href="#为什么选择zookeeper" class="headerlink" title="为什么选择zookeeper"></a>为什么选择zookeeper</h2><p>ZooKeeper是一种为分布式应用所设计的高可用、高性能且一致的开源协调服务，它提供了一项基本服务：<strong>分布式锁服务</strong>。由于ZooKeeper的开源特性，后来我们的开发者在分布式锁的基础上，摸索了出了其他的使用方法：<strong>配置维护、组服务、分布式消息队列</strong>、<strong>分布式通知/协调</strong>等。</p><p>ZooKeeper在实现这些服务时，首先它设计一种新的<strong>数据结构——Znode</strong>，然后在该数据结构的基础上定义了一些<strong>原语</strong>，也就是一些关于该数据结构的一些操作。有了这些数据结构和原语还不够，因为我们的ZooKeeper是工作在一个分布式的环境下，我们的服务是通过消息以网络的形式发送给我们的分布式应用程序，所以还需要一个<strong>通知机制</strong>——Watcher机制。那么总结一下，ZooKeeper所提供的服务主要是通过：数据结构+原语+watcher机制，三个部分来实现的。</p><p>ZooKeeper<strong>性能上的特点</strong>决定了它能够用在大型的、分布式的系统当中。从<strong>可靠性</strong>方面来说，它并不会因为一个节点的错误而崩溃。除此之外，它<strong>严格的序列访问控制</strong>意味着复杂的控制原语可以应用在客户端上。ZooKeeper在一致性、可用性、容错性的保证，也是ZooKeeper的成功之处，它获得的一切成功都与它采用的协议——Zab协议是密不可分的，这些内容将会在后面介绍。</p><h2 id="Zookeeper的设计目标"><a href="#Zookeeper的设计目标" class="headerlink" title="Zookeeper的设计目标"></a>Zookeeper的设计目标</h2><p>ZooKeeper 很简单。ZooKeeper允许分布式进程通过共享的层次命名空间相互协调，该命名空间的组织类似于标准文件系统。名称空间由数据寄存器组成——用ZooKeeper的说法称为znodes——这些寄存器类似于文件和目录。与典型的用于存储的文件系统不同，ZooKeeper数据保存在内存中，这意味着ZooKeeper可以实现高吞吐量和低延迟数。</p><p>ZooKeeper实现非常重视高性能、高可用性和严格有序的访问。ZooKeeper的性能方面意味着它可以用于大型分布式系统。可靠性方面使它不会成为单点故障。严格的排序意味着复杂的同步原语可以在客户端实现。</p><p>ZooKeeper 是复制。与它所协调的分布式进程一样，ZooKeeper本身也打算在一组称为集合的主机上进行复制。</p><p><img src="http://zookeeper.apache.org/doc/current/images/zkservice.jpg" alt="Zookeeper集群部署"></p><p>组成ZooKeeper服务的服务器必须相互了解。它们在内存中维护状态映像，以及持久存储中的事务日志和快照。只要大多数服务器可用，ZooKeeper服务就可用。</p><p>客户端连接到单个ZooKeeper服务器。客户端维护一个TCP连接，通过它发送请求、获取响应、获取监视事件和发送心跳。如果到服务器的TCP连接中断，客户机将连接到另一台服务器。</p><p>ZooKeeper是有序的。ZooKeeper用一个数字来标记每个更新，这个数字反映了所有ZooKeeper事务的顺序。后续操作可以使用该顺序来实现更高级别的抽象，比如同步原语。</p><p>ZooKeeper很快。在“以读取为主”的工作负载中，它的速度特别快。ZooKeeper应用程序运行在数千台机器上，当读操作比写操作更常见时，它的性能最好，比率约为10:1。</p><h2 id="Zookeeper的数据模型"><a href="#Zookeeper的数据模型" class="headerlink" title="Zookeeper的数据模型"></a>Zookeeper的数据模型</h2><p>ZooKeeper拥有一个层次的命名空间，这个和标准的文件系统非常相似，如下图所示。</p><p><img src="https://images0.cnblogs.com/blog/671563/201411/301534562152768.png" alt="ZooKeeper数据模型与文件系统目录树"></p><p>从图中我们可以看出ZooKeeper的数据模型，在结构上和标准文件系统的非常相似，都是采用这种树形层次结构，ZooKeeper树中的每个节点被称为—Znode。和文件系统的目录树一样，ZooKeeper树中的每个节点可以拥有子节点。但也有不同之处：</p><ol><li><p>引用方式</p><p>Zonde通过<strong>路径引用</strong>，如同Unix中的文件路径。路径必须是绝对的，因此他们必须由斜杠字符来<strong>开头</strong>。除此以外，他们必须是唯一的，也就是说每一个路径只有一个表示，因此这些路径不能改变。在ZooKeeper中，路径由Unicode字符串组成，并且有一些限制。字符串”/zookeeper”用以保存管理信息，比如关键配额信息。</p></li><li><p>Znode结构</p><p>ZooKeeper命名空间中的Znode，兼具文件和目录两种特点。既像文件一样维护着数据、元信息、ACL、时间戳等数据结构，又像目录一样可以作为路径标识的一部分。图中的每个节点称为一个Znode。 每个Znode由3部分组成:</p><ul><li><p>stat：此为状态信息, 描述该Znode的版本, 权限等信息</p></li><li><p>data：与该Znode关联的数据</p></li><li><p>children：该Znode下的子节点</p></li></ul><p>ZooKeeper虽然可以关联一些数据，但并没有被设计为常规的数据库或者大数据存储，相反的是，它用来<strong>管理调度数据</strong>，比如分布式应用中的配置文件信息、状态信息、汇集位置等等。这些数据的共同特性就是它们都是很小的数据，通常以KB为大小单位。ZooKeeper的服务器和客户端都被设计为严格检查并限制每个Znode的数据大小至多1M，但常规使用中应该远小于此值。</p></li><li><p>数据访问</p><p>ZooKeeper中的每个节点存储的数据要被<strong>原子性的操作</strong>。也就是说读操作将获取与节点相关的所有数据，写操作也将替换掉节点的所有数据。另外，每一个节点都拥有自己的ACL(访问控制列表)，这个列表规定了用户的权限，即限定了特定用户对目标节点可以执行的操作。</p></li><li><p>节点类型</p><p>ZooKeeper中的节点有两种，分别为<strong>临时节点</strong>和<strong>永久节点</strong>。节点的类型在创建时即被确定，并且不能改变。</p><p>临时节点: </p><ul><li>该节点的生命周期依赖于创建它们的会话。一旦会话(Session)结束，临时节点将被自动删除，当然可以也可以手动删除。虽然每个临时的Znode都会绑定到一个客户端会话，但他们对所有的客户端还是可见的。另外，ZooKeeper的临时节点不允许拥有子节点。</li></ul><p>永久节点：</p><ul><li>该节点的生命周期不依赖于会话，并且只有在客户端显示执行删除操作的时候，他们才能被删除。</li></ul></li><li><p>顺序节点</p><p>当创建Znode的时候，用户可以请求在ZooKeeper的路径结尾添加一个<strong>递增的计数</strong>。这个计数<strong>对于此节点的父节点来说</strong>是唯一的，它的格式为”%10d”(10位数字，没有数值的数位用0补充，例如”0000000001”)。当计数值大于232-1时，计数器将溢出。</p></li><li><p>观察</p><p>客户端可以在节点上设置watch，我们称之为<strong>监视器</strong>。当节点状态发生改变时(Znode的增、删、改)将会触发watch所对应的操作。当watch被触发时，ZooKeeper将会向客户端发送且仅发送一条通知，因为watch只能被触发一次，这样可以减少网络流量。</p></li></ol><h2 id="安装Zookeeper"><a href="#安装Zookeeper" class="headerlink" title="安装Zookeeper"></a>安装Zookeeper</h2><ol><li><p>下载源码包()</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">http:<span class="comment">//archive.cloudera.com/cdh5/cdh/5/zookeeper-3.4.5-cdh5.16.2.tar.gz</span></span><br></pre></td></tr></table></figure></li><li><p>编译</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">yum install ant<span class="comment">//zk的编译不是使用maven的，而是使用ant</span></span><br><span class="line">ant <span class="keyword">package</span></span><br></pre></td></tr></table></figure><p>需要注意两个坑:</p><ol><li>需要指定jdk为1.7</li><li>远端仓库连不上，需要改路径</li></ol></li><li><p>配置环境变量</p></li></ol><h2 id="单节点配置Zookeeper"><a href="#单节点配置Zookeeper" class="headerlink" title="单节点配置Zookeeper"></a>单节点配置Zookeeper</h2><ol><li>修改zoo.cfg文件(需要从zoo_sample.cfg复制一份)</li><li>修改dataDir路径，默认放在/tmp目录下，dataDir主要用于保存Zookeeper中的数据</li><li>启动服务端 <code>bin/zkServer.sh start</code></li><li>启动客户端 <code>bin/zkCli.sh</code></li></ol><h2 id="多节点配置Zookeeper"><a href="#多节点配置Zookeeper" class="headerlink" title="多节点配置Zookeeper"></a>多节点配置Zookeeper</h2><ol><li><p>修改zoo.cfg文件(需要从zoo_sample.cfg复制一份)</p></li><li><p>修改dataDir路径，默认放在/tmp目录下，dataDir主要用于保存Zookeeper中的数据</p></li><li><p>在zoo.cfg文件中配置集群的id、主机和端口号，<code>集群共享内容</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">server<span class="number">.2</span>=hadoop102:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line">server<span class="number">.3</span>=hadoop103:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line">server<span class="number">.4</span>=hadoop104:<span class="number">2888</span>:<span class="number">3888</span></span><br></pre></td></tr></table></figure><p><code>server.A=B:C:D</code><br>A：标识第几号服务器，有一个myid要与之对应<br>B：服务器的地址<br>C：follower与leader通信的端口<br>D：选举leader时的端口</p></li><li><p>在dataDir指定的路径下创建myid文件，添加与之对应的服务器编号，即编号A，<code>集群唯一内容</code>，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。</p></li><li><p>启动集群服务端 <code>bin/zkServer.sh start</code></p></li><li><p>启动集群客户端 ，工作中客户端连接集群命令: </p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">./zkCli.sh -server hadoop102:<span class="number">2181</span>,hadoop103:<span class="number">2181</span>,hadoop104:<span class="number">2181</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="Zookeeper的常用命令"><a href="#Zookeeper的常用命令" class="headerlink" title="Zookeeper的常用命令"></a>Zookeeper的常用命令</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">stat 状态</span><br><span class="line">set修改</span><br><span class="line">ls 查看</span><br><span class="line">ls2 ls+get</span><br><span class="line">delete 删除</span><br><span class="line">rmr 递归删除</span><br><span class="line">get 得到</span><br><span class="line">create 创建</span><br></pre></td></tr></table></figure><ol><li><p><code>stat path [watch]</code>：数据的状态</p><p><code>[zk: localhost:2181(CONNECTED) 1] stat /tunan</code> </p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">cZxid = <span class="number">0x21</span><span class="comment">//节点的id号</span></span><br><span class="line">ctime = Mon Mar <span class="number">02</span> <span class="number">15</span>:<span class="number">27</span>:<span class="number">36</span> CST <span class="number">2020</span><span class="comment">//节点的创建时间</span></span><br><span class="line">mZxid = <span class="number">0x32</span><span class="comment">//节点修改的id号</span></span><br><span class="line">mtime = Mon Mar <span class="number">02</span> <span class="number">15</span>:<span class="number">36</span>:<span class="number">45</span> CST <span class="number">2020</span><span class="comment">//节点修改的时间</span></span><br><span class="line">pZxid = <span class="number">0x38</span><span class="comment">//最后更新的子节点的id</span></span><br><span class="line">cversion = <span class="number">9</span><span class="comment">//节点所拥有的子节点的被修改的版本号</span></span><br><span class="line">dataVersion = <span class="number">1</span><span class="comment">//节点数据版本号</span></span><br><span class="line">aclVersion = <span class="number">0</span><span class="comment">//节点所拥有的ACL版本号</span></span><br><span class="line">ephemeralOwner = <span class="number">0x0</span><span class="comment">//特殊标记 0x0: 永久节点</span></span><br><span class="line">dataLength = <span class="number">3</span><span class="comment">//数据的长度</span></span><br><span class="line">numChildren = <span class="number">5</span><span class="comment">//子节点的个数</span></span><br></pre></td></tr></table></figure></li><li><p><code>set path data [version]</code>：修改数据</p><p><code>[zk: localhost:2181(CONNECTED) 5] set /tunan/aaa 123456</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">cZxid = <span class="number">0x31</span><span class="comment">//这是一个子节点的id跟上面的父节点的id不同</span></span><br><span class="line">ctime = Mon Mar <span class="number">02</span> <span class="number">15</span>:<span class="number">35</span>:<span class="number">31</span> CST <span class="number">2020</span><span class="comment">//子节点的创建时间</span></span><br><span class="line">mZxid = <span class="number">0x3a</span><span class="comment">//子节点的修改id</span></span><br><span class="line">mtime = Tue Mar <span class="number">03</span> <span class="number">11</span>:<span class="number">28</span>:<span class="number">06</span> CST <span class="number">2020</span><span class="comment">//子节点的修改时间</span></span><br><span class="line">pZxid = <span class="number">0x31</span><span class="comment">//最后更新的子节点的id，我们现在更新的是这个节点，所以对应cZxid</span></span><br><span class="line">cversion = <span class="number">0</span><span class="comment">//节点所拥有的子节点的被修改的版本号，明显和父节点不同</span></span><br><span class="line">dataVersion = <span class="number">1</span><span class="comment">//数据被修改过了，所以版本号变了</span></span><br><span class="line">aclVersion = <span class="number">0</span><span class="comment">//节点所拥有的ACL版本号</span></span><br><span class="line">ephemeralOwner = <span class="number">0x0</span><span class="comment">//特殊标记 0x0: 永久节点</span></span><br><span class="line">dataLength = <span class="number">6</span><span class="comment">//数据的长度</span></span><br><span class="line">numChildren = <span class="number">0</span><span class="comment">//子节点的个数</span></span><br></pre></td></tr></table></figure></li><li><p><code>ls path [watch]</code>：查看节点</p><p><code>[zk: localhost:2181(CONNECTED) 8] ls /tunan</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[aaa, ccc, bbb, eee, ddd]</span><br></pre></td></tr></table></figure></li><li><p><code>ls2 path [watch]</code>：查看并获取节点内容，等同于get</p><p><code>[zk: localhost:2181(CONNECTED) 9] ls2 /tunan</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[aaa, ccc, bbb, eee, ddd]<span class="comment">//节点的数据内容</span></span><br></pre></td></tr></table></figure></li><li><p><code>delete path [version]</code>：删除节点</p><p><code>[zk: localhost:2181(CONNECTED) 10] delete /tunan/aaa</code><br><code>[zk: localhost:2181(CONNECTED) 11] ls /tunan</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[ccc, bbb, eee, ddd]</span><br></pre></td></tr></table></figure></li><li><p><code>rmr path</code>：递归删除节点</p><p><code>[zk: localhost:2181(CONNECTED) 12] rmr /tunan/ccc</code><br><code>[zk: localhost:2181(CONNECTED) 13] ls /tunan</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">[bbb, eee, ddd]</span><br></pre></td></tr></table></figure></li><li><p><code>get path [watch]</code>：获取节点内容，等同于ls2</p><p><code>[zk: localhost:2181(CONNECTED) 14] get /tunan/bbb</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="number">222</span></span><br></pre></td></tr></table></figure></li><li><p><code>create [-s] [-e] path data acl</code>：创建节点</p><p>-s：顺序创建</p><p><code>[zk: localhost:2181(CONNECTED) 0] create -s /tunan/bbb 12345</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Created /tunan/bbb0000000007</span><br></pre></td></tr></table></figure><p><code>[zk: localhost:2181(CONNECTED) 1] create -s /tunan/bbb 12345</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Created /tunan/bbb0000000008</span><br></pre></td></tr></table></figure><p>-e：临时创建</p><p><code>[zk: localhost:2181(CONNECTED) 5] create -e /tunan/fff 12345</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Created /tunan/fff</span><br></pre></td></tr></table></figure></li></ol><h2 id="Zookeeper的监听"><a href="#Zookeeper的监听" class="headerlink" title="Zookeeper的监听"></a>Zookeeper的监听</h2><ol><li><p>watch概述</p><p>ZooKeeper可以为所有的<strong>读操作</strong>设置watch，这些读操作包括：exists()、getChildren()及getData()。watch事件是<strong>一次性的触发器</strong>，当watch的对象状态发生改变时，将会触发此对象上watch所对应的事件。watch事件将被<strong>异步</strong>地发送给客户端，并且ZooKeeper为watch机制提供了有序的<strong>一致性保证</strong>。理论上，客户端接收watch事件的时间要快于其看到watch对象状态变化的时间。</p></li><li><p>watch类型</p><p>ZooKeeper所管理的watch可以分为两类：</p><ul><li><p>数据watch(data watches)：<strong>getData</strong>和<strong>exists</strong>负责设置数据watch</p></li><li><p>孩子watch(child watches)：<strong>getChildren</strong>负责设置孩子watch</p></li></ul><p>我们可以通过操作<strong>返回的数据</strong>来设置不同的watch：</p><ul><li>getData和exists：</li><li>getChildren：返回孩子列表</li></ul><p>因此</p><ol><li><p>一个成功的<strong>setData操作</strong>将触发Znode的数据watch</p></li><li><p>一个成功的<strong>create操作</strong>将触发Znode的数据watch以及孩子watch</p></li><li><p>一个成功的<strong>delete操作</strong>将触发Znode的数据watch以及孩子watch</p></li></ol></li><li><p>watch注册与处触发</p><p>图 6.1 watch设置操作及相应的触发器如图下图所示：</p><p><img src="https://images0.cnblogs.com/blog/671563/201411/301534579188980.png" alt="img"></p><ol><li>exists操作上的watch，在被监视的Znode<strong>创建</strong>、<strong>删除</strong>或<strong>数据更新</strong>时被触发。</li><li>getData操作上的watch，在被监视的Znode<strong>删除</strong>或<strong>数据更新</strong>时被触发。在被创建时不能被触发，因为只有</li><li>getChildren操作上的watch，在被监视的Znode的子节点<strong>创建</strong>或<strong>删除</strong>，或是这个Znode自身被<strong>删除</strong>时被触发。可以通过查看watch事件类型来区分是Znode，还是他的子节点被删除：NodeDelete表示Znode被删除，NodeDeletedChanged表示子节点被删除。</li></ol><p>Watch由客户端所连接的ZooKeeper服务器在本地维护，因此watch可以非常容易地设置、管理和分派。当客户端连接到一个新的服务器时，任何的会话事件都将可能触发watch。另外，当从服务器断开连接的时候，watch将不会被接收。但是，当一个客户端重新建立连接的时候，任何先前注册过的watch都会被重新注册。</p></li><li><p>需要注意的几点</p><p>Zookeeper的watch实际上要处理两类事件：</p><ul><li><p>连接状态事件(type=None, path=null)</p><p>这类事件不需要注册，也不需要我们连续触发，我们只要处理就行了。</p></li><li><p>节点事件</p><p>节点的建立，删除，数据的修改。它是one time trigger，我们需要不停的注册触发，还可能发生事件丢失的情况。</p></li></ul><p>上面2类事件都在Watch中处理，也就是重载的<strong>process(Event event)</strong></p><p>节点事件的触发，通过函数exists，getData或getChildren来处理这类函数，有双重作用：</p><ol><li>注册触发事件</li><li>函数本身的功能</li></ol><p>函数的本身的功能又可以用异步的回调函数来实现,重载processResult()过程中处理函数本身的的功能。</p></li><li><p>命令行操作</p><p>监听创建节点(<code>stat命令</code>)</p><p><code>stat /xiaoqi watch</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 13] create /xiaoqi 111</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeCreated path:/xiaoqi</span><br><span class="line">Created /xiaoqi</span><br></pre></td></tr></table></figure><p>监听修改节点(<code>stat命令</code>)</p><p><code>stat /xiaoqi watch</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 16] set /xiaoqi 1234</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeDataChanged path:/xiaoqi</span><br></pre></td></tr></table></figure><p>监听删除节点(<code>stat命令</code>)</p><p><code>stat /xiaoqi watch</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 20] delete /xiaoqi</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeDeleted path:/xiaoqi</span><br></pre></td></tr></table></figure><p>监听父节点删除子节点(<code>ls命令</code>)</p><p><code>ls /xiaoqi watch</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 29] create /xiaoqi/aaa 111</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/xiaoqi</span><br></pre></td></tr></table></figure><p>监听父节点创建子节点(<code>ls命令</code>)</p><p><code>ls /xiaoqi watch</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 31] delete /xiaoqi/aaa</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/xiaoqi</span><br></pre></td></tr></table></figure></li></ol><h2 id="Zookeeper四字命令"><a href="#Zookeeper四字命令" class="headerlink" title="Zookeeper四字命令"></a>Zookeeper四字命令</h2><p><a href="">官网</a></p><p>无须启动zk客户端，直接在命令行输入<code>echo {}| nc localhost 2181</code> 即可</p><p><code>stat:</code> 列出服务器和连接的客户机的简要信息。</p><p>echo stat | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Zookeeper version: <span class="number">3.4</span><span class="number">.5</span>-cdh5<span class="number">.16</span><span class="number">.2</span>--<span class="number">1</span>, built on <span class="number">06</span>/<span class="number">03</span>/<span class="number">2019</span> <span class="number">10</span>:<span class="number">40</span> GMT</span><br><span class="line">Clients:<span class="comment">//客户端</span></span><br><span class="line"> /<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">58056</span>[<span class="number">0</span>](queued=<span class="number">0</span>,recved=<span class="number">1</span>,sent=<span class="number">0</span>)</span><br><span class="line"> /<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">57020</span>[<span class="number">1</span>](queued=<span class="number">0</span>,recved=<span class="number">288</span>,sent=<span class="number">296</span>)</span><br><span class="line"></span><br><span class="line">Latency min/avg/max: <span class="number">0</span>/<span class="number">0</span>/<span class="number">20</span></span><br><span class="line">Received: <span class="number">454</span><span class="comment">//接收</span></span><br><span class="line">Sent: <span class="number">461</span><span class="comment">//发送</span></span><br><span class="line">Connections: <span class="number">2</span><span class="comment">//连接数</span></span><br><span class="line">Outstanding: <span class="number">0</span></span><br><span class="line">Zxid: <span class="number">0x53</span></span><br><span class="line">Mode: standalone<span class="comment">//模式</span></span><br><span class="line">Node count: <span class="number">17</span><span class="comment">//节点数</span></span><br></pre></td></tr></table></figure><p><code>ruok:</code> 测试服务器是否在非错误状态下运行。</p><p>echo ruok | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">imok</span><br></pre></td></tr></table></figure><p><code>dump:</code> 列出未完成的会话和临时节点。这只对leader有效。</p><p>echo dump | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SessionTracker dump:</span><br><span class="line"><span class="function">Session <span class="title">Sets</span> <span class="params">(<span class="number">3</span>)</span>:</span></span><br><span class="line"><span class="function">0 expire at Tue Mar 03 12:29:46 CST 2020:</span></span><br><span class="line"><span class="function">0 expire at Tue Mar 03 12:29:56 CST 2020:</span></span><br><span class="line"><span class="function">1 expire at Tue Mar 03 12:30:06 CST 2020:</span></span><br><span class="line"><span class="function">0x1709e65d9b00001</span></span><br><span class="line"><span class="function">ephemeral nodes dump:</span></span><br><span class="line"><span class="function">Sessions with <span class="title">Ephemerals</span> <span class="params">(<span class="number">1</span>)</span>:</span></span><br><span class="line"><span class="function">0x1709e65d9b00001:</span></span><br><span class="line"><span class="function">/tunan/fff<span class="comment">//临时节点</span></span></span><br></pre></td></tr></table></figure><p><code>conf:</code> 打印关于服务配置的详细信息。</p><p>echo conf | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">clientPort=<span class="number">2181</span><span class="comment">//端口</span></span><br><span class="line">dataDir=/home/hadoop/tmp/zookeeper/version-<span class="number">2</span><span class="comment">//数据目录</span></span><br><span class="line">dataLogDir=/home/hadoop/tmp/zookeeper/version-<span class="number">2</span><span class="comment">//日志目录</span></span><br><span class="line">tickTime=<span class="number">2000</span><span class="comment">//心跳时间，单位毫秒</span></span><br><span class="line">maxClientCnxns=<span class="number">60</span><span class="comment">//最大连接数</span></span><br><span class="line">minSessionTimeout=<span class="number">4000</span><span class="comment">//最小的超时时间</span></span><br><span class="line">maxSessionTimeout=<span class="number">40000</span><span class="comment">//最大的超时时间</span></span><br><span class="line">serverId=<span class="number">0</span><span class="comment">//服务器的id</span></span><br></pre></td></tr></table></figure><p><code>envi:</code> 打印出服务环境的细节</p><p>echo envi | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Environment:</span><br><span class="line">zookeeper.version=<span class="number">3.4</span><span class="number">.5</span>-cdh5<span class="number">.16</span><span class="number">.2</span>--<span class="number">1</span>, built on <span class="number">06</span>/<span class="number">03</span>/<span class="number">2019</span> <span class="number">10</span>:<span class="number">40</span> GMT</span><br><span class="line">host.name=aliyun</span><br><span class="line">java.version=<span class="number">1.8</span><span class="number">.0_144</span></span><br><span class="line">java.vendor=Oracle Corporation</span><br><span class="line">java.home=/usr/java/jdk1<span class="number">.8</span><span class="number">.0_144</span>/jre</span><br><span class="line">java<span class="class">.<span class="keyword">class</span>.<span class="title">path</span></span>=/home/hadoop/app/zookeeper/bin/../build/classes:/home/hadoop/app/zookeeper/bin/../build/lib<span class="comment">/*.jar:/home/hadoop/app/zookeeper/bin/../share/zookeeper/zookeeper-3.4.5-cdh5.16.2.jar:/home/hadoop/app/zookeeper/bin/../share/zookeeper/slf4j-log4j12-1.7.5.jar:/home/hadoop/app/zookeeper/bin/../share/zookeeper/slf4j-api-1.7.5.jar:/home/hadoop/app/zookeeper/bin/../share/zookeeper/netty-3.10.5.Final.jar:/home/hadoop/app/zookeeper/bin/../share/zookeeper/log4j-1.2.16.jar:/home/hadoop/app/zookeeper/bin/../share/zookeeper/jline-2.11.jar:/home/hadoop/app/zookeeper/bin/../src/java/lib/*.jar:/home/hadoop/app/zookeeper/bin/../conf:</span></span><br><span class="line"><span class="comment">java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</span></span><br><span class="line"><span class="comment">java.io.tmpdir=/tmp</span></span><br><span class="line"><span class="comment">java.compiler=&lt;NA&gt;</span></span><br><span class="line"><span class="comment">os.name=Linux</span></span><br><span class="line"><span class="comment">os.arch=amd64</span></span><br><span class="line"><span class="comment">os.version=3.10.0-514.26.2.el7.x86_64</span></span><br><span class="line"><span class="comment">user.name=hadoop</span></span><br><span class="line"><span class="comment">user.home=/home/hadoop</span></span><br><span class="line"><span class="comment">user.dir=/home/hadoop/app/zookeeper-3.4.5-cdh5.16.2/bin</span></span><br></pre></td></tr></table></figure><p><code>mntr:</code> 输出可用于监视集群健康状况的变量列表。</p><p>echo mntr | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">zk_version<span class="number">3.4</span><span class="number">.5</span>-cdh5<span class="number">.16</span><span class="number">.2</span>--<span class="number">1</span>, built on <span class="number">06</span>/<span class="number">03</span>/<span class="number">2019</span> <span class="number">10</span>:<span class="number">40</span> GMT</span><br><span class="line">zk_avg_latency<span class="number">0</span></span><br><span class="line">zk_max_latency<span class="number">20</span></span><br><span class="line">zk_min_latency<span class="number">0</span></span><br><span class="line">zk_packets_received<span class="number">494</span></span><br><span class="line">zk_packets_sent<span class="number">501</span></span><br><span class="line">zk_num_alive_connections<span class="number">2</span></span><br><span class="line">zk_outstanding_requests<span class="number">0</span></span><br><span class="line">zk_server_statestandalone</span><br><span class="line">zk_znode_count<span class="number">17</span></span><br><span class="line">zk_watch_count<span class="number">1</span></span><br><span class="line">zk_ephemerals_count<span class="number">1</span></span><br><span class="line">zk_approximate_data_size<span class="number">253</span></span><br><span class="line">zk_open_file_descriptor_count<span class="number">27</span></span><br><span class="line">zk_max_file_descriptor_count<span class="number">65535</span></span><br><span class="line">zk_fsync_threshold_exceed_count<span class="number">0</span></span><br></pre></td></tr></table></figure><p><code>wchs:</code> 按会话列出关于服务器监视的详细信息。</p><p>echo wchs | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span> connections watching <span class="number">1</span> paths</span><br><span class="line">Total watches:<span class="number">1</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Zookeeper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Zookeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>项目复盘</title>
      <link href="/2020/02/21/offlinedw/8.%E9%A1%B9%E7%9B%AE%E5%A4%8D%E7%9B%98/"/>
      <url>/2020/02/21/offlinedw/8.%E9%A1%B9%E7%9B%AE%E5%A4%8D%E7%9B%98/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>任务分配问题</li><li>hive问题</li><li>etl代码问题</li><li>任务调度问题</li></ol><h2 id="任务分配问题"><a href="#任务分配问题" class="headerlink" title="任务分配问题"></a>任务分配问题</h2><ol><li>立项开始就需要梳理开发流程，精确到每一个开发环节。</li><li>开发定义好接口尽量并行开发。</li><li>建立规范、代码版本同步。</li><li>定义ods层表名,dws层表名。</li><li>有人跟踪项目进度有没有阻碍，进度落后需要帮忙</li></ol><h2 id="hive问题"><a href="#hive问题" class="headerlink" title="hive问题"></a>hive问题</h2><ol><li>建表需要加注释，规范表名。</li><li>重建外部表需要删除数据。</li></ol><h2 id="etl代码问题"><a href="#etl代码问题" class="headerlink" title="etl代码问题"></a>etl代码问题</h2><ol><li>ip解析应该放在setup上。</li><li>searcher 提到全局静态变量</li></ol><h2 id="任务调度"><a href="#任务调度" class="headerlink" title="任务调度"></a>任务调度</h2><ol><li>每个统计任务直接跟sqoop同步任务</li><li>数据倾斜group by 优化 skewindata  = true</li></ol>]]></content>
      
      
      <categories>
          
          <category> OfflineDW </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OfflineDW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban的安装&amp;使用&amp;坑</title>
      <link href="/2020/02/20/azkaban/1/"/>
      <url>/2020/02/20/azkaban/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>Azkaban的安装</li><li>Azkaban的使用</li><li>Git的安装</li></ol><h2 id="Git的安装"><a href="#Git的安装" class="headerlink" title="Git的安装"></a>Git的安装</h2><p>在安装Azkaban之前要安装Git</p><ol><li><p>获取github最新的Git安装包下载链接，进入Linux服务器，执行下载，命令为： </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https://github.com/git/git/archive/v2.17.0.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>压缩包解压，命令为：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -zxvf v2.17.0.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>安装编译源码所需依赖，命令为：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel gcc perl-ExtUtils-MakeMaker</span><br></pre></td></tr></table></figure><p>耐心等待安装，出现提示输入y即可；</p></li><li><p>安装依赖时，yum自动安装了Git，需要卸载旧版本Git，命令为：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum remove git -y</span><br></pre></td></tr></table></figure></li><li><p>进入解压后的文件夹，命令 cd git-2.17.0 ，然后执行编译，命令为:  </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">make prefix=/usr/local/git all</span><br></pre></td></tr></table></figure></li><li><p>安装Git至/usr/local/git路径，命令为:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">make prefix=/usr/local/git install</span><br></pre></td></tr></table></figure></li><li><p>打开环境变量配置文件，命令 vim /etc/profile ，在底部加上Git相关配置信息：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export GIT_HOME=/usr/local/git/bin</span><br><span class="line">export PATH=$GIT_HOME/bin/:$PATH</span><br></pre></td></tr></table></figure></li><li><p>输入命令 <code>git --version</code> ，查看安装的git版本，校验通过，安装成功。</p></li></ol><h2 id="Azkaban的安装"><a href="#Azkaban的安装" class="headerlink" title="Azkaban的安装"></a>Azkaban的安装</h2><ol><li><p>下载<a href="https://github.com/azkaban/azkaban/releases" target="_blank" rel="noopener">Azkaban</a></p></li><li><p>解压</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xzvf 3.81.0.tar.gz -C ../app/</span><br></pre></td></tr></table></figure></li><li><p>安装<a href="https://azkaban.readthedocs.io/en/latest/getStarted.html" target="_blank" rel="noopener">编译文档</a></p><p>在开始编译之前要下载名为<a href="https://services.gradle.org/distributions/gradle-4.6-all.zip" target="_blank" rel="noopener">gradle-4.6-all.zip</a>的包，如果直接让系统下载超级慢，下载好了放在同目录下，并且在<code>/azkaban/gradle/wrapper/gradle-wrapper.properties</code> 中注释下载路径，并指定已经下载好的路径</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">distributionUrl=gradle-4.6-all.zip</span><br><span class="line"><span class="meta">#</span><span class="bash">distributionUrl=https\://services.gradle.org/distributions/gradle-4.6-all.zip</span></span><br></pre></td></tr></table></figure><p>还需要安装gcc环境:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install -y gcc-c++*</span><br></pre></td></tr></table></figure><p>修改maven仓库为阿里云镜像</p><p><code>vim azkaban-3.81.0/build.gradle 54行左右</code> </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">maven &#123;</span><br><span class="line">//这里是阿里云</span><br><span class="line">url &apos;http://maven.aliyun.com/nexus/content/repositories/central/&apos;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>   正式编译</p>   <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./gradlew build installDist -x test</span><br></pre></td></tr></table></figure><p>   编译后生成三个文件</p>   <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">azkaban-exec-server</span><br><span class="line">azkaban-solo-server</span><br><span class="line">azkaban-web-server</span><br></pre></td></tr></table></figure><ol start="4"><li><p>执行<code>azkaban-solo-server</code></p><ol><li><p>解压<code>azkaban-solo-server</code>到<code>app</code>目录下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xzvf azkaban-solo-server/build/distributionsa/zkaban-solo-server-0.1.0-SNAPSHOT.tar.gz ~/app</span><br></pre></td></tr></table></figure></li><li><p>启动: <code>bin/start-solo.sh</code>，生成<code>AzkabanSingleServer</code>进程</p></li><li><p>修改配置(时区、用户)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">时区: default.timezone.id=Asia/Shanghai</span><br><span class="line">用户: &lt;user password="tunan" roles="admin" username="tunan"/&gt;</span><br></pre></td></tr></table></figure></li><li><p>web端登录，端口8081</p></li></ol></li><li><p>azkaban需要至少3G内存，如果内存不足3G，需要设置不检查内存</p><p>azkaban-web-server-2.7.0/plugins/jobtypes/commonprivate.properties</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">memCheck.enabled=false</span><br></pre></td></tr></table></figure></li></ol><h2 id="Azkaban的使用"><a href="#Azkaban的使用" class="headerlink" title="Azkaban的使用"></a>Azkaban的使用</h2><h3 id="单节点部署"><a href="#单节点部署" class="headerlink" title="单节点部署"></a>单节点部署</h3><p><a href="https://azkaban.readthedocs.io/en/latest/createFlows.html" target="_blank" rel="noopener">官方文档</a></p><ol><li><p>创建flow20.projec文件，写入信息: </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">azkaban-flow-version: 2.0</span><br></pre></td></tr></table></figure></li><li><p>创建basic.flow文件，写入信息:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: jobA</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: echo "This is an echoed text."</span><br></pre></td></tr></table></figure></li><li><p>压缩Archive.zip文件，在web页面中提交</p><p><code>坑</code>: 提交的用户和hadoop上的用户不同</p></li><li><p>多依赖案例</p><p>flow20.projec文件相同，basic.flow文件内容不同，文件名可不同</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: jobC</span><br><span class="line">    type: noop</span><br><span class="line">    # jobC depends on jobA and jobB</span><br><span class="line">    dependsOn:</span><br><span class="line">      - jobA</span><br><span class="line">      - jobB</span><br><span class="line"></span><br><span class="line">  - name: jobA</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: echo "This is an echoed text."</span><br><span class="line"></span><br><span class="line">  - name: jobB</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: pwd</span><br></pre></td></tr></table></figure></li><li><p>wc案例</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: tunan-wordcount</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: hadoop jar /home/hadoop/app/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar wordcount /azkaban/data/wc.txt /out</span><br></pre></td></tr></table></figure><ol><li>直接点击程序修改输出文件参数</li><li>使用Flow Parameters修改输出文件参数</li></ol></li><li><p>hive案例</p><p>除了固定的flow20.project文件，需要修改hive.flow文件和添加stat.sh文件</p><p>hive.flow</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: exe hive</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: sh stat.sh</span><br></pre></td></tr></table></figure><p>stat.sh</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line">hive -e "select * from offline_dw.dws_country_traffic"</span><br></pre></td></tr></table></figure><p>上传相同的文件，新版本会覆盖旧的版本</p></li><li><p>调度执行</p><p>在执行页面的左下角点击Schedule即可，并可在Scheduling中查看调度信息，然后在History中查看历史执行信息</p></li></ol><h3 id="多节点部署"><a href="#多节点部署" class="headerlink" title="多节点部署"></a>多节点部署</h3><ol><li><p>在mysql中创建数据库</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE DATABASE azkaban;</span><br></pre></td></tr></table></figure></li><li><p>创建用户</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE USER 'tunan'@'%' IDENTIFIED BY 'tunan';</span><br></pre></td></tr></table></figure></li><li><p>赋权给用户</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span>,<span class="keyword">INSERT</span>,<span class="keyword">UPDATE</span>,<span class="keyword">DELETE</span> <span class="keyword">ON</span> azkaban.* <span class="keyword">to</span> <span class="string">'tunan'</span>@<span class="string">'%'</span> <span class="keyword">WITH</span> <span class="keyword">GRANT</span> <span class="keyword">OPTION</span>;</span><br></pre></td></tr></table></figure></li><li><p>在mysql中生成需要的表</p><p>这里的source可能会遇到权限问题，解决的办法很多，如移动其他位置</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">use</span> azkaban;</span><br><span class="line">source /home/hadoop/app/azkaban/azkaban-db/build/<span class="keyword">install</span>/azkaban-db/<span class="keyword">create</span>-<span class="keyword">all</span>-<span class="keyword">sql</span><span class="number">-0.1</span><span class="number">.0</span>-SNAPSHOT.sql;</span><br></pre></td></tr></table></figure></li><li><p>执行SQL获取动态端口号</p><p><code>mysql&gt; select * from executors ;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">----+--------+-------+--------+</span></span><br><span class="line">| id | host   | port  | active |</span><br><span class="line">+<span class="comment">----+--------+-------+--------+</span></span><br><span class="line">|  1 | aliyun | 46418 |      0 |</span><br><span class="line">+<span class="comment">----+--------+-------+--------+</span></span><br></pre></td></tr></table></figure></li><li><p>激活执行器</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -G "http://aliyun:46418/executor?action=activate"</span><br></pre></td></tr></table></figure></li><li><p>把<code>azkaban-exec-server</code>和<code>azkaban-web-server</code>解压，并启动</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xzvf azkaban-exec-server/build/distributionsa/azkaban-exec-server-0.1.0-SNAPSHOT.tar.gz ~/app</span><br><span class="line">tar -xzvf azkaban-web-server/build/distributionsa/azkaban-web-server-0.1.0-SNAPSHOT.tar.gz ~/app</span><br></pre></td></tr></table></figure></li><li><p>登录web使用，使用方式和单节点一样</p></li></ol><h3 id="二次开发"><a href="#二次开发" class="headerlink" title="二次开发"></a>二次开发</h3><p>azkaban的二次开发需要调用az已经做好的接口</p><p><a href="https://azkaban.readthedocs.io/en/latest/ajaxApi.html#request-parameters-1" target="_blank" rel="noopener">官方文档</a></p><p>命令行获取session.id</p><p><code>curl -k -X POST --data &quot;action=login&amp;username=azkaban&amp;password=azkaban&quot; http://aliyun:8081</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "session.id" : "f1725f59-9d20-4fe2-b89e-3715ef85****",</span><br><span class="line">  "status" : "success"</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="常用功能"><a href="#常用功能" class="headerlink" title="常用功能"></a>常用功能</h2><ol><li>配置代理用户</li><li>配置hadoop.home</li><li>配置Spark作业提交</li></ol>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>结果数据的展示</title>
      <link href="/2020/02/20/offlinedw/7.%E7%BB%93%E6%9E%9C%E6%95%B0%E6%8D%AE%E7%9A%84%E5%B1%95%E7%A4%BA/"/>
      <url>/2020/02/20/offlinedw/7.%E7%BB%93%E6%9E%9C%E6%95%B0%E6%8D%AE%E7%9A%84%E5%B1%95%E7%A4%BA/</url>
      
        <content type="html"><![CDATA[<p>由于时间忙不过来，这篇暂时不更新，仅作为维护项目的完整性存在。。。</p>]]></content>
      
      
      <categories>
          
          <category> OfflineDW </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OfflineDW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>业务数据的抽取</title>
      <link href="/2020/02/19/offlinedw/6.%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%8A%BD%E5%8F%96/"/>
      <url>/2020/02/19/offlinedw/6.%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%8A%BD%E5%8F%96/</url>
      
        <content type="html"><![CDATA[<p>由于时间忙不过来，这篇暂时不更新，仅作为项目的完整性存在。。。</p>]]></content>
      
      
      <categories>
          
          <category> OfflineDW </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OfflineDW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库的分层</title>
      <link href="/2020/02/18/offlinedw/5.%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E5%88%86%E5%B1%82/"/>
      <url>/2020/02/18/offlinedw/5.%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E5%88%86%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<p>系统架构图:</p><p><img src="https://yerias.github.io/offlinedw/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E9%A1%B9%E7%9B%AE%E5%9B%BE_3.jpg" alt="数据仓库架构项目图"></p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>数据仓库为什么要分层</li><li>数据仓库如何分层</li><li>使用脚本将数据执行分层</li><li>数据分析案例</li><li>使用crontab调度脚本(临时)</li></ol><h2 id="数据仓库为什么要分层"><a href="#数据仓库为什么要分层" class="headerlink" title="数据仓库为什么要分层"></a>数据仓库为什么要分层</h2><p>分层的主要原因是在管理数据的时候，能对数据有一个更加清晰的掌控，详细来讲，主要有下面几个原因：</p><ol><li><code>清晰数据结构</code>，每一个数据分层都有它的作用域，这样我们在使用表的时候能更方便地定位和理解。</li><li><code>数据血缘追踪</code>，简单来说，我们最终给业务呈现的是一个能直接使用业务表，但是它的来源有很多，如果有一张来源表出问题了，我们希望能够快速准确地定位到问题，并清楚它的危害范围。</li><li><code>减少重复开发</code>，规范数据分层，开发一些通用的中间层数据，能够减少极大的重复计算。</li><li><code>把复杂问题简单化</code>，将一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。</li><li><code>屏蔽原始数据的异常</code></li><li><code>屏蔽业务的影响</code>，不必改一次业务就需要重新接入数据</li></ol><h2 id="数据仓库如何分层"><a href="#数据仓库如何分层" class="headerlink" title="数据仓库如何分层"></a>数据仓库如何分层</h2><p>数据仓库的分层标准: 不为分层而分层</p><h3 id="标准的分层"><a href="#标准的分层" class="headerlink" title="标准的分层"></a>标准的分层</h3><ul><li>ODS：<code>数据原始层</code>: 直接加载原始数据，不做任何处理(不做ETL)</li><li>DWD：<code>数据明细层</code>，对ODS层进行清洗(ETL)</li><li>DWS：<code>数据服务层</code>，基于DWD做统计分析</li><li>ADS：<code>数据应用层</code>，为各种统计报表提供数据</li></ul><h3 id="我们的分层"><a href="#我们的分层" class="headerlink" title="我们的分层"></a>我们的分层</h3><ul><li>ODS(operate data store)：<code>数据原始层</code>，最接近数据源中数据的一层，数据源中的数据，经过抽取、洗净、传输，也就说传说中的ETL之后，装入本层</li><li>DWS(data warehouse server)：<code>数据明细层</code>，从ODS层中获得的数据按照主题建立各种数据模型。在这里，我们需要了解四个概念：维（dimension）、事实（Fact）、指标（Index）和粒度（ Granularity）。</li><li>ADS：<code>数据应用层</code>，该层主要是提供数据产品和数据分析使用的数据。 比如我们经常说的报表数据，或者说那种大宽表，一般就放在这里。</li></ul><h2 id="使用脚本将数据执行分层"><a href="#使用脚本将数据执行分层" class="headerlink" title="使用脚本将数据执行分层"></a>使用脚本将数据执行分层</h2><p>现在我们知道了数据需要在Hive中分成ODS层、DWS层和ADS层，每一层的建表标准是<code>ods_</code>、<code>dws_</code>、<code>ads_</code></p><ol><li><p>建库</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> offline_dw;</span><br></pre></td></tr></table></figure></li><li><p>建ODS层外部分区表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> ods_access(</span><br><span class="line"><span class="string">`year`</span><span class="keyword">String</span>,</span><br><span class="line"><span class="string">`month`</span><span class="keyword">String</span>,</span><br><span class="line"><span class="string">`day`</span><span class="keyword">String</span>,</span><br><span class="line">country<span class="keyword">String</span>,</span><br><span class="line">province<span class="keyword">String</span>,</span><br><span class="line">city<span class="keyword">String</span>,</span><br><span class="line">area<span class="keyword">String</span>,</span><br><span class="line">proxyIp<span class="keyword">String</span>,</span><br><span class="line">responseTime<span class="built_in">BigInt</span>,</span><br><span class="line">referer<span class="keyword">String</span>,</span><br><span class="line"><span class="string">`method`</span><span class="keyword">String</span>,</span><br><span class="line"><span class="keyword">http</span><span class="keyword">String</span>,</span><br><span class="line"><span class="string">`domain`</span><span class="keyword">String</span>,</span><br><span class="line"><span class="string">`path`</span><span class="keyword">String</span>,</span><br><span class="line">httpCode<span class="keyword">String</span>,</span><br><span class="line">requestSize<span class="built_in">BigInt</span>,</span><br><span class="line">responseSize<span class="built_in">bigInt</span>,</span><br><span class="line"><span class="keyword">cache</span><span class="keyword">String</span>,</span><br><span class="line">userId <span class="built_in">BigInt</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span>(d <span class="keyword">String</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span></span><br><span class="line">location <span class="string">"/item/offline-dw/ods/access/"</span>;</span><br></pre></td></tr></table></figure></li><li><p>写脚本(etl.sh)</p><ol><li>使用MR通过ETL把源数据写入ODS临时目录</li><li>把ODS临时目录中<code>part*</code>开头的数据移动到ODS分区目录下，并指定分区</li><li>接着Hive执行命令新建分区</li></ol><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line"></span><br><span class="line">if [ $# -eq 1 ];then</span><br><span class="line">        time=$1</span><br><span class="line">else</span><br><span class="line">        time=`date --date "1 days ago" +%Y%m%d`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">MR做ETL 注意输入和输出(带时间)</span></span><br><span class="line">hadoop jar /home/hadoop/lib/hadoop-client-1.0.0.jar com.tunan.item.ETLDriver -libjars $LIBJARS /item/offline-dw/raw/access/$time /item/offline-dw/ods_tmp/access/$time</span><br><span class="line"><span class="meta">#</span><span class="bash">删除分区目录(d=时间)</span></span><br><span class="line">hdfs dfs -rm -r -f  /item/offline-dw/ods/access/d=$time</span><br><span class="line"><span class="meta">#</span><span class="bash">创建分区目录(d=时间)</span></span><br><span class="line">hdfs dfs -mkdir -p /item/offline-dw/ods/access/d=$time</span><br><span class="line"><span class="meta">#</span><span class="bash">ods_tmp移动数据到ods((时间目录下的part* d=时间))  </span></span><br><span class="line">hdfs dfs -mv /item/offline-dw/ods_tmp/access/$time/part* /item/offline-dw/ods/access/d=$time</span><br><span class="line"><span class="meta">#</span><span class="bash">删除ods_tmp(带时间)</span></span><br><span class="line">hdfs dfs -rm -r -f /item/offline-dw/ods_tmp/access/$time</span><br><span class="line"><span class="meta">#</span><span class="bash">hive命令行刷新分区 (判断表是否存在，partition(d=<span class="string">"<span class="variable">$time</span>"</span>))</span></span><br><span class="line">hive -e "alter table offline_dw.ods_access add if not exists partition(d=$time)"</span><br></pre></td></tr></table></figure></li></ol><h2 id="数据分析案例-DWS-ADS层"><a href="#数据分析案例-DWS-ADS层" class="headerlink" title="数据分析案例(DWS/ADS层)"></a>数据分析案例(DWS/ADS层)</h2><p>这层的数据可以作为DWS层，我们要什么数据就建什么表(分区)</p><ul><li><p>统计国家流量</p><p>思路: 建分区表，字段分别是国家和流量，最后分组查询，插入表中</p><ol><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> dws_country_traffic(</span><br><span class="line">country <span class="keyword">String</span>,</span><br><span class="line">traffic <span class="built_in">BigInt</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span>(d <span class="keyword">String</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询数据插入表中</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> offline_dw.dws_country_traffic <span class="keyword">partition</span>(d=<span class="string">'$time'</span>) </span><br><span class="line"><span class="keyword">select</span> country,<span class="keyword">sum</span>(responseSize) <span class="keyword">from</span> offline_dw.ods_access <span class="keyword">where</span> d = <span class="string">'$time'</span><span class="keyword">group</span> <span class="keyword">by</span> country;</span><br></pre></td></tr></table></figure></li></ol></li><li><p>统计域名流量</p><p>思路: 建分区表，字段分别是域名和流量，最后分组查询，插入表中</p><ol><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> dws_domain_traffic(</span><br><span class="line"><span class="keyword">domain</span> <span class="keyword">String</span>,</span><br><span class="line">traffic <span class="built_in">BigInt</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span>(d <span class="keyword">String</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询数据插入表中</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> offline_dw.dws_domain_traffic <span class="keyword">partition</span>(d=<span class="string">'$time'</span>) </span><br><span class="line"><span class="keyword">select</span> <span class="keyword">domain</span>,<span class="keyword">sum</span>(responseSize) <span class="keyword">from</span> offline_dw.ods_access <span class="keyword">where</span> d=<span class="string">'$time'</span><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">domain</span>;</span><br></pre></td></tr></table></figure></li></ol></li><li><p>写成脚本方便调度</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line"></span><br><span class="line">if [ $# -eq 1 ];then</span><br><span class="line">        time=$1</span><br><span class="line">else</span><br><span class="line">        time=`date --date '1 days ago' +%Y%m%d`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive -e "insert overwrite table offline_dw.dws_country_traffic partition(d='$time') </span><br><span class="line">select country,sum(responseSize) from offline_dw.ods_access where d='$time' group by country;"</span><br><span class="line"></span><br><span class="line">hive -e "insert overwrite table offline_dw.dws_domain_traffic partition(d='$time') </span><br><span class="line">select domain,sum(responseSize)  from offline_dw.ods_access where d='$time' group by domain;"</span><br></pre></td></tr></table></figure></li></ul><h2 id="使用crontab调度脚本-临时"><a href="#使用crontab调度脚本-临时" class="headerlink" title="使用crontab调度脚本(临时)"></a>使用crontab调度脚本(临时)</h2><p>每天凌晨一点执行etl.sh，每天凌晨两点执行stats.sh</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">crontab -e</span><br><span class="line"></span><br><span class="line">0 1 * * * /home/hadoop/offline_dw/script/etl.sh</span><br><span class="line">0 2 * * * /home/hadoop/offline_dw/script/stats.sh</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> OfflineDW </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OfflineDW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>项目数据的ETL</title>
      <link href="/2020/02/17/offlinedw/4.%E9%A1%B9%E7%9B%AE%E6%95%B0%E6%8D%AE%E7%9A%84ETL/"/>
      <url>/2020/02/17/offlinedw/4.%E9%A1%B9%E7%9B%AE%E6%95%B0%E6%8D%AE%E7%9A%84ETL/</url>
      
        <content type="html"><![CDATA[<p>系统架构图:</p><p><img src="https://yerias.github.io/offlinedw/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E9%A1%B9%E7%9B%AE%E5%9B%BE_2.jpg" alt="数据仓库架构项目图"></p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li><p>为什么要进行ETL</p></li><li><p>什么是ETL</p></li><li><p>ETL该怎么做</p></li><li><p>ETL在服务器上运行需要解决的问题</p></li></ol><h2 id="为什么要进行ETL"><a href="#为什么要进行ETL" class="headerlink" title="为什么要进行ETL"></a>为什么要进行ETL</h2><p>在上一步我们使用Flume采集数据到HDFS，从系统架构图来看现在要进行数据的ETL操作，ETL进程对数据进行规范化、验证、清洗，并最终装载进入数据仓库</p><h2 id="什么是ETL"><a href="#什么是ETL" class="headerlink" title="什么是ETL"></a>什么是ETL</h2><p>ETL 即 Extract Transform Load的首字母 ==&gt; 抽取、转换、加载</p><h2 id="ETL该怎么做"><a href="#ETL该怎么做" class="headerlink" title="ETL该怎么做"></a>ETL该怎么做</h2><p>数据采集到HDFS上指定的目录下，通过MR写入数据，进行ETL操作，并写出到指定的目录下，ETL操作包括定义数据字段的序列化类，把时间解析出年月日，把URL解析为http、domain和path、对异常值进行处理(try/catch)，使用计数器。</p><p>需要注意:</p><ol><li><p>时间解析参考代码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//时间</span></span><br><span class="line">String time = split[<span class="number">0</span>];</span><br><span class="line">SimpleDateFormat format = <span class="keyword">new</span> SimpleDateFormat(<span class="string">"[dd/MM/yyyy:HH:mm:ss +0800]"</span>);</span><br><span class="line">Date date = format.parse(split[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">Calendar calendar = Calendar.getInstance();</span><br><span class="line">calendar.setTime(date);</span><br><span class="line"></span><br><span class="line"><span class="comment">//year</span></span><br><span class="line">String year =String.valueOf(calendar.get(Calendar.YEAR));</span><br><span class="line">access.setYear(year);</span><br><span class="line"></span><br><span class="line"><span class="comment">//month</span></span><br><span class="line"><span class="keyword">int</span>  month = calendar.get(Calendar.MONTH)+<span class="number">1</span>;</span><br><span class="line">access.setMont(month &lt; <span class="number">10</span> ? <span class="string">"0"</span>+month:String.valueOf(month))</span><br><span class="line"></span><br><span class="line"><span class="comment">//day</span></span><br><span class="line"><span class="keyword">int</span> day = calendar.get(Calendar.DAY_OF_MONTH);</span><br><span class="line">access.setDay(day &lt; <span class="number">10</span> ? <span class="string">"0"</span>+day:String.valueOf(day));</span><br></pre></td></tr></table></figure></li><li><p>异常值是舍去还是保留，这跟try/catch如何操作有关系，参考代码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//要数据,设默认值为0</span></span><br><span class="line"><span class="keyword">long</span> responseSize = <span class="number">0L</span>;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    responseSize = Long.parseLong(split[<span class="number">9</span>].trim());</span><br><span class="line">    access.setResponseSize(responseSize);</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">    access.setResponseSize(responseSize);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 不要数据，产生异常直接返回</span></span><br><span class="line">Long responseSize = Long.parseLong(split[<span class="number">9</span>].trim());</span><br><span class="line">access.setResponseSize(responseSize);</span><br></pre></td></tr></table></figure></li><li><p>计数器mapper中的参考代码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">context.getCounter(<span class="string">"ETL"</span>,<span class="string">"SUCCEED"</span>).increment(<span class="number">1</span>);</span><br></pre></td></tr></table></figure></li><li><p>计数器Driver中的参考代码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//通过迭代器获取mapper中的计数器</span></span><br><span class="line">CounterGroup group = job.getCounters().getGroup(<span class="string">"ETL"</span>);</span><br><span class="line">Iterator&lt;Counter&gt; iterator = group.iterator();</span><br><span class="line"><span class="keyword">while</span>(iterator.hasNext())&#123;</span><br><span class="line">Counter counter = iterator.next();</span><br><span class="line">     System.out.println(counter.getName() + <span class="string">"==&gt;"</span> + counter.getValue());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意: 这里可以通过jdbc将计数器结果根据批次写到mysql数据库中</p></li></ol><h2 id="ETL在服务器上运行需要解决的问题"><a href="#ETL在服务器上运行需要解决的问题" class="headerlink" title="ETL在服务器上运行需要解决的问题"></a>ETL在服务器上运行需要解决的问题</h2><p>在本地测试好代码后，上传Jar包到服务器上，跑HDFS上的数据</p><p>首先创建三个文件夹lib、data、script放ETL相关的文件，运行脚本的shell文件就在script目录下</p><p>由于我们把ETL打的瘦包，所以很多数据需要的依赖Jar包得不到，还有ip解析库的数据库也需要上传到本地文件下</p><p>思路是:</p><ol><li><p>把ip解析库放到项目的resources目录下</p></li><li><p>把需要的依赖上传到lib目录下</p></li><li><p>在<code>~/.bashrc</code>文件下导入LIBJARS路径用来指向lib目录下的依赖</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">export LIBJARS=/home/hadoop/lib/LIBJARS</span><br><span class="line">export LIBJARS=$LIBJARS/commons-lang3-<span class="number">3.4</span>.jar,$LIBJARS/qqwry.dat,$LIBJARS/mysql-connector-java-<span class="number">5.1</span><span class="number">.27</span>-bin.jar</span><br></pre></td></tr></table></figure></li><li><p>执行jar包命令写进脚本，执行脚本即可</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">time=<span class="number">20200217</span></span><br><span class="line">hadoop jar hadoop-client-<span class="number">1.0</span><span class="number">.0</span>.jar  com.tunan.ip.ipParseDriver  -libjars $LIBJARS  /item/offline-dw/raw/access/$time /item/offline-dw/tmp/access/$time/</span><br></pre></td></tr></table></figure><p><code>-libjars</code>用来指定外部依赖，<code>$LIBJARS</code>指向<code>~/.bashrc</code>文件中的路径</p><p><code>/item/offline-dw/raw/access/$time</code>是源数据，这个数据一般保存7天后即可删除</p><p><code>/item/offline-dw/tmp/access/$time</code>/是ETL后的数据</p><p><code>$LIBJARS/qqwry.dat</code>是ip解析库的路径</p></li><li><p>还可以把ip解析库在服务器上的路径写死在代码中，就不用手动指定ip解析库的路径了</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> OfflineDW </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OfflineDW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>项目数据的采集</title>
      <link href="/2020/02/16/offlinedw/3.%E9%A1%B9%E7%9B%AE%E6%95%B0%E6%8D%AE%E7%9A%84%E9%87%87%E9%9B%86/"/>
      <url>/2020/02/16/offlinedw/3.%E9%A1%B9%E7%9B%AE%E6%95%B0%E6%8D%AE%E7%9A%84%E9%87%87%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<p>系统架构图:</p><p><img src="https://yerias.github.io/offlinedw/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E9%A1%B9%E7%9B%AE%E5%9B%BE_1.jpg" alt="数据仓库架构项目图"></p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>Flume是用来做什么的</p><p>为什么要使用Flume</p><p>Flume具体怎么用</p><p>java客户端上传消息到Flume写到HDFS</p><p>解决Flume采集数据时生成大量小文件的问题</p><h2 id="为什么要使用Flume"><a href="#为什么要使用Flume" class="headerlink" title="为什么要使用Flume"></a>为什么要使用Flume</h2><p>在开源框架的选择中，因为Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. 所以我们选择了Flume作为数据的采集工具</p><h2 id="Flume是用来做什么的"><a href="#Flume是用来做什么的" class="headerlink" title="Flume是用来做什么的"></a>Flume是用来做什么的</h2><p>从系统架构图上来看，用户只要产生行为，那么日志就会在Nginx服务器中保存，所以我们现在要做的就是把数据从Nginx服务器中使用Flume采集到HDFS上</p><h2 id="Flume具体怎么用"><a href="#Flume具体怎么用" class="headerlink" title="Flume具体怎么用"></a>Flume具体怎么用</h2><h3 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h3><p>Flume就是一个针对日志数据进行采集和汇总的一个框架</p><p>Flume的进程叫做Agent，每个Agent中有Srouce、Channel、Sink</p><p>Flume从使用层面来讲就是写配置文件，其实就是配置我们的Agent，只要学会从官网查配置就行了</p><p><a href="http://flume.apache.org" target="_blank" rel="noopener">Flume官网</a></p><h3 id="Agent"><a href="#Agent" class="headerlink" title="Agent"></a>Agent</h3><p>Source中的常用方式有 avro、exec、spooling、taildir、kafka</p><p>Channel中的常用方式有 memory、kafka、file</p><p>Sink中的常用方式有 hdfs、logger、avro、kafka</p><h3 id="示例配置"><a href="#示例配置" class="headerlink" title="示例配置"></a>示例配置</h3><p>它描述了一个单节点Flume部署。该配置允许用户生成事件，然后将它们记录到控制台。以后写配置只需要在这个配置文件的基础上做出选择性的修改即可</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>有了这个配置文件，我们可以按如下方式启动Flume:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/example.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>输入端:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">telnet localhost 44444</span><br></pre></td></tr></table></figure><h3 id="TAILDIR"><a href="#TAILDIR" class="headerlink" title="TAILDIR"></a>TAILDIR</h3><p>taildir是Source端可以选择的一个类型，它可以同时支持目录和文件，并且支持offset，Flume挂了重启后可以接着上次消费的地方继续消费，下面提供一个配置taildir的文件</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = taildir</span><br><span class="line">a1.sources.r1.positionFile = /home/hadoop/taildir_position.json#这个目录的上一级目录不能存在</span><br><span class="line">a1.sources.r1.filegroups = f1 f2</span><br><span class="line">a1.sources.r1.filegroups.f1 = /home/hadoop/data/flume/example</span><br><span class="line">a1.sources.r1.headers.f1.headerKey1 = value1</span><br><span class="line">a1.sources.r1.filegroups.f2 = /home/hadoop/data/flume/.*log.*</span><br><span class="line">a1.sources.r1.headers.f2.headerKey1 = value2</span><br><span class="line">a1.sources.r1.headers.f2.headerKey2 = value2-<span class="number">2</span></span><br><span class="line">a1.sources.r1.fileHeader = <span class="keyword">true</span></span><br><span class="line">a1.sources.ri.maxBatchCount = <span class="number">1000</span></span><br></pre></td></tr></table></figure><p>其中<code>taildir_position.json</code>文件是用来记录文件读取offset的位置，方便下次继续从offset位置读取</p><p>注意<code>taildir_position.json</code>文件不能存在上级目录，不然会报错</p><h2 id="java客户端上传消息到Flume写到HDFS"><a href="#java客户端上传消息到Flume写到HDFS" class="headerlink" title="java客户端上传消息到Flume写到HDFS"></a>java客户端上传消息到Flume写到HDFS</h2><p>参考博客: <a href="https://blog.csdn.net/lbship/article/details/85336555" target="_blank" rel="noopener">https://blog.csdn.net/lbship/article/details/85336555</a></p><ol><li><p>在java客户端编写代码生成logger数据</p><p>添加依赖</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">   &lt;groupId&gt;org.apache.flume.flume-ng-clients&lt;/groupId&gt;</span><br><span class="line">   &lt;artifactId&gt;flume-ng-log4jappender&lt;/artifactId&gt;</span><br><span class="line">   &lt;version&gt;1.6.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.slf4j&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.7.21&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flume.flume-ng-clients&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flume-ng-log4jappender&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.6.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>生产日志</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> Logger logger = Logger.getLogger(LoggerGenerator<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(i&lt;<span class="number">99</span>)&#123;</span><br><span class="line">        Thread.sleep(<span class="number">500</span>);</span><br><span class="line">        logger.info(<span class="string">"now is : "</span>+i);</span><br><span class="line">        i++;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>在log4j.properties文件中添加代码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">log4j.rootCategory=info,console,flume  <span class="comment">//rootCategory需要指定flume</span></span><br><span class="line">    </span><br><span class="line">log4j.appender.flume=org.apache.flume.clients.log4jappender.Log4jAppender</span><br><span class="line">log4j.appender.flume.Hostname=<span class="number">121.196</span><span class="number">.220</span><span class="number">.143</span></span><br><span class="line">log4j.appender.flume.Port=<span class="number">41414</span></span><br><span class="line">log4j.appender.flume.UnsafeMode=<span class="keyword">true</span></span><br></pre></td></tr></table></figure></li><li><p>在Hostname服务端接收log4j.properties文件中指定的端口即可，注意Source类型需要使用avro(序列化)</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">a1.sources.r1.port = <span class="number">41414</span></span><br></pre></td></tr></table></figure></li><li><p>写出到HDFS，这里需要注意解决时间戳和乱码问题</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = /flume/%y-%m-%d/%H%M/%S#时间需要根据实际情况修改</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = events-</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp=true#解决时间戳问题</span><br><span class="line">a1.sinks.k1.hdfs.fileType=DataStream #解决乱码</span><br></pre></td></tr></table></figure><p>使用<code>useLocalTimeStamp=true</code>参数解决时间戳异常，使用<code>fileType=DataStream</code>解决文件内容乱码问题</p></li></ol><h2 id="解决Flume采集数据时生成大量小文件的问题"><a href="#解决Flume采集数据时生成大量小文件的问题" class="headerlink" title="解决Flume采集数据时生成大量小文件的问题"></a>解决Flume采集数据时生成大量小文件的问题</h2><p>在使用Flume采集数据时，由于默认参数的影响会生产大量小文件，我们先看默认参数</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hdfs.rollInterval<span class="number">30</span>滚动当前文件之前要等待的秒数</span><br><span class="line">hdfs.rollSize<span class="number">1024</span>触发滚动当前文件的大小，单位bytes(B)</span><br><span class="line">hdfs.rollCount<span class="number">10</span>触发滚动当前文件的events数量</span><br></pre></td></tr></table></figure><p>我们看到默认生成文件有三个条件，每30秒、每1M、每10个events，这样的配置会生成大量的小文件，所以我们要对这三个文件进行修改</p><p>最终生成的文件必须综合时间、文件大小、event数量来决定，时间太长或者文件太大都不利于最终生成的文件。该时间还需要配合<code>hdfs.path</code>参数指定的生成文件时间。</p><p>注意<code>sink.type</code>如果是<code>memory</code>模式，注意文件的大小，防止内存不足，太大可以设置<code>sink.type = file</code></p>]]></content>
      
      
      <categories>
          
          <category> OfflineDW </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OfflineDW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>项目开发的流程</title>
      <link href="/2020/02/15/offlinedw/2.%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E6%B5%81%E7%A8%8B/"/>
      <url>/2020/02/15/offlinedw/2.%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>为什么是离线数据仓库</li><li>采集什么日志</li><li>技术实现流程</li></ol><h2 id="为什么是离线数据仓库"><a href="#为什么是离线数据仓库" class="headerlink" title="为什么是离线数据仓库"></a>为什么是离线数据仓库</h2><h3 id="什么是数据仓库"><a href="#什么是数据仓库" class="headerlink" title="什么是数据仓库"></a>什么是数据仓库</h3><p>将多个数据源的数据经过ETL之后，按照一定的主题继承，提供 <code>决策支持</code> 和 <code>联机分析应用</code> 的结构化数据环境</p><h3 id="为什么要建数据仓库"><a href="#为什么要建数据仓库" class="headerlink" title="为什么要建数据仓库"></a>为什么要建数据仓库</h3><p>摆脱多种不同数据源、异构数据库、不同数据格式等等带来的问题</p><h2 id="采集用户行为日志"><a href="#采集用户行为日志" class="headerlink" title="采集用户行为日志"></a>采集用户行为日志</h2><p>既然要建数据仓库，那么第一步需要考虑的是我们的数据从哪里来？来的什么数据？这些数据是做什么的？</p><h3 id="数据是从哪里来的？"><a href="#数据是从哪里来的？" class="headerlink" title="数据是从哪里来的？"></a>数据是从哪里来的？</h3><p><code>数据</code>通过<code>采集团队</code>从<code>ngnix</code>服务器(请求携带的数据经过nginx服务器)、<code>埋点日志</code>(用户行为过程及结果的记录)和<code>SDK</code>(软件开发工具包)通过<code>flume</code>采集产生的数据到HDFS上</p><h3 id="来的是什么数据？"><a href="#来的是什么数据？" class="headerlink" title="来的是什么数据？"></a>来的是什么数据？</h3><p>采集到的是用户的行为数据，包括日志和业务数据，只要是用户访问、搜索、点击、收藏都会产生数据通过flume采集到了HDFS上</p><h3 id="数据可以用来做什么的？"><a href="#数据可以用来做什么的？" class="headerlink" title="数据可以用来做什么的？"></a>数据可以用来做什么的？</h3><p>数据运用在数据仓库中会进行一系列的ETL、调度、建模，最终用来可视化分析。</p><h2 id="技术实现流程"><a href="#技术实现流程" class="headerlink" title="技术实现流程"></a>技术实现流程</h2><h3 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h3><p>从输入==&gt;到输出</p><p>可以使用的工具:</p><ol><li>SDK</li><li>Flume(推介)</li><li>Sqoop</li><li>DataX</li></ol><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>数据预处理也就是对数据进行ETL/清洗操作</p><ol><li>格式处理：时间、IP、URL</li><li>数据拆分：1 col ==&gt; n cols  (URL、UA)</li><li>数据补充：1 col ==&gt; n cols</li></ol><h3 id="数据入库"><a href="#数据入库" class="headerlink" title="数据入库"></a>数据入库</h3><p>数据清洗后的数据导入到你HIVE中的库/表中(大宽表/N多列)</p><ol><li>分析的维度：day or hour</li><li>表：分区外部表</li></ol><h3 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h3><p>根据不同业务的执行SQL并且通过SQOOP存到RDBMS/NoSQL的表中</p><h3 id="数据展示"><a href="#数据展示" class="headerlink" title="数据展示"></a>数据展示</h3><ol><li>WEB ==&gt; RDBMS</li><li>Echarts</li><li>d3</li><li>HUE</li><li>Zeppelin</li></ol><h3 id="项目架构"><a href="#项目架构" class="headerlink" title="项目架构"></a>项目架构</h3><p><img src="https://yerias.github.io/offlinedw/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E9%A1%B9%E7%9B%AE%E5%9B%BE.jpg" alt="数据仓库架构项目图"></p><h3 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h3><ol><li><p>评估你这个业务线需要多少资源，数据量的计算: 一条数据的大小==&gt;*用户数*每个人一天产生的数据量==&gt;*副本数*天数(365)==&gt;*每年30%的上升空间</p></li><li><p>每个节点磁盘多少？内存多少？物理核多少？通过计算得出需要的节点数</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">core：<span class="number">32</span>/<span class="number">64</span> 决定了应用程序的快慢</span><br><span class="line">memory：<span class="number">256</span>/<span class="number">512</span> 决定了应用程序的生死</span><br><span class="line">disk：<span class="number">10</span>T*数量</span><br></pre></td></tr></table></figure></li></ol><h2 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h2><p>对数据进行ETL操作时，N个业务：至少是3*N个SQL(3层)，并且尽可能把一些麻烦的/繁琐的/join等SQL操作提前进行</p>]]></content>
      
      
      <categories>
          
          <category> OfflineDW </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OfflineDW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>项目开发的准备</title>
      <link href="/2020/02/14/offlinedw/1.%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E5%87%86%E5%A4%87/"/>
      <url>/2020/02/14/offlinedw/1.%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E5%87%86%E5%A4%87/</url>
      
        <content type="html"><![CDATA[<hr><p>涉及到具体的文档，这里只描述流程</p><hr><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li><p>项目调研</p></li><li><p>需求分析</p></li><li><p>方案设计</p></li></ol><h2 id="项目调研"><a href="#项目调研" class="headerlink" title="项目调研"></a>项目调研</h2><p>是什么行业? </p><p>关于什么业务？</p><p>调研人员(资深的产品经理/业务分析人员)</p><h2 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h2><p>要做什么?</p><p>做成啥样?</p><ol><li>需求(表层的需求、隐藏的需求、售前团队的需求)</li><li>产出(需求规格说明书、进度规划：甘特图)</li><li>人员(项目经理、产品、leader)</li></ol><h2 id="方案设计"><a href="#方案设计" class="headerlink" title="方案设计"></a>方案设计</h2><h3 id="概要设计"><a href="#概要设计" class="headerlink" title="概要设计"></a>概要设计</h3><ol><li>技术架构(框架的调研、”糙”点的测试报告)</li><li>模块</li><li>功能点</li><li>和甲方确认过程</li></ol><h3 id="详细设计"><a href="#详细设计" class="headerlink" title="详细设计"></a>详细设计</h3><p>详细设计是最复杂的、篇幅最大的，针对具体功能的实现</p><p>基本要求(类、方法(入参、出参)、UML、图表)</p><p>系统要求/非功能需求</p><ol><li>扩展性</li><li>容错性</li><li>是否支持定制化：Azkaban</li><li>监控/告警(发生后)/预警(发生前)</li></ol><h3 id="功能开发"><a href="#功能开发" class="headerlink" title="功能开发"></a>功能开发</h3><p>码农对着文档把方法实现了而已，CICD(持续集成(CI)、持续交付(CD))</p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><ol><li>测试覆盖率 95%</li><li>场景测试(案例)</li><li>功能测试</li><li>联调测试(一个完整流程)</li><li>性能/压力测试</li><li>用户测试</li></ol><h3 id="部署上线"><a href="#部署上线" class="headerlink" title="部署上线"></a>部署上线</h3><ol><li>试运行(“双活”系统做DIFF)</li><li>正式上线</li></ol><h3 id="后期维护"><a href="#后期维护" class="headerlink" title="后期维护"></a>后期维护</h3><p>后期维护的费用并不低，包括开发新功能，修复老bug</p>]]></content>
      
      
      <categories>
          
          <category> OfflineDW </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
            <tag> OfflineDW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkSQL Catalyst详解(转载)</title>
      <link href="/2019/11/05/spark/35/"/>
      <url>/2019/11/05/spark/35/</url>
      
        <content type="html"><![CDATA[<p>最近想来，大数据相关技术与传统型数据库技术很多都是相互融合、互相借鉴的。传统型数据库强势在于其久经考验的SQL优化器经验，弱势在于分布式领域的高可用性、容错性、扩展性等，假以时日，让其经过一定的改造，比如引入Paxos、raft等，强化自己在分布式领域的能力，相信一定会在大数据系统中占有一席之地。相反，大数据相关技术优势在于其天生的扩展性、可用性、容错性等，但其SQL优化器经验却基本全部来自于传统型数据库，当然，针对列式存储大数据SQL优化器会有一定的优化策略。</p><p>本文主要介绍SparkSQL的优化器系统Catalyst，上文讲到其设计思路基本都来自于传统型数据库，而且和大多数当前的大数据SQL处理引擎设计基本相同（Impala、Presto、Hive（Calcite）等），因此通过本文的学习也可以基本了解所有其他SQL处理引擎的工作原理。</p><p>SQL优化器核心执行策略主要分为两个大的方向：<strong>基于规则优化（RBO）</strong>以及<strong>基于代价优化(CBO)</strong>，<code>基于规则优化是一种经验式、启发式地优化思路，更多地依靠前辈总结出来的优化规则，简单易行且能够覆盖到大部分优化逻辑，但是对于核心优化算子Join却显得有点力不从心</code>。举个简单的例子，两个表执行Join到底应该使用BroadcastHashJoin还是SortMergeJoin？当前SparkSQL的方式是通过手工设定参数来确定，如果一个表的数据量小于这个值就使用BroadcastHashJoin，但是这种方案显得很不优雅，很不灵活。<code>基于代价优化就是为了解决这类问题，它会针对每个Join评估当前两张表使用每种Join策略的代价，根据代价估算确定一种代价最小的方案。</code></p><p>本文将会重点介绍基于规则的优化策略，后续文章会详细介绍基于代价的优化策略。下图中红色框框部分将是本文的介绍重点：</p><p><img src="https://yerias.github.io/spark_img/catalyst1.png" alt="catalyst1"></p><h3 id="预备知识－Tree-amp-Rule"><a href="#预备知识－Tree-amp-Rule" class="headerlink" title="预备知识－Tree&amp;Rule"></a>预备知识－Tree&amp;Rule</h3><p>在介绍SQL优化器工作原理之前，有必要首先介绍两个重要的数据结构：Tree和Rule。相信无论对SQL优化器有无了解，都肯定知道SQL语法树这个概念，不错，SQL语法树就是SQL语句通过编译器之后会被解析成一棵树状结构。这棵树会包含很多节点对象，每个节点都拥有特定的数据类型，同时会有0个或多个孩子节点（节点对象在代码中定义为TreeNode对象），下图是个简单的示例：</p><p><img src="https://yerias.github.io/spark_img/catalyst2.png" alt="catalyst2"></p><p>如上图所示，箭头左边表达式有3种数据类型（<strong>Literal</strong>表示常量、<strong>Attribute</strong>表示变量、<strong>Add</strong>表示动作），表示x+(1+2)。映射到右边树状结构后，每一种数据类型就会变成一个节点。另外，Tree还有一个非常重要的特性，可以通过一定的规则进行等价变换，如下图：</p><p><img src="https://yerias.github.io/spark_img/catalyst3.png" alt="catalyst3"></p><p>上图定义了一个<strong>等价变换规则(Rule)</strong>：<code>两个Integer类型的常量相加可以等价转换为一个Integer常量</code>，这个规则其实很简单，对于上文中提到的表达式x+(1+2)来说就可以转变为x+3。对于程序来讲，<code>如何找到两个Integer常量呢？其实就是简单的二叉树遍历算法，每遍历到一个节点，就模式匹配当前节点为Add、左右子节点是Integer常量的结构，定位到之后将此三个节点替换为一个Literal类型的节点。</code></p><p>上面用一个最简单的示例来说明等价变换规则以及如何将规则应用于语法树。在任何一个SQL优化器中，通常会定义大量的Rule（后面会讲到），SQL优化器会遍历语法树中每个节点，针对遍历到的节点模式匹配所有给定规则（Rule），如果有匹配成功的，就进行相应转换，如果所有规则都匹配失败，就继续遍历下一个节点。</p><h3 id="Catalyst工作流程"><a href="#Catalyst工作流程" class="headerlink" title="Catalyst工作流程"></a>Catalyst工作流程</h3><p>任何一个优化器工作原理都大同小异：</p><ol><li><p>SQL语句首先通过Parser(解析器)模块被解析为语法树，此棵树称为Unresolved Logical Plan；</p></li><li><p>Unresolved Logical Plan通过Analyzer(分析器)模块借助于数据元数据解析为Logical Plan；</p></li><li><p>通过各种基于规则的优化策略进行深入优化，得到Optimized Logical Plan；</p></li><li><p>优化后的逻辑执行计划依然是逻辑的，并不能被Spark系统理解，此时需要将此逻辑执行计划转换为Physical Plan；</p></li></ol><p>为了更好的对整个过程进行理解，下文通过一个简单示例进行解释。</p><h4 id="Parser"><a href="#Parser" class="headerlink" title="Parser"></a>Parser</h4><p><code>Parser简单来说是将SQL字符串切分成一个一个Token，再根据一定语义规则解析为一棵语法树。</code>Parser模块目前基本都使用第三方类库ANTLR进行实现，比如Hive、 Presto、SparkSQL等。下图是一个示例性的SQL语句（有两张表，其中people表主要存储用户基本信息，score表存储用户的各种成绩），通过Parser解析后的AST语法树如右图所示：</p><p><img src="https://yerias.github.io/spark_img/catalyst4.png" alt="catalyst4"></p><h4 id="Analyzer"><a href="#Analyzer" class="headerlink" title="Analyzer"></a>Analyzer</h4><p>通过解析后的逻辑执行计划基本有了骨架，但是系统并不知道score、sum这些都是些什么鬼，<strong>此时需要基本的元数据信息来表达这些词素</strong>，最重要的元数据信息主要包括两部分：表的Scheme和基本函数信息，表的scheme主要包括表的基本定义（列名、数据类型）、表的数据格式（Json、Text）、表的物理位置等，基本函数信息主要指类信息。</p><p><strong>Analyzer会再次遍历整个语法树，对树上的每个节点进行数据类型绑定以及函数绑定</strong>，比如people词素会根据元数据表信息解析为包含age、id以及name三列的表，people.age会被解析为数据类型为int的变量，sum会被解析为特定的聚合函数，如下图所示：</p><p><img src="https://yerias.github.io/spark_img/catalyst5.png" alt="catalyst5"></p><p>SparkSQL中Analyzer定义了各种解析规则，有兴趣深入了解的童鞋可以查看Analyzer类，其中定义了基本的解析规则，如下：</p><p><img src="https://yerias.github.io/spark_img/catalyst6.png" alt="catalyst6"></p><h4 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h4><p>优化器是整个Catalyst的核心，上文提到优化器分为基于规则优化和基于代价优化两种，当前SparkSQL 2.1依然没有很好的支持基于代价优化（下文细讲），此处只介绍基于规则的优化策略，基于规则的优化策略实际上就是对语法树进行一次遍历，模式匹配能够满足特定规则的节点，再进行相应的等价转换。因此，<strong>基于规则优化说到底就是一棵树等价地转换为另一棵树</strong>。SQL中经典的优化规则有很多，下文结合示例介绍三种比较常见的规则：<strong>谓词下推（Predicate Pushdown）</strong>、<strong>常量累加（Constant Folding）</strong>和<strong>列值裁剪（Column Pruning）</strong>。</p><p><img src="https://yerias.github.io/spark_img/catalyst7.png" alt="catalyst7"></p><p>上图左边是经过Analyzer解析后的语法树，语法树中两个表先做join，之后再使用age&gt;10对结果进行过滤。大家知道join算子通常是一个非常耗时的算子，耗时多少一般取决于参与join的两个表的大小，如果能够减少参与join两表的大小，就可以大大降低join算子所需时间。<strong>谓词下推就是这样一种功能，它会将过滤操作下推到join之前进行，上图中过滤条件age&gt;0以及id!=null两个条件就分别下推到了join之前。</strong>这样，系统在扫描数据的时候就对数据进行了过滤，参与join的数据量将会得到显著的减少，join耗时必然也会降低。</p><p><img src="https://yerias.github.io/spark_img/catalyst8.png" alt="catalyst8"></p><p><strong>常量累加其实很简单，就是上文中提到的规则  x+(1+2)  -&gt; x+3</strong>，虽然是一个很小的改动，但是意义巨大。示例如果没有进行优化的话，每一条结果都需要执行一次100+80的操作，然后再与变量math_score以及english_score相加，而优化后就不需要再执行100+80操作。</p><p><img src="https://yerias.github.io/spark_img/catalyst9.png" alt="catalyst9"></p><p><strong>列值裁剪</strong>是另一个经典的规则，示例中对于people表来说，并不需要扫描它的所有列值，而只需要列值id，所以在扫描people之后需要将其他列进行裁剪，只留下列id。这个优化一方面大幅度减少了网络、内存数据量消耗，另一方面对于列存数据库（Parquet）来说大大提高了扫描效率。</p><p>除此之外，Catalyst还定义了很多其他优化规则，有兴趣深入了解的童鞋可以查看Optimizer类，下图简单的截取一部分规则：</p><p><img src="https://yerias.github.io/spark_img/catalyst10.png" alt="catalyst10"></p><p>至此，逻辑执行计划已经得到了比较完善的优化，然而，逻辑执行计划依然没办法真正执行，他们只是逻辑上可行，实际上Spark并不知道如何去执行这个东西。比如Join只是一个抽象概念，代表两个表根据相同的id进行合并，然而具体怎么实现这个合并，逻辑执行计划并没有说明。</p><p><img src="https://yerias.github.io/spark_img/catalyst11.png" alt="catalyst11"></p><p>此时就需要将逻辑执行计划转换为物理执行计划，将逻辑上可行的执行计划变为Spark可以真正执行的计划。比如Join算子，Spark根据不同场景为该算子制定了不同的算法策略，有<code>BroadcastHashJoin</code>、<code>ShuffleHashJoin</code>以及<code>SortMergeJoin</code>等（可以将Join理解为一个接口，BroadcastHashJoin是其中一个具体实现），<strong>物理执行计划实际上就是在这些具体实现中挑选一个耗时最小的算法实现</strong>，这个过程涉及到基于代价优化策略，后续文章细讲。</p><h3 id="SparkSQL执行计划"><a href="#SparkSQL执行计划" class="headerlink" title="SparkSQL执行计划"></a>SparkSQL执行计划</h3><p>至此，笔者通过一个简单的示例完整的介绍了Catalyst的整个工作流程，包括Parser阶段、Analyzer阶段、Optimize阶段以及Physical Planning阶段。有同学可能会比较感兴趣Spark环境下如何查看一条具体的SQL的整个过程，在此介绍两种方法：</p><ol><li><p>使用queryExecution方法查看逻辑执行计划，使用explain方法查看物理执行计划，分别如下所示：</p><p><img src="https://yerias.github.io/spark_img/catalyst12.png" alt="catalyst12"></p><p><img src="https://yerias.github.io/spark_img/catalyst13.png" alt="catalyst13"></p></li><li><p>使用Spark WebUI进行查看，如下图所示：</p><p><img src="https://yerias.github.io/spark_img/catalyst14.png" alt="catalyst14"></p></li></ol><p><strong>参考文章：</strong></p><ol><li>Deep Dive into  Spark SQL’s Catalyst Optimizer：<a href="https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html" target="_blank" rel="noopener">https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html</a></li><li>Spark SQL: Relational Data Processing in Spark：<a href="http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf" target="_blank" rel="noopener">http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf</a></li></ol><p><strong>转载自：</strong></p><ol><li><a href="http://hbasefly.com/2017/03/01/sparksql-catalyst/" target="_blank" rel="noopener">http://hbasefly.com/2017/03/01/sparksql-catalyst/</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从源码的角度解析ByKey类算子</title>
      <link href="/2019/11/04/spark/34/"/>
      <url>/2019/11/04/spark/34/</url>
      
        <content type="html"><![CDATA[<h3 id="reduceByKey和groupByKey的区别"><a href="#reduceByKey和groupByKey的区别" class="headerlink" title="reduceByKey和groupByKey的区别"></a>reduceByKey和groupByKey的区别</h3><p>我们都知道reduceByKey和groupByKey的区别在于</p><ol><li>reduceByKey用于对每个key对应的多个value进行merge操作，并且在map端进行了预聚合操作。</li><li>groupByKey也是对每个key进行操作，但只将key对应的value组成一个集合，没有预聚合的操作。</li></ol><p>那么它们的源码使怎样的呢?</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"># reduceByKey源码</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(partitioner: <span class="type">Partitioner</span>, func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">    combineByKeyWithClassTag[<span class="type">V</span>]((v: <span class="type">V</span>) =&gt; v, func, func, partitioner)</span><br><span class="line">&#125;</span><br><span class="line"># groupByKey源码</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])] = self.withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> createCombiner = (v: <span class="type">V</span>) =&gt; <span class="type">CompactBuffer</span>(v)</span><br><span class="line">    <span class="keyword">val</span> mergeValue = (buf: <span class="type">CompactBuffer</span>[<span class="type">V</span>], v: <span class="type">V</span>) =&gt; buf += v</span><br><span class="line">    <span class="keyword">val</span> mergeCombiners = (c1: <span class="type">CompactBuffer</span>[<span class="type">V</span>], c2: <span class="type">CompactBuffer</span>[<span class="type">V</span>]) =&gt; c1 ++= c2</span><br><span class="line">    <span class="keyword">val</span> bufs = combineByKeyWithClassTag[<span class="type">CompactBuffer</span>[<span class="type">V</span>]](</span><br><span class="line">        createCombiner, mergeValue, mergeCombiners, partitioner, mapSideCombine = <span class="literal">false</span>)</span><br><span class="line">    bufs.asInstanceOf[<span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>combineByKeyWithClassTag</code>算子是大多数ByKey类算子的最底层实现，我们看看源码。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">combineByKeyWithClassTag</span></span>[<span class="type">C</span>](</span><br><span class="line">     createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,<span class="comment">//聚合值的类型</span></span><br><span class="line">     mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,<span class="comment">//局部聚合</span></span><br><span class="line">     mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,<span class="comment">//全局集合</span></span><br><span class="line">     partitioner: <span class="type">Partitioner</span>,<span class="comment">//分区数</span></span><br><span class="line">     mapSideCombine: <span class="type">Boolean</span> = <span class="literal">true</span>,<span class="comment">//默认开启map端预聚合</span></span><br><span class="line">     serializer: <span class="type">Serializer</span> = <span class="literal">null</span>)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>从源码中可以看到reduceByKey和groupByKey都调用了combineByKeyWithClassTag算子，区别在于</p><ol><li>reduceByKey中传入combineByKeyWithClassTag的前三个参数做了计算，并且没有修改mapSideCombine参数</li><li>groupByKey中传入combineByKeyWithClassTag的前三个参数没有做计算(集合)，并且修改mapSideCombine的参数为false</li></ol><p>此上两点决定了reduceByKey做了map端的预聚合而groupByKey没做，reduceByKey做了计算返回结果而groupByKey没有做计算返回集合。</p><h3 id="combineByKeyWithClassTag做计算"><a href="#combineByKeyWithClassTag做计算" class="headerlink" title="combineByKeyWithClassTag做计算"></a>combineByKeyWithClassTag做计算</h3><p>下面我们通过直接使用最底层的combineByKeyWithClassTag实现reduceByKey的功能来了解combineByKeyWithTag的计算原理。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"># combineByKey源码</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKey</span></span>[<span class="type">C</span>](</span><br><span class="line">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = self.withScope &#123;</span><br><span class="line">    combineByKeyWithClassTag(createCombiner, mergeValue, mergeCombiners)(<span class="literal">null</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于combineByKey的底层调用的combineByKeyWithClassTag，我们从combineByKey开始</p><p><strong>案例1求和</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>, <span class="number">3</span>), (<span class="number">1</span>, <span class="number">4</span>), (<span class="number">1</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">3</span>), (<span class="number">3</span>, <span class="number">6</span>), (<span class="number">3</span>, <span class="number">8</span>)), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 求和</span></span><br><span class="line"><span class="keyword">val</span> result = rdd.combineByKey(</span><br><span class="line">    x =&gt; x,<span class="comment">// 初始化类型</span></span><br><span class="line">    (a: <span class="type">Int</span>, b: <span class="type">Int</span>) =&gt; a + b,<span class="comment">// 局部聚合</span></span><br><span class="line">    (x: <span class="type">Int</span>, y: <span class="type">Int</span>) =&gt; x + y<span class="comment">// 全局聚合</span></span><br><span class="line">).collect()</span><br><span class="line"></span><br><span class="line">println(result.mkString) <span class="comment">// (3,14)(1,9)(2,3)</span></span><br></pre></td></tr></table></figure><p><strong>案例2求平均值</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 求平均值</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="string">"a"</span>, <span class="number">88</span>), (<span class="string">"b"</span>, <span class="number">95</span>), (<span class="string">"a"</span>, <span class="number">91</span>), (<span class="string">"b"</span>, <span class="number">93</span>), (<span class="string">"a"</span>, <span class="number">95</span>), (<span class="string">"b"</span>, <span class="number">98</span>)), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> result = rdd.combineByKey(</span><br><span class="line">    (_, <span class="number">1</span>),<span class="comment">// 初始化类型</span></span><br><span class="line">    (a: (<span class="type">Int</span>, <span class="type">Int</span>), b: <span class="type">Int</span>) =&gt; (a._1 + b, a._2 + <span class="number">1</span>),<span class="comment">// 局部聚合</span></span><br><span class="line">    (x: (<span class="type">Int</span>, <span class="type">Int</span>), y: (<span class="type">Int</span>, <span class="type">Int</span>)) =&gt; (x._1 + y._1, x._2 + y._2)<span class="comment">// 全局聚合</span></span><br><span class="line">).map&#123;</span><br><span class="line">    <span class="keyword">case</span> (key, value) =&gt; &#123;<span class="comment">// 匹配key,value</span></span><br><span class="line">        (key,(value._1/value._2).toDouble)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;.collect()</span><br><span class="line"></span><br><span class="line">println(result.mkString)<span class="comment">// (b,95.0)(a,91.0)</span></span><br></pre></td></tr></table></figure><p>使用combineByKeyWithClassTag算子重构</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>, <span class="number">3</span>), (<span class="number">1</span>, <span class="number">4</span>), (<span class="number">1</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">3</span>), (<span class="number">3</span>, <span class="number">6</span>), (<span class="number">3</span>, <span class="number">8</span>)), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 求和</span></span><br><span class="line"><span class="keyword">val</span> result = rdd.combineByKeyWithClassTag(</span><br><span class="line">    x =&gt; x,<span class="comment">// 初始化类型</span></span><br><span class="line">    (a: <span class="type">Int</span>, b: <span class="type">Int</span>) =&gt; a + b,<span class="comment">// 局部聚合</span></span><br><span class="line">    (x: <span class="type">Int</span>, y: <span class="type">Int</span>) =&gt; x + y<span class="comment">// 全局聚合</span></span><br><span class="line">).collect()</span><br><span class="line"></span><br><span class="line">println(result.mkString)<span class="comment">// (3,14)(1,9)(2,3)</span></span><br></pre></td></tr></table></figure><p>从代码可以发现，combineByKeyWithClassTag的计算原理无非就是如何实现初始化类型、局部聚合和全局聚合。</p><p>大多数ByKey类的算子都是如此。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Streaming中的state管理</title>
      <link href="/2019/11/03/spark/33/"/>
      <url>/2019/11/03/spark/33/</url>
      
        <content type="html"><![CDATA[<h3 id="有状态转化"><a href="#有状态转化" class="headerlink" title="有状态转化"></a>有状态转化</h3><p><code>依赖之前的批次数据或者中间结果来计算当前批次的数据</code>，不断的把当前的计算和历史时间切片的RDD进行累计。</p><p>Spark Streaming中状态管理函数包括<code>updateStateByKey</code>和<code>mapWithState</code>，都是用来统计全局key的状态的变化的。它们以DStream中的数据进行按key做reduce操作，然后对各个批次的数据进行累加，在有新的数据信息进入或更新时。能够让用户保持想要的不论任何状状。</p><h3 id="updateStateByKey"><a href="#updateStateByKey" class="headerlink" title="updateStateByKey"></a>updateStateByKey</h3><p><strong>概念</strong></p><p>updateStateByKey会统计全局的key的状态，不管又没有数据输入，它会在每一个批次间隔返回之前的key的状态。updateStateByKey会对已存在的key进行state的状态更新，同时还会对每个新出现的key执行相同的更新函数操作。如果通过更新函数对state更新后返回来为none()，此时刻key对应的state状态会被删除（state可以是任意类型的数据的结构）。</p><p><strong>适用场景</strong></p><p>updateStateByKey可以用来统计历史数据，每次输出所有的key值。例如统计不同时间段用户平均消费金额，消费次数，消费总额，网站的不同时间段的访问量等指标</p><p><strong>使用实例</strong></p><ol><li><p>首先会以DStream中的数据进行按key做reduce操作，然后再对各个批次的数据进行累加 。</p></li><li><p>updateStateBykey要求<strong>必须要设置checkpoint点</strong>。</p></li><li><p>updateStateByKey 方法中 updateFunc就要传入的参数。<code>Seq[V]</code>表示当前key对应的所有值，<code>Option[S]</code> 是当前key的历史状态，返回的是新的封装的数据。</p></li></ol><p><strong>代码</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UpdateStateV2</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> checkpoint = <span class="string">"./chk_v2"</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用这种方式拿到的始终是一个StreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> ssc  = <span class="type">StreamingContext</span>.getOrCreate(checkpoint, functionToCreateContext)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 创建StreamingContext</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">functionToCreateContext</span></span>(): <span class="type">StreamingContext</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf,<span class="type">Seconds</span>(<span class="number">50000</span>))   <span class="comment">// new context</span></span><br><span class="line"></span><br><span class="line">    dispose(ssc)</span><br><span class="line"></span><br><span class="line">    ssc.checkpoint(checkpoint)   <span class="comment">// set checkpoint directory</span></span><br><span class="line">    ssc</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 处理具题的业务逻辑</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">dispose</span></span>(ssc: <span class="type">StreamingContext</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"hadoop"</span>, <span class="number">9100</span>) <span class="comment">// create DStreams</span></span><br><span class="line"></span><br><span class="line">    lines</span><br><span class="line">      .flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map((_, <span class="number">1</span>))</span><br><span class="line">      .updateStateByKey(updateFunction)</span><br><span class="line">      .print()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 更新state</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">updateFunction</span></span>(newValues: <span class="type">Seq</span>[<span class="type">Int</span>], oldValues: <span class="type">Option</span>[<span class="type">Int</span>]): <span class="type">Option</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> curr = newValues.sum</span><br><span class="line">    <span class="keyword">val</span> old = oldValues.getOrElse(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> count = curr + old</span><br><span class="line">    <span class="type">Some</span>(count)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="mapWithState"><a href="#mapWithState" class="headerlink" title="mapWithState"></a><strong>mapWithState</strong></h3><p><strong>概念</strong></p><p>mapWithState也会统计全局的key的状态，但是如果没有数据输入，便不会返回之前的key的状态，只会返回batch中存在的key值统计，类似于增量的感觉。</p><p><strong>适用场景</strong></p><p>mapWithState可以用于一些实时性较高，延迟较少的一些场景，例如你在某宝上下单买了个东西，付款之后返回你账户里的余额信息。</p><p><strong>使用实例</strong></p><ol><li><p>如果有初始化的值得需要，可以使用initialState(RDD)来初始化key的值</p></li><li><p>还可以指定timeout函数，该函数的作用是，如果一个key超过timeout设定的时间没有更新值，那么这个key将会失效。这个控制需要在func中实现，必须使用state.isTimingOut()来判断失效的key值。如果在失效时间之后，这个key又有新的值了，则会重新计算。如果没有使用isTimingOut，则会报错。</p></li><li><p>checkpoint 不是必须的</p></li></ol><p><strong>代码</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MapWithState</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> checkpoint = <span class="string">"./chk_v3"</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 拿到 StreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="type">StreamingContext</span>.getOrCreate(checkpoint, functionToCreateContext)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">functionToCreateContext</span></span>(): <span class="type">StreamingContext</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建 StreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    ssc.checkpoint(checkpoint)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对记录做累加操作</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mappingFunc</span> </span>= (word: <span class="type">String</span>, one: <span class="type">Option</span>[<span class="type">Int</span>], state: <span class="type">State</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">      <span class="keyword">if</span>(state.isTimingOut())&#123;</span><br><span class="line">        println(<span class="string">"超时3秒没拿到数据"</span>)</span><br><span class="line">      &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">val</span> sum = one.getOrElse(<span class="number">0</span>) + state.getOption.getOrElse(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">val</span> output = (word, sum)</span><br><span class="line">        state.update(sum)</span><br><span class="line">        output</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 逻辑处理</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"hadoop"</span>, <span class="number">9100</span>)</span><br><span class="line">    lines</span><br><span class="line">      .flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map((_,<span class="number">1</span>))</span><br><span class="line">      .mapWithState(<span class="type">StateSpec</span>.function(mappingFunc)</span><br><span class="line">      .timeout(<span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">      ).print()</span><br><span class="line"></span><br><span class="line">    ssc</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="updateStateByKey和mapWithState的区别"><a href="#updateStateByKey和mapWithState的区别" class="headerlink" title="updateStateByKey和mapWithState的区别"></a>updateStateByKey和mapWithState的区别</h3><p>updateStateByKey可以在指定的批次间隔内返回之前的全部历史数据，包括新增的，改变的和没有改变的。由于updateStateByKey在使用的时候一定要做checkpoint，当数据量过大的时候，checkpoint会占据庞大的数据量，会影响性能，<strong>效率不高</strong>。</p><p>mapWithState只返回变化后的key的值，这样做的好处是，我们可以只是关心那些已经发生的变化的key，对于没有数据输入，则不会返回那些没有变化的key的数据。这样的话，即使数据量很大，checkpoint也不会像updateStateByKey那样，占用太多的存储，<strong>效率比较高</strong>（再生产环境中建议使用这个）。</p><h3 id="SparkStreaming之mapWithState"><a href="#SparkStreaming之mapWithState" class="headerlink" title="SparkStreaming之mapWithState"></a>SparkStreaming之mapWithState</h3><p>与updateStateByKey方法相比，使用mapWithState方法能够得到6倍的低延迟的同时维护的key状态的数量要多10倍，这一性能的提升和扩展性可以从基准测试结果得到验证，所有的结果全部在实践间隔为1秒的batch和相同大小的集群中生成。</p><p>下图比较的是mapWithState方法和updateStateByKey方法处理1秒的batch所消耗的平均时间。在本例子中，我们为同样数量的的key(0.25-1百万)保存状态，然后以统一的速率（30个更新/s）对其进行更新。可以看到mapWithState方法比updateStateByKey方法快了8倍，从而允许更低的端到端的延迟。</p><p><img src="https://yerias.github.io/spark_img/state.png" alt="state"></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark自定义Hbase外部数据源(兼容稀疏存储)</title>
      <link href="/2019/11/02/spark/32/"/>
      <url>/2019/11/02/spark/32/</url>
      
        <content type="html"><![CDATA[<p>这篇文章接着<a href="https://yerias.github.io/2020/03/15/spark/16/">从jdbc的角度解读外部数据源</a>和<a href="https://yerias.github.io/2020/03/17/spark/18/">自定义外部Text数据源</a></p><p>我想以简单的形式在Spark中读取Hbase数据，但是Spark并不支持读取Hbase数据后简单使用。思考能否自己实现这个读取的过程？</p><p>Hbase的读写API，结果数据往往需要处理后使用。我们是否可以将Hbase结果数据通过转化，直接转化为DataFrame的形式，方便我们使用。</p><p>总体思路可以分为几个步骤。</p><ol><li>继承RelationProvider实现createRelation</li><li>继承BaseRelation和TableScan实现StructType和buildScan，分别用来获取Schema和RDD[Row]</li><li>buildScan中需要思考如何去读取HBase的数据，如何把HBase的转成RDD[Row]</li><li>其他还有如何获取外面传进来的数据，以及怎么保存这些数据信息</li></ol><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>我们准备了一份稀疏数据，注意该案例目前只考虑到一个列族并且不考虑行键和时间戳</p><p><img src="https://yerias.github.io/spark_img/hbase_1.png" alt="hbase_1"></p><p>我们期望在经过Spark处理得到这么一份数据</p><p><img src="https://yerias.github.io/spark_img/hbase_2.png" alt="hbase_2"></p><p>如何自定义外部数据源的思以及源码我的其他博客已经介绍过了，我们无法就是干两件事，实现每个数据的StructType，实现StructType对应的RDD[Row]</p><h2 id="自定义外部数据源"><a href="#自定义外部数据源" class="headerlink" title="自定义外部数据源"></a>自定义外部数据源</h2><ol><li><p>老套路，创建DefaultSource实现RelationProvider</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultSource</span> <span class="keyword">extends</span> <span class="title">RelationProvider</span></span>&#123;</span><br><span class="line">    <span class="comment">// 创建Relation</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span></span>(sqlContext: <span class="type">SQLContext</span>, parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">BaseRelation</span> = &#123;</span><br><span class="line">        <span class="comment">// 做一些检查性的工作，然后调用HbaseRelation</span></span><br><span class="line">        <span class="type">HbaseRelation</span>(sqlContext,parameters)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>在创建HbaseRelation之前需要实现两个辅助类</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 存储外部传入的Schema字段名和字段类型</span></span><br><span class="line"><span class="comment"> * @param fieldName 字段名</span></span><br><span class="line"><span class="comment"> * @param fieldType 字段类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkSchema</span>(<span class="params">fieldName:<span class="type">String</span>,fieldType:<span class="type">String</span></span>)</span></span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HBaseDataSourceUtils</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 读取输入的字段类型做转换</span></span><br><span class="line"><span class="comment">     * @param sparkTableScheme 外部传入的Schema</span></span><br><span class="line"><span class="comment">     * @return</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extractSparkFields</span></span>(sparkTableScheme:<span class="type">String</span>):<span class="type">Array</span>[<span class="type">SparkSchema</span>] = &#123;</span><br><span class="line">        <span class="comment">// 除去左右括号以及按逗号切分</span></span><br><span class="line">        <span class="keyword">val</span> columns = sparkTableScheme.trim.drop(<span class="number">1</span>).dropRight(<span class="number">1</span>).split(<span class="string">","</span>)</span><br><span class="line">        <span class="comment">// 拿到切分后的每一对Schema</span></span><br><span class="line">        <span class="keyword">val</span> sparkSchemas: <span class="type">Array</span>[<span class="type">SparkSchema</span>] = columns.map(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = x.split(<span class="string">" "</span>)</span><br><span class="line">            <span class="comment">// 使用SparkSchema封装，这里拿什么封装无所谓，tuple都行</span></span><br><span class="line">            <span class="type">SparkSchema</span>(words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">// 因为是map，所以返回的一个数组</span></span><br><span class="line">        sparkSchemas</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>创建HbaseRelation继承BashRelation和TableScan实现StructType和buildScan</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">HbaseRelation</span>(<span class="params">@transient sqlContext: <span class="type">SQLContext</span>, @transient parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]</span>) <span class="keyword">extends</span> <span class="title">BaseRelation</span> <span class="keyword">with</span> <span class="title">TableScan</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 拿到外部传入的hbase表名</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> hbaseTable: <span class="type">String</span> = parameters.getOrElse(<span class="string">"hbase.table.name"</span>, sys.error(<span class="string">"hbase.table.name is required..."</span>))</span><br><span class="line">    <span class="comment">// 拿到外部传入的Schema</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> sparkTableSchema: <span class="type">String</span> = parameters.getOrElse(<span class="string">"spark.table.schema"</span>, sys.error(<span class="string">"spark.table.schema is required..."</span>))</span><br><span class="line">    <span class="comment">// 拿到外部传入的zookeeper地址</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> zookeeperHostAndPort: <span class="type">String</span> = parameters.getOrElse(<span class="string">"spark.zookeeper.host.port"</span>, sys.error(<span class="string">"spark.zookeeper.host.port is required..."</span>))</span><br><span class="line">    <span class="comment">// TODO 注意 这里可能还需要拿到传入的列族，以实现获取不同列族的数据</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将传入的Schema信息传入extractSparkFields进行解析，返回一个SparkSchema数组</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> sparkFields: <span class="type">Array</span>[<span class="type">SparkSchema</span>] = <span class="type">HBaseDataSourceUtils</span>.extractSparkFields(sparkTableSchema)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">schema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">        <span class="comment">// 拿到每一个SparkSchema</span></span><br><span class="line">        <span class="keyword">val</span> rows: <span class="type">Array</span>[<span class="type">StructField</span>] = sparkFields.map(field =&gt; &#123;</span><br><span class="line">            <span class="comment">// 拿到SparkSchema中的fieldType做模式匹配，封装成我们需要的StructField</span></span><br><span class="line">            <span class="keyword">val</span> structField = field.fieldType.toLowerCase <span class="keyword">match</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="string">"int"</span> =&gt; <span class="type">StructField</span>(field.fieldName, <span class="type">IntegerType</span>)</span><br><span class="line">                <span class="keyword">case</span> <span class="string">"string"</span> =&gt; <span class="type">StructField</span>(field.fieldName, <span class="type">StringType</span>)</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 返回StructField</span></span><br><span class="line">            structField</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">// 使用的map，返回的一个structField数组，直接放入StructType对象中，即拿到最终的StructType</span></span><br><span class="line">        <span class="keyword">new</span> <span class="type">StructType</span>(rows)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 把HBase中的数据转成RDD[Row]</span></span><br><span class="line"><span class="comment">     * 1）怎么样去读取HBase的数据</span></span><br><span class="line"><span class="comment">     * 2）怎么样把HBase的转成RDD[Row]</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">        <span class="comment">// 创建hbase配置文件</span></span><br><span class="line">        <span class="keyword">val</span> hbaseConf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">        <span class="comment">// 设置zookeeper的地址</span></span><br><span class="line">        hbaseConf.set(<span class="string">"hbase.zookeeper.quorum"</span>, zookeeperHostAndPort)</span><br><span class="line">        <span class="comment">// 设置hbase的表名</span></span><br><span class="line">        hbaseConf.set(<span class="type">TableInputFormat</span>.<span class="type">INPUT_TABLE</span>, hbaseTable)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 通过newAPIHadoopRDD拿到对应hbase表中的所有数据</span></span><br><span class="line">        <span class="keyword">val</span> hbaseRDD: <span class="type">RDD</span>[(<span class="type">ImmutableBytesWritable</span>, <span class="type">Result</span>)] = sqlContext.sparkContext.newAPIHadoopRDD(hbaseConf,</span><br><span class="line">            classOf[<span class="type">TableInputFormat</span>],</span><br><span class="line">            classOf[<span class="type">ImmutableBytesWritable</span>],</span><br><span class="line">            classOf[<span class="type">Result</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 拿到我们需要的Result</span></span><br><span class="line">        hbaseRDD.map(_._2).map(result =&gt; &#123;</span><br><span class="line">            <span class="comment">// 创建一个列表，类型且是Any的，方便后面转换成RDD[Row]</span></span><br><span class="line">            <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Any</span>]()</span><br><span class="line">            <span class="comment">// 变量每一行数据</span></span><br><span class="line">            sparkFields.foreach(field =&gt; &#123;</span><br><span class="line">                <span class="comment">// 判断对应列族下的列有没有数据</span></span><br><span class="line">                <span class="keyword">if</span> (result.containsColumn(<span class="type">Bytes</span>.toBytes(<span class="string">"info"</span>), <span class="type">Bytes</span>.toBytes(field.fieldName))) &#123;</span><br><span class="line">                    <span class="comment">// 如果有，则拿到对应的数据，并且通过模式匹配，转换成对应的类型</span></span><br><span class="line">                    field.fieldType <span class="keyword">match</span> &#123;</span><br><span class="line">                        <span class="keyword">case</span> <span class="string">"string"</span> =&gt;</span><br><span class="line">                            <span class="keyword">val</span> tmp: <span class="type">Array</span>[<span class="type">Byte</span>] = result.getValue(<span class="type">Bytes</span>.toBytes(<span class="string">"info"</span>), <span class="type">Bytes</span>.toBytes(field.fieldName))</span><br><span class="line">                            <span class="comment">// 加入buffer</span></span><br><span class="line">                            buffer += <span class="keyword">new</span> <span class="type">String</span>(tmp)</span><br><span class="line">                        <span class="keyword">case</span> <span class="string">"int"</span> =&gt;</span><br><span class="line">                            <span class="keyword">val</span> tmp = result.getValue(<span class="type">Bytes</span>.toBytes(<span class="string">"info"</span>), <span class="type">Bytes</span>.toBytes(field.fieldName))</span><br><span class="line">                            <span class="comment">// 加入buffer</span></span><br><span class="line">                            buffer += <span class="type">Integer</span>.valueOf(<span class="keyword">new</span> <span class="type">String</span>(tmp))</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// 如果没有数据，则对应字段直接存一个空</span></span><br><span class="line">                    buffer += <span class="keyword">new</span> <span class="type">String</span>(<span class="string">""</span>)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            <span class="comment">// 得到最终的RDD[Row]</span></span><br><span class="line">            <span class="type">Row</span>.fromSeq(buffer)</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>SparkHBaseSourceApp类获取数据</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkHBaseSourceApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 主类</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">          .builder()</span><br><span class="line">          .master(<span class="string">"local[2]"</span>)</span><br><span class="line">          .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">          .getOrCreate()</span><br><span class="line">        <span class="keyword">val</span> df = spark.read.format(<span class="string">"com.tunan.spark.sql.extds.hbase"</span>)</span><br><span class="line">          .option(<span class="string">"hbase.table.name"</span>, <span class="string">"student"</span>)</span><br><span class="line">          .option(<span class="string">"spark.table.schema"</span>, <span class="string">"(adress string,age int,email string,girlfriend string ,love string,name string,sex string)"</span>)</span><br><span class="line">          .option(<span class="string">"spark.zookeeper.host.port"</span>, <span class="string">"hadoop:2181"</span>)</span><br><span class="line">          .option(<span class="string">"spark.select.cf"</span>,<span class="string">"info"</span>)</span><br><span class="line">          .load()</span><br><span class="line"></span><br><span class="line">        df.printSchema()</span><br><span class="line"><span class="comment">//        df.show()</span></span><br><span class="line">        df.createOrReplaceTempView(<span class="string">"student"</span>)</span><br><span class="line">        spark.sql(<span class="string">"select * from student where age &gt; 18"</span>).show(<span class="literal">false</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查看结果</p><p><img src="https://yerias.github.io/spark_img/hbase_3.png" alt="hbase_3"></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark内存管理</title>
      <link href="/2019/11/01/spark/31/"/>
      <url>/2019/11/01/spark/31/</url>
      
        <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Spark 作为一个基于内存的分布式计算引擎，其内存管理模块在整个系统中扮演着非常重要的角色。理解 Spark 内存管理的基本原理，有助于更好地开发 Spark 应用程序和进行性能调优。本文旨在梳理出 Spark 内存管理的脉络，抛砖引玉，引出读者对这个话题的深入探讨。本文中阐述的原理基于 Spark 2.1 版本，阅读本文需要读者有一定的 Spark 和 Java 基础，了解 RDD、Shuffle、JVM 等相关概念。</p><p>在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业（Job），并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度，后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能[1]。由于 Driver 的内存管理相对来说较为简单，本文主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。</p><h2 id="堆内和堆外内存规划"><a href="#堆内和堆外内存规划" class="headerlink" title="堆内和堆外内存规划"></a>堆内和堆外内存规划</h2><p>作为一个 JVM 进程，Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM 的堆内（On-heap）空间进行了更为详细的分配，以充分利用内存。同时，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化了内存的使用。</p><p><img src="https://yerias.github.io/spark_img/memoryManager_1.png" alt=""></p><h3 id="堆内内存"><a href="#堆内内存" class="headerlink" title="堆内内存"></a>堆内内存</h3><p>堆内内存的大小，由 Spark 应用程序启动时的 –executor-memory 或 spark.executor.memory 参数配置。Executor 内运行的并发任务共享 JVM 堆内内存，这些任务在缓存 RDD 数据和广播（Broadcast）数据时占用的内存被规划为存储（Storage）内存，而这些任务在执行 Shuffle 时占用的内存被规划为执行（Execution）内存，剩余的部分不做特殊规划，那些 Spark 内部的对象实例，或者用户定义的 Spark 应用程序中的对象实例，均占用剩余的空间。不同的管理模式下，这三部分占用的空间大小各不相同（下面第 2 小节会进行介绍）。</p><p>Spark 对堆内内存的管理是一种逻辑上的”规划式”的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前记录这些内存，我们来看其具体流程：</p><ul><li>申请内存：</li></ul><blockquote><ol><li>Spark 在代码中 new 一个对象实例</li><li>JVM 从堆内内存分配空间，创建对象并返回对象引用</li><li>Spark 保存该对象的引用，记录该对象占用的内存</li></ol></blockquote><ul><li>释放内存：</li></ul><blockquote><ol><li>Spark 记录该对象释放的内存，删除该对象的引用</li><li>等待 JVM 的垃圾回收机制释放该对象占用的堆内内存</li></ol></blockquote><p>我们知道，JVM 的对象可以以序列化的方式存储，序列化的过程是将对象转换为二进制字节流，本质上可以理解为将非连续空间的链式存储转化为连续空间或块存储，在访问时则需要进行序列化的逆过程——反序列化，将字节流转化为对象，序列化的方式可以节省存储空间，但增加了存储和读取时候的计算开销。</p><p>对于 Spark 中序列化的对象，由于是字节流的形式，其占用的内存大小可直接计算，而对于非序列化的对象，其占用的内存是通过周期性地采样近似估算而得，即并不是每次新增的数据项都会计算一次占用的内存大小，这种方法降低了时间开销但是有可能误差较大，导致某一时刻的实际内存有可能远远超出预期[2]。此外，在被 Spark 标记为释放的对象实例，很有可能在实际上并没有被 JVM 回收，导致实际可用的内存小于 Spark 记录的可用内存。所以 Spark 并不能准确记录实际可用的堆内内存，从而也就无法完全避免内存溢出（OOM, Out of Memory）的异常。</p><p>虽然不能精准控制堆内内存的申请和释放，但 Spark 通过对存储内存和执行内存各自独立的规划管理，可以决定是否要在存储内存里缓存新的 RDD，以及是否为新的任务分配执行内存，在一定程度上可以提升内存的利用率，减少异常的出现。</p><h3 id="堆外内存"><a href="#堆外内存" class="headerlink" title="堆外内存"></a>堆外内存</h3><p>为了进一步优化内存的使用以及提高 Shuffle 时排序的效率，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。利用 JDK Unsafe API（从 Spark 2.0 开始，在管理堆外的存储内存时不再基于 Tachyon，而是与堆外的执行内存一样，基于 JDK Unsafe API 实现[3]），Spark 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。堆外内存可以被精确地申请和释放，而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差。</p><p>在默认情况下堆外内存并不启用，可通过配置 spark.memory.offHeap.enabled 参数启用，并由 spark.memory.offHeap.size 参数设定堆外空间的大小。除了没有 other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存。</p><h3 id="内存管理接口"><a href="#内存管理接口" class="headerlink" title="内存管理接口"></a>内存管理接口</h3><p>Spark 为存储内存和执行内存的管理提供了统一的接口——MemoryManager，同一个 Executor 内的任务都调用这个接口的方法来申请或释放内存:</p><p>内存管理接口的主要方法</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//申请存储内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">acquireStorageMemory</span></span>(blockId: <span class="type">BlockId</span>, numBytes: <span class="type">Long</span>, memoryMode: <span class="type">MemoryMode</span>): <span class="type">Boolean</span></span><br><span class="line"><span class="comment">//申请展开内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">acquireUnrollMemory</span></span>(blockId: <span class="type">BlockId</span>, numBytes: <span class="type">Long</span>, memoryMode: <span class="type">MemoryMode</span>): <span class="type">Boolean</span></span><br><span class="line"><span class="comment">//申请执行内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">acquireExecutionMemory</span></span>(numBytes: <span class="type">Long</span>, taskAttemptId: <span class="type">Long</span>, memoryMode: <span class="type">MemoryMode</span>): <span class="type">Long</span></span><br><span class="line"><span class="comment">//释放存储内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">releaseStorageMemory</span></span>(numBytes: <span class="type">Long</span>, memoryMode: <span class="type">MemoryMode</span>): <span class="type">Unit</span></span><br><span class="line"><span class="comment">//释放执行内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">releaseExecutionMemory</span></span>(numBytes: <span class="type">Long</span>, taskAttemptId: <span class="type">Long</span>, memoryMode: <span class="type">MemoryMode</span>): <span class="type">Unit</span></span><br><span class="line"><span class="comment">//释放展开内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">releaseUnrollMemory</span></span>(numBytes: <span class="type">Long</span>, memoryMode: <span class="type">MemoryMode</span>): <span class="type">Unit</span></span><br></pre></td></tr></table></figure><p>我们看到，在调用这些方法时都需要指定其内存模式（MemoryMode），这个参数决定了是在堆内还是堆外完成这次操作。</p><p>MemoryManager 的具体实现上，Spark 1.6 之后默认为统一管理（<a href="https://github.com/apache/spark/blob/v2.1.0/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala" target="_blank" rel="noopener">Unified Memory Manager</a>）方式，1.6 之前采用的静态管理（<a href="https://github.com/apache/spark/blob/v2.1.0/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala" target="_blank" rel="noopener">Static Memory Manager</a>）方式仍被保留，可通过配置 spark.memory.useLegacyMode 参数启用。两种方式的区别在于对空间分配的方式，下面的第 3 小节会分别对这两种方式进行介绍。</p><h2 id="内存空间分配"><a href="#内存空间分配" class="headerlink" title="内存空间分配"></a>内存空间分配</h2><h3 id="静态内存管理"><a href="#静态内存管理" class="headerlink" title="静态内存管理"></a>静态内存管理</h3><p>在 Spark 最初采用的静态内存管理机制下，存储内存、执行内存和其他内存的大小在 Spark 应用程序运行期间均为固定的，但用户可以应用程序启动前进行配置，堆内内存的分配如图 所示：</p><p><img src="https://yerias.github.io/spark_img/memoryManager_2.png" alt=""></p><p>可以看到，可用的堆内内存的大小需要按照下面的方式计算：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">可用的存储内存 = systemMaxMemory * spark.storage.memoryFraction * spark.storage.safetyFraction</span><br><span class="line">可用的执行内存 = systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction</span><br></pre></td></tr></table></figure><p>其中 systemMaxMemory 取决于当前 JVM 堆内内存的大小，最后可用的执行内存或者存储内存要在此基础上与各自的 memoryFraction 参数和 safetyFraction 参数相乘得出。上述计算公式中的两个 safetyFraction 参数，其意义在于在逻辑上预留出 1-safetyFraction 这么一块保险区域，降低因实际内存超出当前预设范围而导致 OOM 的风险（上文提到，对于非序列化对象的内存采样估算会产生误差）。值得注意的是，这个预留的保险区域仅仅是一种逻辑上的规划，在具体使用时 Spark 并没有区别对待，和”其它内存”一样交给了 JVM 去管理。</p><p>堆外的空间分配较为简单，只有存储内存和执行内存，     所示。可用的执行内存和存储内存占用的空间大小直接由参数 spark.memory.storageFraction 决定，由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域。</p><p><img src="https://yerias.github.io/spark_img/memoryManager_3.png" alt=""></p><p>静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成”一半海水，一半火焰”的局面，即存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。由于新的内存管理机制的出现，这种方式目前已经很少有开发者使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。</p><p>最后看一下静态内存管理的源码</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 老版本的静态内存管理机制，storage和excutor是单独的</span></span><br><span class="line"><span class="type">StaticMemoryManager</span>&#123;</span><br><span class="line">getMaxExecutionMemory&#123;</span><br><span class="line">    <span class="keyword">val</span> memoryFraction = conf.getDouble(<span class="string">"spark.shuffle.memoryFraction"</span>, <span class="number">0.2</span>)</span><br><span class="line">    <span class="keyword">val</span> safetyFraction = conf.getDouble(<span class="string">"spark.shuffle.safetyFraction"</span>, <span class="number">0.8</span>)</span><br><span class="line">(systemMaxMemory * memoryFraction * safetyFraction).toLong<span class="number">1000</span>M*<span class="number">0.2</span>*<span class="number">0.8</span>=<span class="number">160</span>M</span><br><span class="line">&#125;</span><br><span class="line">getMaxStorageMemory&#123;</span><br><span class="line">    <span class="keyword">val</span> systemMaxMemory = conf.getLong(<span class="string">"spark.testing.memory"</span>, <span class="type">Runtime</span>.getRuntime.maxMemory)</span><br><span class="line">    <span class="keyword">val</span> memoryFraction = conf.getDouble(<span class="string">"spark.storage.memoryFraction"</span>, <span class="number">0.6</span>)</span><br><span class="line">    <span class="keyword">val</span> safetyFraction = conf.getDouble(<span class="string">"spark.storage.safetyFraction"</span>, <span class="number">0.9</span>)</span><br><span class="line">    (systemMaxMemory * memoryFraction * safetyFraction).toLong<span class="number">1000</span>M*<span class="number">0.6</span>*<span class="number">0.9</span>=<span class="number">540</span>M</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="统一内存管理"><a href="#统一内存管理" class="headerlink" title="统一内存管理"></a>统一内存管理</h3><p>Spark 1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域</p><p><img src="https://yerias.github.io/spark_img/memoryManager_4.png" alt=""></p><p>Executor Memory：主要用于存放 Shuffle、Join、Sort、Aggregation 等计算过程中的临时数据<br>Storage Memory：主要用于存储 spark 的 cache 数据，例如RDD的缓存、unroll数据<br>Other Memory：主要用于存储 RDD 转换操作所需要的数据，例如 RDD 依赖等信息<br>Reserved Memory：系统预留内存，会用来存储Spark内部对象</p><p><img src="https://yerias.github.io/spark_img/memoryManager_5.png" alt=""></p><p>其中最重要的优化在于动态占用机制，其规则如下：</p><blockquote><ul><li>设定基本的存储内存和执行内存区域（spark.storage.storageFraction 参数），该设定确定了双方各自拥有的空间的范围</li><li>双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block）</li><li>执行内存的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后”归还”借用的空间</li><li>存储内存的空间被对方占用后，无法让对方”归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂</li></ul></blockquote><p><img src="https://yerias.github.io/spark_img/memoryManager_6.png" alt=""></p><p>凭借统一内存管理机制，Spark 在一定程度上提高了堆内和堆外内存资源的利用率，降低了开发者维护 Spark 内存的难度，但并不意味着开发者可以高枕无忧。譬如，所以如果存储内存的空间太大或者说缓存的数据过多，反而会导致频繁的全量垃圾回收，降低任务执行时的性能，因为缓存的 RDD 数据通常都是长期驻留内存的 [5] 。所以要想充分发挥 Spark 的性能，需要开发者进一步了解存储内存和执行内存各自的管理方式和实现原理。</p><p>最后看一下统一内存管理的源码</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 新版本统一的内存管理，storage和excutor是共用的</span></span><br><span class="line"><span class="type">UnifiedMemoryManager</span>&#123;</span><br><span class="line"><span class="keyword">val</span> maxMemory = getMaxMemory(conf)&#123;</span><br><span class="line"><span class="keyword">val</span> systemMemory = conf.getLong(<span class="string">"spark.testing.memory"</span>, <span class="type">Runtime</span>.getRuntime.maxMemory)</span><br><span class="line"><span class="keyword">val</span> reservedMemory = conf.getLong(<span class="string">"spark.testing.reservedMemory"</span>,<span class="keyword">if</span> (conf.contains(<span class="string">"spark.testing"</span>)) <span class="number">0</span> <span class="keyword">else</span> <span class="type">RESERVED_SYSTEM_MEMORY_BYTES</span>) <span class="number">300</span>M </span><br><span class="line">&#125;</span><br><span class="line"><span class="number">1000</span>M - <span class="number">300</span>M = <span class="number">700</span>M</span><br><span class="line"><span class="keyword">val</span> usableMemory = systemMemory - reservedMemory</span><br><span class="line">    <span class="keyword">val</span> memoryFraction = conf.getDouble(<span class="string">"spark.memory.fraction"</span>, <span class="number">0.6</span>)</span><br><span class="line">    (usableMemory * memoryFraction).toLong <span class="number">700</span>*<span class="number">0.6</span> = <span class="number">420</span>Mexe和storage总共能用的</span><br><span class="line">    </span><br><span class="line">    storage (<span class="number">1000</span><span class="number">-300</span>)*<span class="number">0.6</span>*<span class="number">0.5</span>=<span class="number">210</span>M  /executor <span class="number">210</span>M</span><br><span class="line">    onHeapStorageRegionSize = (maxMemory * conf.getDouble(<span class="string">"spark.memory.storageFraction"</span>, <span class="number">0.5</span>)).toLong,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Broadcast实现动态更新作业配置</title>
      <link href="/2019/10/31/spark/30/"/>
      <url>/2019/10/31/spark/30/</url>
      
        <content type="html"><![CDATA[<p>由于实时场景对可用性十分敏感，实时作业通常需要避免频繁重启，因此动态加载作业配置（变量）是实时计算里十分常见的需求，比如通常复杂事件处理 (CEP) 的规则或者在线机器学习的模型。尽管常见，实现起来却并没有那么简单，其中最难点在于如何确保节点状态在变更期间的一致性。目前来说一般有两种实现方式：</p><ul><li>轮询拉取方式，即作业算子定时检测在外部系统的配置是否有变更，若有则同步配置。</li><li>控制流方式，即作业除了用于计算的一个或多个普通数据流以外，还有提供一个用于改变作业算子状态的元数据流，也就是控制流。</li></ul><p>轮询拉取方式基于 pull 模式，一般实现是用户在 Stateful 算子(比如 RichMap)里实现后台线程定时从外部系统同步变量。这种方式对于一般作业或许足够，但存在两个缺点分别限制了作业的实时性和准确性的进一步提高：首先，轮询总是有一定的延迟，因此变量的变更不能第一时间生效；其次，这种方式依赖于节点本地时间来进行校准。如果在同一时间有的节点已经检测到变更并更新状态，而有的节点还没有检测到或者还未更新，就会造成短时间内的不一致。</p><p>控制流方式基于 push 模式，变更的检测和节点更新的一致性都由计算框架负责，从用户视角看只需要定义如何更新算子状态并负责将控制事件丢入控制流，后续工作计算框架会自动处理。控制流不同于其他普通数据流的地方在于控制流是以广播形式流动的，否则在有 Keyby 或者 rebalance 等提高并行度分流的算子的情况下就无法将控制事件传达给所有的算子。</p><p>以目前最流行的两个实时计算框架 Spark Streaming 和 Flink 来说，前者是以类似轮询的方式来实现实时作业的更新，而后者则是基于控制流的方式。</p><h2 id="Spark-Streaming-Broadcast-Variable"><a href="#Spark-Streaming-Broadcast-Variable" class="headerlink" title="Spark Streaming Broadcast Variable"></a>Spark Streaming Broadcast Variable</h2><p>Spark Streaming 为用户提供了 Broadcast Varialbe，可以用于节点算子状态的初始化和后续更新。Broacast Variable 是一组只读的变量，它在作业初始化时由 Spark Driver 生成并广播到每个 Executor 节点，随后该节点的 Task 可以复用同一份变量。</p><p>Broadcast Variable 的设计初衷是为了避免大文件，比如 NLP 常用的分词词典，随序列化后的作业对象一起分发，造成重复分发的网络资源浪费和启动时间延长。这类文件的更新频率是相对低的，扮演的角色类似于只读缓存，通过设置 TTL 来定时更新，缓存过期之后 Executor 节点会重新向 Driver 请求最新的变量。</p><p>Broadcast Variable 并不是从设计理念上就支持低延迟的作业状态更新，因此用户想出了不少 Hack 的方法，其中最为常见的方式是：一方面在 Driver 实现后台线程不断更新 Broadcast Variavle，另一方面在作业运行时通过显式地删除 Broadcast Variable 来迫使 Executor 重新从 Driver 拉取最新的 Broadcast Variable。这个过程会发生在两个 micro batch 计算之间，以确保每个 micro batch 计算过程中状态是一致的。</p><p>比起用户在算子内访问外部系统实现更新变量，这种方式的优点在于一致性更有保证。因为 Broadcast Variable 是统一由 Driver 更新并推到 Executor 的，这就保证不同节点的更新时间是一致的。然而相对地，缺点是会给 Driver 带来比较大的负担，因为需要不断分发全量的 Broadcast Variable (试想下一个巨大的 Map，每次只会更新少数 Entry，却要整个 Map 重新分发)。在 Spark 2.0 版本以后，Broadcast Variable 的分发已经从 Driver 单点改为基于 BitTorrent 的 P2P 分发，这一定程度上缓解了随着集群规模提升 Driver 分发变量的压力，但我个人对这种方式能支持到多大规模的部署还是持怀疑态度。另外一点是重新分发 Broadcast Variable 需要阻塞作业进行，这也会使作业的吞吐量和延迟受到比较大的影响。</p><h2 id="Flink-Broadcast-State-amp-Stream"><a href="#Flink-Broadcast-State-amp-Stream" class="headerlink" title="Flink Broadcast State &amp; Stream"></a>Flink Broadcast State &amp; Stream</h2><p>Broadcast Stream 是 Flink 1.5.0 发布的新特性，基于控制流的方式实现了实时作业的状态更新。Broadcast Stream 的创建方式与普通数据流相同，例如从 Kafka Topic 读取，特别之处在于它承载的是控制事件流，会以广播形式将数据发给下游算子的每个实例。Broadcast Stream 需要在作业拓扑的某个节点和普通数据流 (Main Stream) join 到一起。</p><p><img src="https://yerias.github.io/control-stream.png" alt=""></p><p>该节点的算子需要同时处理普通数据流和控制流：一方面它需要读取控制流以更新本地状态 (Broadcast State)，另外一方面需要读取 Main Stream 并根据 Broadcast State 来进行数据转换。由于每个算子实例读到的控制流都是相同的，它们生成的 Broadcast State 也是相同的，从而达到通过控制消息来更新所有算子实例的效果。</p><p>目前 Flink 的 Broadcast Stream 从效果上实现了控制流的作业状态更新，不过在编程模型上有点和一般直觉不同。原因主要在于 Flink 对控制流的处理方式和普通数据流保持了一致，最为明显的一点是控制流除了改变本地 State 还可以产生 output，这很大程度上影响了 Broadcast Stream 的使用方式。Broadcast Stream 的使用方式与普通的 DataStream 差别比较大，即需要和 DataStream 连接成为 BroadcastConnectedStream 后，再通过特殊的 BroadcastProcessFunction 来处理，而 BroadcastProcessFunction 目前只支持 类似于 RichCoFlatMap 效果的操作。RichCoFlatMap 可以间接实现对 Main Stream 的 Map 转换（返回一只有一个元素的集合）和 Filter 转换（返回空集合），但无法实现 Window 类计算。这意味着如果用户希望改变 Window 算子的状态，那么需要将状态管理提前到上游的 BroadcastProcessFunction，然后再通过 BroadcastProcessFunction 的输出来将影响下游 Window 算子的行为。</p><h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BroadcaseAlertV2</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> user: <span class="type">String</span> = <span class="string">"root"</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> password: <span class="type">String</span> = <span class="string">"root"</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> url: <span class="type">String</span> = <span class="string">"jdbc:mysql://hadoop:3306/tunan"</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> prewarningTable: <span class="type">String</span> = <span class="string">"prewarning_config"</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> lastUpdatedAt: <span class="type">Date</span> = <span class="type">Calendar</span>.getInstance.getTime <span class="comment">//上次time</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateAndGet</span></span>(spark: <span class="type">SparkSession</span>, bcAlertList: <span class="type">Broadcast</span>[<span class="type">ListBuffer</span>[<span class="type">String</span>]]): <span class="type">Broadcast</span>[<span class="type">ListBuffer</span>[<span class="type">String</span>]] = &#123;</span><br><span class="line"><span class="keyword">var</span> bcAlertList2: <span class="type">Broadcast</span>[<span class="type">ListBuffer</span>[<span class="type">String</span>]] = <span class="literal">null</span></span><br><span class="line"><span class="keyword">val</span> currentDate: <span class="type">Date</span> = <span class="type">Calendar</span>.getInstance.getTime <span class="comment">//当前time</span></span><br><span class="line"><span class="keyword">val</span> diff: <span class="type">Long</span> = currentDate.getTime - lastUpdatedAt.getTime <span class="comment">//time差值</span></span><br><span class="line"><span class="keyword">if</span> (bcAlertList == <span class="literal">null</span> || diff &gt;= <span class="number">60000</span>) &#123; <span class="comment">//第一次进来bcAlertList为null，后面每次刷新为60S ，这一步检测可以提前到调用代码中</span></span><br><span class="line"><span class="keyword">if</span> (bcAlertList != <span class="literal">null</span>) &#123;</span><br><span class="line">bcAlertList.unpersist() <span class="comment">// 如果bcAlertList存在，即不是第一次调用，先清除缓存</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">lastUpdatedAt = currentDate <span class="comment">//上次登录时间更新为当前时间</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// spark sql 读取MySQL数据库</span></span><br><span class="line"><span class="keyword">val</span> alterDs: <span class="type">Dataset</span>[<span class="type">Row</span>] = spark.read.format(<span class="string">"jdbc"</span>)</span><br><span class="line">  .option(<span class="string">"url"</span>, url)</span><br><span class="line">  .option(<span class="string">"dbtable"</span>, prewarningTable)</span><br><span class="line">  .option(<span class="string">"user"</span>, user)</span><br><span class="line">  .option(<span class="string">"password"</span>, password)</span><br><span class="line">  .load</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> alertList: <span class="type">ListBuffer</span>[<span class="type">String</span>] = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">String</span>]</span><br><span class="line"><span class="comment">// 数据库查询的结果转换为List</span></span><br><span class="line"><span class="keyword">val</span> warninfo: util.<span class="type">List</span>[<span class="type">Row</span>] = alterDs.collectAsList</span><br><span class="line"><span class="comment">//java集合转换为scala，循环add</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>._</span><br><span class="line"><span class="keyword">for</span> (row_warninfo &lt;- warninfo) &#123;</span><br><span class="line">alertList.add(row_warninfo.get(<span class="number">0</span>).toString)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//定义广播变量bcAlertList</span></span><br><span class="line">bcAlertList2 = spark.sparkContext.broadcast(alertList)</span><br><span class="line">&#125;</span><br><span class="line">bcAlertList2</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>背景: Spark Streaming在源源不断的处理log，其中有一个操作就是过滤出日志内容带有高危关键词的数据，如果高危关键词写死在代码中，那么更新关键词将变得非常麻烦，目前的解决办法就是将高危关键词存储在MySQL中，再由Spark代码动态加载到作业中，由bordcast分发，最后参与作业的计算</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>实时作业运行时动态加载变量可以令大大提升实时作业的灵活性和适应更多应用场景，目前无论是 Flink 还是 Spark Streaming 对动态加载变量的支持都不是特别完美。Spark Streaming 受限于 Micro Batch 的计算模型（虽然现在 2.3 版本引入 Continuous Streaming 来支持流式处理，但离成熟还需要一定时间），将作业变量作为一致性和实时性要求相对低的节点本地缓存，并不支持低延迟地、低成本地更新作业变量。Flink 将变量更新视为特殊的控制事件流，符合 Even Driven 的流式计算框架定位，目前在业界已有比较成熟的应用。不过美中不足的是编程模型的易用性上有提高空间：控制流目前只能用于和数据流的 join，这意味着下游节点无法继续访问控制流或者需要把控制流数据插入到数据流中（这种方式并不优雅），从而降低了编程模型的灵活性。个人认为最好的情况是大部分的算子都可以被拓展为具有 BroadcastOperator，就像 RichFunction 一样，它们可以接收一个数据流和一个至多个控制流，并维护对应的 BroadcastState，这样控制流的接入成本将显著下降。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-17+Side+Inputs+for+DataStream+API" target="_blank" rel="noopener">FLIP-17 Side Inputs for DataStream API</a><br>2.<a href="https://data-artisans.com/blog/bettercloud-dynamic-alerting-apache-flink" target="_blank" rel="noopener">Dynamically Configured Stream Processing: How BetterCloud Built an Alerting System with Apache Flink®</a><br>3.<a href="https://mux.com/blog/using-control-streams-to-manage-apache-flink-applications/" target="_blank" rel="noopener">Using Control Streams to Manage Apache Flink Applications</a><br>4.<a href="https://stackoverflow.com/questions/33372264/how-can-i-update-a-broadcast-variable-in-spark-streaming" target="_blank" rel="noopener">StackOverFlow - ow can I update a broadcast variable in spark streaming?</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark性能优化指南之Shuffle调优(转载)</title>
      <link href="/2019/10/30/spark/29/"/>
      <url>/2019/10/30/spark/29/</url>
      
        <content type="html"><![CDATA[<h2 id="调优概述"><a href="#调优概述" class="headerlink" title="调优概述"></a>调优概述</h2><p>大多数Spark作业的性能主要就是消耗在了shuffle环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对shuffle过程进行调优。但是也必须提醒大家的是，影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解shuffle的原理，以及相关参数的说明，同时给出各个参数的调优建议。</p><h2 id="ShuffleManager发展概述"><a href="#ShuffleManager发展概述" class="headerlink" title="ShuffleManager发展概述"></a>ShuffleManager发展概述</h2><p>在Spark的源码中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。</p><p>在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。</p><p>因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。</p><p>下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。</p><h2 id="HashShuffleManager运行原理"><a href="#HashShuffleManager运行原理" class="headerlink" title="HashShuffleManager运行原理"></a>HashShuffleManager运行原理</h2><h3 id="未经优化的HashShuffleManager"><a href="#未经优化的HashShuffleManager" class="headerlink" title="未经优化的HashShuffleManager"></a>未经优化的HashShuffleManager</h3><p>下图说明了未经优化的HashShuffleManager的原理。这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。</p><p>我们先从shuffle write开始说起。shuffle write阶段，主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子（比如reduceByKey），而将每个task处理的数据按key进行“分类”。所谓“分类”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。</p><p>那么每个执行shuffle write的task，要为下一个stage创建多少个磁盘文件呢？很简单，下一个stage的task有多少个，当前stage的每个task就要创建多少份磁盘文件。比如下一个stage总共有100个task，那么当前stage的每个task都要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，每个Executor执行5个Task，那么每个Executor上总共就要创建500个磁盘文件，所有Executor上会创建5000个磁盘文件。由此可见，未经优化的shuffle write操作所产生的磁盘文件的数量是极其惊人的。</p><p>接着我们来说说shuffle read。shuffle read，通常就是一个stage刚开始时要做的事情。此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task给下游stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。</p><p>shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。</p><p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%988.png" alt=""></p><h3 id="优化后的HashShuffleManager"><a href="#优化后的HashShuffleManager" class="headerlink" title="优化后的HashShuffleManager"></a>优化后的HashShuffleManager</h3><p>下图说明了优化后的HashShuffleManager的原理。这里说的优化，是指我们可以设置一个参数，spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。</p><p>开启consolidate机制之后，在shuffle write过程中，task就不是为下游stage的每个task创建一个磁盘文件了。此时会出现shuffleFileGroup的概念，每个shuffleFileGroup会对应一批磁盘文件，磁盘文件的数量与下游stage的task数量是相同的。一个Executor上有多少个CPU core，就可以并行执行多少个task。而第一批并行执行的每个task都会创建一个shuffleFileGroup，并将数据写入对应的磁盘文件内。</p><p>当Executor的CPU core执行完一批task，接着执行下一批task时，下一批task就会复用之前已有的shuffleFileGroup，包括其中的磁盘文件。也就是说，此时task会将数据写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，consolidate机制允许不同的task复用同一批磁盘文件，这样就可以有效将多个task的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升shuffle write的性能。</p><p>假设第二个stage有100个task，第一个stage有50个task，总共还是有10个Executor，每个Executor执行5个task。那么原本使用未经优化的HashShuffleManager时，每个Executor会产生500个磁盘文件，所有Executor会产生5000个磁盘文件的。但是此时经过优化之后，每个Executor创建的磁盘文件的数量的计算公式为：CPU core的数量 * 下一个stage的task数量。也就是说，每个Executor此时只会创建100个磁盘文件，所有Executor只会创建1000个磁盘文件。</p><p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%989.png" alt=""></p><h2 id="SortShuffleManager运行原理"><a href="#SortShuffleManager运行原理" class="headerlink" title="SortShuffleManager运行原理"></a>SortShuffleManager运行原理</h2><p>SortShuffleManager的运行机制主要分成两种，一种是普通运行机制，另一种是bypass运行机制。当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为200），就会启用bypass机制。</p><h3 id="普通运行机制"><a href="#普通运行机制" class="headerlink" title="普通运行机制"></a>普通运行机制</h3><p>下图说明了普通的SortShuffleManager的原理。在该模式下，数据会先写入一个内存数据结构中，此时根据不同的shuffle算子，可能选用不同的数据结构。如果是reduceByKey这种聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存；如果是join这种普通的shuffle算子，那么会选用Array数据结构，直接写入内存。接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。</p><p>在溢写到磁盘文件之前，会先根据key对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。默认的batch数量是10000条，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过Java的BufferedOutputStream实现的。BufferedOutputStream是Java的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。</p><p>一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，由于一个task就只对应一个磁盘文件，也就意味着该task为下游stage的task准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。</p><p>SortShuffleManager由于有一个磁盘文件merge的过程，因此大大减少了文件数量。比如第一个stage有50个task，总共有10个Executor，每个Executor执行5个task，而第二个stage有100个task。由于每个task最终只有一个磁盘文件，因此此时每个Executor上只有5个磁盘文件，所有Executor只有50个磁盘文件。</p><p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%9810.png" alt=""></p><h3 id="bypass运行机制"><a href="#bypass运行机制" class="headerlink" title="bypass运行机制"></a>bypass运行机制</h3><p>下图说明了bypass SortShuffleManager的原理。bypass运行机制的触发条件如下： * shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。 * 不是聚合类的shuffle算子（比如reduceByKey）。</p><p>此时task会为每个下游task都创建一个临时磁盘文件，并将数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。</p><p>该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。</p><p>而该机制与普通SortShuffleManager运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。</p><p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%9811.png" alt=""></p><h2 id="shuffle相关参数调优"><a href="#shuffle相关参数调优" class="headerlink" title="shuffle相关参数调优"></a>shuffle相关参数调优</h2><p>以下是Shffule过程中的一些主要参数，这里详细讲解了各个参数的功能、默认值以及基于实践经验给出的调优建议。</p><h3 id="spark-shuffle-file-buffer"><a href="#spark-shuffle-file-buffer" class="headerlink" title="spark.shuffle.file.buffer"></a>spark.shuffle.file.buffer</h3><ul><li>默认值：32k</li><li>参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。</li><li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li></ul><h3 id="spark-reducer-maxSizeInFlight"><a href="#spark-reducer-maxSizeInFlight" class="headerlink" title="spark.reducer.maxSizeInFlight"></a>spark.reducer.maxSizeInFlight</h3><ul><li>默认值：48m</li><li>参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。</li><li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li></ul><h3 id="spark-shuffle-io-maxRetries"><a href="#spark-shuffle-io-maxRetries" class="headerlink" title="spark.shuffle.io.maxRetries"></a>spark.shuffle.io.maxRetries</h3><ul><li>默认值：3</li><li>参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。</li><li>调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。</li></ul><h3 id="spark-shuffle-io-retryWait"><a href="#spark-shuffle-io-retryWait" class="headerlink" title="spark.shuffle.io.retryWait"></a>spark.shuffle.io.retryWait</h3><ul><li>默认值：5s</li><li>参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。</li><li>调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。</li></ul><h3 id="spark-shuffle-memoryFraction"><a href="#spark-shuffle-memoryFraction" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.shuffle.memoryFraction</h3><ul><li>默认值：0.2</li><li>参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。</li><li>调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。</li></ul><h3 id="spark-shuffle-manager"><a href="#spark-shuffle-manager" class="headerlink" title="spark.shuffle.manager"></a>spark.shuffle.manager</h3><ul><li>默认值：sort</li><li>参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。</li><li>调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。</li></ul><h3 id="spark-shuffle-sort-bypassMergeThreshold"><a href="#spark-shuffle-sort-bypassMergeThreshold" class="headerlink" title="spark.shuffle.sort.bypassMergeThreshold"></a>spark.shuffle.sort.bypassMergeThreshold</h3><ul><li>默认值：200</li><li>参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。</li><li>调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。</li></ul><h3 id="spark-shuffle-consolidateFiles"><a href="#spark-shuffle-consolidateFiles" class="headerlink" title="spark.shuffle.consolidateFiles"></a>spark.shuffle.consolidateFiles</h3><ul><li>默认值：false</li><li>参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。</li><li>调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。</li></ul><p>本文分别讲解了开发过程中的优化原则、运行前的资源参数设置调优、运行中的数据倾斜的解决方案、为了精益求精的shuffle调优。希望大家能够在阅读本文之后，记住这些性能调优的原则以及方案，在Spark作业开发、测试以及运行的过程中多尝试，只有这样，我们才能开发出更优的Spark作业，不断提升其性能。</p><hr><p>参考博客: <a href="https://tech.meituan.com/2016/05/12/spark-tuning-pro.html" target="_blank" rel="noopener">https://tech.meituan.com/2016/05/12/spark-tuning-pro.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark性能优化指南之数据倾斜调优(转载)</title>
      <link href="/2019/10/29/spark/28/"/>
      <url>/2019/10/29/spark/28/</url>
      
        <content type="html"><![CDATA[<h2 id="调优概述"><a href="#调优概述" class="headerlink" title="调优概述"></a>调优概述</h2><p>有的时候，我们可能会遇到大数据计算中一个最棘手的问题——数据倾斜，此时Spark作业的性能会比期望差很多。数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。</p><h2 id="数据倾斜发生时的现象"><a href="#数据倾斜发生时的现象" class="headerlink" title="数据倾斜发生时的现象"></a>数据倾斜发生时的现象</h2><ul><li>绝大多数task执行得都非常快，但个别task执行极慢。比如，总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时。这种情况很常见。</li><li>原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。</li></ul><h2 id="数据倾斜发生的原理"><a href="#数据倾斜发生的原理" class="headerlink" title="数据倾斜发生的原理"></a>数据倾斜发生的原理</h2><p>数据倾斜的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。</p><p>因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。</p><p>下图就是一个很清晰的例子：hello这个key，在三个节点上对应了总共7条数据，这些数据都会被拉取到同一个task中进行处理；而world和you这两个key分别才对应1条数据，所以另外两个task只要分别处理1条数据即可。此时第一个task的运行时间可能是另外两个task的7倍，而整个stage的运行速度也由运行最慢的那个task所决定。</p><p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%982.png" alt=""></p><h2 id="如何定位导致数据倾斜的代码"><a href="#如何定位导致数据倾斜的代码" class="headerlink" title="如何定位导致数据倾斜的代码"></a>如何定位导致数据倾斜的代码</h2><p>数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。</p><h3 id="某个task执行特别慢的情况"><a href="#某个task执行特别慢的情况" class="headerlink" title="某个task执行特别慢的情况"></a>某个task执行特别慢的情况</h3><p>首先要看的，就是数据倾斜发生在第几个stage中。</p><p>如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以在Spark Web UI上深入看一下当前这个stage各个task分配的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。</p><p>比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完；而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而运行时间特别长的task需要处理几千KB的数据，处理的数据量差了10倍。此时更加能够确定是发生了数据倾斜。</p><p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%983.png" alt=""></p><p>知道数据倾斜发生在哪一个stage之后，接着我们就需要根据stage划分原理，推算出来发生倾斜的那个stage对应代码中的哪一部分，这部分代码中肯定会有一个shuffle类算子。精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。</p><p>这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。 * stage0，主要是执行从textFile到map操作，以及执行shuffle write操作。shuffle write操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。 * stage1，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行shuffle read操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">val</span> lines = sc.textFile(<span class="string">"hdfs://..."</span>)</span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> pairs = words.map((_, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line"> </span><br><span class="line">wordCounts.collect().foreach(println(_))</span><br></pre></td></tr></table></figure><p>通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。比如我们在Spark Web UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是由educeByKey算子导致的数据倾斜问题。比如某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。</p><h3 id="某个task莫名其妙内存溢出的情况"><a href="#某个task莫名其妙内存溢出的情况" class="headerlink" title="某个task莫名其妙内存溢出的情况"></a>某个task莫名其妙内存溢出的情况</h3><p>这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。</p><p>但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。</p><h2 id="查看导致数据倾斜的key的数据分布情况"><a href="#查看导致数据倾斜的key的数据分布情况" class="headerlink" title="查看导致数据倾斜的key的数据分布情况"></a>查看导致数据倾斜的key的数据分布情况</h2><p>知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD/Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。</p><p>此时根据你执行操作的情况不同，可以有很多种查看key分布的方式： 1. 如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。 2. 如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。</p><p>举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sampledPairs = pairs.sample(<span class="literal">false</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">val</span> sampledWordCounts = sampledPairs.countByKey()</span><br><span class="line">sampledWordCounts.foreach(println(_))</span><br></pre></td></tr></table></figure><h2 id="数据倾斜的解决方案"><a href="#数据倾斜的解决方案" class="headerlink" title="数据倾斜的解决方案"></a>数据倾斜的解决方案</h2><h3 id="解决方案一：使用Hive-ETL预处理数据"><a href="#解决方案一：使用Hive-ETL预处理数据" class="headerlink" title="解决方案一：使用Hive ETL预处理数据"></a>解决方案一：使用Hive ETL预处理数据</h3><p><strong>方案适用场景：</strong>导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。</p><p><strong>方案实现思路：</strong>此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。</p><p><strong>方案实现原理：</strong>这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。</p><p><strong>方案优点：</strong>实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p><p><strong>方案缺点：</strong>治标不治本，Hive ETL中还是会发生数据倾斜。</p><p><strong>方案实践经验：</strong>在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p><p><strong>项目实践经验：</strong>在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。</p><h3 id="解决方案二：过滤少数导致倾斜的key"><a href="#解决方案二：过滤少数导致倾斜的key" class="headerlink" title="解决方案二：过滤少数导致倾斜的key"></a>解决方案二：过滤少数导致倾斜的key</h3><p><strong>方案适用场景：</strong>如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p><p><strong>方案实现思路：</strong>如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。</p><p><strong>方案实现原理：</strong>将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。</p><p><strong>方案优点：</strong>实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p><p><strong>方案缺点：</strong>适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p><p><strong>方案实践经验：</strong>在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</p><h3 id="解决方案三：提高shuffle操作的并行度"><a href="#解决方案三：提高shuffle操作的并行度" class="headerlink" title="解决方案三：提高shuffle操作的并行度"></a>解决方案三：提高shuffle操作的并行度</h3><p><strong>方案适用场景：</strong>如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。</p><p><strong>方案实现思路：</strong>在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。</p><p><strong>方案实现原理：</strong>增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。</p><p><strong>方案优点：</strong>实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p><p><strong>方案缺点：</strong>只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p><p><strong>方案实践经验：</strong>该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</p><p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%984.png" alt=""></p><h3 id="解决方案四：两阶段聚合（局部聚合-全局聚合）"><a href="#解决方案四：两阶段聚合（局部聚合-全局聚合）" class="headerlink" title="解决方案四：两阶段聚合（局部聚合+全局聚合）"></a>解决方案四：两阶段聚合（局部聚合+全局聚合）</h3><p><strong>方案适用场景：</strong>对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。</p><p><strong>方案实现思路：</strong>这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。</p><p><strong>方案实现原理：</strong>将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p><p><strong>方案优点：</strong>对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p><p><strong>方案缺点：</strong>仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。</p><p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%985.png" alt=""></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 第一步，给RDD中的每个key都打上一个随机前缀。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt; randomPrefixRdd = rdd.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Long</span>&gt;, <span class="type">String</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="type">Random</span> random = <span class="keyword">new</span> <span class="type">Random</span>();</span><br><span class="line">                int prefix = random.nextInt(<span class="number">10</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt;(prefix + <span class="string">"_"</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 第二步，对打上随机前缀的key进行局部聚合。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt; localAggrRdd = randomPrefixRdd.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Function2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Long</span> call(<span class="type">Long</span> v1, <span class="type">Long</span> v2) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 第三步，去除RDD中每个key的随机前缀。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; removedRandomPrefixRdd = localAggrRdd.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>,<span class="type">Long</span>&gt;, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                long originalKey = <span class="type">Long</span>.valueOf(tuple._1.split(<span class="string">"_"</span>)[<span class="number">1</span>]);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt;(originalKey, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 第四步，对去除了随机前缀的RDD进行全局聚合。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; globalAggrRdd = removedRandomPrefixRdd.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Function2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Long</span> call(<span class="type">Long</span> v1, <span class="type">Long</span> v2) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><h3 id="解决方案五：将reduce-join转为map-join"><a href="#解决方案五：将reduce-join转为map-join" class="headerlink" title="解决方案五：将reduce join转为map join"></a>解决方案五：将reduce join转为map join</h3><p><strong>方案适用场景：</strong>在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p><p><strong>方案实现思路：</strong>不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</p><p><strong>方案实现原理：</strong>普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。</p><p><strong>方案优点：</strong>对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p><p><strong>方案缺点：</strong>适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</p><p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%986.png" alt=""></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 首先将数据量比较小的RDD的数据，collect到Driver中来。</span></span><br><span class="line"><span class="type">List</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt;&gt; rdd1Data = rdd1.collect()</span><br><span class="line"><span class="comment">// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。</span></span><br><span class="line"><span class="comment">// 可以尽可能节省内存空间，并且减少网络传输性能开销。</span></span><br><span class="line"><span class="keyword">final</span> <span class="type">Broadcast</span>&lt;<span class="type">List</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 对另外一个RDD执行map类操作，而不再是join类操作。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; joinedRdd = rdd2.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="comment">// 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。</span></span><br><span class="line">                <span class="type">List</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt;&gt; rdd1Data = rdd1DataBroadcast.value();</span><br><span class="line">                <span class="comment">// 可以将rdd1的数据转换为一个Map，便于后面进行join操作。</span></span><br><span class="line">                <span class="type">Map</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt; rdd1DataMap = <span class="keyword">new</span> <span class="type">HashMap</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt;();</span><br><span class="line">                <span class="keyword">for</span>(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt; data : rdd1Data) &#123;</span><br><span class="line">                    rdd1DataMap.put(data._1, data._2);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 获取当前RDD数据的key以及value。</span></span><br><span class="line">                <span class="type">String</span> key = tuple._1;</span><br><span class="line">                <span class="type">String</span> value = tuple._2;</span><br><span class="line">                <span class="comment">// 从rdd1数据Map中，根据key获取到可以join到的数据。</span></span><br><span class="line">                <span class="type">Row</span> rdd1Value = rdd1DataMap.get(key);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(key, <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;(value, rdd1Value));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 这里得提示一下。</span></span><br><span class="line"><span class="comment">// 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。</span></span><br><span class="line"><span class="comment">// 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。</span></span><br><span class="line"><span class="comment">// rdd2中每条数据都可能会返回多条join后的数据。</span></span><br></pre></td></tr></table></figure><h3 id="解决方案六：采样倾斜key并分拆join操作"><a href="#解决方案六：采样倾斜key并分拆join操作" class="headerlink" title="解决方案六：采样倾斜key并分拆join操作"></a>解决方案六：采样倾斜key并分拆join操作</h3><p><strong>方案适用场景：</strong>两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。</p><p><strong>方案实现思路：</strong> * 对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。 * 然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。 * 接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。 * 再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。 * 而另外两个普通的RDD就照常join即可。 * 最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。</p><p><strong>方案实现原理：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。</p><p><strong>方案优点：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。</p><p><strong>方案缺点：</strong>如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。</p><p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%987.png" alt=""></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 首先从包含了少数几个导致数据倾斜key的rdd1中，采样10%的样本数据。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; sampledRDD = rdd1.sample(<span class="literal">false</span>, <span class="number">0.1</span>);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 对样本数据RDD统计出每个key的出现次数，并按出现次数降序排序。</span></span><br><span class="line"><span class="comment">// 对降序排序后的数据，取出top 1或者top 100的数据，也就是key最多的前n个数据。</span></span><br><span class="line"><span class="comment">// 具体取出多少个数据量最多的key，由大家自己决定，我们这里就取1个作为示范。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; mappedSampledRDD = sampledRDD.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt;(tuple._1, <span class="number">1</span>L);</span><br><span class="line">            &#125;     </span><br><span class="line">        &#125;);</span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; countedSampledRDD = mappedSampledRDD.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Function2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Long</span> call(<span class="type">Long</span> v1, <span class="type">Long</span> v2) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; reversedSampledRDD = countedSampledRDD.mapToPair( </span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Long</span>&gt;, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt;(tuple._2, tuple._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"><span class="keyword">final</span> <span class="type">Long</span> skewedUserid = reversedSampledRDD.sortByKey(<span class="literal">false</span>).take(<span class="number">1</span>).get(<span class="number">0</span>)._2;</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 从rdd1中分拆出导致数据倾斜的key，形成独立的RDD。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; skewedRDD = rdd1.filter(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Function</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">Boolean</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Boolean</span> call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"><span class="comment">// 从rdd1中分拆出不导致数据倾斜的普通key，形成独立的RDD。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; commonRDD = rdd1.filter(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Function</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">Boolean</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Boolean</span> call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> !tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125; </span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// rdd2，就是那个所有key的分布相对较为均匀的rdd。</span></span><br><span class="line"><span class="comment">// 这里将rdd2中，前面获取到的key对应的数据，过滤出来，分拆成单独的rdd，并对rdd中的数据使用flatMap算子都扩容100倍。</span></span><br><span class="line"><span class="comment">// 对扩容的每条数据，都打上0～100的前缀。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt; skewedRdd2 = rdd2.filter(</span><br><span class="line">         <span class="keyword">new</span> <span class="type">Function</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Row</span>&gt;, <span class="type">Boolean</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Boolean</span> call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).flatMapToPair(<span class="keyword">new</span> <span class="type">PairFlatMapFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Row</span>&gt;, <span class="type">String</span>, <span class="type">Row</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Iterable</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; call(</span><br><span class="line">                    <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="type">Random</span> random = <span class="keyword">new</span> <span class="type">Random</span>();</span><br><span class="line">                <span class="type">List</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; list = <span class="keyword">new</span> <span class="type">ArrayList</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt;();</span><br><span class="line">                <span class="keyword">for</span>(int i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">                    list.add(<span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;(i + <span class="string">"_"</span> + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> list;</span><br><span class="line">            &#125;</span><br><span class="line">              </span><br><span class="line">        &#125;);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 将rdd1中分拆出来的导致倾斜的key的独立rdd，每条数据都打上100以内的随机前缀。</span></span><br><span class="line"><span class="comment">// 然后将这个rdd1中分拆出来的独立rdd，与上面rdd2中分拆出来的独立rdd，进行join。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; joinedRDD1 = skewedRDD.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">String</span>, <span class="type">String</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="type">Random</span> random = <span class="keyword">new</span> <span class="type">Random</span>();</span><br><span class="line">                int prefix = random.nextInt(<span class="number">100</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(prefix + <span class="string">"_"</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .join(skewedUserid2infoRDD)</span><br><span class="line">        .mapToPair(<span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>,<span class="type">Tuple2</span>&lt;<span class="type">String</span>,<span class="type">Row</span>&gt;&gt;, <span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt;() &#123;</span><br><span class="line">                        <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        public <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; call(</span><br><span class="line">                            <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; tuple)</span><br><span class="line">                            <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                            long key = <span class="type">Long</span>.valueOf(tuple._1.split(<span class="string">"_"</span>)[<span class="number">1</span>]);</span><br><span class="line">                            <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt;(key, tuple._2);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 将rdd1中分拆出来的包含普通key的独立rdd，直接与rdd2进行join。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; joinedRDD2 = commonRDD.join(rdd2);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 将倾斜key join后的结果与普通key join后的结果，uinon起来。</span></span><br><span class="line"><span class="comment">// 就是最终的join结果。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; joinedRDD = joinedRDD1.union(joinedRDD2);</span><br></pre></td></tr></table></figure><h3 id="解决方案七：使用随机前缀和扩容RDD进行join"><a href="#解决方案七：使用随机前缀和扩容RDD进行join" class="headerlink" title="解决方案七：使用随机前缀和扩容RDD进行join"></a>解决方案七：使用随机前缀和扩容RDD进行join</h3><p><strong>方案适用场景：</strong>如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。</p><p><strong>方案实现思路：</strong> * 该方案的实现思路基本和“解决方案六”类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。 * 然后将该RDD的每条数据都打上一个n以内的随机前缀。 * 同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。 * 最后将两个处理后的RDD进行join即可。</p><p><strong>方案实现原理：</strong>将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。</p><p><strong>方案优点：</strong>对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</p><p><strong>方案缺点：</strong>该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</p><p><strong>方案实践经验：</strong>曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt; expandedRDD = rdd1.flatMapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFlatMapFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Row</span>&gt;, <span class="type">String</span>, <span class="type">Row</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Iterable</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="type">List</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; list = <span class="keyword">new</span> <span class="type">ArrayList</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt;();</span><br><span class="line">                <span class="keyword">for</span>(int i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">                    list.add(<span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;(<span class="number">0</span> + <span class="string">"_"</span> + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> list;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; mappedRDD = rdd2.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">String</span>, <span class="type">String</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="type">Random</span> random = <span class="keyword">new</span> <span class="type">Random</span>();</span><br><span class="line">                int prefix = random.nextInt(<span class="number">100</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(prefix + <span class="string">"_"</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 将两个处理后的RDD进行join即可。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; joinedRDD = mappedRDD.join(expandedRDD);</span><br></pre></td></tr></table></figure><h3 id="解决方案八：多种方案组合使用"><a href="#解决方案八：多种方案组合使用" class="headerlink" title="解决方案八：多种方案组合使用"></a>解决方案八：多种方案组合使用</h3><p>在实践中发现，很多情况下，如果只是处理较为简单的数据倾斜场景，那么使用上述方案中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方案组合起来使用。比如说，我们针对出现了多个数据倾斜环节的Spark作业，可以先运用解决方案一和二，预处理一部分数据，并过滤一部分数据来缓解；其次可以对某些shuffle操作提升并行度，优化其性能；最后还可以针对不同的聚合或join操作，选择一种方案来优化其性能。大家需要对这些方案的思路和原理都透彻理解之后，在实践中根据各种不同的情况，灵活运用多种方案，来解决自己的数据倾斜问题。</p><hr><p>参考博客 : <a href="https://tech.meituan.com/2016/05/12/spark-tuning-pro.html" target="_blank" rel="noopener">https://tech.meituan.com/2016/05/12/spark-tuning-pro.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark性能优化指南之资源参数调优(转载)</title>
      <link href="/2019/10/28/spark/27/"/>
      <url>/2019/10/28/spark/27/</url>
      
        <content type="html"><![CDATA[<h2 id="调优概述"><a href="#调优概述" class="headerlink" title="调优概述"></a>调优概述</h2><p>在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。</p><h2 id="Spark作业基本运行原理"><a href="#Spark作业基本运行原理" class="headerlink" title="Spark作业基本运行原理"></a>Spark作业基本运行原理</h2><p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%981.png" alt=""></p><p>详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。</p><p>在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。</p><p>Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。</p><p>当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。</p><p>因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。</p><p>task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。</p><p>以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。</p><h2 id="资源参数调优"><a href="#资源参数调优" class="headerlink" title="资源参数调优"></a>资源参数调优</h2><p>了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。</p><h3 id="num-executors"><a href="#num-executors" class="headerlink" title="num-executors"></a>num-executors</h3><ul><li>参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</li><li>参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。</li></ul><h3 id="executor-memory"><a href="#executor-memory" class="headerlink" title="executor-memory"></a>executor-memory</h3><ul><li>参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</li><li>参数调优建议：每个Executor进程的内存设置4G<del>8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3</del>1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</li></ul><h3 id="executor-cores"><a href="#executor-cores" class="headerlink" title="executor-cores"></a>executor-cores</h3><ul><li>参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</li><li>参数调优建议：Executor的CPU core数量设置为2<del>4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3</del>1/2左右比较合适，也是避免影响其他同学的作业运行。</li></ul><h3 id="driver-memory"><a href="#driver-memory" class="headerlink" title="driver-memory"></a>driver-memory</h3><ul><li>参数说明：该参数用于设置Driver进程的内存。</li><li>参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。</li></ul><h3 id="spark-default-parallelism"><a href="#spark-default-parallelism" class="headerlink" title="spark.default.parallelism"></a>spark.default.parallelism</h3><ul><li>参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。</li><li>参数调优建议：Spark作业的默认task数量为500<del>1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2</del>3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。</li></ul><h3 id="spark-storage-memoryFraction"><a href="#spark-storage-memoryFraction" class="headerlink" title="spark.storage.memoryFraction"></a>spark.storage.memoryFraction</h3><ul><li>参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</li><li>参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><h3 id="spark-shuffle-memoryFraction"><a href="#spark-shuffle-memoryFraction" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.shuffle.memoryFraction</h3><ul><li>参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</li><li>参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><p>资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。</p><h2 id="资源参数参考示例"><a href="#资源参数参考示例" class="headerlink" title="资源参数参考示例"></a>资源参数参考示例</h2><p>以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --master yarn-cluster \</span><br><span class="line">  --num-executors 100 \</span><br><span class="line">  --executor-memory 6G \</span><br><span class="line">  --executor-cores 4 \</span><br><span class="line">  --driver-memory 1G \</span><br><span class="line">  --conf spark.default.parallelism=1000 \</span><br><span class="line">  --conf spark.storage.memoryFraction=0.5 \</span><br><span class="line">  --conf spark.shuffle.memoryFraction=0.3 \</span><br></pre></td></tr></table></figure><p>根据实践经验来看，大部分Spark作业经过本次基础篇所讲解的开发调优与资源调优之后，一般都能以较高的性能运行了，足以满足我们的需求。但是在不同的生产环境和项目背景下，可能会遇到其他更加棘手的问题（比如各种数据倾斜），也可能会遇到更高的性能要求。为了应对这些挑战，需要使用更高级的技巧来处理这类问题。在后续的《Spark性能优化指南——高级篇》中，我们会详细讲解数据倾斜调优以及Shuffle调优。</p><hr><p>参考博客: <a href="https://tech.meituan.com/2016/04/29/spark-tuning-basic.html" target="_blank" rel="noopener">https://tech.meituan.com/2016/04/29/spark-tuning-basic.html</a>    </p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark性能优化指南之开发调优(转载)</title>
      <link href="/2019/10/27/spark/14/"/>
      <url>/2019/10/27/spark/14/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>调优概述</li><li>原则一：避免创建重复的RDD</li><li>原则二：尽可能复用同一个RDD</li><li>原则三：对多次使用的RDD进行持久化</li><li>原则四：尽量避免使用shuffle类算子</li><li>原则五：使用map-side预聚合的shuffle操作</li><li>原则六：使用高性能的算子</li><li>原则七：广播大变量</li><li>原则八：使用Kryo优化序列化性能</li><li>原则九：优化数据结构</li><li>原则十：Data Locality本地化级别</li></ul><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。</p><p>然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。</p><p>Spark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。</p><p>笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。</p><p>本文作为Spark性能优化指南的基础篇，主要讲解开发调优以及资源调优。</p><h2 id="调优概述"><a href="#调优概述" class="headerlink" title="调优概述"></a>调优概述</h2><p>Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。</p><h2 id="原则一：避免创建重复的RDD"><a href="#原则一：避免创建重复的RDD" class="headerlink" title="原则一：避免创建重复的RDD"></a>原则一：避免创建重复的RDD</h2><p>通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。</p><p>我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。</p><p>一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。</p><p>一个简单的例子</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。</span></span><br><span class="line"><span class="comment">// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。</span></span><br><span class="line"><span class="comment">// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd2.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。</span></span><br><span class="line"><span class="comment">// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。</span></span><br><span class="line"><span class="comment">// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。</span></span><br><span class="line"><span class="comment">// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><h2 id="原则二：尽可能复用同一个RDD"><a href="#原则二：尽可能复用同一个RDD" class="headerlink" title="原则二：尽可能复用同一个RDD"></a>原则二：尽可能复用同一个RDD</h2><p>除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。</p><p>一个简单的例子</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 错误的做法。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。</span></span><br><span class="line"><span class="comment">// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line"><span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; rdd2 = rdd1.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分别对rdd1和rdd2执行了不同的算子操作。</span></span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd2.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。</span></span><br><span class="line"><span class="comment">// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 其实在这种情况下完全可以复用同一个RDD。</span></span><br><span class="line"><span class="comment">// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。</span></span><br><span class="line"><span class="comment">// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd1.map(tuple._2...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。</span></span><br><span class="line"><span class="comment">// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。</span></span><br><span class="line"><span class="comment">// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。</span></span><br></pre></td></tr></table></figure><h2 id="原则三：对多次使用的RDD进行持久化"><a href="#原则三：对多次使用的RDD进行持久化" class="headerlink" title="原则三：对多次使用的RDD进行持久化"></a>原则三：对多次使用的RDD进行持久化</h2><p>当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。</p><p>Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。</p><p>因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。</p><p>对多次使用的RDD进行持久化的代码示例</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"><span class="comment">// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。</span></span><br><span class="line"><span class="comment">// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。</span></span><br><span class="line"><span class="comment">// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).cache()</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。</span></span><br><span class="line"><span class="comment">// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。</span></span><br><span class="line"><span class="comment">// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。</span></span><br><span class="line"><span class="comment">// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><p>对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。</p><h3 id="Spark的持久化级别"><a href="#Spark的持久化级别" class="headerlink" title="Spark的持久化级别"></a>Spark的持久化级别</h3><table><thead><tr><th>持久化级别</th><th>含义解释</th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。</td></tr><tr><td>MEMORY_AND_DISK</td><td>使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</td></tr><tr><td>MEMORY_ONLY_SER</td><td>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>MEMORY_AND_DISK_SER</td><td>基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>DISK_ONLY</td><td>使用未序列化的Java对象格式，将数据全部写入磁盘文件中。</td></tr><tr><td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等.</td><td>对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。</td></tr></tbody></table><h3 id="如何选择一种最合适的持久化策略"><a href="#如何选择一种最合适的持久化策略" class="headerlink" title="如何选择一种最合适的持久化策略"></a>如何选择一种最合适的持久化策略</h3><ul><li>默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。</li><li>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</li><li>如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。</li><li>通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</li></ul><h2 id="原则四：尽量避免使用shuffle类算子"><a href="#原则四：尽量避免使用shuffle类算子" class="headerlink" title="原则四：尽量避免使用shuffle类算子"></a>原则四：尽量避免使用shuffle类算子</h2><p>如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。</p><p>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。</p><p>因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p><p>Broadcast与map进行join代码示例</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 传统的join操作会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Broadcast+map的join操作，不会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 使用Broadcast将一个数据量较小的RDD作为广播变量。</span></span><br><span class="line"><span class="keyword">val</span> rdd2Data = rdd2.collect()</span><br><span class="line"><span class="keyword">val</span> rdd2DataBroadcast = sc.broadcast(rdd2Data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span></span><br><span class="line"><span class="comment">// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。</span></span><br><span class="line"><span class="comment">// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.map(rdd2DataBroadcast...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span></span><br><span class="line"><span class="comment">// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span></span><br></pre></td></tr></table></figure><h2 id="原则五：使用map-side预聚合的shuffle操作"><a href="#原则五：使用map-side预聚合的shuffle操作" class="headerlink" title="原则五：使用map-side预聚合的shuffle操作"></a>原则五：使用map-side预聚合的shuffle操作</h2><p>如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。</p><p>所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。</p><p>比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</p><p><img src="https://yerias.github.io/spark_img/shuffle1.png" alt="shuffle1"></p><p><img src="https://yerias.github.io/spark_img/shuffle2.png" alt="shuffle2"></p><h2 id="原则六：使用高性能的算子"><a href="#原则六：使用高性能的算子" class="headerlink" title="原则六：使用高性能的算子"></a>原则六：使用高性能的算子</h2><p>除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。</p><h3 id="使用reduceByKey-aggregateByKey替代groupByKey"><a href="#使用reduceByKey-aggregateByKey替代groupByKey" class="headerlink" title="使用reduceByKey/aggregateByKey替代groupByKey"></a>使用reduceByKey/aggregateByKey替代groupByKey</h3><p>详情见“原则五：使用map-side预聚合的shuffle操作”。</p><h3 id="使用mapPartitions替代普通map"><a href="#使用mapPartitions替代普通map" class="headerlink" title="使用mapPartitions替代普通map"></a>使用mapPartitions替代普通map</h3><p>mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！</p><h3 id="使用foreachPartitions替代foreach"><a href="#使用foreachPartitions替代foreach" class="headerlink" title="使用foreachPartitions替代foreach"></a>使用foreachPartitions替代foreach</h3><p>原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。</p><h3 id="使用filter之后进行coalesce操作"><a href="#使用filter之后进行coalesce操作" class="headerlink" title="使用filter之后进行coalesce操作"></a>使用filter之后进行coalesce操作</h3><p>通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。</p><h3 id="使用repartitionAndSortWithinPartitions替代repartition与sort类操作"><a href="#使用repartitionAndSortWithinPartitions替代repartition与sort类操作" class="headerlink" title="使用repartitionAndSortWithinPartitions替代repartition与sort类操作"></a>使用repartitionAndSortWithinPartitions替代repartition与sort类操作</h3><p>repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。</p><h2 id="原则七：广播大变量"><a href="#原则七：广播大变量" class="headerlink" title="原则七：广播大变量"></a>原则七：广播大变量</h2><p>有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。</p><p>在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。</p><p>因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。</p><p>广播大变量的代码示例</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 以下代码在算子函数中，使用了外部的变量。</span></span><br><span class="line"><span class="comment">// 此时没有做任何特殊操作，每个task都会有一份list1的副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line">rdd1.map(list1...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下代码将list1封装成了Broadcast类型的广播变量。</span></span><br><span class="line"><span class="comment">// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。</span></span><br><span class="line"><span class="comment">// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。</span></span><br><span class="line"><span class="comment">// 每个Executor内存中，就只会驻留一份广播变量副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line"><span class="keyword">val</span> list1Broadcast = sc.broadcast(list1)</span><br><span class="line">rdd1.map(list1Broadcast...)</span><br></pre></td></tr></table></figure><h2 id="原则八：使用Kryo优化序列化性能"><a href="#原则八：使用Kryo优化序列化性能" class="headerlink" title="原则八：使用Kryo优化序列化性能"></a>原则八：使用Kryo优化序列化性能</h2><p>在Spark中，主要有三个地方涉及到了序列化：</p><ul><li>在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。</li><li>将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。</li><li>使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。</li></ul><p>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。</p><p>以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建SparkConf对象。</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class="line"><span class="comment">// 设置序列化器为KryoSerializer。</span></span><br><span class="line">conf.set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line"><span class="comment">// 注册要序列化的自定义类型。</span></span><br><span class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</span><br></pre></td></tr></table></figure><h2 id="原则九：优化数据结构"><a href="#原则九：优化数据结构" class="headerlink" title="原则九：优化数据结构"></a>原则九：优化数据结构</h2><p>Java中，有三种类型比较耗费内存：</p><ul><li>对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。</li><li>字符串，每个字符串内部都有一个字符数组以及长度等额外信息。</li><li>集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。</li></ul><p>因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。</p><p>但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。</p><h2 id="原则十：Data-Locality本地化级别"><a href="#原则十：Data-Locality本地化级别" class="headerlink" title="原则十：Data Locality本地化级别"></a>原则十：Data Locality本地化级别</h2><p><strong>PROCESS_LOCAL</strong>：进程本地化，代码和数据在同一个进程中，也就是在同一个executor中；计算数据的task由executor执行，数据在executor的BlockManager中；性能最好</p><p><strong>NODE_LOCAL</strong>：节点本地化，代码和数据在同一个节点中；比如说，数据作为一个HDFS block块，就在节点上，而task在节点上某个executor中运行；或者是，数据和task在一个节点上的不同executor中；数据需要在进程间进行传输<br><strong>NO_PREF</strong>：对于task来说，数据从哪里获取都一样，没有好坏之分<br><strong>RACK_LOCAL</strong>：机架本地化，数据和task在一个机架的两个节点上；数据需要通过网络在节点之间进行传输<br><strong>ANY</strong>：数据和task可能在集群中的任何地方，而且不在一个机架中，性能最差</p><p>spark.locality.wait，默认是3s</p><p>Spark在Driver上，对Application的每一个stage的task，进行分配之前，都会计算出每个task要计算的是哪个分片数据，RDD的某个partition；Spark的task分配算法，优先，会希望每个task正好分配到它要计算的数据所在的节点，这样的话，就不用在网络间传输数据；</p><p>但是可能task没有机会分配到它的数据所在的节点，因为可能那个节点的计算资源和计算能力都满了；所以呢，这种时候，通常来说，Spark会等待一段时间，默认情况下是3s钟（不是绝对的，还有很多种情况，对不同的本地化级别，都会去等待），到最后，实在是等待不了了，就会选择一个比较差的本地化级别，比如说，将task分配到靠它要计算的数据所在节点，比较近的一个节点，然后进行计算。</p><p>但是对于第二种情况，通常来说，肯定是要发生数据传输，task会通过其所在节点的BlockManager来获取数据，BlockManager发现自己本地没有数据，会通过一个getRemote()方法，通过TransferService（网络数据传输组件）从数据所在节点的BlockManager中，获取数据，通过网络传输回task所在节点。</p><p>对于我们来说，当然不希望是类似于第二种情况的了。最好的，当然是task和数据在一个节点上，直接从本地executor的BlockManager中获取数据，纯内存，或者带一点磁盘IO；如果要通过网络传输数据的话，那么实在是，性能肯定会下降的，大量网络传输，以及磁盘IO，都是性能的杀手。</p><p><strong>什么时候要调节这个参数？</strong><br>观察日志，spark作业的运行日志，推荐大家在测试的时候，先用client模式，在本地就直接可以看到比较全的日志。<br>日志里面会显示，starting task。。。，PROCESS LOCAL、NODE LOCAL，观察大部分task的数据本地化级别。</p><p>如果大多都是PROCESS_LOCAL，那就不用调节了<br>如果是发现，好多的级别都是NODE_LOCAL、ANY，那么最好就去调节一下数据本地化的等待时长<br>调节完，应该是要反复调节，每次调节完以后，再来运行，观察日志<br>看看大部分的task的本地化级别有没有提升；看看，整个spark作业的运行时间有没有缩短</p><p>但是注意别本末倒置，本地化级别倒是提升了，但是因为大量的等待时长，spark作业的运行时间反而增加了，那就还是不要调节了。</p><p>spark.locality.wait，默认是3s；可以改成6s，10s</p><p>默认情况下，下面3个的等待时长，都是跟上面那个是一样的，都是3s</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">spark.locality.wait.process<span class="comment">//建议60s</span></span><br><span class="line">spark.locality.wait.node<span class="comment">//建议30s</span></span><br><span class="line">spark.locality.wait.rack<span class="comment">//建议20s</span></span><br></pre></td></tr></table></figure><hr><p>参考博客: <a href="https://tech.meituan.com/spark-tuning-basic.html" target="_blank" rel="noopener">https://tech.meituan.com/spark-tuning-basic.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark性能优化指南</title>
      <link href="/2019/10/26/spark/26/"/>
      <url>/2019/10/26/spark/26/</url>
      
        <content type="html"><![CDATA[<p><img src="https://yerias.github.io/spark_img/%E8%B0%83%E4%BC%98.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark各个版本特性</title>
      <link href="/2019/10/25/spark/25/"/>
      <url>/2019/10/25/spark/25/</url>
      
        <content type="html"><![CDATA[<p>参考博客：<a href="https://www.maxinhong.com/2020/04/03/68.spark各个版本特性/#more" target="_blank" rel="noopener">https://www.maxinhong.com/2020/04/03/68.spark%E5%90%84%E4%B8%AA%E7%89%88%E6%9C%AC%E7%89%B9%E6%80%A7/#more</a></p><hr><h1 id="各个版本特性（官方文档）"><a href="#各个版本特性（官方文档）" class="headerlink" title="各个版本特性（官方文档）"></a>各个版本特性（官方文档）</h1><p><a href="https://spark.apache.org/releases/" target="_blank" rel="noopener">https://spark.apache.org/releases/</a><br><a href="https://spark.apache.org/news/index.html" target="_blank" rel="noopener">https://spark.apache.org/news/index.html</a></p><h2 id="Spark-0-6-x"><a href="#Spark-0-6-x" class="headerlink" title="Spark 0.6.x"></a>Spark 0.6.x</h2><ul><li>Standalone部署模式进行了简化</li></ul><h2 id="Spark-0-7"><a href="#Spark-0-7" class="headerlink" title="Spark 0.7"></a>Spark 0.7</h2><ul><li>Python API</li><li>增加Spark Streaming</li><li>支持maven build</li></ul><h2 id="Spark-0-8"><a href="#Spark-0-8" class="headerlink" title="Spark 0.8"></a>Spark 0.8</h2><ul><li>支持MLlib库</li><li>hadoop yarn正式支持</li></ul><h2 id="Spark-0-9"><a href="#Spark-0-9" class="headerlink" title="Spark 0.9"></a>Spark 0.9</h2><ul><li>用SparkConf类来配置SparkContext</li><li>spark streaming正式版发布</li><li>GraphX的测试版出现</li><li>mllib库升级，支持python</li><li>core升级</li></ul><h2 id="Spark-1-0"><a href="#Spark-1-0" class="headerlink" title="Spark 1.0"></a>Spark 1.0</h2><ul><li>提出spark-submit脚本和history-server</li><li>yarn安全模式整合</li><li>spark sql被提出</li><li>java8的支持</li></ul><h2 id="Spark-1-1"><a href="#Spark-1-1" class="headerlink" title="Spark 1.1"></a>Spark 1.1</h2><ul><li>spark增强了磁盘（非内存）的排序的速率</li></ul><h2 id="Spark-1-2"><a href="#Spark-1-2" class="headerlink" title="Spark 1.2"></a>Spark 1.2</h2><ul><li>shuffle大升级</li><li>Graphx正式版发布</li></ul><h2 id="Spark-1-3"><a href="#Spark-1-3" class="headerlink" title="Spark 1.3"></a>Spark 1.3</h2><ul><li>新增DataFrame API</li><li>Spark SQL正式脱离alpha版本</li></ul><h2 id="Spark-1-4"><a href="#Spark-1-4" class="headerlink" title="Spark 1.4"></a>Spark 1.4</h2><ul><li>正式引入SparkR</li><li>Spark Core为应用提供了REST API来获取各种信息</li></ul><h2 id="Spark-1-5"><a href="#Spark-1-5" class="headerlink" title="Spark 1.5"></a>Spark 1.5</h2><ul><li>Spark1.5重点是对性能的提升，引入钨丝项目，该项目通过对几个底层框架的重构进一步优化Spark性能</li></ul><h2 id="Spark-1-6"><a href="#Spark-1-6" class="headerlink" title="Spark 1.6"></a>Spark 1.6</h2><ul><li>新增Dataset API</li></ul><h2 id="Spark-2-0"><a href="#Spark-2-0" class="headerlink" title="Spark 2.0"></a>Spark 2.0</h2><ul><li>用sparksession实现hivecontext和sqlcontext统一</li><li>合并dataframe和datasets</li></ul><h2 id="Spark-2-1"><a href="#Spark-2-1" class="headerlink" title="Spark 2.1"></a>Spark 2.1</h2><ul><li>提升ORC格式文件的读写性能</li></ul><h2 id="Spark-2-2"><a href="#Spark-2-2" class="headerlink" title="Spark 2.2"></a>Spark 2.2</h2><ul><li>Structured Streaming的生产环境支持已经就绪</li></ul><h2 id="Spark-2-3"><a href="#Spark-2-3" class="headerlink" title="Spark 2.3"></a>Spark 2.3</h2><ul><li>Structured Streaming 引入了低延迟的连续处理</li><li>支持 stream-to-stream joins</li></ul><h2 id="Spark-2-4"><a href="#Spark-2-4" class="headerlink" title="Spark 2.4"></a>Spark 2.4</h2><ul><li>Scala 2.12</li><li>添加了35个高阶函数</li></ul>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SS+Kafka提交服务器&amp;窗口函数&amp;SS调优</title>
      <link href="/2019/10/24/spark/24/"/>
      <url>/2019/10/24/spark/24/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>SS+Kafka提交服务器</li><li>窗口函数</li><li>SS调优</li></ol><h2 id="SS-Kafka提交服务器"><a href="#SS-Kafka提交服务器" class="headerlink" title="SS+Kafka提交服务器"></a>SS+Kafka提交服务器</h2><p>由于Spark自身没有spark-streaming-kafka的依赖，所以Spark Streaming+Kafka的Application跑在服务器上需要添加spark-streaming-kafka的依赖，共有三种添加依赖的方式</p><ol><li>直接在IDEA中打胖包，但是服务器上有的东西需要标识为privated，不然依赖重复了，这种方式不推介</li><li>提交Application的时候使用–packages参数，格式为: <code>groupId:artifactId:version</code>，这种方式需要在有网络的情况下才能使用</li><li>使用–jars 传入依赖，推介，这里有个技巧，可以将需要的 jar 包放在固定目录下，需要传入依赖的时候只需要使用 $(echo /home/hadoop/lib/*.jar | tr ‘ ‘ ‘,’)即可以将目录下的 jar 包全都拼接上去</li></ol><p>简单的WC案例</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">wc_ss_kafka</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> groupId:<span class="type">String</span> = _</span><br><span class="line">  <span class="keyword">var</span> topic:<span class="type">String</span> = _</span><br><span class="line">  <span class="keyword">var</span> brokers:<span class="type">String</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (args.length&lt;<span class="number">3</span>)&#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Usage: com.tunan.spark.streming.kafka.wc.wc_ss_kafka &lt;groupId&gt; &lt;topic&gt; &lt;brokers&gt;"</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    groupId = args(<span class="number">0</span>)</span><br><span class="line">    topic = args(<span class="number">1</span>)</span><br><span class="line">    brokers = args(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="string">"bootstrap.servers"</span> -&gt; brokers,</span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">"group.id"</span> -&gt; groupId,</span><br><span class="line">      <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"earliest"</span>, <span class="comment">//latest</span></span><br><span class="line">      <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(topic)</span><br><span class="line">    <span class="keyword">val</span> stream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    stream.map(x=&gt;(x.value(),<span class="number">1</span>)).reduceByKey(_+_).foreachRDD(rdd =&gt;&#123;</span><br><span class="line">      rdd.foreach(println)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>作业提交</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">  --class com.tunan.spark.streming.kafka.wc.wc_ss_kafka \</span><br><span class="line">  --master yarn \</span><br><span class="line">  --deploy-mode client \</span><br><span class="line">  --jars $(echo /home/hadoop/lib/*.jar | tr ' ' ',') \</span><br><span class="line">  /home/hadoop/jar/tunan-spark-streaming-kafka-1.0.jar \</span><br><span class="line">  wc_group_id_for_each_stream test hadoop:9090,hadoop:9091,hadoop:9092</span><br></pre></td></tr></table></figure><p>查看结果<img src="https://yerias.github.io/spark_img/ss_kafka_submit.png" alt=""></p><h2 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h2><p>前面有介绍窗口函数，现在我们来看一下如何使用</p><p>在工作中常常有这样的需求:</p><ol><li>每隔5秒钟统计前10秒钟的数据</li><li>每隔1分钟统计前05分钟的数据</li></ol><p>这类每隔多少统计前多少时间的操作就时窗口操作</p><p>我们以一个例子来说明窗口操作。 对之前的单词计数的示例进行扩展，每10秒钟对过去30秒的数据进行wordcount。为此，我们必须在最近30秒的DStream数据中对键值对应用<code>reduceByKey</code>操作。这是通过使用<code>reduceByKeyAndWindow</code>操作完成的。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 每隔10秒统计前30秒的数据</span></span><br><span class="line">stream.map(x=&gt;(x.value(),<span class="number">1</span>)).reduceByKeyAndWindow((a:<span class="type">Int</span>,b:<span class="type">Int</span>)=&gt;a+b,<span class="type">Seconds</span>(<span class="number">30</span>),<span class="type">Seconds</span>(<span class="number">10</span>))</span><br><span class="line">.foreachRDD(rdd =&gt;&#123;</span><br><span class="line">    rdd.foreach(println)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>一些常见的窗口操作如下表所示。所有这些操作都用到了上述两个参数：<code>windowLength</code>和<code>slideInterval</code>。</p><ul><li><p><code>window(windowLength, slideInterval)</code></p><p>基于源DStream产生的窗口化的批数据计算一个新的DStream</p></li><li><p><code>countByWindow(windowLength, slideInterval)</code></p><p>返回流中元素的一个滑动窗口数</p></li><li><p><code>reduceByWindow(func, windowLength, slideInterval)</code></p><p>返回一个单元素流。利用函数func聚集滑动时间间隔的流的元素创建这个单元素流。函数必须是相关联的以使计算能够正确的并行计算。</p></li><li><p><code>reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])</code></p><p>应用到一个(K,V)对组成的DStream上，返回一个由(K,V)对组成的新的DStream。每一个key的值均由给定的reduce函数聚集起来。注意：在默认情况下，这个算子利用了Spark默认的并发任务数去分组。你可以用numTasks参数设置不同的任务数</p></li><li><p><code>reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])</code></p><p>上述<code>reduceByKeyAndWindow()</code>的更高效的版本，其中使用前一窗口的reduce计算结果递增地计算每个窗口的reduce值。这是通过对进入滑动窗口的新数据进行reduce操作，以及“逆减（inverse reducing）”离开窗口的旧数据来完成的。一个例子是当窗口滑动时对键对应的值进行“一加一减”操作。但是，它仅适用于“可逆减函数（invertible reduce functions）”，即具有相应“反减”功能的减函数（作为参数invFunc）。 像<code>reduceByKeyAndWindow</code>一样，通过可选参数可以配置reduce任务的数量。 请注意，使用此操作必须启用检查点。</p></li><li><p><code>countByValueAndWindow(windowLength, slideInterval, [numTasks])</code></p><p>应用到一个(K,V)对组成的DStream上，返回一个由(K,V)对组成的新的DStream。每个key的值都是它们在滑动窗口中出现的频率。</p></li></ul><h2 id="SS调优"><a href="#SS调优" class="headerlink" title="SS调优"></a>SS调优</h2><p>通过spark-shell运行Spark Streaming + Kafka程序，查看Web UI界面</p><p><img src="https://yerias.github.io/spark_img/Web_UI_2.jpg" alt=""></p><p>我们需要作业的性能最高，那么需要一个最佳时间</p><ol><li>在下一个批次启动作业之前一定要运行完前一个批次数据的处理</li><li>batch time: 根据需求来定的</li></ol><p>影响任务运行时长的要素：</p><ol><li>数据规模(增加kafka分区数==&gt;增加Spark分区==&gt;增加task)</li><li>batch time</li><li>业务复杂度</li></ol><p>kafka限速</p><p><img src="https://yerias.github.io/spark_img/Web_UI_1.jpg" alt=""></p><p>我们看到在设置auto.offset.reset = earliest后，即从头消费，如果累积的数据量特别大，那么在第一次消费的就会撑爆Kafka，必须限制每秒多少条数据</p><table><thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th></tr></thead><tbody><tr><td><code>spark.streaming.kafka.maxRatePerPartition</code></td><td>not set</td><td>每个Kafka分区读取数据的最大速率</td></tr></tbody></table><p>在设置maxRatePerPartition的值时，数据量=设置的值*分区数*读取时间，加入设置的值为10,分区为3，读取时间为10s，那么每次出来的数据量: 10*3*10=300</p><p><strong>优点</strong></p><ol><li>如果有很多数据量没有处理，并且从头开始，为了防止过载</li><li>高峰期限速，防止Kafka处理能力不够挂掉</li></ol><p><strong>缺点</strong></p><ol><li>是个固定值 ==&gt; 背压(backpressure 1.5版本引入)</li></ol><p>背压(backpressure )，在Spark1.5引入，它可以在运行时根据前一个批次数据的运行情况动态调整后续批次读入的数据量</p><p>打开参数：spark.streaming.backpressure.enabled </p><p>上限参数：spark.streaming.kafka.maxRatePerPartition</p><p>初始参数：spark.streaming.backpressure.initialRate</p><table><thead><tr><th align="left">Property Name</th><th align="left">Default</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><code>spark.streaming.backpressure.enabled</code></td><td align="left">false</td><td align="left">使Spark流能够根据当前的批调度延迟和处理时间来控制接收速率，从而使系统接收的速度只取决于系统能够处理的速度。</td></tr><tr><td align="left"><code>spark.streaming.kafka.maxRatePerPartition</code></td><td align="left">not set</td><td align="left">每个Kafka分区读取数据的最大速率</td></tr><tr><td align="left"><code>spark.streaming.backpressure.initialRate</code></td><td align="left">not set</td><td align="left">启用背压机制时每个接收器接收第一批数据的初始最大接收速率</td></tr></tbody></table><p>到此，Kafka数据量过载的问题完全解决</p><p>最后引入一个关于StreamingContext关闭时的参数</p><table><thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th></tr></thead><tbody><tr><td><code>spark.streaming.stopGracefullyOnShutdown</code></td><td>false</td><td>如果是“true”，Spark会在JVM关闭时优雅地关闭“StreamingContext”，而不是立即关闭。</td></tr></tbody></table><p>其他调优可参考官网或者<a href="https://blog.csdn.net/Johnson8702/article/details/88944368" target="_blank" rel="noopener">博客</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 读写压缩文件的一次简单尝试</title>
      <link href="/2019/10/23/spark/23/"/>
      <url>/2019/10/23/spark/23/</url>
      
        <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>我认为以节省存储空间为角度出发，Spark作业中的读写压缩文件是必不可少的话题，当然这在MR作业中也有体现和实际解决这种问题，现在我们就要在Spark中解决这种问题。</p><p>如果需要安装Lzo可以看我的其他<a href="https://yerias.github.io/2018/10/15/hadoop/13/">文章</a></p><p>源文件是一份access的原始数据</p><p><img src="https://yerias.github.io/spark_img/access.log.png" alt=""></p><p>我们在上传到服务器上的时候，使用lzop命令压缩该文件，得到压缩后的文件，上传HDFSF并对该文件创建索引，得到我们代码中需要处理的源文件</p><p><img src="https://yerias.github.io/spark_img/compress-1.png" alt=""></p><p>需要注意的是未压缩的文件是1G，压缩后为314M</p><p>下图是我们想要做的事情</p><p><img src="https://yerias.github.io/spark_img/format%E8%AF%BB%E5%86%99.jpg" alt=""></p><p><strong>各种压缩格式的性能比较</strong></p><table><thead><tr><th>压缩格式</th><th>工具</th><th>算法</th><th>扩展名</th><th>是否支持分割</th><th>Hadoop编码/解码器</th><th>hadoop自带</th></tr></thead><tbody><tr><td>DEFLATE</td><td>N/A</td><td>DEFLATE</td><td>.deflate</td><td>No</td><td>org.apache.hadoop.io.compress.DefalutCodec</td><td>是</td></tr><tr><td>gzip</td><td>gzip</td><td>DEFLATE</td><td>.gz</td><td>No</td><td>org.apache.hadoop.io.compress.GzipCodec</td><td>是</td></tr><tr><td>bzip2</td><td>bzip2</td><td>bzip2</td><td>.bz2</td><td>yes</td><td>org.apache.hadoop.io.compress.Bzip2Codec</td><td>是</td></tr><tr><td>LZO</td><td>Lzop</td><td>LZO</td><td>.lzo</td><td>yes(建索引)</td><td>com.hadoop.compression.lzo.LzoCodec</td><td>是</td></tr><tr><td>LZ4</td><td>N/A</td><td>LZ4</td><td>.lz4</td><td>No</td><td>org.apache.hadoop.io.compress.Lz4Codec</td><td>否</td></tr><tr><td>Snappy</td><td>N/A</td><td>Snappy</td><td>.snappy</td><td>No</td><td>org.apache.hadoop.io.compress.SnappyCodec</td><td>否</td></tr></tbody></table><p>压缩比：Snappy&lt;LZ4&lt;LZO&lt;GZIP&lt;BZIP2</p><p><strong>各种压缩格式的性能优缺点</strong></p><table><thead><tr><th>压缩格式</th><th>特点</th></tr></thead><tbody><tr><td>lzo</td><td>优点：压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便<br/>缺点：不支持split</td></tr><tr><td>snappy</td><td>优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便<br/>缺点：压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）</td></tr><tr><td>gzip</td><td>优点：压缩速度快；支持hadoop native库<br/>缺点：不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令</td></tr><tr><td>bzip2</td><td>优点：支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便<br/>缺点：压缩/解压速度慢；不支持native</td></tr></tbody></table><h3 id="读LZO文件写HDFS"><a href="#读LZO文件写HDFS" class="headerlink" title="读LZO文件写HDFS"></a>读LZO文件写HDFS</h3><p>需要注意的是在Spark中读取HDFS上的压缩文件，需要使用newAPIHadoopFile接口，并且传入LzoTextInputFormat，这个依赖不好解决，仓库是twitter的，下载不了，最好的办法是去github下载源码，install到本地</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.hadoop.gplcompression<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-lzo<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.4.20<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>读取文件的具体代码</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> linesRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.newAPIHadoopFile(in, classOf[<span class="type">LzoTextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>])  .map(x =&gt; x._2.toString)</span><br></pre></td></tr></table></figure><p>现在我们已经读进来了，可以实现我们的业务逻辑，那么最后在Spark Core中如何写出去呢? 只需要使用saveAsTextFile</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">etl(linesRDD,hashMap)  .filter(x =&gt; x.responseSize != <span class="number">0</span>).saveAsTextFile(out)</span><br></pre></td></tr></table></figure><h3 id="读LZO文件写HDFS也为LZO文件"><a href="#读LZO文件写HDFS也为LZO文件" class="headerlink" title="读LZO文件写HDFS也为LZO文件"></a>读LZO文件写HDFS也为LZO文件</h3><p>读LZO文件上面的方法同样适用，而写为LZO文件只需要在saveAsTextFile中指定LzopCodec即可，适用于所有的格式压缩</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">etl(linesRDD,hashMap)  .filter(x =&gt; x.responseSize != <span class="number">0</span>).saveAsTextFile(out,classOf[<span class="type">LzopCodec</span>])</span><br></pre></td></tr></table></figure><h3 id="读LZO文件写MYSQL"><a href="#读LZO文件写MYSQL" class="headerlink" title="读LZO文件写MYSQL"></a>读LZO文件写MYSQL</h3><p>读写表和读写文件不同，读写表更适合用Spark SQL来实现，现在我们的表的字段非常多，不适用于使用tuple的实现方式，从而自定义外部数据源，在TableScan中也是实现与上面相同代码，就能将lzo文件分片读写</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">    logError(<span class="string">"进入buildScan方法"</span>)</span><br><span class="line">    <span class="comment">// 使用RDD拿到Lzo本文内容</span></span><br><span class="line">    <span class="keyword">val</span> lines = sqlContext.sparkContext.newAPIHadoopFile(path, classOf[<span class="type">LzoTextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>])</span><br><span class="line">    .map(_._2.toString)</span><br><span class="line">    <span class="comment">// 拿到响应的schema信息</span></span><br><span class="line">    <span class="keyword">val</span> fields = schema.fields</span><br><span class="line">    <span class="comment">// 拿到每行数据，做简单处理，返回RDD[Row]</span></span><br><span class="line">    lines.map(_.split(<span class="string">","</span>).map(_.trim)).map(_.zipWithIndex.map &#123;</span><br><span class="line">        <span class="keyword">case</span> (value, index) =&gt;</span><br><span class="line">        <span class="type">Utils</span>.caseTo(value, fields(index).dataType)</span><br><span class="line">    &#125;).map(x =&gt; <span class="type">Row</span>.fromSeq(x))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我在这里尝试了压缩写和普通写，都可以查询到数据</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">        properties.put(<span class="string">"user"</span>,<span class="string">"root"</span>)</span><br><span class="line">        properties.put(<span class="string">"password"</span>,<span class="string">"root"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> url = <span class="string">"jdbc:mysql://hadoop:3306/tunan?useUnicode=true&amp;characterEncoding=utf-8"</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!flat)&#123;</span><br><span class="line">            <span class="comment">// 保存到MySQL</span></span><br><span class="line">            result.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">              .jdbc(url,<span class="string">"domain_group"</span>,properties)</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="comment">// 保存到MySQL,压缩格式为lzo</span></span><br><span class="line">            result.write.format(<span class="string">"orc"</span>).option(<span class="string">"compression"</span>,<span class="string">"lzo"</span>).mode(<span class="string">"overwrite"</span>)</span><br><span class="line">              .jdbc(url,<span class="string">"domain_group"</span>,properties)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>查询：</p><p><img src="https://yerias.github.io/spark_img/compress-2.png" alt=""></p><h3 id="读LZO文件写HIVE"><a href="#读LZO文件写HIVE" class="headerlink" title="读LZO文件写HIVE"></a>读LZO文件写HIVE</h3><p>读的方式也是一样的，使用外部数据源的方式直接得到DataFrame，在写的时候指定了存储格式，而不指定压缩格式的话，会默认指定压缩格式为Snappy</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">textDf.write.format(<span class="string">"parquet"</span>).mode(<span class="string">"overwrite"</span>).saveAsTable(<span class="string">"store_format.parquet_tb"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://yerias.github.io/spark_img/compress-3.png" alt=""></p><h3 id="读LZO文件压缩写HIVE"><a href="#读LZO文件压缩写HIVE" class="headerlink" title="读LZO文件压缩写HIVE"></a>读LZO文件压缩写HIVE</h3><p>读Lzo的方式如上，在这里我们尝试了几种格式的压缩写</p><ol><li><p>指定了存储格式为parquet，压缩格式为lzo</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">textDf.write.format(<span class="string">"parquet"</span>).option(<span class="string">"compression"</span>,<span class="string">"lzo"</span>).mode(<span class="string">"overwrite"</span>).saveAsTable(<span class="string">"store_format.parquet_tb"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://yerias.github.io/spark_img/compress-4.png" alt=""></p><p>可以看得到压缩格式和存储格式发生了变化，我们继续查表</p><p><img src="https://yerias.github.io/spark_img/compress-5.png" alt=""></p></li><li><p>现在我们将压缩格式改为orc，存储格式还是使用lzo</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">textDf.write.format(<span class="string">"orc"</span>).option(<span class="string">"compression"</span>,<span class="string">"lzo"</span>).mode(<span class="string">"overwrite"</span>).saveAsTable(<span class="string">"store_format.parquet_tb"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://yerias.github.io/spark_img/compress-6.png" alt=""></p><p>我们发现orc和lzo格式并不能一起使用，查出来的是无效的结果</p></li><li><p>继续使用压缩格式为orc，存储格式改为snappy</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">textDf.write.format(<span class="string">"orc"</span>).option(<span class="string">"compression"</span>,<span class="string">"snappy"</span>).mode(<span class="string">"overwrite"</span>).saveAsTable(<span class="string">"store_format.parquet_tb"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://yerias.github.io/spark_img/compress-7.png" alt=""></p><p>查询结果：</p><p><img src="https://yerias.github.io/spark_img/compress-5.png" alt=""></p></li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li>使用Lzo压缩的压缩比相比于Snappy较高</li><li>无论哪种压缩格式在MySQL中都是无效的</li><li>Parquet可以和Snappy或者Lzo搭配使用，ORC可以和Snappy或者Bzip搭配使用</li><li>ORC不能和Lzo搭配使用</li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka Offset管理</title>
      <link href="/2019/10/22/spark/22/"/>
      <url>/2019/10/22/spark/22/</url>
      
        <content type="html"><![CDATA[<p>Kafka中的每个partition都由一系列有序的、不可变的消息组成，这些消息被连续的追加到partition中。partition中的每个消息都有一个连续的序号，用于partition唯一标识一条消息。</p><p><strong>Offset记录着下一条将要发送给Consumer的消息的序号。</strong></p><p>Offset从语义上来看拥有两种：Current Offset和Committed Offset。</p><h2 id="Current-Offset"><a href="#Current-Offset" class="headerlink" title="Current Offset"></a>Current Offset</h2><p>Current Offset保存在Consumer客户端中，它表示Consumer希望收到的下一条消息的序号。它仅仅在pull()方法中使用。例如，Consumer第一次调用pull()方法后收到了20条消息，那么Current Offset就被设置为20。这样Consumer下一次调用pull()方法时，Kafka就知道应该从序号为21的消息开始读取。这样就能够保证每次Consumer pull消息时，都能够收到不重复的消息。</p><h2 id="Committed-Offset"><a href="#Committed-Offset" class="headerlink" title="Committed Offset"></a>Committed Offset</h2><p>Committed Offset保存在Broker上，它表示Consumer已经确认消费过的消息的序号。主要通过<code>commitSync</code>和<code>commitAsync</code> API来操作。举个例子，Consumer通过pull() 方法收到20条消息后，此时Current Offset就是20，经过一系列的逻辑处理后，并没有调用<code>consumer.commitAsync()</code>或<code>consumer.commitSync()</code>来提交Committed Offset，那么此时Committed Offset依旧是0。</p><p>Committed Offset主要用于Consumer Rebalance。在Consumer Rebalance的过程中，一个partition被分配给了一个Consumer，那么这个Consumer该从什么位置开始消费消息呢？答案就是Committed Offset。另外，如果一个Consumer消费了5条消息（pull并且成功commitSync）之后宕机了，重新启动之后它仍然能够从第6条消息开始消费，因为Committed Offset已经被Kafka记录为5。</p><p>总结一下，Current Offset是针对Consumer的pull过程的，它可以保证每次pull都返回不重复的消息；而Committed Offset是用于Consumer Rebalance过程的，它能够保证新的Consumer能够从正确的位置开始消费一个partition，从而避免重复消费。</p><h2 id="Offset存储模型"><a href="#Offset存储模型" class="headerlink" title="Offset存储模型"></a>Offset存储模型</h2><p>由于一个partition只能固定的交给一个消费者组中的一个消费者消费，因此Kafka保存offset时并不直接为每个消费者保存，而是以groupid-topic-partition -&gt; offset的方式保存。如图所示：</p><p><img src="https://yerias.github.io/spark_img/group-topic-partition-offset.jpg" alt="group-topic-partition-offset"></p><p><strong>Kafka在保存Offset的时候，实际上是将Consumer Group和partition对应的offset以消息的方式保存在__consumers_offsets这个topic中</strong>。</p><p>下图展示了__consumers_offsets中保存的offset消息的格式：</p><p><img src="https://yerias.github.io/spark_img/consumers_offsets.jpg" alt=""></p><p><img src="https://yerias.github.io/spark_img/consumers_offsets_data.jpg" alt=""></p><p>如图所示，一条offset消息的格式为groupid-topic-partition -&gt; offset。因此consumer pull消息时，已知groupid和topic，又通过Coordinator分配partition的方式获得了对应的partition，自然能够通过Coordinator查找__consumers_offsets的方式获得最新的offset了。</p><h2 id="Kafka-Offset-的几种管理方式"><a href="#Kafka-Offset-的几种管理方式" class="headerlink" title="Kafka Offset 的几种管理方式"></a>Kafka Offset 的几种管理方式</h2><ul><li><code>Spark Checkpoint：</code>在 Spark Streaming 执行Checkpoint 操作时，将 Kafka Offset 一并保存到 HDFS 中。这种方式的问题在于：当 Spark Streaming 应用升级或更新时，以及当Spark 本身更新时，Checkpoint 可能无法恢复。因而，不推荐采用这种方式。</li><li><code>HBASE、Redis 等外部 NOSQL 数据库：</code>这一方式可以支持大吞吐量的 Offset 更新，但它最大的问题在于：用户需要自行编写 HBASE 或 Redis 的读写程序，并且需要维护一个额外的组件。</li><li><code>ZOOKEEPER：</code>老版本的位移offset是提交到zookeeper中的，目录结构是 ：/consumers/&lt;group.id&gt;/offsets/ &lt;topic&gt;/&lt;partitionId&gt; ，但是由于 ZOOKEEPER 的写入能力并不会随着 ZOOKEEPER 节点数量的增加而扩大，因而，当存在频繁的 Offset 更新时，ZOOKEEPER 集群本身可能成为瓶颈。因而，不推荐采用这种方式。</li><li><code>KAFKA 自身的一个特殊 Topic（__consumer_offsets）中</code>：这种方式支持大吞吐量的Offset 更新，又不需要手动编写 Offset 管理程序或者维护一套额外的集群，但是Kafka不支持事物，不能保证输出的幂等性(Exactly once)。</li></ul><p><img src="https://yerias.github.io/spark_img/offsets-manager.png" alt="offsets-manager"></p><h2 id="Redis实现Offset幂等性消费"><a href="#Redis实现Offset幂等性消费" class="headerlink" title="Redis实现Offset幂等性消费"></a>Redis实现Offset幂等性消费</h2><h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><p>Spark Streaming实时消费kafka数据的时候，程序停止或者Kafka节点挂掉会导致数据丢失，Spark Streaming也没有设置CheckPoint(节点挂了，数据容易丢失)，所以每次出现问题的时候，重启程序，而程序的消费方式是Direct，所以在程序down掉的这段时间Kafka上的数据是消费不到的，虽然可以设置offset为smallest，但是会导致重复消费，重新overwrite hive上的数据，但是不允许重复消费的场景就不能这样做。</p><h3 id="原理阐述"><a href="#原理阐述" class="headerlink" title="原理阐述"></a>原理阐述</h3><p>在Spark Streaming中消费 Kafka 数据的时候，有两种方式分别是 ：</p><ol><li><p>基于 Receiver-based 的 createStream 方法。receiver从Kafka中获取的数据都是存储在Spark Executor的内存中的，然后Spark Streaming启动的job会去处理那些数据。然而，在默认的配置下，这种方式可能会因为底层的失败而丢失数据。如果要启用高可靠机制，让数据零丢失，就必须启用Spark Streaming的<strong>预写日志机制</strong>（Write Ahead Log，WAL）。该机制会同步地将接收到的Kafka数据写入分布式文件系统（比如HDFS）上的预写日志中。所以，即使底层节点出现了失败，也可以使用预写日志中的数据进行恢复。本文对此方式不研究，有兴趣的可以自己实现，个人不喜欢这个方式。</p></li><li><p>Direct Approach (No Receivers) 方式的 createDirectStream 方法，但是第二种使用方式中  kafka 的 offset 是保存在 checkpoint 中的，如果程序重启的话，会丢失一部分数据，我使用的是这种方式。KafkaUtils.createDirectStream。本文将用代码说明如何将 kafka 中的 offset 保存到 Redis 中，以及如何从 Redis 中读取已存在的 offset。参数auto.offset.reset为latest的时候程序才会读取redis的offset。</p></li></ol><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>OffsetsManager trait</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">OffsetsManager</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">obtainOffsets</span></span>(topics: <span class="type">Array</span>[<span class="type">String</span>], groupId: <span class="type">String</span>): <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">storeOffsets</span></span>(offsetRanges: <span class="type">Array</span>[<span class="type">OffsetRange</span>], groupId: <span class="type">String</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>RedisOffsetsManager object</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RedisOffsetsManager</span> <span class="keyword">extends</span> <span class="title">OffsetsManager</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 拿到offset</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">obtainOffsets</span></span>(topics: <span class="type">Array</span>[<span class="type">String</span>], groupId: <span class="type">String</span>): <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>] = &#123;</span><br><span class="line">        <span class="comment">// 定义fromOffsets、jedis、offsetMap</span></span><br><span class="line">        <span class="keyword">var</span> fromOffsets: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>] = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">var</span> jedis: <span class="type">Jedis</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">var</span> offsetMap: util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 初始化jedis</span></span><br><span class="line">            jedis = <span class="type">RedisUtils</span>.getJedis</span><br><span class="line">            <span class="comment">// 拿到offsetMap</span></span><br><span class="line">            offsetMap = jedis.hgetAll(topics(<span class="number">0</span>) + <span class="string">"_"</span> + groupId)</span><br><span class="line">            <span class="comment">// 导入java集合转scala的依赖</span></span><br><span class="line">            <span class="keyword">import</span> scala.collection.<span class="type">JavaConverters</span>._</span><br><span class="line">            offsetMap.asScala.foreach(row =&gt; &#123;</span><br><span class="line">                <span class="comment">// 拿到topicPartition</span></span><br><span class="line">                <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(topics(<span class="number">0</span>), row._1.toInt)</span><br><span class="line">                <span class="comment">// 将topicPartition和offset放入fromOffsets</span></span><br><span class="line">                fromOffsets += topicPartition -&gt; row._2.toLong</span><br><span class="line">            &#125;)</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e:<span class="type">Exception</span> =&gt; e.printStackTrace()</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">// 关闭jedis</span></span><br><span class="line">            jedis.close()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 返回fromOffsets</span></span><br><span class="line">        fromOffsets</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// TODO 存储offset</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">storeOffsets</span></span>(offsetRanges: <span class="type">Array</span>[<span class="type">OffsetRange</span>], groupId: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 定义jedis</span></span><br><span class="line">        <span class="keyword">var</span> jedis:<span class="type">Jedis</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 初始化jedis</span></span><br><span class="line">            jedis = <span class="type">RedisUtils</span>.getJedis</span><br><span class="line">            offsetRanges.foreach(o  =&gt; &#123;</span><br><span class="line">                <span class="comment">// 存储offset</span></span><br><span class="line">                jedis.hset(o.topic + <span class="string">"_"</span> + groupId, o.partition + <span class="string">""</span>, o.untilOffset + <span class="string">""</span>)</span><br><span class="line">            &#125;)</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e:<span class="type">Exception</span> =&gt; e.printStackTrace()</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">// 关闭jedis0</span></span><br><span class="line"></span><br><span class="line">            jedis.close()</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里offsets保存在Redis，如果是MySQL同理，只需要实现obtainOffsets和storeOffsets方法</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MySQLOffsetsManager</span> <span class="keyword">extends</span> <span class="title">OffsetsManager</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">obtainOffsets</span></span>(topics: <span class="type">Array</span>[<span class="type">String</span>], groupId: <span class="type">String</span>): <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>] = ???</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">storeOffsets</span></span>(offsetRanges: <span class="type">Array</span>[<span class="type">OffsetRange</span>], groupId: <span class="type">String</span>): <span class="type">Unit</span> = ???</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>OffsetsRedis</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">OffsetsRedis</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 消费者组</span></span><br><span class="line">        <span class="keyword">val</span> groupId = <span class="string">"use_a_separate_group_id_for_each_stream_3"</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">            <span class="string">"bootstrap.servers"</span> -&gt; <span class="string">"hadoop:9090,hadoop:9091,hadoop:9092"</span>,</span><br><span class="line">            <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">            <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">            <span class="string">"group.id"</span> -&gt; groupId,</span><br><span class="line">            <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"earliest"</span>, <span class="comment">//latest</span></span><br><span class="line">            <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 拿到topic</span></span><br><span class="line">        <span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">"test"</span>) <span class="comment">// topic</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// TODO 拿到偏移量</span></span><br><span class="line">        <span class="keyword">val</span> fromOffsets = <span class="type">RedisOffsetsManager</span>.obtainOffsets(topics,groupId) <span class="comment">//Map[TopicPartition, Long]()</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> stream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">            ssc,</span><br><span class="line">            <span class="type">PreferConsistent</span>,</span><br><span class="line">            <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams, fromOffsets)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        stream.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (!rdd.isEmpty()) &#123;</span><br><span class="line">                <span class="comment">// 拿到offsetRanges 包括topic、partition、fromOffset、untilOffset</span></span><br><span class="line">                <span class="keyword">val</span> offsetRanges: <span class="type">Array</span>[<span class="type">OffsetRange</span>] = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 已经拿到了offset，可以开始业务逻辑处理</span></span><br><span class="line">                <span class="keyword">val</span> result = rdd.map(row =&gt; &#123;</span><br><span class="line">                    (row.value(), <span class="number">1</span>)</span><br><span class="line">                &#125;).reduceByKey(_ + _)</span><br><span class="line">                .collect()</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 事物写入redis</span></span><br><span class="line">                <span class="keyword">var</span> jedis: <span class="type">Jedis</span> = <span class="literal">null</span></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    jedis = <span class="type">RedisUtils</span>.getJedis</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 提交业务逻辑</span></span><br><span class="line">                    <span class="keyword">for</span> (pair &lt;- result) &#123;</span><br><span class="line">                        jedis.hincrBy(<span class="string">"wc_redis_ss"</span>, pair._1, pair._2)</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 写offset</span></span><br><span class="line">                    <span class="type">RedisOffsetsManager</span>.storeOffsets(offsetRanges,groupId)</span><br><span class="line"></span><br><span class="line">                &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                    <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">                    e.printStackTrace()</span><br><span class="line">                &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                    jedis.close()</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                println(<span class="string">s"拉取数据中..."</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">// 开始作业</span></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="exactly-once方案"><a href="#exactly-once方案" class="headerlink" title="exactly once方案"></a>exactly once方案</h3><p>准确的说也不是严格的方案，要根据实际的业务场景来配合。</p><p>现在的方案是保存rdd的最后一个offset，我们可以考虑在处理完一个消息之后就更新offset，保存offset和业务处理做成一个事务，当出现Exception的时候，都进行回退，或者将出现问题的offset和消息发送到另一个kafka或者保存到数据库，另行处理错误的消息。代码demo如下</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">stream.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">    <span class="keyword">if</span> (!rdd.isEmpty()) &#123;</span><br><span class="line">        <span class="comment">// 拿到offsetRanges 包括topic、partition、fromOffset、untilOffset</span></span><br><span class="line">        <span class="keyword">val</span> offsetRanges: <span class="type">Array</span>[<span class="type">OffsetRange</span>] = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 已经拿到了offset，可以开始业务逻辑处理</span></span><br><span class="line">        <span class="keyword">val</span> result = rdd.map(row =&gt; &#123;</span><br><span class="line">            (row.value(), <span class="number">1</span>)</span><br><span class="line">        &#125;).reduceByKey(_ + _)</span><br><span class="line">        .collect()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 事物写入redis</span></span><br><span class="line">        <span class="keyword">var</span> pipeline: <span class="type">Pipeline</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">var</span> jedis: <span class="type">Jedis</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            jedis = <span class="type">RedisUtils</span>.getJedis</span><br><span class="line">            pipeline = jedis.pipelined()</span><br><span class="line">            pipeline.multi()</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 提交业务逻辑</span></span><br><span class="line">            <span class="keyword">for</span> (pair &lt;- result) &#123;</span><br><span class="line">                pipeline.hincrBy(<span class="string">"wc_redis_ss"</span>, pair._1, pair._2)</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 写offset</span></span><br><span class="line">            offsetRanges.foreach(o =&gt; &#123;</span><br><span class="line">                pipeline.hset(topics(<span class="number">0</span>) + <span class="string">"_"</span> + groupId, o.partition + <span class="string">""</span>, o.untilOffset + <span class="string">""</span>)</span><br><span class="line">            &#125;)</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 提交&amp;同步</span></span><br><span class="line">            pipeline.exec()</span><br><span class="line">            pipeline.sync()</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">            <span class="comment">// 失败回滚</span></span><br><span class="line">            pipeline.discard()</span><br><span class="line">            e.printStackTrace()</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">// 关闭连接</span></span><br><span class="line">            pipeline.close()</span><br><span class="line">            jedis.close()</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        println(<span class="string">s"拉取数据中..."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p><code>注意：</code>pipeline不能在Redis集群中使用，但是适用于主从架构</p><h2 id="语义"><a href="#语义" class="headerlink" title="语义"></a>语义</h2><p>数据流系统的语义通常根据系统可以处理每个记录的次数来捕获。在所有可能的操作条件下(除了故障等)，系统可以提供三种类型的保证。</p><ul><li>At most once: 每个记录要么处理一次，要么根本不处理。</li><li>At least once:  每个记录将被处理一次或多次。这比最多一次强，因为它确保不会丢失任何数据。但是可能有重复的。</li><li>Exactly once: 每条记录将被精确处理一次——没有数据会丢失，也没有数据会被多次处理。这显然是三者中最有力的保证。</li></ul><h2 id="auto-offset-reset参数"><a href="#auto-offset-reset参数" class="headerlink" title="auto.offset.reset参数"></a>auto.offset.reset参数</h2><p>Kafka 默认是定期帮你自动提交位移的（enable.auto.commit=true）。有时候，我们需要采用自己来管理位移提交，这时候需要设置 enable.auto.commit=false。</p><ul><li><p>earliest: 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费</p></li><li><p>latest: 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据</p></li><li><p>none: topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常</p></li></ul><p>默认建议用earliest。设置该参数后 kafka出错后重启，找到未消费的offset可以继续消费。</p><p>而latest 这个设置容易丢失消息，假如kafka出现问题，还有数据往topic中写，这个时候重启kafka，这个设置会从最新的offset开始消费,中间出问题的哪些就不管了。 </p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SS黑名单管理&amp;窗口&amp;闭包&amp;SS对接Kafka&amp;KafkaRDD</title>
      <link href="/2019/10/21/spark/21/"/>
      <url>/2019/10/21/spark/21/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>黑名单管理</li><li>窗口</li><li>闭包</li><li>SS对接Kafka</li><li>KafkaRDD</li></ol><h2 id="黑名单管理"><a href="#黑名单管理" class="headerlink" title="黑名单管理"></a>黑名单管理</h2><p>Spark Streaming在计算流式数据时，有时候需要过滤一些数据，比如一些特殊的字段，或者利用爬虫爬取数据的恶意ip，又或者那些帮助某些无良商家刷广告的人，那么我们有一个黑名单，来过滤或者禁止他们的访问</p><p>思路：</p><ol><li>准备一个管理黑名单的文件，读进RDD作为key，并添加一个value值为true</li><li>Spark Streaming接收流式数据，使用transform转换为RDD，拿到key用来做join，value为数据内容</li><li>两个RDD做left join，并过滤掉value值为true的数据</li></ol><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取黑名单数据，并做简单处理</span></span><br><span class="line">    <span class="keyword">val</span> blacks = <span class="type">List</span>(<span class="string">"tunan"</span>)</span><br><span class="line">    <span class="keyword">val</span> blackRDD = ssc.sparkContext.parallelize(blacks)</span><br><span class="line">    <span class="keyword">val</span> blackMapRDD = blackRDD.map((_,<span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从socket拿到流式数据</span></span><br><span class="line">    <span class="keyword">val</span> stream = ssc.socketTextStream(<span class="string">"hadoop"</span>, <span class="number">9100</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 数据转换成RDD</span></span><br><span class="line">    stream.transform( rdd =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> mapRDD= rdd.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.split(<span class="string">","</span>)</span><br><span class="line">            <span class="comment">// 拿到做join的key，value为数据内容</span></span><br><span class="line">            (words(<span class="number">0</span>), row)</span><br><span class="line">        &#125;)</span><br><span class="line"><span class="comment">// 流式数据为基表，做left join</span></span><br><span class="line">        <span class="keyword">val</span> joinRDD = mapRDD.leftOuterJoin(blackMapRDD)</span><br><span class="line"><span class="comment">// 过滤掉黑名单数据</span></span><br><span class="line">        <span class="keyword">val</span> filterRDD = joinRDD.filter(_._2._2.getOrElse(<span class="literal">false</span>) != <span class="literal">true</span>)</span><br><span class="line">        <span class="comment">// 返回数据内容</span></span><br><span class="line">        filterRDD.map(_._2._1)</span><br><span class="line">    &#125;).print()</span><br><span class="line">    </span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="窗口-了解，Flink详解"><a href="#窗口-了解，Flink详解" class="headerlink" title="窗口(了解，Flink详解)"></a>窗口(了解，Flink详解)</h2><p>Spark Streaming还提供了窗口计算功能，允许在数据的滑动窗口上应用转换操作。下图说明了滑动窗口的工作方式：<img src="E:%5Chexo%5Cyeriasblog%5Cthemes%5Cmelody%5Csource%5Cspark_img%5Cstreaming-dstream-window.png" alt="streaming-dstream-window"></p><p>如图所示，每当窗口滑过<code>originalDStream</code>时，落在窗口内的源RDD被组合并被执行操作以产生<code>windowedDStream</code>的RDD。在上面的例子中，操作应用于最近3个时间单位的数据，并以2个时间单位滑动。这表明任何窗口操作都需要指定两个参数：</p><ul><li><p>窗口长度（windowlength） - 窗口的时间长度（上图的示例中为：3）。</p></li><li><p>滑动间隔（slidinginterval） - 两次相邻的窗口操作的间隔（即每次滑动的时间长度）（上图示例中为：2）。</p><p>这两个参数必须是源DStream的批间隔的倍数（上图示例中为：1）。</p></li></ul><p>我们以一个例子来说明窗口操作。 对之前的单词计数的示例进行扩展，每10秒钟对过去30秒的数据进行wordcount。为此，我们必须在最近30秒的pairs DStream数据中对(word, 1)键值对应用<code>reduceByKey</code>操作。这是通过使用<code>reduceByKeyAndWindow</code>操作完成的。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 执行wordcount</span></span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> wordPair = words.map(x =&gt; (x, <span class="number">1</span>))</span><br><span class="line"><span class="comment">//val wordCountResult = wordPair.reduceByKey(_ + _)</span></span><br><span class="line"><span class="keyword">val</span> wordCountResult = wordPair.reduceByKeyAndWindow(</span><br><span class="line">    (a: <span class="type">Int</span>, b: <span class="type">Int</span>) =&gt; (a + b), <span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>一些常见的窗口操作如下表所示。所有这些操作都用到了上述两个参数：<code>windowLength</code>和<code>slideInterval</code>。</p><ul><li><p><code>window(windowLength, slideInterval)</code></p><p>基于源DStream产生的窗口化的批数据计算一个新的DStream</p></li><li><p><code>countByWindow(windowLength, slideInterval)</code></p><p>返回流中元素的一个滑动窗口数</p></li><li><p><code>reduceByWindow(func, windowLength, slideInterval)</code></p><p>返回一个单元素流。利用函数func聚集滑动时间间隔的流的元素创建这个单元素流。函数必须是相关联的以使计算能够正确的并行计算。</p></li><li><p><code>reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])</code></p><p>应用到一个(K,V)对组成的DStream上，返回一个由(K,V)对组成的新的DStream。每一个key的值均由给定的reduce函数聚集起来。注意：在默认情况下，这个算子利用了Spark默认的并发任务数去分组。你可以用numTasks参数设置不同的任务数</p></li><li><p><code>reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])</code></p><p>上述<code>reduceByKeyAndWindow()</code>的更高效的版本，其中使用前一窗口的reduce计算结果递增地计算每个窗口的reduce值。这是通过对进入滑动窗口的新数据进行reduce操作，以及“逆减（inverse reducing）”离开窗口的旧数据来完成的。一个例子是当窗口滑动时对键对应的值进行“一加一减”操作。但是，它仅适用于“可逆减函数（invertible reduce functions）”，即具有相应“反减”功能的减函数（作为参数invFunc）。 像<code>reduceByKeyAndWindow</code>一样，通过可选参数可以配置reduce任务的数量。 请注意，使用此操作必须启用检查点。</p></li><li><p><code>countByValueAndWindow(windowLength, slideInterval, [numTasks])</code></p><p>应用到一个(K,V)对组成的DStream上，返回一个由(K,V)对组成的新的DStream。每个key的值都是它们在滑动窗口中出现的频率。</p></li></ul><h2 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h2><p>Spark的难点之一是理解<strong>跨集群执行代码时变量和方法的范围和生命周期</strong>。在范围之外修改变量的RDD操作可能经常引起混淆。在下面的示例中，我们将查看使用foreach()递增计数器的代码，但是其他操作也可能出现类似的问题。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> counter = <span class="number">0</span></span><br><span class="line"><span class="keyword">var</span> rdd = sc.parallelize(data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Wrong: Don't do this!!</span></span><br><span class="line">rdd.foreach(x =&gt; counter += x)</span><br><span class="line"></span><br><span class="line">println(<span class="string">"Counter value: "</span> + counter)</span><br></pre></td></tr></table></figure><p>上述代码的行为是未定义的，可能无法按预期工作。为了执行作业，Spark将RDD操作的处理分解为tasks，每个任务由executor执行。在执行之前，Spark计算task的闭包。闭包是那些executor在RDD上执行其计算时必须可见的变量和方法(在本例中为foreach())。这个闭包被序列化并发送给每个executor 。</p><p>闭包中发送给每个executor 的变量现在都是副本，因此，当在foreach函数中引用counter时，它不再是driver 上的计数器。在executors的内存中仍然有一个计数器，但它对executor不再可见!executor只看到来自序列化闭包的副本。因此，counter的最终值仍然是零，因为<strong>counter上的所有操作都引用了序列化闭包中的值</strong>。</p><p>一般来说，像循环或局部定义方法这样的闭包结构不应该用来改变全局状态。Spark不保证闭包外部引用的对象的突变行为。一些这样做的代码可能在本地模式下工作，但那只是偶然的，而且这样的代码在分布式模式下不会像预期的那样工作。如果需要全局聚合，则使用Accumulator。</p><h2 id="SS对接Kafka"><a href="#SS对接Kafka" class="headerlink" title="SS对接Kafka"></a>SS对接Kafka</h2><p>Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者在网站中的所有动作流数据。 这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。 这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。 对于像Hadoop一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka的目的是通过Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消息。</p><p>SS是Spark上的一个流式处理框架，可以面向海量数据实现高吞吐量、高容错的实时计算。SS支持多种类型数据源，包括Kafka、Flume、twitter、zeroMQ、Kinesis以及TCP sockets等。SS实时接收数据流，并按照一定的时间间隔将连续的数据流拆分成一批批离散的数据集；然后应用诸如map、reduce、join和window等丰富的API进行复杂的数据处理；最后提交给Spark引擎进行运算，得到批量结果数据，因此其也被称为准实时处理系统。而结果也能保存在很多地方，如HDFS，数据库等。另外SS也能和MLlib（机器学习）以及GraphX（图计算）完美融合。</p><p>下面我们就来一个SS对接Kafka的案例(Kafka的研究看我的其他博客)</p><p>Kafka Product API</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 设置配置文件，这些配置文件都是源码中找的</span></span><br><span class="line">    <span class="keyword">val</span> props = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop:9090,hadoop:9091,hadoop:9092"</span>);</span><br><span class="line">    props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line">    props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">    props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">    <span class="comment">// 创建producer</span></span><br><span class="line">    <span class="keyword">val</span> producer = <span class="keyword">new</span> <span class="type">KafkaProducer</span>[<span class="type">String</span>, <span class="type">String</span>](props)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 循环发送数据</span></span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">100</span>)&#123;</span><br><span class="line">        <span class="keyword">val</span> par = i%<span class="number">3</span> <span class="comment">// 数组走的分区</span></span><br><span class="line">        producer.send(<span class="keyword">new</span> <span class="type">ProducerRecord</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="string">"test"</span>,par,<span class="string">""</span>,<span class="type">Integer</span>.toString(i)));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 关闭producer</span></span><br><span class="line">    producer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Spark Streaming  Consumer</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建ssc</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 连接kafka配置参数</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">        <span class="string">"bootstrap.servers"</span> -&gt; <span class="string">"hadoop:9090,hadoop:9091,hadoop:9092"</span>,</span><br><span class="line">        <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">        <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">        <span class="string">"group.id"</span> -&gt; <span class="string">"use_a_separate_group_id_for_each_stream"</span>,</span><br><span class="line">        <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"earliest"</span>, <span class="comment">//latest</span></span><br><span class="line">        <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">// 可以设置多个topic</span></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">"test"</span>)</span><br><span class="line">    <span class="comment">// 创建DirectStream</span></span><br><span class="line">    <span class="keyword">val</span> stream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">        ssc,</span><br><span class="line">        <span class="type">PreferConsistent</span>,</span><br><span class="line">        <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">// 业务逻辑</span></span><br><span class="line">    stream.map(x =&gt; (x.value(),<span class="number">1</span>)).reduceByKey(_+_)</span><br><span class="line">    .foreachRDD(rdd =&gt; &#123;</span><br><span class="line">        <span class="comment">// 分区操作</span></span><br><span class="line">        rdd.foreachPartition(partition =&gt; &#123;</span><br><span class="line">            <span class="comment">// 一个分区连一个jedis</span></span><br><span class="line">            <span class="keyword">val</span> jedis = <span class="type">RedisUtils</span>.getJedis</span><br><span class="line">            partition.foreach(fields =&gt;&#123;</span><br><span class="line">                <span class="comment">// 保存到Hash中</span></span><br><span class="line">                jedis.hincrBy(<span class="string">"wc_redis"</span>,fields._1,fields._2)</span><br><span class="line">            &#125;)</span><br><span class="line">            <span class="comment">// 关闭连接</span></span><br><span class="line">            jedis.close()</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">// 启动程序</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>现在，无论在哪里写进Kafka的数据，都可以从Spark Streaming的客户端写出来，我们这里保存的是Redis，保存在MySQL是同样的思路。</p><h2 id="KafkaRDD"><a href="#KafkaRDD" class="headerlink" title="KafkaRDD"></a>KafkaRDD</h2><p>最后我们看一下如何在代码中拿到Kafka的Offset</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">        <span class="string">"bootstrap.servers"</span> -&gt; <span class="string">"hadoop:9090,hadoop:9091,hadoop:9092"</span>,</span><br><span class="line">        <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">        <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">        <span class="string">"group.id"</span> -&gt; <span class="string">"use_a_separate_group_id_for_each_stream"</span>,</span><br><span class="line">        <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"earliest"</span>, <span class="comment">//latest</span></span><br><span class="line">        <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">"test"</span>)</span><br><span class="line">    <span class="comment">// stream不能做任何操作，否则得到的不是一个KafkaRDD</span></span><br><span class="line">    <span class="keyword">val</span> stream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">        ssc,</span><br><span class="line">        <span class="type">PreferConsistent</span>,</span><br><span class="line">        <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 必须先拿到HasOffsetRanges，才能开始业务逻辑</span></span><br><span class="line">    stream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">        <span class="comment">// 通过rdd.asInstanceOf[HasOffsetRanges]拿到KafkaRDD，它保存了每个分区的offset</span></span><br><span class="line">        <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">        <span class="comment">// KafkaRDD维护了topic、partition、fromOffset、untilOffset</span></span><br><span class="line">        offsetRanges.foreach &#123; o =&gt;</span><br><span class="line">            println(<span class="string">s"<span class="subst">$&#123;o.topic&#125;</span> <span class="subst">$&#123;o.partition&#125;</span> <span class="subst">$&#123;o.fromOffset&#125;</span> <span class="subst">$&#123;o.untilOffset&#125;</span>"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 启动程序</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>注意：</strong></p><ol><li>对HasOffsetRanges的类型转换只有在对createDirectStream的结果调用的第一个方法中完成时才会成功，而不是在随后的方法链中。</li><li>RDD分区和Kafka分区之间的一一映射会在RDD发生shuffle或者repartition操作之后改变，比如reduceByKey或window</li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Streaming简介&amp;Spark Streaming的内部结构&amp;StreamingContext对象&amp;离散流（DStream）&amp;IDEA开发Spark Streaming</title>
      <link href="/2019/10/20/spark/20/"/>
      <url>/2019/10/20/spark/20/</url>
      
        <content type="html"><![CDATA[<hr><p>参考博客: <a href="https://vinxikk.github.io/2018/05/29/spark/" target="_blank" rel="noopener">https://vinxikk.github.io/2018/05/29/spark/</a></p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>Spark Streaming简介</li><li>Spark Streaming的内部结构</li><li>StreamingContext对象</li><li>离散流（DStream）</li><li>IDEA开发Spark Streaming</li></ol><h2 id="Spark-Streaming简介"><a href="#Spark-Streaming简介" class="headerlink" title="Spark Streaming简介"></a>Spark Streaming简介</h2><p>Spark Streaming是核心Spark API的扩展，实现<strong>可扩展</strong>、<strong>高吞吐量</strong>、<strong>可容错的实时数据流处理</strong>。数据可以从诸如Kafka，Flume，Kinesis或TCP套接字等众多来源获取，并且可以使用由高级函数（如map，reduce，join和window）开发的复杂算法进行流数据处理。最后，处理后的数据可以被推送到文件系统，数据库和实时仪表板。而且还可以在数据流上应用Spark提供的机器学习和图处理算法。</p><p><img src="https://yerias.github.io/spark_img/what-is-spark-streaming.png" alt="what-is-spark-streaming"></p><h2 id="Spark-Streaming的内部结构"><a href="#Spark-Streaming的内部结构" class="headerlink" title="Spark Streaming的内部结构"></a>Spark Streaming的内部结构</h2><p>在内部，它的工作原理如下。Spark Streaming接收实时输入数据流，并将数据切分成批，然后由Spark引擎对其进行处理，最后生成“批”形式的结果流。</p><p><img src="https://yerias.github.io/spark_img/streaming-batch-process.png" alt="streaming-batch-process"></p><p>Spark Streaming将连续的数据流抽象为discretizedstream(DStream)。在内部，DStream由一个RDD序列表示。</p><h2 id="StreamingContext对象"><a href="#StreamingContext对象" class="headerlink" title="StreamingContext对象"></a>StreamingContext对象</h2><p>初始化<code>StreamingContext</code>：</p><p>方式一，从<code>SparkConf</code>对象中创建：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建一个Context对象：StreamingContext</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"MyNetworkWordCount"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line"><span class="comment">//指定批处理的时间间隔</span></span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br></pre></td></tr></table></figure><p>方式二，从现有的<code>SparkContext</code>实例中创建：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>说明：</p><ul><li>appName参数是应用程序在集群UI上显示的名称。</li><li>master是Spark，Mesos或YARN集群的URL，或者一个特殊的<code>“local [*]”</code>字符串来让程序以本地模式运行。</li><li>当在集群上运行程序时，不需要在程序中硬编码master参数，而是使用spark-submit提交应用程序并将master的URL以脚本参数的形式传入。但是，对于本地测试和单元测试，您可以通过<code>“local[*]”</code>来运行Spark Streaming程序（请确保本地系统中的cpu核心数够用）。</li><li><code>StreamingContext</code>会内在的创建一个<code>SparkContext</code>的实例（所有Spark功能的起始点），你可以通过<code>ssc.sparkContext</code>访问到这个实例。</li><li>批处理的时间窗口长度必须根据应用程序的延迟要求和可用的集群资源进行设置。</li></ul><p>注意：</p><ul><li>一旦一个<code>StreamingContext</code>开始运作，就不能设置或添加新的流计算。</li><li>一旦一个上下文被停止，它将无法重新启动。</li><li>同一时刻，一个JVM中只能有一个<code>StreamingContext</code>处于活动状态。</li><li><code>StreamingContext</code>上的<code>stop()</code>方法也会停止<code>SparkContext</code>。 要仅停止<code>StreamingContext</code>（保持<code>SparkContext</code>活跃），请将<code>stop()</code> 方法的可选参数<code>stopSparkContext</code>设置为<code>false</code>。</li><li>只要前一个<code>StreamingContext</code>在下一个<code>StreamingContext</code>被创建之前停止（不停止<code>SparkContext</code>），<code>SparkContext</code>就可以被重用来创建多个<code>StreamingContext</code>。</li></ul><h2 id="离散流（DStream）"><a href="#离散流（DStream）" class="headerlink" title="离散流（DStream）"></a>离散流（DStream）</h2><p><code>DiscretizedStream</code>(<code>DStream</code>) 是Spark Streaming对流式数据的基本抽象。它表示连续的数据流，这些连续的数据流可以是从数据源接收的输入数据流，也可以是通过对输入数据流执行转换操作而生成的经处理的数据流。在内部，<code>DStream</code>由一系列连续的RDD表示，如下图：</p><p><img src="https://yerias.github.io/spark_img/streaming-dstream-1.png" alt="streaming-dstream-1"></p><p>我们将一行行文本组成的流转换为单词流，具体做法为：将<code>flatMap</code>操作应用于名为lines的 DStream中的每个RDD上，以生成words DStream的RDD。如下图所示：</p><p><img src="https://yerias.github.io/spark_img/streaming-dstream-2.png" alt="streaming-dstream-2"></p><p>但是DStream和RDD也有区别，下面画图说明：</p><p><img src="https://yerias.github.io/spark_img/streaming-dstream-3.png" alt="streaming-dstream-3"></p><p><img src="https://yerias.github.io/spark_img/streaming-dstream-4.png" alt="streaming-dstream-4"></p><h2 id="IDEA开发Spark-Streaming"><a href="#IDEA开发Spark-Streaming" class="headerlink" title="IDEA开发Spark Streaming"></a>IDEA开发Spark Streaming</h2><p>要编写自己的Spark流程序，必须将以下依赖项添加到Maven项目中。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-streaming_2<span class="number">.12</span>&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;<span class="number">2.4</span><span class="number">.5</span>&lt;/version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="SocketFile简单的单词计数"><a href="#SocketFile简单的单词计数" class="headerlink" title="SocketFile简单的单词计数"></a>SocketFile简单的单词计数</h3><ol><li><p>实现代scala代码逻辑</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 拿到StreamingContext对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    dispose(ssc)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启StreamingContext</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">dispose</span></span>(ssc: <span class="type">StreamingContext</span>) = &#123;</span><br><span class="line">    <span class="comment">//输入记录</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"hadoop"</span>, <span class="number">9100</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//逻辑处理</span></span><br><span class="line">    <span class="keyword">val</span> words = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> pair = words.map(x =&gt; (x, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> result = pair.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//输出记录</span></span><br><span class="line">    result.print()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>使用nc发送消息</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">$ nc -lk <span class="number">9100</span></span><br><span class="line"></span><br><span class="line">hello world</span><br></pre></td></tr></table></figure></li><li><p>客户端接收消息</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">-------------------------------------------</span><br><span class="line"><span class="type">Time</span>: <span class="number">1357008430000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(hello,<span class="number">1</span>)</span><br><span class="line">(world,<span class="number">1</span>)</span><br><span class="line">...</span><br></pre></td></tr></table></figure></li></ol><p>DStreams 是表示从源端接收的输入数据的数据流。在这个简单的示例中，行是一个输入DStream，因为它表示从netcat服务器接收到的数据流。每个输入DStream(本节后面讨论的文件流除外)都与接收方(Scala doc、Java doc)对象相关联，接收方接收来自源的数据并将其存储在Spark内存中进行处理。</p><p><code>注意</code>：Spark流应用程序需要分配足够的Core来处理接收到的数据，以及运行接收方。设置core的数量要大于Receivers的数量。</p><h3 id="Checkpoint维护State"><a href="#Checkpoint维护State" class="headerlink" title="Checkpoint维护State"></a>Checkpoint维护State</h3><p>什么是updateStateByKey?</p><ul><li><strong>updateStateByKey</strong>(<em>func</em>)可以返回一个新“state”的DStream，其中通过对键的前一个状态和键的新值应用给定的函数来更新每个键的状态。这可以用来维护每个键的任意状态数据。</li></ul><p>什么是Checkpoint?</p><ul><li>Checkpoint可以通过在一个容错的、可靠的文件系统中设置一个目录来启用，Checkpoint信息将被保存到这个目录中。这是通过使用streamingContext.checkpoint(checkpointDirectory)实现的。</li></ul><ol><li><p>下面案例也是单词计数，只不过需求变成了<strong>求当天到现在为止的单词计数</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    dispose(ssc)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 处理逻辑</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">dispose</span></span>(ssc: <span class="type">StreamingContext</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"hadoop"</span>, <span class="number">9100</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置checkpoint目录，保存offset</span></span><br><span class="line">    ssc.checkpoint(<span class="string">"./chk"</span>)</span><br><span class="line">    <span class="comment">// updateStateByKey：维护记录的state</span></span><br><span class="line">    lines.flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>)).updateStateByKey(updateFunction)</span><br><span class="line">    .print()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 实现对新值和旧值的累加</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateFunction</span></span>(newValues: <span class="type">Seq</span>[<span class="type">Int</span>], oldValues: <span class="type">Option</span>[<span class="type">Int</span>]): <span class="type">Option</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> curr = newValues.sum</span><br><span class="line">    <span class="keyword">val</span> old = oldValues.getOrElse(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> count = curr + old</span><br><span class="line">    <span class="type">Some</span>(count)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>使用nc发送消息</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">$ nc -lk <span class="number">9100</span></span><br><span class="line"></span><br><span class="line">a a a b b c</span><br><span class="line">a a a b b c</span><br></pre></td></tr></table></figure></li><li><p>客户端接收消息</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line"><span class="type">Time</span>: <span class="number">1587439170000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,<span class="number">2</span>)</span><br><span class="line">(a,<span class="number">3</span>)</span><br><span class="line">(c,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line"><span class="type">Time</span>: <span class="number">1587439175000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,<span class="number">4</span>)</span><br><span class="line">(a,<span class="number">6</span>)</span><br><span class="line">(c,<span class="number">2</span>)</span><br></pre></td></tr></table></figure></li></ol><p>上面的代码一直运行，结果可以一直累加，但是代码一旦停止运行，再次运行时，结果会不会接着上一次进行计算，上一次的计算结果丢失了，主要原因上每次程序运行都会初始化一个程序入口，而2次运行的程序入口不是同一个入口，所以会导致第一次计算的结果丢失</p><p>第一次的运算结果状态保存在Driver里面，所以我们如果想用上一次的计算结果，我们需要将上一次的Driver里面的运行结果状态取出来，而上面的代码有一个checkpoint方法，它会把上一次Driver里面的运算结果状态保存在checkpoint的目录里面，我们在第二次启动程序时，从checkpoint里面取出上一次的运行结果状态，把这次的Driver状态恢复成和上一次Driver一样的状态</p><h3 id="Checkpoint维护State-HA"><a href="#Checkpoint维护State-HA" class="headerlink" title="Checkpoint维护State HA"></a>Checkpoint维护State HA</h3><p>以下代码参考<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing" target="_blank" rel="noopener">官网</a></p><p>如果想让应用程序从驱动程序故障中恢复，我们应该重写代码，让它具备下面的功能</p><ul><li>当程序第一次启动时，它将创建一个新的StreamingContext，设置所有的流，然后调用start()。</li><li>当程序在失败后重新启动时，它将从Checkpoint目录中的Checkpoint数据重新创建一个StreamingContext。</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> checkpoint = <span class="string">"./chk_v2"</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 拿到StreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> ssc  = <span class="type">StreamingContext</span>.getOrCreate(checkpoint, functionToCreateContext)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建StreamingContext</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">functionToCreateContext</span></span>(): <span class="type">StreamingContext</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf,<span class="type">Seconds</span>(<span class="number">50000</span>))   <span class="comment">// new context</span></span><br><span class="line"></span><br><span class="line">    dispose(ssc)</span><br><span class="line"></span><br><span class="line">    ssc.checkpoint(checkpoint)   <span class="comment">// set checkpoint directory</span></span><br><span class="line">    ssc</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 处理具题的业务逻辑</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">dispose</span></span>(ssc: <span class="type">StreamingContext</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"hadoop"</span>, <span class="number">9100</span>) <span class="comment">// create DStreams</span></span><br><span class="line"></span><br><span class="line">    lines</span><br><span class="line">    .flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    .map((_, <span class="number">1</span>))</span><br><span class="line">    .updateStateByKey(updateFunction)</span><br><span class="line">    .print()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 更新state</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateFunction</span></span>(newValues: <span class="type">Seq</span>[<span class="type">Int</span>], oldValues: <span class="type">Option</span>[<span class="type">Int</span>]): <span class="type">Option</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> curr = newValues.sum</span><br><span class="line">    <span class="keyword">val</span> old = oldValues.getOrElse(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> count = curr + old</span><br><span class="line">    <span class="type">Some</span>(count)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="源码中维护State的方法"><a href="#源码中维护State的方法" class="headerlink" title="源码中维护State的方法"></a>源码中维护State的方法</h3><p>在阅读源码中的Example模块下Streaming下的StatefulNetworkWordCount object时，发现了一种维护State的写法</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 保存checkpoint</span></span><br><span class="line"><span class="keyword">val</span> checkpoint = <span class="string">"./chk_v3"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 拿到 StreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="type">StreamingContext</span>.getOrCreate(checkpoint, functionToCreateContext)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 StreamingContext</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">functionToCreateContext</span></span>(): <span class="type">StreamingContext</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    ssc.checkpoint(checkpoint)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对记录做累加操作</span></span><br><span class="line">    <span class="keyword">val</span> mappingFunc = (word: <span class="type">String</span>, one: <span class="type">Option</span>[<span class="type">Int</span>], state: <span class="type">State</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">        <span class="keyword">if</span>(state.isTimingOut())&#123;</span><br><span class="line">            println(<span class="string">"超时3秒没拿到数据"</span>)</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">val</span> sum = one.getOrElse(<span class="number">0</span>) + state.getOption.getOrElse(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">val</span> output = (word, sum)</span><br><span class="line">            state.update(sum)</span><br><span class="line">            output</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 逻辑处理</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"hadoop"</span>, <span class="number">9100</span>)</span><br><span class="line">    lines</span><br><span class="line">    .flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    .map((_,<span class="number">1</span>))</span><br><span class="line">    .mapWithState(<span class="type">StateSpec</span>.function(mappingFunc)</span><br><span class="line">                  .timeout(<span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">                 ).print()</span><br><span class="line"></span><br><span class="line">    ssc</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Idea加载Spark源码，并且在控制台查询SQL</title>
      <link href="/2019/10/19/spark/19/"/>
      <url>/2019/10/19/spark/19/</url>
      
        <content type="html"><![CDATA[<h3 id="编译源码"><a href="#编译源码" class="headerlink" title="编译源码"></a>编译源码</h3><ol><li><p>下载Spark源码</p><p>本次案例，我们使用Apche版本，版本为：spark-2.4.5</p><p>下载链接：<a href="https://github.com/apache/spark" target="_blank" rel="noopener">https://github.com/apache/spark</a></p><p><em>20200421更新：</em></p><p>一般使用spark对应版本的scala编译最好，如果使用不同版本的scala编译，需要修改主pom文件</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.12.10<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">scala.binary.version</span>&gt;</span>2.12<span class="tag">&lt;/<span class="name">scala.binary.version</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>编译Spark源码</p><p>在编译Spark源码之前，需要修改一些东西，原因是scope规定provided会报ClassNotFoundException</p><ul><li><p>修改hive-thriftserver模块下的pom.xm文件</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.eclipse.jetty<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jetty-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--      &lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.eclipse.jetty<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jetty-servlet<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--      &lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>修改主pom.xml文件</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.eclipse.jetty<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jetty-http<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;jetty.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--        &lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.eclipse.jetty<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jetty-continuation<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;jetty.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--        &lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.eclipse.jetty<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jetty-servlet<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;jetty.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--        &lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.eclipse.jetty<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jetty-servlets<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;jetty.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--        &lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.eclipse.jetty<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jetty-proxy<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;jetty.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--        &lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.eclipse.jetty<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jetty-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;jetty.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--        &lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.eclipse.jetty<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jetty-util<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;jetty.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--        &lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.eclipse.jetty<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jetty-security<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;jetty.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--       &lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.eclipse.jetty<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jetty-plus<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;jetty.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  &lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.eclipse.jetty<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jetty-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;jetty.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--        &lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果还有其他类似的ClassNotFoundException，都是这个原因引起的，注释即可</p></li></ul><p>使用<code>git-bash</code>编译</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mvn clean package -DskipTests=true</span><br><span class="line"></span><br><span class="line">## 经过漫长的等待，出现如下界面时，表示编译成功(忘记保留了，这里先用hive的)</span><br><span class="line">[INFO] Reactor Summary:</span><br><span class="line">[INFO]</span><br><span class="line">[INFO] Hive 1.1.0-cdh5.16.2 ............................... SUCCESS [  3.119 s]</span><br><span class="line">[INFO] Hive Classifications ............................... SUCCESS [  2.406 s]</span><br><span class="line">[INFO] Hive Shims Common .................................. SUCCESS [  3.327 s]</span><br><span class="line">[INFO] Hive Shims 0.23 .................................... SUCCESS [  3.494 s]</span><br><span class="line">[INFO] Hive Shims Scheduler ............................... SUCCESS [  2.423 s]</span><br><span class="line">[INFO] Hive Shims ......................................... SUCCESS [  1.463 s]</span><br><span class="line">[INFO] Hive Common ........................................ SUCCESS [  8.382 s]</span><br><span class="line">[INFO] Hive Serde ......................................... SUCCESS [  8.001 s]</span><br><span class="line">[INFO] Hive Metastore ..................................... SUCCESS [ 28.285 s]</span><br><span class="line">[INFO] Hive Ant Utilities ................................. SUCCESS [  1.668 s]</span><br><span class="line">[INFO] Spark Remote Client ................................ SUCCESS [  4.915 s]</span><br><span class="line">[INFO] Hive Query Language ................................ SUCCESS [01:36 min]</span><br><span class="line">[INFO] Hive Service ....................................... SUCCESS [ 22.921 s]</span><br><span class="line">[INFO] Hive Accumulo Handler .............................. SUCCESS [  5.496 s]</span><br><span class="line">[INFO] Hive JDBC .......................................... SUCCESS [  5.797 s]</span><br><span class="line">[INFO] Hive Beeline ....................................... SUCCESS [  3.957 s]</span><br><span class="line">[INFO] Hive CLI ........................................... SUCCESS [  4.060 s]</span><br><span class="line">[INFO] Hive Contrib ....................................... SUCCESS [  4.321 s]</span><br><span class="line">[INFO] Hive HBase Handler ................................. SUCCESS [  5.518 s]</span><br><span class="line">[INFO] Hive HCatalog ...................................... SUCCESS [  1.399 s]</span><br><span class="line">[INFO] Hive HCatalog Core ................................. SUCCESS [  5.933 s]</span><br><span class="line">[INFO] Hive HCatalog Pig Adapter .......................... SUCCESS [  4.632 s]</span><br><span class="line">[INFO] Hive HCatalog Server Extensions .................... SUCCESS [  4.477 s]</span><br><span class="line">[INFO] Hive HCatalog Webhcat Java Client .................. SUCCESS [  4.903 s]</span><br><span class="line">[INFO] Hive HCatalog Webhcat .............................. SUCCESS [  7.452 s]</span><br><span class="line">[INFO] Hive HCatalog Streaming ............................ SUCCESS [  4.306 s]</span><br><span class="line">[INFO] Hive HWI ........................................... SUCCESS [  3.461 s]</span><br><span class="line">[INFO] Hive ODBC .......................................... SUCCESS [  3.061 s]</span><br><span class="line">[INFO] Hive Shims Aggregator .............................. SUCCESS [  0.840 s]</span><br><span class="line">[INFO] Hive TestUtils ..................................... SUCCESS [  1.077 s]</span><br><span class="line">[INFO] Hive Packaging 1.1.0-cdh5.16.2 ..................... SUCCESS [  4.194 s]</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 04:22 min</span><br><span class="line">[INFO] Finished at: 2020-04-12T18:50:46+08:00</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure></li><li><p>将源码导入IDEA </p><p>源码以Maven方式，导入IDEA后，等待依赖加载完成</p><p>在编译之前需要删除spark-sql下的test包下的streaming包，不然会在<code>Build Project</code>时进入这里，引起<code>java.lang.OutOfMemoryError: GC overhead limit exceeded</code>异常</p><p>点击<code>Build Project</code>编译</p></li></ol><h3 id="本地调试Spark-SQL"><a href="#本地调试Spark-SQL" class="headerlink" title="本地调试Spark SQL"></a>本地调试Spark SQL</h3><ol><li><p>找到<code>hive-thriftserver</code>模块，在<code>main</code>下，新建<code>resources</code>目录，并标记为资源目录</p></li><li><p>拷贝集群上如下配置文件到<code>resources</code>目录中</p><p>hive-site.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://hadoop:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>指向的是运行metastore服务的主机<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>服务器需启动 metastore 服务</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">hive --service metastore &amp;</span><br></pre></td></tr></table></figure></li><li><p>运行<code>SparkSQLCLIDriver</code></p><p>在运行之前，需要在VM options中添加参数</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">-<span class="type">Dspark</span>.master=local[<span class="number">2</span>] -<span class="type">Djline</span>.<span class="type">WindowsTerminal</span>.directConsole=<span class="literal">false</span></span><br></pre></td></tr></table></figure><p>控制台输出如下信息</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">Spark</span> master: local[<span class="number">2</span>], <span class="type">Application</span> <span class="type">Id</span>: local<span class="number">-1587372819248</span></span><br><span class="line">spark-sql (<span class="keyword">default</span>)&gt; show databases;</span><br><span class="line">show databases;</span><br><span class="line">databaseName</span><br><span class="line">access_dw</span><br><span class="line"><span class="keyword">default</span></span><br><span class="line">offline_dw</span><br><span class="line">store_format</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自定义外部Text数据源</title>
      <link href="/2019/10/18/spark/18/"/>
      <url>/2019/10/18/spark/18/</url>
      
        <content type="html"><![CDATA[<p>这里接着上次的解读jdbc数据源，现在我们自己实现一个text的外部数据源</p><hr><ol><li><p>创建DefaultSource类实现RelationProviderTrait，注意这里的类名必须是DefaultSource，源码中写死了</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultSource</span>  <span class="keyword">extends</span> <span class="title">RelationProvider</span></span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span></span>(sqlContext: <span class="type">SQLContext</span>, parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">BaseRelation</span> = &#123;</span><br><span class="line">      <span class="comment">//拿到client传入的参数path</span></span><br><span class="line">    <span class="keyword">val</span> path = parameters.get(<span class="string">"path"</span>)</span><br><span class="line">      <span class="comment">//判断path是否存在</span></span><br><span class="line">    path <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(p) =&gt;<span class="keyword">new</span> <span class="type">TextDataSourceRelation</span>(sqlContext,p)</span><br><span class="line">      <span class="keyword">case</span> _ =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"path is required ..."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>自定义Relation，继承BashRelation和TableScan，拿到Schema和RDD[Row]</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextDataSourceRelation</span>(<span class="params">context:<span class="type">SQLContext</span>,path:<span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">BaseRelation</span> <span class="keyword">with</span> <span class="title">TableScan</span> <span class="keyword">with</span> <span class="title">Logging</span></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">sqlContext</span></span>: <span class="type">SQLContext</span> = context</span><br><span class="line"></span><br><span class="line">   <span class="comment">//重写StructType接口的方式实现Schema</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">schema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>&#123;</span><br><span class="line">    <span class="type">List</span>(</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"id"</span>,<span class="type">StringType</span>,<span class="literal">true</span>),</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"name"</span>,<span class="type">StringType</span>,<span class="literal">true</span>),</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"sex"</span>,<span class="type">StringType</span>,<span class="literal">true</span>),</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"sal"</span>,<span class="type">DoubleType</span>,<span class="literal">true</span>),</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"comm"</span>,<span class="type">DoubleType</span>,<span class="literal">true</span>)</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写buildScan拿到RDD[Row]</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">      <span class="comment">//拿到文本数据</span></span><br><span class="line">    <span class="keyword">val</span> textRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sqlContext.sparkContext.textFile(path)</span><br><span class="line">      <span class="comment">//拿到每个StructField</span></span><br><span class="line">    <span class="keyword">val</span> schemaField: <span class="type">Array</span>[<span class="type">StructField</span>] = schema.fields</span><br><span class="line">      <span class="comment">//对每行数据逗号切分，并且去掉空格，返回集合</span></span><br><span class="line">    textRDD.map(_.split(<span class="string">","</span>).map(_.trim))</span><br><span class="line">      <span class="comment">//对集合中的每个元素操作，通过zipWithIndex算子可以拿到元素的内容和对应的索引号</span></span><br><span class="line">      .map(row =&gt; row.zipWithIndex.map &#123;</span><br><span class="line">          <span class="comment">//模式匹配，拿到了value和index，然后对其做操作</span></span><br><span class="line">        <span class="keyword">case</span> (value, index) =&gt; &#123;</span><br><span class="line">            <span class="comment">//通过schemaField和index拿到列名</span></span><br><span class="line">          <span class="keyword">val</span> columnName = schemaField(index).name</span><br><span class="line">            <span class="comment">//判断当前的列名是否是sex，并在工具类中做匹配，对value转换类型</span></span><br><span class="line">          <span class="type">Utils</span>.caseTo(<span class="keyword">if</span> (columnName.equalsIgnoreCase(<span class="string">"sex"</span>)) &#123;</span><br><span class="line">              <span class="comment">//如果列名是sex，列下元素是1、2或者3，则返回对应的字符</span></span><br><span class="line">            <span class="keyword">if</span> (value == <span class="string">"1"</span>) &#123;</span><br><span class="line">              <span class="string">"男"</span></span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (value == <span class="string">"2"</span>) &#123;</span><br><span class="line">              <span class="string">"女"</span></span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              <span class="string">"未知"</span></span><br><span class="line">            &#125;</span><br><span class="line">              <span class="comment">//如果列名不是sex，则直接返回元素</span></span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            value</span><br><span class="line">              <span class="comment">//传入dataType的类型，在工具类中做匹配，使value与schema的类型一致</span></span><br><span class="line">          &#125;, schemaField(index).dataType)</span><br><span class="line">      &#125;</span><br><span class="line">        <span class="comment">//结果是个集合，转换成RDD[Row]</span></span><br><span class="line">      &#125;).map(x =&gt; <span class="type">Row</span>.fromSeq(x))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>自定义Utils类</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Utils</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">caseTo</span></span>(value:<span class="type">String</span>,dataType: <span class="type">DataType</span>) =&#123;</span><br><span class="line">      <span class="comment">//模式匹配，转换value的类型</span></span><br><span class="line">    dataType <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> _:<span class="type">DoubleType</span> =&gt; value.toDouble</span><br><span class="line">      <span class="keyword">case</span> _:<span class="type">LongType</span> =&gt; value.toLong</span><br><span class="line">      <span class="keyword">case</span> _:<span class="type">StringType</span> =&gt; value</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>测试</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">"local[2]"</span>)</span><br><span class="line">      .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> textDF: <span class="type">DataFrame</span> = spark.read.format(<span class="string">"com.tunan.spark.sql.extds.text"</span>).load(<span class="string">"tunan-spark-sql/extds"</span>)</span><br><span class="line"></span><br><span class="line">    textDF.printSchema()</span><br><span class="line">    textDF.show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出结果</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- name: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- sex: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- sal: double (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- comm: double (nullable = <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line"><span class="type">ERROR</span> <span class="type">TextDataSourceRelation</span>: 进入buildScan方法</span><br><span class="line">+---+----+----+-------+------+</span><br><span class="line">| id|name| sex|    sal|  comm|</span><br><span class="line">+---+----+----+-------+------+</span><br><span class="line">|  <span class="number">1</span>|张三|  男|<span class="number">10000.0</span>|<span class="number">1000.0</span>|</span><br><span class="line">|  <span class="number">2</span>|李四|  男|<span class="number">12000.0</span>|<span class="number">2000.0</span>|</span><br><span class="line">|  <span class="number">3</span>|王五|  女|<span class="number">12500.0</span>|<span class="number">1000.0</span>|</span><br><span class="line">|  <span class="number">4</span>|赵六|未知|<span class="number">20000.0</span>|<span class="number">2000.0</span>|</span><br><span class="line">|  <span class="number">5</span>|图南|  男|<span class="number">21000.0</span>|<span class="number">1000.0</span>|</span><br><span class="line">|  <span class="number">6</span>|小七|  女|<span class="number">10000.0</span>|<span class="number">1500.0</span>|</span><br><span class="line">+---+----+----+-------+------+</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据列自动推导&amp;数据错误执行模式&amp;UDAF&amp;UDTF&amp;解读Spark SQL执行计划优化</title>
      <link href="/2019/10/17/spark/17/"/>
      <url>/2019/10/17/spark/17/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>数据列自动推导</li><li>数据错误执行模式</li><li>UDAF</li><li>UDTF</li><li>解读Spark SQL执行计划优化</li></ol><h2 id="数据列自动推导"><a href="#数据列自动推导" class="headerlink" title="数据列自动推导"></a>数据列自动推导</h2><ol><li><p>源数据</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">a|b|c</span><br><span class="line"><span class="number">1</span>|<span class="number">2</span>|<span class="number">3</span></span><br><span class="line"><span class="number">4</span>|tunan|<span class="number">6</span></span><br><span class="line"><span class="number">7</span>|<span class="number">8</span>|<span class="number">9.0</span></span><br></pre></td></tr></table></figure></li><li><p>代码处理</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">    .builder()</span><br><span class="line">    .master(<span class="string">"local[2]"</span>)</span><br><span class="line">    .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    .getOrCreate()</span><br><span class="line">   </span><br><span class="line">  <span class="keyword">val</span> csvDF: <span class="type">DataFrame</span> = spark.read</span><br><span class="line">    .format(<span class="string">"csv"</span>)</span><br><span class="line">    .option(<span class="string">"header"</span>,<span class="string">"true"</span>)</span><br><span class="line">    .option(<span class="string">"sep"</span>,<span class="string">"|"</span>)</span><br><span class="line">    .option(<span class="string">"interSchema"</span>,<span class="string">"true"</span>)</span><br><span class="line">    .load(<span class="string">"tunan-spark-sql/data/test.csv"</span>)</span><br><span class="line">   </span><br><span class="line">  csvDF.printSchema()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>打印数据Schema信息</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- a: integer (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- b: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- c: double (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure></li></ol><h2 id="数据错误执行模式"><a href="#数据错误执行模式" class="headerlink" title="数据错误执行模式"></a>数据错误执行模式</h2><p>在Spark中，读取数据时，遇到错误数据或者脏数据时，我们可以使用option设置mode，区分将错误数据是默认处理<code>PERMISSIVE</code>，还是丢弃数据<code>DROPMALFORMED</code>，还是快速失败<code>FAILFAST</code>，这些方法可以在ParseMode.scala</p><ol><li><p>源数据</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">"a"</span>:<span class="number">1</span>,<span class="string">"b"</span>:<span class="number">2</span>,<span class="string">"c"</span>:<span class="number">3</span>&#125;</span><br><span class="line">&#123;<span class="string">"a"</span>:<span class="number">4</span>,:<span class="number">5</span>,<span class="string">"c"</span>:<span class="number">6</span>&#125;</span><br><span class="line">&#123;<span class="string">"a"</span>:<span class="number">7</span>,<span class="string">"b"</span>:<span class="number">8</span>,<span class="string">"c"</span>:<span class="number">9</span>&#125;</span><br></pre></td></tr></table></figure></li><li><p>读数据</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> jsonDF: <span class="type">DataFrame</span> = spark.read.json(<span class="string">"tunan-spark-sql/data/test.json"</span>)</span><br><span class="line">jsonDF.show()</span><br></pre></td></tr></table></figure><p>结果:</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+----------------+----+----+----+</span><br><span class="line">| _corrupt_record|   a|   b|   c|</span><br><span class="line">+----------------+----+----+----+</span><br><span class="line">|            <span class="literal">null</span>|   <span class="number">1</span>|   <span class="number">2</span>|   <span class="number">3</span>|</span><br><span class="line">|&#123;<span class="string">"a"</span>:<span class="number">4</span>,:<span class="number">5</span>,<span class="string">"c"</span>:<span class="number">6</span>&#125;|<span class="literal">null</span>|<span class="literal">null</span>|<span class="literal">null</span>|</span><br><span class="line">|            <span class="literal">null</span>|   <span class="number">7</span>|   <span class="number">8</span>|   <span class="number">9</span>|</span><br><span class="line">+----------------+----+----+----+</span><br></pre></td></tr></table></figure><p>如果没有在option中设置mode选项，默认为<code>PERMISSIVE</code>，通过_corrupt_record列打印出错误信息</p></li><li><p>使用option设置mode为<code>DROPMALFORMED</code>，如果碰到错误的数据，则自动丢弃</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> jsonDF: <span class="type">DataFrame</span> = spark.read.option(<span class="string">"mode"</span>,<span class="string">"DROPMALFORMED"</span>).json(<span class="string">"tunan-spark-sql/data/test.json"</span>)</span><br><span class="line">jsonDF.show()</span><br></pre></td></tr></table></figure><p>结果:</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+---+---+---+</span><br><span class="line">|  a|  b|  c|</span><br><span class="line">+---+---+---+</span><br><span class="line">|  <span class="number">1</span>|  <span class="number">2</span>|  <span class="number">3</span>|</span><br><span class="line">|  <span class="number">7</span>|  <span class="number">8</span>|  <span class="number">9</span>|</span><br><span class="line">+---+---+---+</span><br></pre></td></tr></table></figure></li></ol><h2 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h2><ol><li><p>自定义一个UDAF的class或者object，作为具体的逻辑实现，需要继承<code>UserDefinedAggregateFunction</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AgeAvgUDAF</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span></span>&#123;</span><br><span class="line">    <span class="comment">//输入类型</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"input"</span>,<span class="type">DoubleType</span>,<span class="literal">true</span>)::<span class="type">Nil</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//聚合内部中的buffer类型</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"sums"</span>,<span class="type">DoubleType</span>,<span class="literal">true</span>)::</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"num"</span>,<span class="type">LongType</span>,<span class="literal">true</span>)::<span class="type">Nil</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//输入数据类型</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//输入数据类型是否和输出数据类型相等</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//聚合内部buffer的初始化</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      buffer(<span class="number">0</span>) = <span class="number">0.0</span></span><br><span class="line">      buffer(<span class="number">1</span>) = <span class="number">0</span>L</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分区内更新聚合buffer</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      buffer.update(<span class="number">0</span>,buffer.getDouble(<span class="number">0</span>)+input.getDouble(<span class="number">0</span>))</span><br><span class="line">      buffer.update(<span class="number">1</span>,buffer.getLong(<span class="number">1</span>)+<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分区间合并</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      buffer1.update(<span class="number">0</span>,buffer1.getDouble(<span class="number">0</span>)+buffer2.getDouble(<span class="number">0</span>))</span><br><span class="line">      buffer1.update(<span class="number">1</span>,buffer1.getLong(<span class="number">1</span>)+buffer2.getLong(<span class="number">1</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//最终计算</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">      buffer.getDouble(<span class="number">0</span>)/buffer.getLong(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li><li><p>注册并使用UDAF</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">   <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">     .builder()</span><br><span class="line">     .master(<span class="string">"local[2]"</span>)</span><br><span class="line">     .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">     .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义数据源</span></span><br><span class="line">   <span class="keyword">val</span> list = <span class="keyword">new</span> util.<span class="type">ArrayList</span>[<span class="type">Row</span>]()</span><br><span class="line">   list.add(<span class="type">Row</span>(<span class="string">"zhangsan"</span>,<span class="number">18</span>,<span class="string">"男"</span>))</span><br><span class="line">   list.add(<span class="type">Row</span>(<span class="string">"lisi"</span>,<span class="number">20</span>,<span class="string">"男"</span>))</span><br><span class="line">   list.add(<span class="type">Row</span>(<span class="string">"wangwu"</span>,<span class="number">26</span>,<span class="string">"女"</span>))</span><br><span class="line">   list.add(<span class="type">Row</span>(<span class="string">"翠翠"</span>,<span class="number">18</span>,<span class="string">"女"</span>))</span><br><span class="line">   list.add(<span class="type">Row</span>(<span class="string">"闰土"</span>,<span class="number">8</span>,<span class="string">"男"</span>))</span><br><span class="line"></span><br><span class="line">   <span class="comment">// 自定义Schema</span></span><br><span class="line">   <span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">     <span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">StringType</span>, <span class="literal">true</span>)::</span><br><span class="line">     <span class="type">StructField</span>(<span class="string">"age"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>)::</span><br><span class="line">     <span class="type">StructField</span>(<span class="string">"sex"</span>, <span class="type">StringType</span>, <span class="literal">true</span>)::<span class="type">Nil</span></span><br><span class="line">   )</span><br><span class="line">   </span><br><span class="line"><span class="comment">//创建df</span></span><br><span class="line">   <span class="keyword">val</span> df = spark.createDataFrame(list, schema)</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//创建视图</span></span><br><span class="line">   df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//注册UDAF</span></span><br><span class="line">   spark.udf.register(<span class="string">"age_avg_udaf"</span>,<span class="type">AgeAvgUDAF</span>)</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//使用UDAF</span></span><br><span class="line">   spark.sql(<span class="string">"select sex,age_avg_udaf(age) as ave_age from people group by sex"</span>).show()</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li><li><p>结果展示</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+---+---------+</span><br><span class="line">|sex|  ave_age|</span><br><span class="line">+---+---------+</span><br><span class="line">| 男|    <span class="number">15.33</span>|</span><br><span class="line">| 女|     <span class="number">22.0</span>|</span><br><span class="line">+---+---------+</span><br></pre></td></tr></table></figure></li></ol><h2 id="UDTF"><a href="#UDTF" class="headerlink" title="UDTF"></a>UDTF</h2><p>UDTF还在研究，先搞个简单的案例</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ExplodeUDTF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">"local[2]"</span>)</span><br><span class="line">      .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 自定义schema  </span></span><br><span class="line">    <span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"teacher"</span>, <span class="type">StringType</span>, <span class="literal">true</span>) ::</span><br><span class="line">        <span class="type">StructField</span>(<span class="string">"sources"</span>, <span class="type">StringType</span>, <span class="literal">true</span>) :: <span class="type">Nil</span></span><br><span class="line">    )</span><br><span class="line"><span class="comment">// 自定义数据源</span></span><br><span class="line">    <span class="keyword">val</span> list = <span class="keyword">new</span> util.<span class="type">ArrayList</span>[<span class="type">Row</span>]()</span><br><span class="line">    list.add(<span class="type">Row</span>(<span class="string">"tunan"</span>, <span class="string">"hive,spark,flink"</span>))</span><br><span class="line">    list.add(<span class="type">Row</span>(<span class="string">"xiaoqi"</span>, <span class="string">"cdh,kafka,hbase"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建临时视图</span></span><br><span class="line">    <span class="keyword">val</span> df = spark.createDataFrame(list, schema)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// 使用flatMap拆分</span></span><br><span class="line">    df.flatMap(x =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> line = <span class="keyword">new</span> <span class="type">ListBuffer</span>[(<span class="type">String</span>, <span class="type">String</span>)]()</span><br><span class="line">      <span class="keyword">val</span> sources = x.getString(<span class="number">1</span>).split(<span class="string">","</span>)</span><br><span class="line">      <span class="keyword">for</span> (source &lt;- sources)&#123;</span><br><span class="line">        line.append((x.getString(<span class="number">0</span>),source))</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//返回</span></span><br><span class="line">      line</span><br><span class="line">    &#125;).toDF(<span class="string">"teacher"</span>,<span class="string">"source"</span>).show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+-------+------+</span><br><span class="line">|teacher|source|</span><br><span class="line">+-------+------+</span><br><span class="line">|  tunan|  hive|</span><br><span class="line">|  tunan| spark|</span><br><span class="line">|  tunan| flink|</span><br><span class="line">| xiaoqi|   cdh|</span><br><span class="line">| xiaoqi| kafka|</span><br><span class="line">| xiaoqi| hbase|</span><br><span class="line">+-------+------+</span><br></pre></td></tr></table></figure><h2 id="解读Spark-SQL执行计划优化"><a href="#解读Spark-SQL执行计划优化" class="headerlink" title="解读Spark SQL执行计划优化"></a>解读Spark SQL执行计划优化</h2><ol><li><p>建空表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> sqltest (<span class="keyword">key</span> <span class="keyword">string</span>,<span class="keyword">value</span> <span class="keyword">string</span>)</span><br></pre></td></tr></table></figure></li><li><p>执行SQL</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">explain</span> <span class="keyword">extended</span> <span class="keyword">select</span> a.key*(<span class="number">3</span>*<span class="number">5</span>),b.value <span class="keyword">from</span> sqltest a <span class="keyword">join</span> sqltest b <span class="keyword">on</span> a.key=b.key <span class="keyword">and</span> a.key &gt;<span class="number">3</span>;</span><br></pre></td></tr></table></figure></li><li><p>解读执行计划</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">// 解析逻辑计划，做些简单的解析</span><br><span class="line">== Parsed Logical Plan ==</span><br><span class="line">'Project [unresolvedalias(('a.key * (3 * 5)), None), 'b.value]</span><br><span class="line">+- 'Join Inner, (('a.key = 'b.key) &amp;&amp; ('a.key &gt; 3))</span><br><span class="line">   :- 'SubqueryAlias `a`</span><br><span class="line">   :  +- 'UnresolvedRelation `sqltest`</span><br><span class="line">   +- 'SubqueryAlias `b`</span><br><span class="line">      +- 'UnresolvedRelation `sqltest`</span><br><span class="line"></span><br><span class="line">// 分析逻辑计划，解析出了数据类型，拿到数据库和表，拿到了序列化方式                           </span><br><span class="line">== Analyzed Logical Plan ==</span><br><span class="line">(CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE)): double, value: string</span><br><span class="line">Project [(cast(key<span class="comment">#2 as double) * cast((3 * 5) as double)) AS (CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE))#6, value#5]</span></span><br><span class="line">+- Join Inner, ((key<span class="comment">#2 = key#4) &amp;&amp; (cast(key#2 as int) &gt; 3))</span></span><br><span class="line">   :- SubqueryAlias `a`</span><br><span class="line">   :  +- SubqueryAlias `default`.`sqltest`</span><br><span class="line">   :     +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key<span class="comment">#2, value#3]</span></span><br><span class="line">   +- SubqueryAlias `b`</span><br><span class="line">      +- SubqueryAlias `default`.`sqltest`</span><br><span class="line">         +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key<span class="comment">#4, value#5]</span></span><br><span class="line"></span><br><span class="line">// 优化逻辑计划，数值类型的运算直接拿到结果，解析过滤条件</span><br><span class="line">== Optimized Logical Plan ==</span><br><span class="line">Project [(cast(key<span class="comment">#2 as double) * 15.0) AS (CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE))#6, value#5]</span></span><br><span class="line">+- Join Inner, (key<span class="comment">#2 = key#4)</span></span><br><span class="line">   :- Project [key<span class="comment">#2]</span></span><br><span class="line">   :  +- Filter (isnotnull(key<span class="comment">#2) &amp;&amp; (cast(key#2 as int) &gt; 3))</span></span><br><span class="line">   :     +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key<span class="comment">#2, value#3]</span></span><br><span class="line">   +- Filter ((cast(key<span class="comment">#4 as int) &gt; 3) &amp;&amp; isnotnull(key#4))</span></span><br><span class="line">      +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key<span class="comment">#4, value#5]</span></span><br><span class="line"></span><br><span class="line">//物理计划，join方式为SortMergeJoin，数据使用hashpartitioning保存，扫描表的方式是HiveTableRelation</span><br><span class="line">== Physical Plan ==</span><br><span class="line">*(5) Project [(cast(key#2 as double) * 15.0) AS (CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE))#6, value#5]</span><br><span class="line">+- *(5) SortMergeJoin [key#2], [key#4], Inner</span><br><span class="line">   :- *(2) Sort [key#2 ASC NULLS FIRST], false, 0</span><br><span class="line">   :  +- Exchange hashpartitioning(key<span class="comment">#2, 200)</span></span><br><span class="line">   :     +- *(1) Filter (isnotnull(key#2) &amp;&amp; (cast(key#2 as int) &gt; 3))</span><br><span class="line">   :        +- Scan hive default.sqltest [key<span class="comment">#2], HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#2, value#3]</span></span><br><span class="line">   +- *(4) Sort [key#4 ASC NULLS FIRST], false, 0</span><br><span class="line">      +- Exchange hashpartitioning(key<span class="comment">#4, 200)</span></span><br><span class="line">         +- *(3) Filter ((cast(key#4 as int) &gt; 3) &amp;&amp; isnotnull(key#4))</span><br><span class="line">            +- Scan hive default.sqltest [key<span class="comment">#4, value#5], HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#4, value#5]</span></span><br></pre></td></tr></table></figure><p>可以简单的看做四步，分别是解析逻辑计划、分析逻辑计划、优化逻辑计划、物理执行计划</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从jdbc的角度解读外部数据源</title>
      <link href="/2019/10/16/spark/16/"/>
      <url>/2019/10/16/spark/16/</url>
      
        <content type="html"><![CDATA[<h2 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h2><p>首先了解三个trait，分别是BaseRelation、TableScan/PrunedScan/PrunedFilteredScan、<del>InsertableRelation</del>、RelationProvider，他们的功能在源码中解读。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//代表了一个抽象的数据源。该数据源由一行行有着已知schema的数据组成（关系表）。</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BaseRelation</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sqlContext</span></span>: <span class="type">SQLContext</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">schema</span></span>: <span class="type">StructType</span><span class="comment">//schema *</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sizeInBytes</span></span>: <span class="type">Long</span> = sqlContext.conf.defaultSizeInBytes</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">needConversion</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">unhandledFilters</span></span>(filters: <span class="type">Array</span>[<span class="type">Filter</span>]): <span class="type">Array</span>[<span class="type">Filter</span>] = filters</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//用于扫描整张表，将数据返回成RDD[Row]。</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Stable</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">TableScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//用于裁剪整张表，将数据返回成RDD[Row]。</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Stable</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">PrunedScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(requiredColumns: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">Row</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//用于裁剪并过滤整张表，将数据返回成RDD[Row]。</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Stable</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">PrunedFilteredScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(requiredColumns: <span class="type">Array</span>[<span class="type">String</span>], filters: <span class="type">Array</span>[<span class="type">Filter</span>]): <span class="type">RDD</span>[<span class="type">Row</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//插入数据的时候实现，设置overwrite是否为true</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Stable</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">InsertableRelation</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(data: <span class="type">DataFrame</span>, overwrite: <span class="type">Boolean</span>): <span class="type">Unit</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//为自定义的数据源类型生成一个新的Relation对象</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">RelationProvider</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//创建一个新的Relation</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span></span>(sqlContext: <span class="type">SQLContext</span>, parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">BaseRelation</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="jdbc实现"><a href="#jdbc实现" class="headerlink" title="jdbc实现"></a>jdbc实现</h2><p>JdbcRelationProvider (最初也是最终的地方)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcRelationProvider</span> <span class="keyword">extends</span> <span class="title">CreatableRelationProvider</span></span></span><br><span class="line"><span class="class"><span class="keyword">with</span> <span class="title">RelationProvider</span> <span class="keyword">with</span> <span class="title">DataSourceRegister</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">shortName</span></span>(): <span class="type">String</span> = <span class="string">"jdbc"</span> <span class="comment">//简称</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span></span>(</span><br><span class="line">        sqlContext: <span class="type">SQLContext</span>,</span><br><span class="line">        parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">BaseRelation</span> = &#123;<span class="comment">//所有options参数以map形式传入</span></span><br><span class="line">        <span class="keyword">val</span> jdbcOptions = <span class="keyword">new</span> <span class="type">JDBCOptions</span>(parameters) <span class="comment">//把参数传入和系统参数匹配</span></span><br><span class="line">        <span class="keyword">val</span> resolver = sqlContext.conf.resolver <span class="comment">//忽略大小写</span></span><br><span class="line">        <span class="keyword">val</span> timeZoneId = sqlContext.conf.sessionLocalTimeZone <span class="comment">//拿到时区</span></span><br><span class="line">        <span class="keyword">val</span> schema = <span class="type">JDBCRelation</span>.getSchema(resolver, jdbcOptions) <span class="comment">//传入参数，拿到schema</span></span><br><span class="line">        <span class="keyword">val</span> parts = <span class="type">JDBCRelation</span>.columnPartition(schema, resolver, timeZoneId,  jdbcOptions) <span class="comment">//拿到分区</span></span><br><span class="line">        <span class="type">JDBCRelation</span>(schema, parts, jdbcOptions)(sqlContext.sparkSession)  <span class="comment">//拿到RDD[R]</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>getSchema </p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSchema</span></span>(resolver: <span class="type">Resolver</span>, jdbcOptions: <span class="type">JDBCOptions</span>): <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> tableSchema = <span class="type">JDBCRDD</span>.resolveTable(jdbcOptions)   <span class="comment">//传入参数，解析table，拿到Schame</span></span><br><span class="line">    jdbcOptions.customSchema <span class="keyword">match</span> &#123; <span class="comment">//模式匹配</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(customSchema) =&gt; <span class="type">JdbcUtils</span>.getCustomSchema( </span><br><span class="line">            tableSchema, customSchema, resolver)<span class="comment">// 返回定制的Schema</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt; tableSchema <span class="comment">//返回直接的Schema</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>resolveTable (阶段一: 拿Schema)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resolveTable</span></span>(options: <span class="type">JDBCOptions</span>): <span class="type">StructType</span> = &#123; <span class="comment">//传入参数，拿到Schame</span></span><br><span class="line">    <span class="keyword">val</span> url = options.url <span class="comment">//拿到url：jdbc:mysql://hadoop:3306/</span></span><br><span class="line">    <span class="keyword">val</span> table = options.tableOrQuery<span class="comment">//拿到table：access_dw.dws_ad_phone_type_dist</span></span><br><span class="line">    <span class="keyword">val</span> dialect = <span class="type">JdbcDialects</span>.get(url) <span class="comment">//拿到方言：MySQLDialect</span></span><br><span class="line">    <span class="keyword">val</span> conn: <span class="type">Connection</span> = <span class="type">JdbcUtils</span>.createConnectionFactory(options)() <span class="comment">//创建连接</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> statement = conn.prepareStatement(dialect.getSchemaQuery(table)) <span class="comment">//拿到sql：com.mysql.jdbc.JDBC42PreparedStatement@5bda157e: SELECT * FROM access_dw.dws_ad_phone_type_dist WHERE 1=0</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            statement.setQueryTimeout(options.queryTimeout) <span class="comment">//设置超时时间</span></span><br><span class="line">            <span class="keyword">val</span> rs = statement.executeQuery() <span class="comment">//执行查询，返回一个查询产生的数据的ResultSet对象</span></span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="type">JdbcUtils</span>.getSchema(rs, dialect, alwaysNullable = <span class="literal">true</span>)  <span class="comment">//传入数据rs，拿到schema，接着下面的内容</span></span><br><span class="line">            &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                rs.close()</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            statement.close()</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        conn.close()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>getSchema (阶段一: 拿Schema)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSchema</span></span>(</span><br><span class="line">    resultSet: <span class="type">ResultSet</span>, <span class="comment">//查询表返回的rs(表结构)</span></span><br><span class="line">    dialect: <span class="type">JdbcDialect</span>, <span class="comment">//MySQL方言</span></span><br><span class="line">    alwaysNullable: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">StructType</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> rsmd = resultSet.getMetaData <span class="comment">//拿到表的元数据</span></span><br><span class="line">  <span class="keyword">val</span> ncols = rsmd.getColumnCount  <span class="comment">//拿到需要的字段的列的数量</span></span><br><span class="line">  <span class="keyword">val</span> fields = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">StructField</span>](ncols) <span class="comment">//创建一个StructField类型的数组，拼接fields</span></span><br><span class="line">  <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">  <span class="keyword">while</span> (i &lt; ncols) &#123; <span class="comment">//循环出每个column</span></span><br><span class="line">    <span class="keyword">val</span> columnName = rsmd.getColumnLabel(i + <span class="number">1</span>) <span class="comment">//返回列名：phoneSystemType</span></span><br><span class="line">    <span class="keyword">val</span> dataType = rsmd.getColumnType(i + <span class="number">1</span>) <span class="comment">//返回数据类型：12</span></span><br><span class="line">    <span class="keyword">val</span> typeName = rsmd.getColumnTypeName(i + <span class="number">1</span>) <span class="comment">//返回数据类型的名称：VARCHAR</span></span><br><span class="line">    <span class="keyword">val</span> fieldSize = rsmd.getPrecision(i + <span class="number">1</span>)  <span class="comment">//返回字段大小：64</span></span><br><span class="line">    <span class="keyword">val</span> fieldScale = rsmd.getScale(i + <span class="number">1</span>)<span class="comment">//返回scale：0</span></span><br><span class="line">    <span class="keyword">val</span> isSigned = &#123; <span class="comment">//判断是否有符号</span></span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        rsmd.isSigned(i + <span class="number">1</span>) <span class="comment">//是否有符号：false</span></span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="comment">// Workaround for HIVE-14684:</span></span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">SQLException</span> <span class="keyword">if</span></span><br><span class="line">        e.getMessage == <span class="string">"Method not supported"</span> &amp;&amp;</span><br><span class="line">          rsmd.getClass.getName == <span class="string">"org.apache.hive.jdbc.HiveResultSetMetaData"</span> =&gt; <span class="literal">true</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> nullable = <span class="keyword">if</span> (alwaysNullable) &#123; <span class="comment">//判断是否可为空</span></span><br><span class="line">      <span class="literal">true</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      rsmd.isNullable(i + <span class="number">1</span>) != <span class="type">ResultSetMetaData</span>.columnNoNulls</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> metadata = <span class="keyword">new</span> <span class="type">MetadataBuilder</span>().putLong(<span class="string">"scale"</span>, fieldScale)</span><br><span class="line">    <span class="keyword">val</span> columnType =</span><br><span class="line">      dialect.getCatalystType(dataType, typeName, fieldSize, metadata).getOrElse(</span><br><span class="line">        getCatalystType(dataType, fieldSize, fieldScale, isSigned)) <span class="comment">// 传入参数拿到类型：StringType</span></span><br><span class="line">    fields(i) = <span class="type">StructField</span>(columnName, columnType, nullable) <span class="comment">//传入列名，数据类型，是否可为空，创建StructField，并加入到fields中</span></span><br><span class="line">    i = i + <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">StructType</span>(fields) <span class="comment">//传入所有的StructField构建StructType，并返回，到这里拿到最终的Schema</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>JDBCRelation (阶段一: 拿Schema)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[sql] <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">JDBCRelation</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    override val schema: <span class="type">StructType</span>, //拿到<span class="type">Schema</span></span></span></span><br><span class="line"><span class="class"><span class="params">    parts: <span class="type">Array</span>[<span class="type">Partition</span>], //得到分区</span></span></span><br><span class="line"><span class="class"><span class="params">    jdbcOptions: <span class="type">JDBCOptions</span></span>)(<span class="params">@transient val sparkSession: <span class="type">SparkSession</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">BaseRelation</span> <span class="title">//实现BaseRelation，必然拿到了Schema</span></span></span><br><span class="line"><span class="class">  <span class="keyword">with</span> <span class="title">PrunedFilteredScan</span> <span class="title">//实现裁剪并且过滤的扫描表</span></span></span><br><span class="line"><span class="class">  <span class="keyword">with</span> <span class="title">InsertableRelation</span> </span>&#123; <span class="comment">//实现插入的模式</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">sqlContext</span></span>: <span class="type">SQLContext</span> = sparkSession.sqlContext</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> needConversion: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//检查JDBCRDD.compileFilter是否可以接受输入过滤器</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">unhandledFilters</span></span>(filters: <span class="type">Array</span>[<span class="type">Filter</span>]): <span class="type">Array</span>[<span class="type">Filter</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (jdbcOptions.pushDownPredicate) &#123;</span><br><span class="line">      filters.filter(<span class="type">JDBCRDD</span>.compileFilter(_, <span class="type">JdbcDialects</span>.get(jdbcOptions.url)).isEmpty)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      filters</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 构建Scan</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(requiredColumns: <span class="type">Array</span>[<span class="type">String</span>], filters: <span class="type">Array</span>[<span class="type">Filter</span>]): <span class="type">RDD</span>[<span class="type">Row</span>] = &#123;<span class="comment">//requiredColumns：需要的列，filters：过滤条件</span></span><br><span class="line">    <span class="comment">// 依赖类型擦除：将RDD[InternalRow]传递回RDD[Row]</span></span><br><span class="line">    <span class="type">JDBCRDD</span>.scanTable(</span><br><span class="line">      sparkSession.sparkContext, <span class="comment">//上下文环境</span></span><br><span class="line">      schema, <span class="comment">//Schema</span></span><br><span class="line">      requiredColumns, <span class="comment">//需要的列</span></span><br><span class="line">      filters, <span class="comment">//过滤条件</span></span><br><span class="line">      parts,   <span class="comment">//分区</span></span><br><span class="line">      jdbcOptions).asInstanceOf[<span class="type">RDD</span>[<span class="type">Row</span>]] <span class="comment">//最终的结果转换成RDD[Row]类型</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(data: <span class="type">DataFrame</span>, overwrite: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    data.write</span><br><span class="line">      .mode(<span class="keyword">if</span> (overwrite) <span class="type">SaveMode</span>.<span class="type">Overwrite</span> <span class="keyword">else</span> <span class="type">SaveMode</span>.<span class="type">Append</span>)</span><br><span class="line">      .jdbc(jdbcOptions.url, jdbcOptions.tableOrQuery, jdbcOptions.asProperties)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> partitioningInfo = <span class="keyword">if</span> (parts.nonEmpty) <span class="string">s" [numPartitions=<span class="subst">$&#123;parts.length&#125;</span>]"</span> <span class="keyword">else</span> <span class="string">""</span></span><br><span class="line">    <span class="comment">// 计划输出中不应包含凭据，表信息就足够了。</span></span><br><span class="line">    <span class="string">s"JDBCRelation(<span class="subst">$&#123;jdbcOptions.tableOrQuery&#125;</span>)"</span> + partitioningInfo</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>scanTable (阶段二: 拿RDD[Row])</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scanTable</span></span>(</span><br><span class="line">    sc: <span class="type">SparkContext</span>, </span><br><span class="line">    schema: <span class="type">StructType</span>,</span><br><span class="line">    requiredColumns: <span class="type">Array</span>[<span class="type">String</span>],</span><br><span class="line">    filters: <span class="type">Array</span>[<span class="type">Filter</span>],</span><br><span class="line">    parts: <span class="type">Array</span>[<span class="type">Partition</span>],</span><br><span class="line">    options: <span class="type">JDBCOptions</span>): <span class="type">RDD</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> url = options.url <span class="comment">//拿到客户端传入的rul</span></span><br><span class="line">    <span class="keyword">val</span> dialect = <span class="type">JdbcDialects</span>.get(url) <span class="comment">//拿到方言</span></span><br><span class="line">    <span class="keyword">val</span> quotedColumns = requiredColumns.map(colName =&gt; dialect.quoteIdentifier(colName)) <span class="comment">//拿到需要的列</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">JDBCRDD</span>(<span class="comment">//传入参数，返回RDD[InternalRow]</span></span><br><span class="line">        sc,</span><br><span class="line">        <span class="type">JdbcUtils</span>.createConnectionFactory(options),</span><br><span class="line">        pruneSchema(schema, requiredColumns),</span><br><span class="line">        quotedColumns,</span><br><span class="line">        filters,</span><br><span class="line">        parts,</span><br><span class="line">        url,</span><br><span class="line">        options)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>JDBCRDD(阶段二: 拿RDD[Row])</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 表示通过JDBC访问的数据库中的表的RDD。</span></span><br><span class="line"><span class="keyword">private</span>[jdbc] <span class="class"><span class="keyword">class</span> <span class="title">JDBCRDD</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    sc: <span class="type">SparkContext</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    getConnection: (</span>) <span class="title">=&gt;</span> <span class="title">Connection</span>,</span></span><br><span class="line"><span class="class">    <span class="title">schema</span></span>: <span class="type">StructType</span>,</span><br><span class="line">    columns: <span class="type">Array</span>[<span class="type">String</span>],</span><br><span class="line">    filters: <span class="type">Array</span>[<span class="type">Filter</span>],</span><br><span class="line">    partitions: <span class="type">Array</span>[<span class="type">Partition</span>],</span><br><span class="line">    url: <span class="type">String</span>,</span><br><span class="line">    options: <span class="type">JDBCOptions</span>)</span><br><span class="line"><span class="keyword">extends</span> <span class="type">RDD</span>[<span class="type">InternalRow</span>](sc, <span class="type">Nil</span>) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 索引与此RDD对应的分区列表。</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = partitions</span><br><span class="line"></span><br><span class="line">    <span class="comment">// `columns` 作为一个字符串注入到SQL查询</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> columnList: <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sb = <span class="keyword">new</span> <span class="type">StringBuilder</span>()</span><br><span class="line">        columns.foreach(x =&gt; sb.append(<span class="string">","</span>).append(x))</span><br><span class="line">        <span class="keyword">if</span> (sb.isEmpty) <span class="string">"1"</span> <span class="keyword">else</span> sb.substring(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// `filters`, 作为一个where语句注入到SQL查询</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> filterWhereClause: <span class="type">String</span> =</span><br><span class="line">    filters</span><br><span class="line">    .flatMap(<span class="type">JDBCRDD</span>.compileFilter(_, <span class="type">JdbcDialects</span>.get(url)))</span><br><span class="line">    .map(p =&gt; <span class="string">s"(<span class="subst">$p</span>)"</span>).mkString(<span class="string">" AND "</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果当前分区有where语句，那么就拼接</span></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getWhereClause</span></span>(part: <span class="type">JDBCPartition</span>): <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span> (part.whereClause != <span class="literal">null</span> &amp;&amp; filterWhereClause.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="string">"WHERE "</span> + <span class="string">s"(<span class="subst">$filterWhereClause</span>)"</span> + <span class="string">" AND "</span> + <span class="string">s"(<span class="subst">$&#123;part.whereClause&#125;</span>)"</span></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (part.whereClause != <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="string">"WHERE "</span> + part.whereClause</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (filterWhereClause.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="string">"WHERE "</span> + filterWhereClause</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="string">""</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对JDBC驱动程序运行SQL查询。</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(thePart: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">        <span class="keyword">var</span> closed = <span class="literal">false</span></span><br><span class="line">        <span class="keyword">var</span> rs: <span class="type">ResultSet</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">var</span> stmt: <span class="type">PreparedStatement</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">var</span> conn: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>() &#123;</span><br><span class="line">            <span class="keyword">if</span> (closed) <span class="keyword">return</span></span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (<span class="literal">null</span> != rs) &#123;</span><br><span class="line">                    rs.close()</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; logWarning(<span class="string">"Exception closing resultset"</span>, e)</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (<span class="literal">null</span> != stmt) &#123;</span><br><span class="line">                    stmt.close()</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; logWarning(<span class="string">"Exception closing statement"</span>, e)</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (<span class="literal">null</span> != conn) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (!conn.isClosed &amp;&amp; !conn.getAutoCommit) &#123;</span><br><span class="line">                        <span class="keyword">try</span> &#123;</span><br><span class="line">                            conn.commit()</span><br><span class="line">                        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                            <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logWarning(<span class="string">"Exception committing transaction"</span>, e)</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                    conn.close()</span><br><span class="line">                &#125;</span><br><span class="line">                logInfo(<span class="string">"closed connection"</span>)</span><br><span class="line">            &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; logWarning(<span class="string">"Exception closing connection"</span>, e)</span><br><span class="line">            &#125;</span><br><span class="line">            closed = <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        context.addTaskCompletionListener[<span class="type">Unit</span>]&#123; context =&gt; close() &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> inputMetrics = context.taskMetrics().inputMetrics</span><br><span class="line">        <span class="keyword">val</span> part = thePart.asInstanceOf[<span class="type">JDBCPartition</span>]</span><br><span class="line">        conn = getConnection()</span><br><span class="line">        <span class="keyword">val</span> dialect = <span class="type">JdbcDialects</span>.get(url)</span><br><span class="line">        <span class="keyword">import</span> scala.collection.<span class="type">JavaConverters</span>._</span><br><span class="line">        dialect.beforeFetch(conn, options.asProperties.asScala.toMap)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 这在通过JDBC读取表/查询之前执行一个通用的SQL语句(或PL/SQL块)。</span></span><br><span class="line">        <span class="comment">// 使用此功能初始化数据库会话环境，例如用于优化和/或故障排除。</span></span><br><span class="line">        options.sessionInitStatement <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="type">Some</span>(sql) =&gt;</span><br><span class="line">            <span class="keyword">val</span> statement = conn.prepareStatement(sql)</span><br><span class="line">            logInfo(<span class="string">s"Executing sessionInitStatement: <span class="subst">$sql</span>"</span>)</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                statement.setQueryTimeout(options.queryTimeout)</span><br><span class="line">                statement.execute() <span class="comment">//最终执行的就是jdbc</span></span><br><span class="line">            &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                statement.close()</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 返回RDD[InternalRow]</span></span><br><span class="line">        <span class="type">CompletionIterator</span>[<span class="type">InternalRow</span>, <span class="type">Iterator</span>[<span class="type">InternalRow</span>]](</span><br><span class="line">            <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context, rowsIterator), close())</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终一套debug走下来，其实就是两步</p><ol><li>第二步通过jdbc查元数据，拿到Schema</li><li>第二步通过jdbc查数据拿到RDD[Row]</li></ol><p><strong>最终的创建DataFrame由框架解决</strong></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RDD转换DadaFrame&amp;使用SQL操作数据源&amp;跨数据源join&amp;SQL与DF与DS的比较&amp;Spark元数据管理: catalog</title>
      <link href="/2019/10/15/spark/15/"/>
      <url>/2019/10/15/spark/15/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>RDD转换DadaFrame</li><li>使用SQL操作数据源</li><li>跨数据源join</li><li>SQL与DF与DS的比较</li><li>Spark元数据管理: catalog</li></ol><h2 id="RDD转换DadaFrame"><a href="#RDD转换DadaFrame" class="headerlink" title="RDD转换DadaFrame"></a>RDD转换DadaFrame</h2><ol><li><p>第一种方式是使用反射来推断包含特定对象类型的RDD的模式</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">reflect</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().master(<span class="string">"local[2]"</span>).appName(<span class="keyword">this</span>.getClass.getSimpleName).getOrCreate()</span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-sql/data/top.txt"</span></span><br><span class="line">        <span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = spark.sparkContext.textFile(in)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = lines.split(<span class="string">","</span>)</span><br><span class="line">            people(words(<span class="number">0</span>), words(<span class="number">1</span>), words(<span class="number">2</span>).toInt)</span><br><span class="line">        &#125;)</span><br><span class="line">        mapRDD.toDF().show()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">people</span>(<span class="params">name:<span class="type">String</span>,subject:<span class="type">String</span>,grade:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure><p>通过反射class的这种方式可以获得Schema创建DataFrame，简单通用，但是在<strong>创建外部数据源的场景下不适用</strong></p></li><li><p>第二种方法是通过编程接口，通过StructType可以构造Schema，然后将其应用于现有的RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">interface</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().master(<span class="string">"local[2]"</span>).appName(<span class="keyword">this</span>.getClass.getSimpleName).getOrCreate()</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-sql/data/top.txt"</span></span><br><span class="line">        <span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = spark.sparkContext.textFile(in)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//在原RDD上创建rowRDD</span></span><br><span class="line">        <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = lines.split(<span class="string">","</span>)</span><br><span class="line">            <span class="type">Row</span>(words(<span class="number">0</span>), words(<span class="number">1</span>), words(<span class="number">2</span>).toDouble)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建和上一步对应的行结构类型的StructType</span></span><br><span class="line">        <span class="keyword">val</span> innerStruct =</span><br><span class="line">            <span class="type">StructType</span>(</span><br><span class="line">            <span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">StringType</span>, <span class="literal">false</span>) ::</span><br><span class="line">            <span class="type">StructField</span>(<span class="string">"subject"</span>, <span class="type">StringType</span>, <span class="literal">false</span>) ::</span><br><span class="line">            <span class="type">StructField</span>(<span class="string">"grade"</span>, <span class="type">DoubleType</span>, <span class="literal">false</span>) :: <span class="type">Nil</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将schema和Rows结合，创建出DF</span></span><br><span class="line">        <span class="keyword">val</span> df = spark.createDataFrame(mapRDD, innerStruct)</span><br><span class="line"></span><br><span class="line">        df.show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>步骤：</p><ol><li>在原RDD上创建rowRDD</li><li>创建和上一步对应的行结构类型的StructType</li><li>将schema和Rows结合，创建出DF</li></ol></li></ol><h2 id="使用SQL操作数据源"><a href="#使用SQL操作数据源" class="headerlink" title="使用SQL操作数据源"></a>使用SQL操作数据源</h2><p>在官网的<a href="http://spark.apache.org/docs/latest/sql-data-sources.html" target="_blank" rel="noopener">Data Sources</a> 下，每个数据源下都有一个Sql选项卡，其中就是对应的SQL采集源数据，并生成对应的SQL视图的代码，如：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">spark-sql (default)&gt; CREATE TEMPORARY VIEW jsonTable</span><br><span class="line">                   &gt; USING org.apache.spark.sql.json</span><br><span class="line">                   &gt; OPTIONS (</span><br><span class="line">                   &gt;   path "file:///home/hadoop/data/people.json"</span><br><span class="line">                   &gt; );</span><br><span class="line">Response code</span><br><span class="line"></span><br><span class="line">spark-sql (default)&gt; SELECT * FROM jsonTable;</span><br><span class="line">agename</span><br><span class="line">NULLMichael</span><br><span class="line">30Andy</span><br><span class="line">19Justin</span><br></pre></td></tr></table></figure><h2 id="跨数据源join"><a href="#跨数据源join" class="headerlink" title="跨数据源join"></a>跨数据源join</h2><p>跨数据源join是Spark非常好用的一个特性，从不同的数据源拿到spark中，再从spark写出去，简直轻而易举。</p><p>下面我们将验证从hive和mysql中分别拿出一个表join(在idea中操作时，需要先连上hive)</p><ol><li><p>jdbc</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> jdbcDF = spark.read</span><br><span class="line">    .format(<span class="string">"jdbc"</span>)</span><br><span class="line">    .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://hadoop/?characterEncoding=utf-8&amp;useSSL=false"</span>)</span><br><span class="line">    .option(<span class="string">"dbtable"</span>, <span class="string">"tunan.dept"</span>)</span><br><span class="line">    .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">    .option(<span class="string">"password"</span>, <span class="string">"root"</span>)</span><br><span class="line">    .load()</span><br></pre></td></tr></table></figure></li><li><p>hive</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> hiveDF = spark.sql(<span class="string">"select * from default.emp"</span>)</span><br></pre></td></tr></table></figure></li><li><p>join</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> joinDF: <span class="type">DataFrame</span> = jdbcDF.join(hiveDF, <span class="string">"deptno"</span>)</span><br></pre></td></tr></table></figure></li><li><p>查看结果</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">&gt;&gt; joinDF.show(<span class="literal">false</span>)</span><br><span class="line">+------+----------+-----+-----+------+---------+----+----------+------+------+</span><br><span class="line">|deptno|     dname|level|empno| ename|      job| jno|      date|   sal| prize|</span><br><span class="line">+------+----------+-----+-----+------+---------+----+----------+------+------+</span><br><span class="line">|    <span class="number">10</span>|<span class="type">ACCOUNTING</span>| <span class="number">1700</span>| <span class="number">7566</span>| <span class="type">JONES</span>|  <span class="type">MANAGER</span>|<span class="number">7839</span>|  <span class="number">1981</span><span class="number">-4</span><span class="number">-2</span>|<span class="number">2975.0</span>|  <span class="literal">null</span>|</span><br><span class="line">|    <span class="number">10</span>|<span class="type">ACCOUNTING</span>| <span class="number">1700</span>| <span class="number">7521</span>|  <span class="type">WARD</span>| <span class="type">SALESMAN</span>|<span class="number">7698</span>| <span class="number">1981</span><span class="number">-2</span><span class="number">-22</span>|<span class="number">1250.0</span>| <span class="number">500.0</span>|</span><br><span class="line">|    <span class="number">20</span>|  <span class="type">RESEARCH</span>| <span class="number">1800</span>| <span class="number">7934</span>|<span class="type">MILLER</span>|    <span class="type">CLERK</span>|<span class="number">7782</span>| <span class="number">1982</span><span class="number">-1</span><span class="number">-23</span>|<span class="number">1300.0</span>|  <span class="literal">null</span>|</span><br><span class="line">|    <span class="number">20</span>|  <span class="type">RESEARCH</span>| <span class="number">1800</span>| <span class="number">7902</span>|  <span class="type">FORD</span>|  <span class="type">ANALYST</span>|<span class="number">7566</span>| <span class="number">1981</span><span class="number">-12</span><span class="number">-3</span>|<span class="number">3000.0</span>|  <span class="literal">null</span>|</span><br><span class="line">....</span><br><span class="line">+------+----------+-----+-----+------+---------+----+----------+------+------+</span><br></pre></td></tr></table></figure></li><li><p>我们并不需要全部的数据，下面我们将经过处理选择我们需要的数据</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 定义视实体类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">EmpDept</span>(<span class="params">deptno:<span class="type">String</span>,dname:<span class="type">String</span>,level:<span class="type">Int</span>,empno:<span class="type">String</span>,ename:<span class="type">String</span>,job:<span class="type">String</span>,jno:<span class="type">String</span>,date:<span class="type">String</span>,sal:<span class="type">Double</span>,prize:<span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Result</span>(<span class="params">empno:<span class="type">String</span>,ename:<span class="type">String</span>,deptno:<span class="type">String</span>,dname:<span class="type">String</span>,prize:<span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//DF转换成DS</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">joinDS</span></span>: <span class="type">Dataset</span>[<span class="type">EmpDept</span>] = joinDF.as[<span class="type">EmpDept</span>]</span><br><span class="line"><span class="comment">//从DS中拿到数据，反射的方式拿到Schema信息</span></span><br><span class="line"><span class="keyword">val</span> mapDS = joinDS.map(x =&gt; <span class="type">Result</span>(x.empno, x.ename, x.deptno, x.dname,x.prize))</span><br></pre></td></tr></table></figure></li><li><p>保存数据</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 保存到文件</span></span><br><span class="line">mapDS.write.format(<span class="string">"orc"</span>).save(<span class="string">"tunan-spark-sql/out"</span>)</span><br><span class="line"><span class="comment">// 保存到MySQL数据库</span></span><br><span class="line">mapDS.write.format(<span class="string">"jdbc"</span>)</span><br><span class="line">.option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://hadoop/?characterEncoding=utf-8&amp;useSSL=false"</span>)</span><br><span class="line">.option(<span class="string">"dbtable"</span>, <span class="string">"tunan.join_result"</span>)</span><br><span class="line">.option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">.option(<span class="string">"password"</span>, <span class="string">"root"</span>)</span><br><span class="line">.mode(<span class="string">"overwrite"</span>)</span><br><span class="line">.save()</span><br></pre></td></tr></table></figure><p>查看结果:</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+-----+------+------+----------+------+</span><br><span class="line">|empno| ename|deptno|     dname| prize|</span><br><span class="line">+-----+------+------+----------+------+</span><br><span class="line">| <span class="number">7566</span>| <span class="type">JONES</span>|    <span class="number">10</span>|<span class="type">ACCOUNTING</span>|  <span class="literal">null</span>|</span><br><span class="line">| <span class="number">7521</span>|  <span class="type">WARD</span>|    <span class="number">10</span>|<span class="type">ACCOUNTING</span>| <span class="number">500.0</span>|</span><br><span class="line">| <span class="number">7934</span>|<span class="type">MILLER</span>|    <span class="number">20</span>|  <span class="type">RESEARCH</span>|  <span class="literal">null</span>|</span><br><span class="line">| <span class="number">7902</span>|  <span class="type">FORD</span>|    <span class="number">20</span>|  <span class="type">RESEARCH</span>|  <span class="literal">null</span>|</span><br><span class="line">...</span><br><span class="line">+-----+------+------+----------+------+</span><br></pre></td></tr></table></figure></li></ol><h2 id="SQL与DF与DS的比较"><a href="#SQL与DF与DS的比较" class="headerlink" title="SQL与DF与DS的比较"></a>SQL与DF与DS的比较</h2><p>小问题：spark.read.load()   这句代码没用指定读取格式，那么它的默认格式是什么？</p><p>现在我们需要对比的是SQL、DF、DS三者对Syntax Errors和Analysis Errors的不同程度的响应</p><p>在上一步中，我们将joinDF转化成了joinDS，现在我们就看看他们在选择需要的列的时候，做了什么样的执行计划</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> selectDF: <span class="type">DataFrame</span> = joinDF.select(<span class="string">"ename"</span>)</span><br><span class="line"><span class="keyword">val</span> selectDS: <span class="type">Dataset</span>[<span class="type">String</span>] = joinDS.map(_.ename)</span><br><span class="line"></span><br><span class="line">println(selectDF.queryExecution.optimizedPlan.numberedTreeString)</span><br><span class="line">println(<span class="string">"-------------"</span>)</span><br><span class="line">println(selectDS.queryExecution.optimizedPlan.numberedTreeString)</span><br></pre></td></tr></table></figure><p>很明显selectDS做出了优化</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="number">00</span> <span class="type">Project</span> [ename#<span class="number">7</span>]</span><br><span class="line"><span class="number">01</span> +- <span class="type">Join</span> <span class="type">Inner</span>, (deptno#<span class="number">0</span> = deptno#<span class="number">13</span>)</span><br><span class="line"><span class="number">02</span>    :- <span class="type">Project</span> [deptno#<span class="number">0</span>]</span><br><span class="line"><span class="number">03</span>    :  +- <span class="type">Filter</span> isnotnull(deptno#<span class="number">0</span>)</span><br><span class="line"><span class="number">04</span>    :     +- <span class="type">Relation</span>[deptno#<span class="number">0</span>,dname#<span class="number">1</span>,level#<span class="number">2</span>] <span class="type">JDBCRelation</span>(tunan.dept) [numPartitions=<span class="number">1</span>]</span><br><span class="line"><span class="number">05</span>    +- <span class="type">Project</span> [ename#<span class="number">7</span>, deptno#<span class="number">13</span>]</span><br><span class="line"><span class="number">06</span>       +- <span class="type">Filter</span> isnotnull(deptno#<span class="number">13</span>)</span><br><span class="line"><span class="number">07</span>          +- <span class="type">HiveTableRelation</span> `<span class="keyword">default</span>`.`emp`, org.apache.hadoop.hive.serde2.<span class="keyword">lazy</span>.<span class="type">LazySimpleSerDe</span>, [empno#<span class="number">6</span>, ename#<span class="number">7</span>, job#<span class="number">8</span>, jno#<span class="number">9</span>, date#<span class="number">10</span>, sal#<span class="number">11</span>, prize#<span class="number">12</span>, deptno#<span class="number">13</span>]</span><br><span class="line"></span><br><span class="line">-------------</span><br><span class="line"></span><br><span class="line"><span class="number">00</span> <span class="type">SerializeFromObject</span> [staticinvoke(<span class="class"><span class="keyword">class</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">unsafe</span>.<span class="title">types</span>.<span class="title">UTF8String</span>, <span class="title">StringType</span>, <span class="title">fromString</span>, <span class="title">input</span>[0, java.lang.<span class="type">String</span>, true], <span class="title">true</span>, <span class="title">false</span>) <span class="title">AS</span> <span class="title">value#47</span>]</span></span><br><span class="line"><span class="class">01 <span class="title">+-</span> <span class="title">MapElements</span> <span class="title">com</span>.<span class="title">tunan</span>.<span class="title">spark</span>.<span class="title">sql</span>.<span class="title">join</span>.<span class="title">JdbcJoinHive$$$Lambda$1097/374205056@10f8e2d2</span>, <span class="title">class</span> <span class="title">com</span>.<span class="title">tunan</span>.<span class="title">spark</span>.<span class="title">sql</span>.<span class="title">join</span>.<span class="title">JdbcJoinHive$EmpDept</span>, [<span class="type">StructField</span>(deptno,<span class="type">StringType</span>,true), <span class="type">StructField</span>(dname,<span class="type">StringType</span>,true), <span class="type">StructField</span>(level,<span class="type">StringType</span>,true), <span class="type">StructField</span>(empno,<span class="type">StringType</span>,true), <span class="type">StructField</span>(ename,<span class="type">StringType</span>,true), <span class="type">StructField</span>(job,<span class="type">StringType</span>,true), <span class="type">StructField</span>(jno,<span class="type">StringType</span>,true), <span class="type">StructField</span>(date,<span class="type">StringType</span>,true), <span class="type">StructField</span>(sal,<span class="type">DoubleType</span>,false), <span class="type">StructField</span>(prize,<span class="type">StringType</span>,true)], <span class="title">obj#46</span></span>: java.lang.<span class="type">String</span></span><br><span class="line"><span class="number">02</span>    +- <span class="type">DeserializeToObject</span> newInstance(<span class="class"><span class="keyword">class</span> <span class="title">com</span>.<span class="title">tunan</span>.<span class="title">spark</span>.<span class="title">sql</span>.<span class="title">join</span>.<span class="title">JdbcJoinHive$EmpDept</span>), <span class="title">obj#45</span></span>: com.tunan.spark.sql.join.<span class="type">JdbcJoinHive</span>$<span class="type">EmpDept</span></span><br><span class="line"><span class="number">03</span>       +- <span class="type">Project</span> [deptno#<span class="number">0</span>, dname#<span class="number">1</span>, level#<span class="number">2</span>, empno#<span class="number">6</span>, ename#<span class="number">7</span>, job#<span class="number">8</span>, jno#<span class="number">9</span>, date#<span class="number">10</span>, sal#<span class="number">11</span>, prize#<span class="number">12</span>]</span><br><span class="line"><span class="number">04</span>          +- <span class="type">Join</span> <span class="type">Inner</span>, (deptno#<span class="number">0</span> = deptno#<span class="number">13</span>)</span><br><span class="line"><span class="number">05</span>             :- <span class="type">Filter</span> isnotnull(deptno#<span class="number">0</span>)</span><br><span class="line"><span class="number">06</span>             :  +- <span class="type">Relation</span>[deptno#<span class="number">0</span>,dname#<span class="number">1</span>,level#<span class="number">2</span>] <span class="type">JDBCRelation</span>(tunan.dept) [numPartitions=<span class="number">1</span>]</span><br><span class="line"><span class="number">07</span>             +- <span class="type">Filter</span> isnotnull(deptno#<span class="number">13</span>)</span><br><span class="line"><span class="number">08</span>                +- <span class="type">HiveTableRelation</span> `<span class="keyword">default</span>`.`emp`, org.apache.hadoop.hive.serde2.<span class="keyword">lazy</span>.<span class="type">LazySimpleSerDe</span>, [empno#<span class="number">6</span>, ename#<span class="number">7</span>, job#<span class="number">8</span>, jno#<span class="number">9</span>, date#<span class="number">10</span>, sal#<span class="number">11</span>, prize#<span class="number">12</span>, deptno#<span class="number">13</span>]</span><br></pre></td></tr></table></figure><p>由此我们可以根据在Spark SQL应用中，选择列的时候，SQL、DF、DS三者做一个比较</p><table><thead><tr><th></th><th>SQL</th><th>DF</th><th>DS</th></tr></thead><tbody><tr><td>Syntax Errors</td><td>runtime</td><td>compile</td><td>compile</td></tr><tr><td>Analysis Errors</td><td>runtime</td><td>runtime</td><td>compile</td></tr></tbody></table><p>在执行SQL的时候，无论是语法错误还是运行错误，都无法在编译时就提前暴露出来</p><p>在执行DF的时候，算子如果写错了，会提前暴露出来，但是写的列名只有在运行的时候才会检查是否正确</p><p>在执行DS的时候，由于case class反射的机制，算子和列名都可以提前到代码编写时就检测到错误</p><p><strong>所以最优的执行顺序为 DS &gt; DF &gt; SQL</strong></p><h3 id="面试题：RDD、DS、DF的区别"><a href="#面试题：RDD、DS、DF的区别" class="headerlink" title="面试题：RDD、DS、DF的区别"></a>面试题：RDD、DS、DF的区别</h3><ol><li>RDD不支持SQL</li><li>DF每一行都是Row类型，不能直接访问字段，必须解析才行</li><li>DS每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获 得每一行的信息</li><li>DataFrame与Dataset均支持spark sql的操作，比如select，group by之类，还 能注册临时表/视窗，进行sql语句操作</li><li>可以看出，Dataset在需要访问列中的某个字段时是非常方便的，然而，如果要 写一些适配性很强的函数时，如果使用Dataset，行的类型又不确定，可能是各种case class，无法实现适配，这时候用DataFrame即Dataset[Row]就能比较好的解决问题。</li></ol><h3 id="面试题：RDD和Dataset-DataFrame中的Persist的默认缓存级别"><a href="#面试题：RDD和Dataset-DataFrame中的Persist的默认缓存级别" class="headerlink" title="面试题：RDD和Dataset/DataFrame中的Persist的默认缓存级别"></a>面试题：RDD和Dataset/DataFrame中的Persist的默认缓存级别</h3><ul><li>Dataset 中的缓存级别是 MEMORY_AND_DISK</li><li>RDD 中的缓存级别是 MEMORY_ONLY</li></ul><h3 id="面试题：Spark-RDD和Spark-SQL的的cache有什么区别"><a href="#面试题：Spark-RDD和Spark-SQL的的cache有什么区别" class="headerlink" title="面试题：Spark RDD和Spark SQL的的cache有什么区别"></a>面试题：Spark RDD和Spark SQL的的cache有什么区别</h3><ul><li><p>Spark RDD的cache是lazy的，需要action才会执行cache操作</p></li><li><p>Spark SQL的cache是egaer的，马上就cache了</p></li></ul><h2 id="Spark元数据管理-catalog"><a href="#Spark元数据管理-catalog" class="headerlink" title="Spark元数据管理: catalog"></a>Spark元数据管理: catalog</h2><p>拿到catalog</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().master(<span class="string">"local[2]"</span>).appName(<span class="keyword">this</span>.getClass.getSimpleName).enableHiveSupport().getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> catalog: <span class="type">Catalog</span> = spark.catalog</span><br></pre></td></tr></table></figure><ol><li><p>展示所有数据库</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dbList: <span class="type">Dataset</span>[<span class="type">Database</span>] = catalog.listDatabases()</span><br><span class="line">dbList.show(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure></li><li><p>当前数据库</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">catalog.currentDatabase</span><br></pre></td></tr></table></figure></li><li><p>只展示名字</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line">dbList.map(_.name).show()</span><br></pre></td></tr></table></figure></li><li><p>展示指定库的所有表</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">catalog.setCurrentDatabase(<span class="string">"offline_dw"</span>)</span><br><span class="line">catalog.listTables().show(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure></li><li><p>过滤表</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> listTable = catalog.listTables()</span><br><span class="line">listTable.filter(<span class="symbol">'name</span> === <span class="string">"dws_country_traffic"</span>).show(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure></li><li><p>判断某个表是否缓存</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">catalog.isCached(<span class="string">"dws_country_traffic"</span>)</span><br><span class="line">catalog.cacheTable(<span class="string">"dws_country_traffic"</span>)</span><br><span class="line">catalog.isCached(<span class="string">"dws_country_traffic"</span>)</span><br><span class="line">catalog.uncacheTable(<span class="string">"dws_country_traffic"</span>)</span><br></pre></td></tr></table></figure><p><strong>注意：catalog的cacheTable是lazy的</strong></p></li><li><p>展示所有函数</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">catalog.listFunctions().show(<span class="number">1000</span>，<span class="literal">false</span>)</span><br></pre></td></tr></table></figure></li><li><p>注册函数，再次展示</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">spark.udf.register(<span class="string">"udf_string_length"</span>,(word:<span class="type">String</span>) =&gt; &#123;</span><br><span class="line">    word.split(<span class="string">","</span>).length</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">catalog.listFunctions().filter(<span class="symbol">'name</span> === <span class="string">"udf_string_length"</span>).show(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark源码之解读spark-shell脚本</title>
      <link href="/2019/10/13/spark/13/"/>
      <url>/2019/10/13/spark/13/</url>
      
        <content type="html"><![CDATA[<p>该篇文章主要分析一下Spark源码中启动spark-shell脚本的处理逻辑，从spark-shell一步步深入进去看看任务提交的整体流程</p><ol><li><p>spark-shell脚本解读</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 初始化cygwin=<span class="literal">false</span></span></span><br><span class="line">cygwin=false</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查你的系统是否属于cygwin</span></span><br><span class="line">case "$(uname)" in</span><br><span class="line">  CYGWIN*) cygwin=true;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置shell的模式为POSIX标准模式</span></span><br><span class="line">set -o posix</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检测是否设置过SPARK_HOME环境变量</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">export _SPARK_CMD_USAGE="Usage: ./bin/spark-shell [options]</span><br><span class="line">Scala REPL options:</span><br><span class="line">  -I &lt;file&gt;                   preload &lt;file&gt;, enforcing line-by-line interpretation"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">scala默认不会使用java classpath，需要手动设置一下让scala使用java</span></span><br><span class="line">SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true"</span><br><span class="line"></span><br><span class="line">function main() &#123;</span><br><span class="line"><span class="meta">  #</span><span class="bash"> 判断是否是cygwin</span></span><br><span class="line">  if $cygwin; then</span><br><span class="line">    # 关闭echo回显，设置读操作最少1个字符</span><br><span class="line">    stty -icanon min 1 -echo &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">    export SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Djline.terminal=unix"</span><br><span class="line">    # 启动spark-submit 执行org.apache.spark.repl.Main类，并设置应用的名字，传递参数</span><br><span class="line">    "$&#123;SPARK_HOME&#125;"/bin/spark-submit --class org.apache.spark.repl.Main --name "Spark shell" "$@"</span><br><span class="line">    # 开启echo回显</span><br><span class="line">    stty icanon echo &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">  else</span><br><span class="line">    export SPARK_SUBMIT_OPTS</span><br><span class="line">    # 启动spark-submit 执行org.apache.spark.repl.Main类，并设置应用的名字，传递参数</span><br><span class="line">    "$&#123;SPARK_HOME&#125;"/bin/spark-submit --class org.apache.spark.repl.Main --name "Spark shell" "$@"</span><br><span class="line">  fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">exit_status=127</span><br><span class="line">saved_stty=""</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> restore stty settings (<span class="built_in">echo</span> <span class="keyword">in</span> particular)</span></span><br><span class="line">function restoreSttySettings() &#123;</span><br><span class="line">  stty $saved_stty</span><br><span class="line">  saved_stty=""</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 判断是否恢复终端设置</span></span><br><span class="line">function onExit() &#123;</span><br><span class="line">  if [[ "$saved_stty" != "" ]]; then</span><br><span class="line">    restoreSttySettings</span><br><span class="line">  fi</span><br><span class="line">  exit $exit_status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 捕获INT中断信号，然就执行onExit方法</span></span><br><span class="line">trap onExit INT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 保存了当前的终端配置</span></span><br><span class="line">saved_stty=$(stty -g 2&gt;/dev/null)</span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果收到退出命令，就恢复stty状态</span></span><br><span class="line">if [[ ! $? ]]; then</span><br><span class="line">  saved_stty=""</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 调用main方法，并传递所有的参数</span></span><br><span class="line">main "$@"</span><br><span class="line"></span><br><span class="line">exit_status=$?</span><br><span class="line">onExit</span><br></pre></td></tr></table></figure></li><li><p>上面启动了spark-submit，接下来我们解读该脚本</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 检查是否设置了<span class="variable">$&#123;SPARK_HOME&#125;</span></span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在Python 3.3+中禁用字符串的随机哈希</span></span><br><span class="line">export PYTHONHASHSEED=0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动spark-class，并传递了org.apache.spark.deploy.SparkSubmit作为第一个参数，然后把前面Spark-shell的参数都传给spark-class</span></span><br><span class="line">exec "$&#123;SPARK_HOME&#125;"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"</span><br></pre></td></tr></table></figure></li><li><p>在spark-submit中又启动了spark-class，继续解读</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 检查/设置SPARK_HOME</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置一些环境变量</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;"/bin/load-spark-env.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 找到bin/java,并赋值给RUNNER变量</span></span><br><span class="line">if [ -n "$&#123;JAVA_HOME&#125;" ]; then</span><br><span class="line">  RUNNER="$&#123;JAVA_HOME&#125;/bin/java"</span><br><span class="line">else</span><br><span class="line">  if [ "$(command -v java)" ]; then</span><br><span class="line">    RUNNER="java"</span><br><span class="line">  else</span><br><span class="line">    echo "JAVA_HOME is not set" &gt;&amp;2</span><br><span class="line">    exit 1</span><br><span class="line">  fi  </span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 拿到Spark的Jar包</span></span><br><span class="line">if [ -d "$&#123;SPARK_HOME&#125;/jars" ]; then</span><br><span class="line">  SPARK_JARS_DIR="$&#123;SPARK_HOME&#125;/jars"</span><br><span class="line">else</span><br><span class="line">  SPARK_JARS_DIR="$&#123;SPARK_HOME&#125;/assembly/target/scala-$SPARK_SCALA_VERSION/jars"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ ! -d "$SPARK_JARS_DIR" ] &amp;&amp; [ -z "$SPARK_TESTING$SPARK_SQL_TESTING" ]; then</span><br><span class="line">  echo "Failed to find Spark jars directory ($SPARK_JARS_DIR)." 1&gt;&amp;2</span><br><span class="line">  echo "You need to build Spark with the target \"package\" before running this program." 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">else</span><br><span class="line">  LAUNCH_CLASSPATH="$SPARK_JARS_DIR/*"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果需要，将启动程序构建目录添加到类路径。</span></span><br><span class="line">if [ -n "$SPARK_PREPEND_CLASSES" ]; then</span><br><span class="line">  LAUNCH_CLASSPATH="$&#123;SPARK_HOME&#125;/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line">if [[ -n "$SPARK_TESTING" ]]; then</span><br><span class="line">  unset YARN_CONF_DIR</span><br><span class="line">  unset HADOOP_CONF_DIR</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 封装了真正的执行的spark的类</span></span><br><span class="line">build_command() &#123;</span><br><span class="line">  "$RUNNER" -Xmx128m -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@"</span><br><span class="line">  printf "%d\0" $?</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 关闭posix模式，因为它不允许进程替换</span></span><br><span class="line">set +o posix</span><br><span class="line"></span><br><span class="line">CMD=()</span><br><span class="line"><span class="meta">#</span><span class="bash"> 首先循环读取ARG参数，加入到CMD中</span></span><br><span class="line">while IFS= read -d '' -r ARG; do</span><br><span class="line">  CMD+=("$ARG")</span><br><span class="line"><span class="meta">#</span><span class="bash"> 执行Spark的类</span></span><br><span class="line">done &lt; &lt;(build_command "$@")</span><br><span class="line">COUNT=$&#123;#CMD[@]&#125;</span><br><span class="line">LAST=$((COUNT - 1))</span><br><span class="line">LAUNCHER_EXIT_CODE=$&#123;CMD[$LAST]&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then</span><br><span class="line">  echo "$&#123;CMD[@]&#125;" | head -n-1 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $LAUNCHER_EXIT_CODE != 0 ]; then</span><br><span class="line">  exit $LAUNCHER_EXIT_CODE</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">CMD=("$&#123;CMD[@]:0:$LAST&#125;")</span><br><span class="line">exec "$&#123;CMD[@]&#125;"</span><br></pre></td></tr></table></figure></li></ol><p>可以看到虽然是执行了spark-shell，但是最终执行的是<code>org.apache.spark.launcher.Main</code>类，也就是说spark-shell的最底层是使用java来启动的</p><p>他们的执行流程大致如下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark-shell&#123;</span><br><span class="line">spark-submit&#123;</span><br><span class="line">spark-class&#123;</span><br><span class="line">   build_command() &#123;</span><br><span class="line">   "$RUNNER" -Xmx128m -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@"</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark中的序列化</title>
      <link href="/2019/10/12/spark/12/"/>
      <url>/2019/10/12/spark/12/</url>
      
        <content type="html"><![CDATA[<p>在写Spark应用时，常常会碰到序列化的问题。例如，在Driver端的程序中创建了一个对象，而在各个Executor端会用到这个对象——由于Driver端的代码和Executor端的代码在不同的JVM中，甚至在不同的节点上，因此必然要有相应</p><h2 id="Java框架进行序列化"><a href="#Java框架进行序列化" class="headerlink" title="Java框架进行序列化"></a>Java框架进行序列化</h2><p>在默认情况下，Spark会使用Java的ObjectOutputStream框架对对象进行序列化，并且可以与任何实现java.io.Serializable的类一起工作。您还可以通过扩展java.io.Externalizable来更紧密地控制序列化的性能。Java序列化是灵活的，但通常相当慢，并且会导致许多类的大型序列化格式。</p><p>测试代码：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">String</span>, name: <span class="type">String</span>, age: <span class="type">Int</span>, gender: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">SerializationDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"SerializationDemo"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> stduentArr = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Student</span>]()</span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">1000000</span>) &#123;</span><br><span class="line">            stduentArr += (<span class="type">Student</span>(i + <span class="string">""</span>, i + <span class="string">"a"</span>, <span class="number">10</span>, <span class="string">"male"</span>))</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> <span class="type">JavaSerialization</span> = sc.parallelize(stduentArr)</span><br><span class="line">        <span class="type">JavaSerialization</span>.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>).count()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="type">Thread</span>.sleep(<span class="number">10000</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在Web界面查看：33.2M</p><h2 id="Kryo框架进行序列化"><a href="#Kryo框架进行序列化" class="headerlink" title="Kryo框架进行序列化"></a>Kryo框架进行序列化</h2><p>Spark还可以使用Kryo库（Spark 2.x）来更快地序列化对象。Kryo比Java（通常多达10倍）要快得多，也更紧凑，但是不支持所有可串行化类型，并且要求您提前注册您将在程序中使用的类，以获得最佳性能。</p><ol><li><p>不注册使用的类测试</p><p>在conf中配置spark.serializer = org.apache.spark.serializer.KryoSerializer来使用kryo序列化</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">String</span>, name: <span class="type">String</span>, age: <span class="type">Int</span>, gender: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">SerializationDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">        .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">        .setAppName(<span class="string">"SerializationDemo"</span>)</span><br><span class="line">        .set(<span class="string">"spark.serializer"</span>,<span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> stduentArr = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Student</span>]()</span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">1000000</span>) &#123;</span><br><span class="line">            stduentArr += (<span class="type">Student</span>(i + <span class="string">""</span>, i + <span class="string">"a"</span>, <span class="number">10</span>, <span class="string">"male"</span>))</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> <span class="type">JavaSerialization</span> = sc.parallelize(stduentArr)</span><br><span class="line">        <span class="type">JavaSerialization</span>.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>).count()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="type">Thread</span>.sleep(<span class="number">10000</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在Web界面查看：53.2M</p><p>这是因为使用Kryo时，不将使用的类注册，往往会得到比java序列化占用更大的内存</p></li><li><p>注册使用的类测试</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">String</span>, name: <span class="type">String</span>, age: <span class="type">Int</span>, gender: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">SerializationDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">        .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">        .setAppName(<span class="string">"SerializationDemo"</span>)</span><br><span class="line">        .set(<span class="string">"spark.serializer"</span>,<span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line">        .registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">Student</span>])) <span class="comment">// 将自定义的类注册到Kryo</span></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> stduentArr = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Student</span>]()</span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">1000000</span>) &#123;</span><br><span class="line">            stduentArr += (<span class="type">Student</span>(i + <span class="string">""</span>, i + <span class="string">"a"</span>, <span class="number">10</span>, <span class="string">"male"</span>))</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> <span class="type">JavaSerialization</span> = sc.parallelize(stduentArr)</span><br><span class="line">        <span class="type">JavaSerialization</span>.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>).count()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="type">Thread</span>.sleep(<span class="number">10000</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在Web界面查看：21.7 M</p><p>在conf中注册</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure></li></ol><h2 id="总结及拓展"><a href="#总结及拓展" class="headerlink" title="总结及拓展"></a>总结及拓展</h2><p>Kryo serialization 性能和序列化大小都比默认提供的 Java serialization 要好，但是<strong>使用Kryo需要将自定义的类先注册进去</strong>，使用起来比 Java serialization麻烦。自从Spark 2.x 以来，我们在使用简单类型、简单类型数组或字符串类型的简单类型来调整RDDs时，在内部使用Kryo序列化器。</p><p>通过查找sparkcontext初始化的源码，可以发现某些类型已经在sparkcontext初始化的时候被注册进去。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Component which configures serialization, compression and encryption for various Spark</span></span><br><span class="line"><span class="comment"> * components, including automatic selection of which [[Serializer]] to use for shuffles.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">SerializerManager</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    defaultSerializer: <span class="type">Serializer</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    conf: <span class="type">SparkConf</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    encryptionKey: <span class="type">Option</span>[<span class="type">Array</span>[<span class="type">Byte</span>]]</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(defaultSerializer: <span class="type">Serializer</span>, conf: <span class="type">SparkConf</span>) = <span class="keyword">this</span>(defaultSerializer, conf, <span class="type">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> kryoSerializer = <span class="keyword">new</span> <span class="type">KryoSerializer</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> stringClassTag: <span class="type">ClassTag</span>[<span class="type">String</span>] = implicitly[<span class="type">ClassTag</span>[<span class="type">String</span>]]</span><br><span class="line">    <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> primitiveAndPrimitiveArrayClassTags: <span class="type">Set</span>[<span class="type">ClassTag</span>[_]] = &#123;</span><br><span class="line">        <span class="keyword">val</span> primitiveClassTags = <span class="type">Set</span>[<span class="type">ClassTag</span>[_]](</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Boolean</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Byte</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Char</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Double</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Float</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Int</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Long</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Null</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Short</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">val</span> arrayClassTags = primitiveClassTags.map(_.wrap)</span><br><span class="line">        primitiveClassTags ++ arrayClassTags</span><br></pre></td></tr></table></figure><p>也就是说，Boolean、Byte、Char、Double、Float、Int、Long、Null、Short这些类型修饰的属性，自动使用kryo序列化。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkSQL&amp;DataFrame的read和write&amp;SparkSQL做统计分析&amp;UDF函数&amp;存储格式的转换</title>
      <link href="/2019/10/11/spark/11/"/>
      <url>/2019/10/11/spark/11/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>SparkSQL</li><li>DataFrame的read和write</li><li>SparkSQL做统计分析</li><li>UDF函数</li><li>存储格式的转换</li></ol><h2 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h2><h3 id="认识SparkSQL"><a href="#认识SparkSQL" class="headerlink" title="认识SparkSQL"></a>认识SparkSQL</h3><ol><li><p>SparkSQL的进化之路</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.0以前：</span><br><span class="line">   Shark</span><br><span class="line">1.1.x开始：</span><br><span class="line">   SparkSQL(只是测试性的) SQL</span><br><span class="line">1.3.x:</span><br><span class="line">   SparkSQL(正式版本)+Dataframe</span><br><span class="line">1.5.x:</span><br><span class="line">SparkSQL 钨丝计划</span><br><span class="line">1.6.x：</span><br><span class="line">   SparkSQL+DataFrame+DataSet(测试版本)</span><br><span class="line">2.x.x:</span><br><span class="line">   SparkSQL+DataFrame+DataSet(正式版本)</span><br><span class="line">   SparkSQL:还有其他的优化</span><br><span class="line">   StructuredStreaming(DataSet)</span><br></pre></td></tr></table></figure></li><li><p>什么是SparkSQL?</p><p>spark SQL是spark的一个模块，主要用于进行结构化数据的处理。它提供的最核心的编程抽象就是DataFrame。</p></li><li><p>SparkSQL的作用</p><p>提供一个编程抽象（DataFrame） 并且作为分布式 SQL 查询引擎</p><p>DataFrame：它可以根据很多源进行构建，包括：<strong>结构化的数据文件，hive中的表，外部的关系型数据库，以及RDD</strong></p></li><li><p>运行原理</p><p>将 Spark SQL 转化为 RDD， 然后提交到集群执行</p></li><li><p>特点</p><ol><li>容易整合</li><li>统一的数据访问方式</li><li>兼容 Hive</li><li>标准的数据连接</li></ol></li><li><p>spark-sql</p><p>spark-sql是一个Spark专属的SQL命令行交互工具，在使用spark-sql之前要把hive-site.xml 拷贝到Spark/Conf下，spark-sql和spark-shell用法一样，但是在引入外部依赖的时候，spark-sql需要用–jars和–driver-class-path同时引入依赖才不会报错</p></li><li><p>持久化</p><p>在spark-sql中的持久化Table命令是: cache table xxx，清除持久化 uncache table xxx</p><p>spark-SQL中的cache和uncache都是eager的，立即执行的</p><p><strong>考点：RDD和SparkSQL的cache有什么区别？</strong></p><ul><li>RDD中的cache是lazy的 spark-SQL中的cache是eager的</li></ul></li><li><p>遗留问题</p><p>–files/–jars    传进去的东西清不掉</p></li></ol><h3 id="SparkSession"><a href="#SparkSession" class="headerlink" title="SparkSession"></a>SparkSession</h3><p>SparkSession是Spark 2.0引如的新概念。SparkSession为用户提供了统一的切入点，来让用户学习spark的各项功能。 </p><p>在spark的早期版本中，SparkContext是spark的主要切入点，由于RDD是主要的API，我们通过sparkcontext来创建和操作RDD。对于每个其他的API，我们需要使用不同的context。例如，对于Streming，我们需要使用StreamingContext；对于sql，使用sqlContext；对于Hive，使用hiveContext。但是随着DataSet和DataFrame的API逐渐成为标准的API，就需要为他们建立接入点。所以在spark2.0中，引入SparkSession作为DataSet和DataFrame API的切入点，SparkSession封装了SparkConf、SparkContext和SQLContext。为了向后兼容，SQLContext和HiveContext也被保存下来。 </p><p>SparkSession实质上是SQLContext和HiveContext的组合（未来可能还会加上StreamingContext），所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。</p><p>特点：</p><ol><li><p><strong>为用户提供一个统一的切入点使用Spark 各项功能</strong></p></li><li><p><strong>允许用户通过它调用 DataFrame 和 Dataset 相关 API 来编写程序</strong></p></li><li><p><strong>减少了用户需要了解的一些概念，可以很容易的与 Spark 进行交互</strong></p></li><li><p><strong>与 Spark 交互之时不需要显示的创建 SparkConf, SparkContext 以及 SQlContext，这些对象已经封闭在 SparkSession 中</strong></p></li></ol><h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><p>在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame与RDD的主要区别在于，前者带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得Spark SQL得以洞察更多的结构信息，从而对藏于DataFrame背后的数据源以及作用于DataFrame之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在stage层面进行简单、通用的流水线优化。</p><p><img src="https://yerias.github.io/spark_img/RDD%E5%92%8CDataFrame%E7%9A%84%E5%AD%98%E5%82%A8%E5%86%85%E5%AE%B9%E6%AF%94%E8%BE%83.png" alt="RDD和DataFrame的存储内容比较"></p><h2 id="DataFrame的read和write"><a href="#DataFrame的read和write" class="headerlink" title="DataFrame的read和write"></a>DataFrame的read和write</h2><h3 id="json"><a href="#json" class="headerlink" title="json"></a>json</h3><ol><li><p>数据的读取[DataFrameReader]</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">rdd2df</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-sql/data/people.json"</span></span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"><span class="comment">//  读取json数据</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.format(<span class="string">"json"</span>).load(in)</span><br><span class="line">        <span class="comment">//  使用$"" 导入隐式转换</span></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        <span class="comment">//  可以使用UDF</span></span><br><span class="line">        df.select($<span class="string">"name"</span>,$<span class="string">"age"</span>).show(<span class="number">2</span>,<span class="literal">false</span>)</span><br><span class="line">        <span class="comment">//  不可以使用UDF 适合大部分场景</span></span><br><span class="line">        df.select(<span class="string">"name"</span>,<span class="string">"age"</span>).show()</span><br><span class="line">        <span class="comment">//  不推介，写着复杂</span></span><br><span class="line">        df.select(df(<span class="string">"name"</span>),df(<span class="string">"age"</span>)).show(<span class="number">2</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>select方法用于选择要输出的列，推介使用 $”col” 和 “col” 的方法</p><ol><li>使用select可以选取打印的列，空值为null</li><li>show()默认打印20条数据，可以指定条数</li><li>truncate默认为true，截取长度，可以设置为false</li></ol><p>select方法有三种不同的写法，fliter也有</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.select(df(<span class="string">"name"</span>),df(<span class="string">"age"</span>)).filter(<span class="symbol">'name</span> === <span class="string">"Andy"</span>).show()<span class="comment">//推介使用</span></span><br><span class="line">df.select(df(<span class="string">"name"</span>),df(<span class="string">"age"</span>)).filter(df(<span class="string">"name"</span>) === <span class="string">"Andy"</span>).show()</span><br><span class="line">df.select(df(<span class="string">"name"</span>),df(<span class="string">"age"</span>)).filter(<span class="string">"name = 'Andy'"</span>).show()</span><br></pre></td></tr></table></figure><p>printSchema()方法可以查看数据的Schema信息</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.printSchema()</span><br><span class="line">------------------------------------------------</span><br><span class="line">root</span><br><span class="line"> |-- age: long (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- name: string (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure></li><li><p>数据的存储[DataFrameWriter]</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> selectDf: <span class="type">DataFrame</span> = df.select($<span class="string">"name"</span>, $<span class="string">"age"</span>)</span><br><span class="line"><span class="comment">//  写出json数据</span></span><br><span class="line">selectDf.write.format(<span class="string">"json"</span>).mode(<span class="string">"overwrite"</span>).save(out)</span><br></pre></td></tr></table></figure><p>这里需要知道的一个概念是Save Modes</p><p>Save操作可以选择使用SaveMode，它指定目标如果存在，如何处理现有数据。重要的是要认识到，这些保存模式不利用任何锁定，也不是原子性的。此外，在执行覆盖时，在写入新数据之前将删除数据。</p><table><thead><tr><th align="left">Scala/Java</th><th align="left">Any Language</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><code>SaveMode.ErrorIfExists</code> (default)</td><td align="left"><code>&quot;error&quot; or &quot;errorifexists&quot;</code> (default)</td><td align="left">在将DataFrame保存到数据源时，如果数据已经存在，则会抛出error。</td></tr><tr><td align="left"><code>SaveMode.Append</code></td><td align="left"><code>&quot;append&quot;</code></td><td align="left">在将DataFrame保存到数据源时，如果数据/表已经存在，则DataFrame的内容将被append到现有数据中。</td></tr><tr><td align="left"><code>SaveMode.Overwrite</code></td><td align="left"><code>&quot;overwrite&quot;</code></td><td align="left">overwrite模式意味着在将DataFrame保存到数据源时，如果数据/表已经存在，则现有数据将被DataFrame的内容覆盖。</td></tr><tr><td align="left"><code>SaveMode.Ignore</code></td><td align="left"><code>&quot;ignore&quot;</code></td><td align="left">ignore模式意味着在将DataFrame保存到数据源时，如果数据已经存在，则save操作不保存DataFrame的内容，也不更改现有数据。这类似于SQL中的<code>CREATE TABLE IF NOT EXISTS</code>。</td></tr></tbody></table></li></ol><h3 id="text"><a href="#text" class="headerlink" title="text"></a>text</h3><ol><li><p>数据的读取</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">text2df</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-sql/data/people.txt"</span></span><br><span class="line">        <span class="keyword">val</span> out = <span class="string">"tunan-spark-sql/out"</span></span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">        <span class="comment">//DataFrame不能直接split，且调用map返回的是一个Dataset</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.format(<span class="string">"text"</span>).load(in)</span><br><span class="line">        <span class="keyword">val</span> mapDF: <span class="type">Dataset</span>[(<span class="type">String</span>, <span class="type">String</span>)] = df.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.toString().split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">        mapDF.show()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//DataFrame转换为RDD后，再toDF，返回的是一个DataFrame</span></span><br><span class="line">        <span class="keyword">val</span> mapRDD2DF: <span class="type">DataFrame</span> = df.rdd.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.toString().split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;).toDF()</span><br><span class="line">        mapRDD2DF.show()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用textFile方法读取文本文件直接返回的是一个Dataset</span></span><br><span class="line">        <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(in)</span><br><span class="line">        <span class="keyword">val</span> mapDs: <span class="type">Dataset</span>[(<span class="type">String</span>, <span class="type">String</span>)] = ds.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">        mapDs.show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>文本数据读进来的一行在一个字段里面，所以要使用map算子，在map中split</p><ol><li>直接read.format()读进来的是DataFrame，map中不能直接split</li><li>DataFrame通过.rdd的方式转换成RDD，map中也不能直接split</li><li>通过read.textFile()的方式读进来的是Dataset，map中可以split</li></ol></li><li><p>数据的存储</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.format(<span class="string">"text"</span>).load(in)</span><br><span class="line"><span class="keyword">val</span> mapDF = df.map(row =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> words = row.toString().split(<span class="string">","</span>)</span><br><span class="line">    <span class="comment">// 拼接成一列</span></span><br><span class="line">    words(<span class="number">0</span>) +<span class="string">","</span>+words(<span class="number">1</span>)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">mapDF.write.format(<span class="string">"text"</span>).mode(<span class="string">"overwrite"</span>).save(out)</span><br></pre></td></tr></table></figure><p>文本数据写出去的时候</p><ol><li>不支持int类型，如果存在int类型，会报错，解决办法是toString，转换成字符串</li><li>只能作为一列输出，如果是多列，会报错，解决办法是拼接起来，组成一列</li></ol><p><strong>文本数据压缩输出，只要是Spark支持的压缩的格式，都可以指定</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapDF.write</span><br><span class="line">    .format(<span class="string">"text"</span>)</span><br><span class="line">    <span class="comment">// 添加压缩操作</span></span><br><span class="line">    .option(<span class="string">"compression"</span>,<span class="string">"gzip"</span>)</span><br><span class="line">    .mode(<span class="string">"overwrite"</span>)</span><br><span class="line">    .save(out)</span><br></pre></td></tr></table></figure></li></ol><h3 id="csv"><a href="#csv" class="headerlink" title="csv"></a>csv</h3><ol><li><p>数据的读取</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">csv2df</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-sql/data/people.csv"</span></span><br><span class="line">        <span class="keyword">val</span> out = <span class="string">"tunan-spark-sql/out"</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read</span><br><span class="line">            .format(<span class="string">"csv"</span>)</span><br><span class="line">            .option(<span class="string">"header"</span>, <span class="string">"true"</span>)</span><br><span class="line">            .option(<span class="string">"sep"</span>, <span class="string">";"</span>)</span><br><span class="line">            .option(<span class="string">"interSchema"</span>,<span class="string">"true"</span>)</span><br><span class="line">            .load(in)</span><br><span class="line">        df.show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>csv读取数据注意使用几个参数</p><ol><li>指定表头：<code>option(&quot;header&quot;, &quot;true&quot;)</code></li><li>指定分隔符：<code>option(&quot;sep&quot;, &quot;;&quot;)</code></li><li>类型自动推测：<code>option(&quot;interSchema&quot;,&quot;true&quot;)</code></li></ol></li></ol><h3 id="jdbc"><a href="#jdbc" class="headerlink" title="jdbc"></a>jdbc</h3><p>在操作jdbc之前要导入两个依赖，一个是mysql-jdbc，用来连接mysql，一个是config，用来解决硬编码的问题</p><p>依赖：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.typesafe<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>config<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.38<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>application.conf文件</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">db.<span class="keyword">default</span>.driver=<span class="string">"com.mysql.jdbc.Driver"</span></span><br><span class="line">db.<span class="keyword">default</span>.url=<span class="string">"jdbc:mysql://hadoop/listener?characterEncoding=utf-8&amp;useSSL=false"</span></span><br><span class="line">db.<span class="keyword">default</span>.user=<span class="string">"root"</span></span><br><span class="line">db.<span class="keyword">default</span>.password=<span class="string">"root"</span></span><br><span class="line">db.<span class="keyword">default</span>.source=<span class="string">"dws_ad_phone_type_dist"</span></span><br><span class="line">db.<span class="keyword">default</span>.target=<span class="string">"dws_ad_phone_type_dist_1"</span></span><br><span class="line">db.<span class="keyword">default</span>.db=<span class="string">"access_dw"</span></span><br><span class="line"></span><br><span class="line"># Connection Pool settings</span><br><span class="line">db.<span class="keyword">default</span>.poolInitialSize=<span class="number">10</span></span><br><span class="line">db.<span class="keyword">default</span>.poolMaxSize=<span class="number">20</span></span><br><span class="line">db.<span class="keyword">default</span>.connectionTimeoutMillis=<span class="number">1000</span></span><br></pre></td></tr></table></figure><ol><li><p>数据的读取</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">mysql2df</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取配置文件中的值，db.default开头</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="type">ConfigFactory</span>.load()</span><br><span class="line">        <span class="keyword">val</span> driver = conf.getString(<span class="string">"db.default.driver"</span>)</span><br><span class="line">        <span class="keyword">val</span> url = conf.getString(<span class="string">"db.default.url"</span>)</span><br><span class="line">        <span class="keyword">val</span> user = conf.getString(<span class="string">"db.default.user"</span>)</span><br><span class="line">        <span class="keyword">val</span> password = conf.getString(<span class="string">"db.default.password"</span>)</span><br><span class="line">        <span class="keyword">val</span> source = conf.getString(<span class="string">"db.default.source"</span>)</span><br><span class="line">        <span class="keyword">val</span> target = conf.getString(<span class="string">"db.default.target"</span>)</span><br><span class="line">        <span class="keyword">val</span> db = conf.getString(<span class="string">"db.default.db"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取数据库的内容</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read</span><br><span class="line">            .format(<span class="string">"jdbc"</span>)</span><br><span class="line">            .option(<span class="string">"url"</span>, url)</span><br><span class="line">            .option(<span class="string">"dbtable"</span>, <span class="string">s"<span class="subst">$db</span>.<span class="subst">$source</span>"</span>)<span class="comment">//库名.源表</span></span><br><span class="line">            .option(<span class="string">"user"</span>, user)</span><br><span class="line">            .option(<span class="string">"password"</span>, password)</span><br><span class="line">            .option(<span class="string">"driver"</span>, driver)</span><br><span class="line">            .load()</span><br><span class="line">        <span class="comment">//使用DataFrame创建临时表提供spark.sql查询</span></span><br><span class="line">        df.createOrReplaceTempView(<span class="string">"phone_type_dist"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//spark.sql写SQL返回一个DataFrame</span></span><br><span class="line">        <span class="keyword">val</span> sqlDF: <span class="type">DataFrame</span> = spark.sql(<span class="string">"select * from phone_type_dist where phoneSystemType = 'IOS'"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>使用df.createOrReplaceTempView()方法创建一个DataFrame数据生成的临时表，提供spark.sql()使用SQL操作数据，返回的也是一个DataFrame</li></ul></li><li><p>数据的存储</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//接着上面返回的sqlDF: DataFrame</span></span><br><span class="line">sqlDF.write</span><br><span class="line">    .format(<span class="string">"jdbc"</span>)</span><br><span class="line">    .option(<span class="string">"url"</span>, url)</span><br><span class="line">    .option(<span class="string">"dbtable"</span>, <span class="string">s"<span class="subst">$db</span>.<span class="subst">$target</span>"</span>)<span class="comment">//库名.目标表</span></span><br><span class="line">    .option(<span class="string">"user"</span>, user)</span><br><span class="line">    .option(<span class="string">"password"</span>, password)</span><br><span class="line">    .option(<span class="string">"driver"</span>,driver)</span><br><span class="line">    .save()</span><br></pre></td></tr></table></figure></li></ol><h2 id="SparkSQL做统计分析"><a href="#SparkSQL做统计分析" class="headerlink" title="SparkSQL做统计分析"></a>SparkSQL做统计分析</h2><ol><li><p>数据</p></li><li><p>需求：求每个国家的每个域名的访问流量排名前2</p></li><li><p>SQL实现</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GroupTopN</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">//读取数据</span></span><br><span class="line">        <span class="keyword">val</span> ds = spark.read.textFile(in)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        <span class="comment">//为生成需要的表格做准备</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = ds.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">3</span>), words(<span class="number">12</span>), words(<span class="number">15</span>).toLong)</span><br><span class="line">        &#125;).toDF(<span class="string">"country"</span>, <span class="string">"domain"</span>, <span class="string">"traffic"</span>)</span><br><span class="line"></span><br><span class="line">        df.createOrReplaceTempView(<span class="string">"access"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 每个国家的域名流量前2</span></span><br><span class="line">        <span class="keyword">val</span> topNSQL=<span class="string">""</span><span class="string">"select</span></span><br><span class="line"><span class="string">                      |*</span></span><br><span class="line"><span class="string">                      |from (</span></span><br><span class="line"><span class="string">                      |select</span></span><br><span class="line"><span class="string">                      |t.*,row_number() over(partition by country order by sum_traffic desc) r</span></span><br><span class="line"><span class="string">                      |from</span></span><br><span class="line"><span class="string">                      |(</span></span><br><span class="line"><span class="string">                      |select country,domain,sum(traffic) as sum_traffic from access group by country,domain</span></span><br><span class="line"><span class="string">                      |) t</span></span><br><span class="line"><span class="string">                      |) rt</span></span><br><span class="line"><span class="string">                      |where rt.r &lt;=2 "</span><span class="string">""</span>.stripMargin</span><br><span class="line"></span><br><span class="line">        spark.sql(topNSQL).show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>如果只要求traffic的降序，可以使用API直接写出来</p><p>分组，求和，别名，降序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//traffic降序排序</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">df.groupBy(<span class="string">"country"</span>,<span class="string">"domain"</span>).agg(sum(<span class="string">"traffic"</span>).as(<span class="string">"sum_traffic"</span>)).sort($<span class="string">"sum_traffic"</span>.desc).show()</span><br></pre></td></tr></table></figure><p><strong>注意看源码中案例仿写</strong></p></li><li><p>结果展示</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+----------------+-----------------+-----------+---+</span><br><span class="line">|         country|           domain|sum_traffic|  r|</span><br><span class="line">+----------------+-----------------+-----------+---+</span><br><span class="line">|            中国| www.bilibili.com|   <span class="number">24265886</span>|  <span class="number">1</span>|</span><br><span class="line">|            中国|www.ruozedata.com|    <span class="number">4187637</span>|  <span class="number">2</span>|</span><br><span class="line">|          利比亚| www.bilibili.com|      <span class="number">22816</span>|  <span class="number">1</span>|</span><br><span class="line">|          利比亚|  ruoze.ke.qq.com|      <span class="number">15970</span>|  <span class="number">2</span>|</span><br><span class="line">|            加纳| www.bilibili.com|     <span class="number">138659</span>|  <span class="number">1</span>|</span><br><span class="line">|            加纳|www.ruozedata.com|      <span class="number">17988</span>|  <span class="number">2</span>|</span><br><span class="line">|        利比里亚| www.bilibili.com|      <span class="number">20593</span>|  <span class="number">1</span>|</span><br><span class="line">|        利比里亚|  ruoze.ke.qq.com|       <span class="number">7466</span>|  <span class="number">2</span>|</span><br><span class="line">+----------------+-----------------+-----------+---+</span><br></pre></td></tr></table></figure></li></ol><h2 id="UDF函数"><a href="#UDF函数" class="headerlink" title="UDF函数"></a>UDF函数</h2><ol><li><p>数据</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">大狗小破车,渣团,热刺,我纯</span><br><span class="line">桶子利物浦</span><br><span class="line">二娃南大王,西班牙人</span><br></pre></td></tr></table></figure></li><li><p>需求：求出每个人的爱好的个数</p></li><li><p>SQLs实现</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LoveLength</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取文本内容</span></span><br><span class="line">        <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(in)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//文本转换成DF</span></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = ds.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.split(<span class="string">"\t"</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;).toDF(<span class="string">"name"</span>, <span class="string">"love"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建UDF</span></span><br><span class="line">        spark.udf.register(<span class="string">"length"</span>, (love: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">            love.split(<span class="string">","</span>).length</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//DF创建临时表</span></span><br><span class="line">        df.createOrReplaceTempView(<span class="string">"udf_love"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//在sql中使用UDF函数</span></span><br><span class="line">        spark.sql(<span class="string">"select name,love,length(love) as love_length from udf_love"</span>).show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>上面是使用SQL的解决方案，还可以使用API的方法</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//自定义的udf需要返回值</span></span><br><span class="line"><span class="keyword">val</span> loveLengthUDF: <span class="type">UserDefinedFunction</span> = spark.udf.register(<span class="string">"length"</span>, (love: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">    love.split(<span class="string">","</span>).length</span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment">//df.select中传入UDF函数</span></span><br><span class="line">df.select($<span class="string">"name"</span>,$<span class="string">"love"</span>,loveLengthUDF($<span class="string">"love"</span>)).show()</span><br></pre></td></tr></table></figure></li><li><p>结果展示</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+----+---------------------+----------------+</span><br><span class="line">|大狗|小破车,渣团,热刺,我纯   |               <span class="number">4</span>|</span><br><span class="line">|桶子|               利物浦 |               <span class="number">1</span>|</span><br><span class="line">|二娃|      南大王,西班牙人  |               <span class="number">2</span>|</span><br><span class="line">+----+---------------------+----------------+</span><br></pre></td></tr></table></figure></li></ol><h2 id="存储格式的转换"><a href="#存储格式的转换" class="headerlink" title="存储格式的转换"></a>存储格式的转换</h2><p>Spark读text文件进行清洗，清洗完以后直接以我们想要的列式存储格式输出，如果按以前的方式要经过很多复杂的步骤</p><p>用Spark的时候只需要在<code>df.write.format(&quot;orc&quot;).mode().save()</code>中指定格式即可，如orc，现在就很方便了，想转成什么格式，只要format支持就ok</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">text2orc</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-sql/data/people.txt"</span></span><br><span class="line">        <span class="keyword">val</span> out = <span class="string">"tunan-spark-sql/out"</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> df = spark.read.textFile(in)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//对文本文件做处理</span></span><br><span class="line">        df.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>),words(<span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">            .toDF(<span class="string">"name"</span>,<span class="string">"age"</span>)<span class="comment">//这一步解决了数据没有表头的问题</span></span><br><span class="line">            .write</span><br><span class="line">            .mode(<span class="string">"overwrite"</span>)<span class="comment">//save mode</span></span><br><span class="line">            .format(<span class="string">"orc"</span>)<span class="comment">//save format</span></span><br><span class="line">            .save(out)<span class="comment">//save path</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>20200416更新：</em> df.write.format(“…”).option(“compression”,”…”)   ==&gt; 存储格式+压缩格式</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark之分组TopN模块</title>
      <link href="/2019/10/10/spark/10/"/>
      <url>/2019/10/10/spark/10/</url>
      
        <content type="html"><![CDATA[<p>在Spark中，分组TopN好写，但是如果想写出性能好的代码却也很难。下面我们将通过写TopN的方式，找出问题，解决问题。</p><ol><li><p>直接reduceByKey完成分组求和排序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> in = <span class="string">"file:///home/hadoop/data/site.log"</span></span><br><span class="line">    <span class="comment">//连接SparkMaster</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">        ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)<span class="comment">//((domain,url),1)</span></span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = mapRDD.reduceByKey(_ + _).groupBy(x =&gt; x._1._1).mapValues( x=&gt; x.toList.sortBy(x =&gt; -x._2).map(x =&gt; (x._1._1,x._1._2,x._2)).take(<span class="number">2</span>))</span><br><span class="line">    result.foreach(println)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该方法虽然直接，但是在reduceByKey和groupBy分别进过了shuffle，而且x.toList是一个非常吃内存的操作，如果数据量大，直接OOM</p></li><li><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> in = <span class="string">"tunan-spark-core/data/site.log"</span></span><br><span class="line">    <span class="comment">//连接SparkMaster</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">        ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> domains = <span class="type">Array</span>(<span class="string">"www.google.com"</span>, <span class="string">"www.ruozedata.com"</span>, <span class="string">"www.baidu.com"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (domain &lt;- domains)&#123;</span><br><span class="line">        mapRDD.filter(x =&gt; x._1._1.equals(domain)).reduceByKey(_+_).sortBy(x =&gt; -x._2).take(<span class="number">2</span>).foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>核心思想：把需要分组分类的数据提前拿出来，在filter中过滤，每次执行一个分组，虽然减少了一次shuffle，但是我们不可能每次都把需要的数据都能提前拿到数据</p></li><li><p>使用ditinct.collect返回的数组替换人为创建的数组</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> in = <span class="string">"tunan-spark-core/data/site.log"</span></span><br><span class="line">    <span class="comment">//连接SparkMaster</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">        ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> domains = mapRDD.map(x =&gt; x._1._1).distinct().collect()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (domain &lt;- domains)&#123;</span><br><span class="line">        mapRDD.filter( x =&gt; domain.equals(x._1._1)).reduceByKey(_+_).sortBy(x =&gt; -x._2).take(<span class="number">2</span>).foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>有人说distinct性能不好，但是我们这里使用去重的是domain，这个数据量并不是很大，可以勉强接受，现在每次都使用for循环来处理数据，能不能更加优化一下呢</p></li><li><p>使用分区执行替换for循环</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> in = <span class="string">"tunan-spark-core/data/site.log"</span></span><br><span class="line">    <span class="comment">//连接SparkMaster</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">        ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> domains = mapRDD.map(x =&gt; x._1._1).distinct().collect()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapPartRDD = mapRDD.reduceByKey(<span class="keyword">new</span> <span class="type">MyPartitioner</span>(domains), _ + _).mapPartitions(partition =&gt; &#123;</span><br><span class="line">        partition.toList.sortBy(x =&gt; -x._2).take(<span class="number">2</span>).iterator</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    mapPartRDD.foreach(println)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>自定义的分区类</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span>(<span class="params">domains:<span class="type">Array</span>[<span class="type">String</span>]</span>) <span class="keyword">extends</span> <span class="title">Partitioner</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> map = mutable.<span class="type">HashMap</span>[<span class="type">String</span>,<span class="type">Int</span>]()</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until (domains.length))&#123;</span><br><span class="line">        map(domains(i)) = i</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = domains.length</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> domain = key.asInstanceOf[(<span class="type">String</span>, <span class="type">String</span>)]._1</span><br><span class="line">        map(domain)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这么做的好处是原本一起计算的RDD，现在每个分区里面去计算了，虽然toList内存占用大，但是还凑合，最终的版本就是把toList替换掉。</p></li><li><p>使用TreeSet替换toList实现最终的排序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-core/data/site.log"</span></span><br><span class="line">        <span class="comment">//连接SparkMaster</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">            ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> domains = mapRDD.map(x =&gt; x._1._1).distinct().collect()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> ord: <span class="type">Ordering</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = <span class="keyword">new</span> <span class="type">Ordering</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)]() &#123;</span><br><span class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: ((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>), y: ((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)): <span class="type">Int</span> = &#123;</span><br><span class="line">                <span class="keyword">if</span> (!x._1.equals(y._1) &amp;&amp; x._2 == y._2) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//  降序排</span></span><br><span class="line">                y._2 - x._2</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> treeSort = mapRDD.reduceByKey(<span class="keyword">new</span> <span class="type">MyPartitioner</span>(domains), _ + _).mapPartitions(partition =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> set = mutable.<span class="type">TreeSet</span>.empty(ord)</span><br><span class="line">            partition.foreach(x =&gt; &#123;</span><br><span class="line">                set.add(x)</span><br><span class="line">                <span class="keyword">if</span> (set.size &gt; <span class="number">2</span>) &#123;</span><br><span class="line">                    set.remove(set.lastKey) <span class="comment">//移除最后一个</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            set.toIterator</span><br><span class="line">        &#125;).collect()</span><br><span class="line">        treeSort.foreach(println)</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>使用TreeSet实现自定义排序器，使之每次维护的只有需要的极少量数据，这样占用内存少，效率最高。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>经典案例&amp;多目录输出&amp;计数器&amp;持久化&amp;广播变量</title>
      <link href="/2019/10/09/spark/9/"/>
      <url>/2019/10/09/spark/9/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>经典案例</li><li>多目录输出</li><li>计数器</li><li>持久化</li><li>广播变量</li></ol><h2 id="经典案例"><a href="#经典案例" class="headerlink" title="经典案例"></a>经典案例</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 用户     节目            展示 点击</span></span><br><span class="line"><span class="comment"> * 001,一起看|电视剧|军旅|亮剑,1,1</span></span><br><span class="line"><span class="comment"> * 001,一起看|电视剧|军旅|亮剑,1,0</span></span><br><span class="line"><span class="comment"> * 002,一起看|电视剧|军旅|士兵突击,1,1</span></span><br><span class="line"><span class="comment"> * ==&gt;</span></span><br><span class="line"><span class="comment"> * 001,一起看,2,1</span></span><br><span class="line"><span class="comment"> * 001,电视剧,2,1</span></span><br><span class="line"><span class="comment"> * 001,军旅,2,1</span></span><br><span class="line"><span class="comment"> * 001,亮剑,2,1</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">exercise02</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> linesRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"tunan-spark-core/data/test2.txt"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用map返回的是一个数组，我不要数组，就使用flatMap</span></span><br><span class="line">        <span class="keyword">import</span> com.tunan.spark.utils.<span class="type">ImplicitAspect</span>.rdd2RichRDD</span><br><span class="line">        <span class="keyword">val</span> map2RDD: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), (<span class="type">Int</span>, <span class="type">Int</span>))] = linesRDD.flatMap(line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">","</span>)</span><br><span class="line">            <span class="keyword">val</span> programs: <span class="type">Array</span>[<span class="type">String</span>] = words(<span class="number">1</span>).split(<span class="string">"\\|"</span>)</span><br><span class="line">            <span class="keyword">val</span> mapRDD: <span class="type">Array</span>[((<span class="type">String</span>, <span class="type">String</span>), (<span class="type">Int</span>, <span class="type">Int</span>))] = programs.map(program =&gt; ((words(<span class="number">0</span>), program), (words(<span class="number">2</span>).toInt, words(<span class="number">3</span>).toInt)))</span><br><span class="line">            mapRDD</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="keyword">val</span> groupRDD: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Iterable</span>[(<span class="type">Int</span>, <span class="type">Int</span>)])] = map2RDD.groupByKey()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//这里是mapValues很好的一个使用案例</span></span><br><span class="line">        <span class="keyword">val</span> mapVRDD: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), (<span class="type">Int</span>, <span class="type">Int</span>))] = groupRDD.mapValues(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> imps: <span class="type">Int</span> = x.map(_._1).sum</span><br><span class="line">            <span class="keyword">val</span> check: <span class="type">Int</span> = x.map(_._2).sum</span><br><span class="line">            (imps, check)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//格式化输出</span></span><br><span class="line">        mapVRDD.map(x =&gt; &#123;</span><br><span class="line">            (x._1._1,x._1._2,x._2._1,x._2._1)</span><br><span class="line">        &#125;).print()</span><br><span class="line"></span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="多目录输出"><a href="#多目录输出" class="headerlink" title="多目录输出"></a>多目录输出</h2><ol><li><p>实现多目录输出自定义类</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.<span class="type">NullWritable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.lib.<span class="type">MultipleTextOutputFormat</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyMultipleTextOutputFormat</span> <span class="keyword">extends</span> <span class="title">MultipleTextOutputFormat</span>[<span class="type">Any</span>,<span class="type">Any</span>] </span>&#123;</span><br><span class="line">    <span class="comment">//生成最终生成的key的类型，这里不要，给Null</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">generateActualKey</span></span>(key: <span class="type">Any</span>, value: <span class="type">Any</span>): <span class="type">Any</span> = <span class="type">NullWritable</span>.get()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//生成最终生成的value的类型，这里是String</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">generateActualValue</span></span>(key: <span class="type">Any</span>, value: <span class="type">Any</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">        value.asInstanceOf[<span class="type">String</span>]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//生成文件名</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">generateFileNameForKeyValue</span></span>(key: <span class="type">Any</span>, value: <span class="type">Any</span>, name: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="string">s"<span class="subst">$key</span>/<span class="subst">$name</span>"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>主类，使用<code>saveAsHadoopFile(path, keyClass, valueClass, fm.runtimeClass.asInstanceOf[Class[F]])</code>方法保存数据，指定参数</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MultipleDirectory</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> out = <span class="string">"tunan-spark-core/out"</span></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">        <span class="type">CheckHDFSOutPath</span>.ifExistsDeletePath(sc.hadoopConfiguration,out)</span><br><span class="line">        <span class="comment">//读取数组，转换成键值对的格式</span></span><br><span class="line">        <span class="keyword">val</span> lines = sc.textFile(<span class="string">"tunan-spark-core/ip/access-result/*"</span>)</span><br><span class="line">        <span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = lines.map(line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = line.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">12</span>), line)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">//多目录保存文件</span></span><br><span class="line">        mapRDD.saveAsHadoopFile(out,classOf[<span class="type">String</span>],classOf[<span class="type">String</span>],classOf[<span class="type">MyMultipleTextOutputFormat</span>])</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>结果</p><p><img src="https://yerias.github.io/spark_img/%E5%A4%9A%E7%9B%AE%E5%BD%95%E8%BE%93%E5%87%BA.jpg" alt="多目录输出"></p></li></ol><p>在spark程序中，当一个传递给Spark操作(例如map和reduce)的函数在远程节点上面运行时，Spark操作实际上操作的是这个函数所用变量的一个<strong>独立副本</strong>。这些变量会被复制到每台机器上，并且这些变量在远程机器上的所有更新都不会传递回驱动程序。通常跨任务的读写变量是低效的，但是，Spark还是为两种常见的使用模式提供了两种有限的共享变量：广播变量（broadcast variable）和累加器（accumulator）</p><h2 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h2><h3 id="为什么要定义计数器？"><a href="#为什么要定义计数器？" class="headerlink" title="为什么要定义计数器？"></a>为什么要定义计数器？</h3><p>在spark应用程序中，我们经常会有这样的需求，如<em>异常监控</em>，<em>调试</em>，<em>记录符合某特性的数据的数目</em>，这种需求都需要用到计数器，如果一个变量不被声明为一个累加器，那么它将在被改变时不会再driver端进行全局汇总，即在分布式运行时每个task运行的只是原始变量的一个副本，并不能改变原始变量的值，但是当这个变量被声明为累加器后，该变量就会有分布式计数的功能。</p><h3 id="图解计数器"><a href="#图解计数器" class="headerlink" title="图解计数器"></a>图解计数器</h3><p>错误的图解</p><p><img src="https://yerias.github.io/spark_img/%E7%B4%AF%E5%8A%A0%E5%99%A8%E9%94%99%E8%AF%AF%E7%9A%84%E5%9B%BE%E8%A7%A3.png" alt="累加器错误的图解"></p><p>正确的图解</p><p><img src="https://yerias.github.io/spark_img/%E7%B4%AF%E5%8A%A0%E5%99%A8%E6%AD%A3%E7%A1%AE%E7%9A%84%E5%9B%BE%E8%A7%A3.png" alt="累加器正确的图解"></p><p>计数器种类很多，但是经常用的就是两种，<code>longAccumulator</code>和<code>collectionAccumulator</code></p><p><strong>需要注意的是计数器是lazy的，只有触发action才会进行计数，在不持久化的情况下重复触发action，计数器会重复累加</strong></p><h3 id="LongAccumulator"><a href="#LongAccumulator" class="headerlink" title="LongAccumulator"></a>LongAccumulator</h3><p>Accumulators 是只能通过associative和commutative操作“added”的变量，因此可以有效地并行支持。它们可用于实现计数器(如MapReduce)和Spark本身支持数字类型的累加器，程序员还<strong>可以添加对新类型的支持</strong>。</p><p><code>longAccumulator</code>通过累加的方式计数</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyLongAccumulator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">        <span class="keyword">var</span> acc = sc.longAccumulator(<span class="string">"计数"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> forRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">            <span class="comment">// 计数器做累加</span></span><br><span class="line">            acc.add(<span class="number">1</span>L)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">// action操作 </span></span><br><span class="line">        forRDD.count()</span><br><span class="line">       </span><br><span class="line">        println(acc.value)<span class="comment">// 9</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用<code>longAccumulator</code>做计数的时候要小心重复执行action导致的acc.value的变化</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyLongAccumulatorV2</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">        <span class="comment">//生成计数器</span></span><br><span class="line">        <span class="keyword">val</span> acc = sc.longAccumulator(<span class="string">"计数"</span>)</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">8</span>)</span><br><span class="line">        <span class="keyword">val</span> forRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">            <span class="comment">//计数器做累加</span></span><br><span class="line">            acc.add(<span class="number">1</span>L)</span><br><span class="line">        &#125;)</span><br><span class="line">        forRDD.count()</span><br><span class="line">        println(acc.value)<span class="comment">//8</span></span><br><span class="line">        forRDD.count()</span><br><span class="line">        println(acc.value)<span class="comment">//16</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于重复执行了count()，累加器的数量成倍增长，解决这种错误累加也很简单，就是在count之前调用forRDD的cache方法(或persist)，这样在count后数据集就会被缓存下来，reduce操作就会读取缓存的数据集，而无需从头开始计算。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyLongAccumulator</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">        <span class="comment">//生成计数器</span></span><br><span class="line">        <span class="keyword">val</span> acc = sc.longAccumulator(<span class="string">"计数"</span>)</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">8</span>)</span><br><span class="line">        <span class="keyword">val</span> forRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">            <span class="comment">//计数器做累加</span></span><br><span class="line">            acc.add(<span class="number">1</span>L)</span><br><span class="line">        &#125;)</span><br><span class="line">        forRDD.cache().count()</span><br><span class="line">        println(acc.value)<span class="comment">//8</span></span><br><span class="line">        forRDD.count()</span><br><span class="line">        println(acc.value)<span class="comment">//8</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="CollectionAccumulator"><a href="#CollectionAccumulator" class="headerlink" title="CollectionAccumulator"></a>CollectionAccumulator</h3><p><code>collectionAccumulator</code>，集合计数器，计数器中保存的是集合元素，通过泛型指定。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：id后三位相同的加入计数器</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyCollectionAccumulator</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sc  = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//生成集合计数器</span></span><br><span class="line">        <span class="keyword">val</span> acc = sc.collectionAccumulator[<span class="type">People</span>](<span class="string">"集合计数器"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//生成RDD</span></span><br><span class="line">        <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">People</span>] = sc.parallelize(<span class="type">Array</span>(<span class="type">People</span>(<span class="string">"tunan"</span>, <span class="number">100000</span>), <span class="type">People</span>(<span class="string">"xiaoqi"</span>, <span class="number">100001</span>), <span class="type">People</span>(<span class="string">"张三"</span>, <span class="number">100222</span>), <span class="type">People</span>(<span class="string">"李四"</span>, <span class="number">100003</span>)))</span><br><span class="line"></span><br><span class="line"> <span class="comment">//map操作</span></span><br><span class="line">        rdd.map(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> id2 = x.id.toString.reverse</span><br><span class="line">            <span class="comment">//满足条件就加入计数器，</span></span><br><span class="line">            <span class="keyword">if</span> (id2(<span class="number">0</span>) == id2(<span class="number">1</span>) &amp;&amp; id2(<span class="number">0</span>) ==id2(<span class="number">2</span>))&#123;</span><br><span class="line">                acc.add(x)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).count()<span class="comment">//触发action</span></span><br><span class="line"></span><br><span class="line">        println(acc.value)<span class="comment">//[People(张三,100222), People(tunan,100000)]</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">People</span>(<span class="params">name:<span class="type">String</span>,id:<span class="type">Long</span></span>)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>注意事项：</strong></p><ol><li><p>计数器在Driver端定义赋初始值，计数器只能在Driver端读取最后的值，在Excutor端更新。</p></li><li><p>计数器不是一个调优的操作，因为如果不这样做，结果是错的</p></li></ol><h2 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h2><p>Spark中最重要的功能之一是跨操作在内存中持久化数据集。持久化一个RDD时，每个节点在内存中存储它计算的任何分区，并在该数据集(或从中派生的数据集)的其他操作中重构它们。这使得将来的操作要快得多(通常超过10倍)。缓存是迭代算法和快速交互使用的关键工具。</p><p>可以使用其上的persist()或cache()方法将RDD标记为持久的。第一次在操作中计算它时，它将保存在节点的内存中。Spark的缓存是容错的——如果一个RDD的任何分区丢失了，它将使用最初创建它的转换自动重新计算。</p><p>持久化的存储级别很多，常用的是MEMORY_ONLY、MEMORY_ONLY_SER、MEMORY_AND_DISK</p><table><thead><tr><th align="left">Storage Level</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left">MEMORY_ONLY</td><td align="left">将RDD作为不序列化的Java对象存储在JVM中。如果RDD不适合内存，那么一些分区将不会被缓存，而是在需要它们时动态地重新计算。这是默认级别。</td></tr><tr><td align="left">MEMORY_AND_DISK</td><td align="left">将RDD作为不序列化的Java对象存储在JVM中。如果RDD不适合内存，那么将不适合的分区存储在磁盘上，并在需要时从那里读取它们。</td></tr><tr><td align="left">MEMORY_ONLY_SER (Java and Scala)</td><td align="left">将RDD存储为序列化的Java对象(每个分区一个字节数组)。这通常比反序列化对象更节省空间，特别是在使用快速序列化器时，但读取时需要更多cpu。</td></tr></tbody></table><h3 id="如何选择它们？"><a href="#如何选择它们？" class="headerlink" title="如何选择它们？"></a><strong>如何选择它们？</strong></h3><p>Storage Level的选择是内存和CPU的权衡</p><ol><li>内存多：MEMORY_ONLY (不进行序列化)</li><li>CPU跟的上：MEMORY_ONLY_SER (进行了序列化，推介)</li><li>不建议写Disk</li></ol><p>使用cache()和persist()进行持久化操作，它们都是<strong>lazy</strong>的，需要action才能触发，默认使用MEMORY_ONLY</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> forRDD.cache</span></span><br><span class="line">res18: forRDD.type = MapPartitionsRDD[9] at map at &lt;console&gt;:27</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> forRDD.count</span></span><br><span class="line">res19: Long = 8</span><br></pre></td></tr></table></figure><p>结果可以在Web UI的<strong>Storage</strong>中查看</p><p>如果需要<strong>清除缓存</strong>，使用unpersist()，清除缓存数据是立即执行的</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> forRDD.unpersist()</span></span><br><span class="line">res8: forRDD.type = MapPartitionsRDD[3] at map at &lt;console&gt;:28</span><br></pre></td></tr></table></figure><p><strong>怎么修改存储级别？</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> forRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="comment">//计数器做累加</span></span><br><span class="line">    acc.add(<span class="number">1</span>L)</span><br><span class="line">&#125;).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>).count()</span><br></pre></td></tr></table></figure><p>StorageLevel是个object，需要的级别都可以从里面拿出来</p><h4 id="考点：cache和persist有什么区别？"><a href="#考点：cache和persist有什么区别？" class="headerlink" title="考点：cache和persist有什么区别？"></a><strong>考点：cache和persist有什么区别？</strong></h4><ul><li>cache调用的persist，persist调用的persist(storage level)</li></ul><h4 id="考点：序列化和非序列化有什么区别？"><a href="#考点：序列化和非序列化有什么区别？" class="headerlink" title="考点：序列化和非序列化有什么区别？"></a><strong>考点：序列化和非序列化有什么区别？</strong></h4><ul><li>序列化将对象转换成字节数组了，节省空间，占CPU</li></ul><h3 id="Removing-Data"><a href="#Removing-Data" class="headerlink" title="Removing Data"></a>Removing Data</h3><p>Spark自动监视每个节点上的缓存使用情况，并以最近最少使用(LRU)的方式删除旧的数据分区。如果想要手动删除一个RDD，而不是等待它从缓存中消失，那么可以使用RDD.unpersist()方法。</p><p>伪代码以及画图表示出什么是LRU？</p><h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><h3 id="为什么要将变量定义成广播变量？"><a href="#为什么要将变量定义成广播变量？" class="headerlink" title="为什么要将变量定义成广播变量？"></a>为什么要将变量定义成广播变量？</h3><p>如果我们要在分布式计算里面分发大对象，例如：<em>字典</em>，<em>集合</em>，<em>黑白名单</em>等，这个都会由Driver端进行分发，一般来讲，如果这个变量不是广播变量，那么每个task就会分发一份，这在<strong>task数目十分多的情况下Driver的带宽会成为系统的瓶颈，而且会大量消耗task服务器上的资源</strong>，如果将这个变量声明为广播变量，那么知识每个executor拥有一份，这个executor启动的task会共享这个变量，节省了通信的成本和服务器的资源。</p><h3 id="广播变量图解"><a href="#广播变量图解" class="headerlink" title="广播变量图解"></a>广播变量图解</h3><p>错误的，不使用广播变量</p><p><img src="https://yerias.github.io/spark_img/%E4%B8%8D%E4%BD%BF%E7%94%A8%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F.png" alt="不使用广播变量"></p><p>正确的，使用广播变量的情况</p><p><img src="https://yerias.github.io/spark_img/%E4%BD%BF%E7%94%A8%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F.png" alt="使用广播变量"></p><h3 id="小表广播案例"><a href="#小表广播案例" class="headerlink" title="小表广播案例"></a>小表广播案例</h3><p>使用广播变量的场景很多， 我们都知道spark 一种常见的优化方式就是小表广播， 使用 map join 来代替 reduce join， 我们通过把小的数据集广播到各个节点上，节省了一次特别 expensive 的 shuffle 操作。</p><p>比如driver 上有一张数据量很小的表， 其他节点上的task 都需要 lookup 这张表， 那么 driver 可以先把这张表 copy 到这些节点，这样 task 就可以在本地查表了。</p><ol><li><p>Fact table  航线(起点机场, 终点机场, 航空公司, 起飞时间)</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SEA,JFK,DL,7:00</span><br><span class="line">SFO,LAX,AA,7:05</span><br><span class="line">SFO,JFK,VX,7:05</span><br><span class="line">JFK,LAX,DL,7:10</span><br><span class="line">LAX,SEA,DL,7:10</span><br></pre></td></tr></table></figure></li><li><p>Dimension table 机场(简称, 全称, 城市, 所处城市简称)</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">JFK,John F. Kennedy International Airport,New York,NY</span><br><span class="line">LAX,Los Angeles International Airport,Los Angeles,CA</span><br><span class="line">SEA,Seattle-Tacoma International Airport,Seattle,WA</span><br><span class="line">SFO,San Francisco International Airport,San Francisco,CA</span><br></pre></td></tr></table></figure></li><li><p>Dimension table  航空公司(简称,全称)</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">AA,American Airlines</span><br><span class="line">DL,Delta Airlines</span><br><span class="line">VX,Virgin America</span><br></pre></td></tr></table></figure></li><li><p>思路：将机场维度表和航空公司维度表进行广播，生成Map，航线事实表从广播变量中通过key拿到value(计算在每个executor上)</p></li><li><p>代码</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BroadcastApp</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Fact table  航线(起点机场, 终点机场, 航空公司, 起飞时间)</span></span><br><span class="line">        <span class="keyword">val</span> flights = sc.textFile(<span class="string">"tunan-spark-core/broadcast/flights.txt"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Dimension table 机场(简称, 全称, 城市, 所处城市简称)</span></span><br><span class="line">        <span class="keyword">val</span> airports: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"tunan-spark-core/broadcast/airports.txt"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Dimension table  航空公司(简称,全称)</span></span><br><span class="line">        <span class="keyword">val</span> airlines = sc.textFile(<span class="string">"tunan-spark-core/broadcast/airlines.txt"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 最终统计结果：</span></span><br><span class="line"><span class="comment">         * 出发城市           终点城市           航空公司名称         起飞时间</span></span><br><span class="line"><span class="comment">         * Seattle           New York       Delta Airlines          7:00</span></span><br><span class="line"><span class="comment">         * San Francisco     Los Angeles    American Airlines       7:05</span></span><br><span class="line"><span class="comment">         * San Francisco     New York       Virgin America          7:05</span></span><br><span class="line"><span class="comment">         * New York          Los Angeles    Delta Airlines          7:10</span></span><br><span class="line"><span class="comment">         * Los Angeles       Seattle        Delta Airlines          7:10</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//广播Dimension Table airport，生成Map</span></span><br><span class="line">        <span class="keyword">val</span> airportsBC = sc.broadcast(airports.map(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = x.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">2</span>))</span><br><span class="line">        &#125;).collectAsMap())</span><br><span class="line"></span><br><span class="line">        <span class="comment">//广播Dimension Table airlines，生成Map</span></span><br><span class="line">        <span class="keyword">val</span> airlinesBC = sc.broadcast(airlines.map(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = x.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;).collectAsMap())</span><br><span class="line"></span><br><span class="line">        <span class="comment">//通过key获取value</span></span><br><span class="line">        flights.map(lines =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = lines.split(<span class="string">","</span>)</span><br><span class="line">            <span class="keyword">val</span> a = airportsBC.value.get(words(<span class="number">0</span>)).get</span><br><span class="line">            <span class="keyword">val</span> b = airportsBC.value.get(words(<span class="number">1</span>)).get</span><br><span class="line">            <span class="keyword">val</span> c = airlinesBC.value.get(words(<span class="number">2</span>)).get</span><br><span class="line">            a+<span class="string">"    "</span>+b+<span class="string">"    "</span>+c+<span class="string">"    "</span>+words(<span class="number">3</span>)</span><br><span class="line">        &#125;).foreach(println)</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>结果</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">New York    Los Angeles     Delta Airlines    7:10</span><br><span class="line">Los Angeles     Seattle    Delta Airlines    7:10</span><br><span class="line">Seattle    New York    Delta Airlines    7:00</span><br><span class="line">San Francisco   Los Angeles     American Airlines 7:05</span><br><span class="line">San Francisco   New York    Virgin America    7:05</span><br></pre></td></tr></table></figure></li></ol><h3 id="为什么只能-broadcast-只读的变量"><a href="#为什么只能-broadcast-只读的变量" class="headerlink" title="为什么只能 broadcast 只读的变量"></a>为什么只能 broadcast 只读的变量</h3><p>这就涉及一致性的问题，如果变量可以被更新，那么一旦变量被某个节点更新，其他节点要不要一块更新？如果多个节点同时在更新，更新顺序是什么？怎么做同步？ 仔细想一下， 每个都很头疼， spark 目前就索性搞成了只读的。  因为分布式强一致性真的很蛋疼</p><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ol><li><p>变量一旦被定义为一个广播变量，那么这个变量只能读，不能修改</p></li><li><p>能不能将一个RDD使用广播变量广播出去？因为RDD是不存储数据的。<strong>可以将RDD的结果广播出去。</strong></p></li><li><p>广播变量只能在Driver端定义，<strong>不能在Executor端定义。</strong></p></li><li><p>在Driver端可以修改广播变量的值，<strong>在Executor端无法修改广播变量的值。</strong></p></li><li><p>如果executor端用到了Driver的变量，<strong>不使用广播变量在Executor有多少task就有多少Driver端的变量副本。</strong></p></li><li><p>如果Executor端用到了Driver的变量，<strong>使用广播变量在每个Executor中只有一份Driver端的变量副本。</strong></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark之短信告警</title>
      <link href="/2019/10/08/spark/8/"/>
      <url>/2019/10/08/spark/8/</url>
      
        <content type="html"><![CDATA[<p>下面的案例继续延续Spark监控中的邮件监控，在监控中检测到数据异常，需要发送邮件告警</p><p>发送邮件工具类</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">MsgUtils</span> </span>&#123;</span><br><span class="line">    public static void send(<span class="type">String</span> recivers, <span class="type">String</span> title, <span class="type">String</span> content) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">Properties</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>();</span><br><span class="line">        properties.setProperty(<span class="string">"mail.host"</span>,<span class="string">"smtp.qq.com"</span>);</span><br><span class="line">        properties.setProperty(<span class="string">"mail.transport.protocol"</span>,<span class="string">"smtp"</span>);</span><br><span class="line">        properties.setProperty(<span class="string">"mail.smtp.auth"</span>, <span class="string">"true"</span>);</span><br><span class="line">        properties.setProperty(<span class="string">"mail.smtp.ssl.enable"</span>,<span class="string">"false"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">MailSSLSocketFactory</span> factory = <span class="keyword">new</span> <span class="type">MailSSLSocketFactory</span>();</span><br><span class="line">        factory.setTrustAllHosts(<span class="literal">true</span>);</span><br><span class="line">        properties.put(<span class="string">"mail.smtp.ssl.socketFactory"</span>, factory);</span><br><span class="line"></span><br><span class="line">        <span class="type">Authenticator</span> authenticator = <span class="keyword">new</span> <span class="type">Authenticator</span>() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">protected</span> <span class="type">PasswordAuthentication</span> getPasswordAuthentication() &#123;</span><br><span class="line">                <span class="type">String</span> username = <span class="string">"发送者qq邮箱"</span>;</span><br><span class="line">                <span class="type">String</span> password = <span class="string">"发送者qq授权码"</span>;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">PasswordAuthentication</span>(username, password);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="type">Session</span> session = <span class="type">Session</span>.getInstance(properties, authenticator);</span><br><span class="line"></span><br><span class="line">        <span class="type">MimeMessage</span> message = <span class="keyword">new</span> <span class="type">MimeMessage</span>(session);</span><br><span class="line">        <span class="type">InternetAddress</span> from = <span class="keyword">new</span> <span class="type">InternetAddress</span>(<span class="string">"发送者qq邮箱"</span>);</span><br><span class="line">        message.setFrom(from);</span><br><span class="line">        <span class="type">InternetAddress</span>[] tos = <span class="type">InternetAddress</span>.parse(recivers);</span><br><span class="line">        message.setRecipients(<span class="type">Message</span>.<span class="type">RecipientType</span>.<span class="type">TO</span>, tos);</span><br><span class="line">        message.setSubject(title);</span><br><span class="line">        message.setContent(content, <span class="string">"text/html;charset=UTF-8"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">Transport</span>.send(message);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(<span class="type">String</span>[] args) <span class="keyword">throws</span> <span class="type">Exception</span>&#123;</span><br><span class="line">        send(<span class="string">"接收邮箱"</span>, <span class="string">"测试"</span>, <span class="string">"测试内容"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Spark监控中调用</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">MsgUtils</span>.send(<span class="string">"接收者邮箱,接收者邮箱"</span>, <span class="string">"ERROR：数据异常"</span>, <span class="string">s"jobID: <span class="subst">$jobId</span> 数据异常，请马上检查: <span class="subst">$&#123;listener.toString&#125;</span>s"</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark之监控模块</title>
      <link href="/2019/10/07/spark/7/"/>
      <url>/2019/10/07/spark/7/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>Spark自带监控</li><li>Spark接口监控</li><li>Spark自定义监控</li></ol><h2 id="Spark自带监控"><a href="#Spark自带监控" class="headerlink" title="Spark自带监控"></a><a href="http://spark.apache.org/docs/latest/monitoring.html#spark-history-server-configuration-options" target="_blank" rel="noopener">Spark自带监控</a></h2><p>第一种监控方式是Spark自带的，由于Spark Web UI界面只在sc的生命周期内有效，所以我们需要存储日志，在Spark sc 生命周期结束后重构UI界面。</p><p>首先看官方文档配置，这里只是简单配置</p><ol><li><p>修改spark.default.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">#开启日志存储</span><br><span class="line">spark.eventLog.enabled true</span><br><span class="line">#指定日志存储的HDFS目录</span><br><span class="line">spark.eventLog.dir hdfs://hadoop:9000/spark-logs</span><br><span class="line">#开启日志存储7天自动删除</span><br><span class="line">spark.history.fs.cleaner.enabled true</span><br></pre></td></tr></table></figure></li><li><p>修改spark.env.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">#指定日志恢复目录，就是上面的日志存储目录</span><br><span class="line">SPARK_HISTORY_OPTS = "-Dspark.history.fs.logDirectory=hdfs://hadoop:9000/spark-logs"</span><br></pre></td></tr></table></figure></li><li><p>在 sc 的生命周期外打开历史UI界面</p></li></ol><h2 id="Spark接口监控"><a href="#Spark接口监控" class="headerlink" title="Spark接口监控"></a><a href="http://spark.apache.org/docs/latest/monitoring.html#rest-api" target="_blank" rel="noopener">Spark接口监控</a></h2><p>首先看官方文档配置，这里只是简单介绍</p><p>查看application列表：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">http:<span class="comment">//hadoop:18080/api/v1/applications</span></span><br></pre></td></tr></table></figure><p>查看application的所有job</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">http:<span class="comment">//hadoop:18080/history/application_1585632916452_0002/jobs/</span></span><br></pre></td></tr></table></figure><h2 id="Spark自定义监控"><a href="#Spark自定义监控" class="headerlink" title="Spark自定义监控"></a><a href="http://spark.apache.org/docs/latest/monitoring.html#executor-task-metrics" target="_blank" rel="noopener">Spark自定义监控</a></h2><p>metrics: 数据信息</p><p>spark 提供了一系列整个任务生命周期中各个阶段变化的事件监听机制，通过这一机制可以在任务的各个阶段做一些自定义的各种动作。SparkListener便是这些阶段的事件监听接口类 通过实现这个类中的各种方法便可实现自定义的事件处理动作。</p><p>自定义监听sparListener后的注册方式有两种：</p><p><strong>方法1：</strong>conf 配置中指定</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">.setAppName(getClass.getSimpleName)</span><br><span class="line">.setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line"><span class="comment">//监听类注册</span></span><br><span class="line">.set(<span class="string">"spark.extraListeners"</span>, <span class="string">"com.tunan.spark.listener.MySparkListener"</span>)</span><br><span class="line">.getOrCreate()</span><br></pre></td></tr></table></figure><p><strong>方法2：</strong>sparkContext 类中指定</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc.addSparkListener(<span class="keyword">new</span> <span class="type">MySparkAppListener</span>)</span><br></pre></td></tr></table></figure><h3 id="SparkListerner"><a href="#SparkListerner" class="headerlink" title="SparkListerner"></a>SparkListerner</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//SparkListener 下各个事件对应的函数名非常直白，即如字面所表达意思。</span></span><br><span class="line"><span class="comment">//想对哪个阶段的事件做一些自定义的动作，变继承SparkListener实现对应的函数即可</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkListener</span> <span class="keyword">extends</span> <span class="title">SparkListenerInterface</span> </span>&#123;</span><br><span class="line">  <span class="comment">//阶段完成时触发的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStageCompleted</span></span>(stageCompleted: <span class="type">SparkListenerStageCompleted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//阶段提交时触发的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStageSubmitted</span></span>(stageSubmitted: <span class="type">SparkListenerStageSubmitted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//任务启动时触发的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskStart</span></span>(taskStart: <span class="type">SparkListenerTaskStart</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//下载任务结果的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskGettingResult</span></span>(taskGettingResult: <span class="type">SparkListenerTaskGettingResult</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//任务结束的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskEnd</span></span>(taskEnd: <span class="type">SparkListenerTaskEnd</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//job启动的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onJobStart</span></span>(jobStart: <span class="type">SparkListenerJobStart</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//job结束的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onJobEnd</span></span>(jobEnd: <span class="type">SparkListenerJobEnd</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//环境变量被更新的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onEnvironmentUpdate</span></span>(environmentUpdate: <span class="type">SparkListenerEnvironmentUpdate</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//块管理被添加的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBlockManagerAdded</span></span>(blockManagerAdded: <span class="type">SparkListenerBlockManagerAdded</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBlockManagerRemoved</span></span>(</span><br><span class="line">      blockManagerRemoved: <span class="type">SparkListenerBlockManagerRemoved</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//取消rdd缓存的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onUnpersistRDD</span></span>(unpersistRDD: <span class="type">SparkListenerUnpersistRDD</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//app启动的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onApplicationStart</span></span>(applicationStart: <span class="type">SparkListenerApplicationStart</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//app结束的事件 [以下各事件也如同函数名所表达各个阶段被触发的事件不在一一标注]</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onApplicationEnd</span></span>(applicationEnd: <span class="type">SparkListenerApplicationEnd</span>): <span class="type">Unit</span> = &#123; &#125; </span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorMetricsUpdate</span></span>(</span><br><span class="line">      executorMetricsUpdate: <span class="type">SparkListenerExecutorMetricsUpdate</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorAdded</span></span>(executorAdded: <span class="type">SparkListenerExecutorAdded</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorRemoved</span></span>(executorRemoved: <span class="type">SparkListenerExecutorRemoved</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorBlacklisted</span></span>(</span><br><span class="line">      executorBlacklisted: <span class="type">SparkListenerExecutorBlacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorUnblacklisted</span></span>(</span><br><span class="line">      executorUnblacklisted: <span class="type">SparkListenerExecutorUnblacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onNodeBlacklisted</span></span>(</span><br><span class="line">      nodeBlacklisted: <span class="type">SparkListenerNodeBlacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onNodeUnblacklisted</span></span>(</span><br><span class="line">      nodeUnblacklisted: <span class="type">SparkListenerNodeUnblacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBlockUpdated</span></span>(blockUpdated: <span class="type">SparkListenerBlockUpdated</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onOtherEvent</span></span>(event: <span class="type">SparkListenerEvent</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h3><p>首先看官方文档配置，这里只是简单案例</p><ol><li>自定义监控类，继承SparkListener</li><li>重写onTaskEnd方法，拿到taskMetrics</li><li>从taskMetrics获取各种数据信息</li><li>注册到被监听的类</li></ol><p><strong>第1-3步代码</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySparkListener</span>(<span class="params">conf:<span class="type">SparkConf</span></span>) <span class="keyword">extends</span> <span class="title">SparkListener</span> <span class="keyword">with</span> <span class="title">Logging</span></span>&#123;</span><br><span class="line">    <span class="comment">//监听每个Task结束</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskEnd</span></span>(taskEnd: <span class="type">SparkListenerTaskEnd</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//获取应用程序名称</span></span><br><span class="line">        <span class="keyword">val</span> appName = conf.get(<span class="string">"spark.app.name"</span>)</span><br><span class="line">        <span class="comment">//获取作业的taskMetrics</span></span><br><span class="line">        <span class="keyword">val</span> metrics = taskEnd.taskMetrics</span><br><span class="line">        <span class="comment">//使用对象接收参数</span></span><br><span class="line">        <span class="keyword">val</span> metricsObject = <span class="type">Metrics</span>(appName,taskEnd.stageId, taskEnd.taskInfo.taskId, metrics.inputMetrics.bytesRead, metrics.shuffleReadMetrics.totalBytesRead, metrics.outputMetrics.bytesWritten, metrics.shuffleWriteMetrics.bytesWritten)</span><br><span class="line">        <span class="comment">//输出字符串类型的metricsObject</span></span><br><span class="line">        logError(metricsObject.toString)</span><br><span class="line">        <span class="comment">//输出Json类型的metricsObject</span></span><br><span class="line">        logError(<span class="type">Json</span>(<span class="type">DefaultFormats</span>).write(metricsObject))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//定义case class对象</span></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Metrics</span>(<span class="params">appName:<span class="type">String</span>,stageId:<span class="type">Long</span>,taskId:<span class="type">Long</span>,bytesRead:<span class="type">Long</span>,bytesWritten:<span class="type">Long</span>,shuffleReadMetrics:<span class="type">Long</span>,shuffleWriteMetrics:<span class="type">Long</span></span>)</span>&#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = <span class="string">s"appName:<span class="subst">$appName</span>,stageId:<span class="subst">$stageId</span>,taskId:<span class="subst">$taskId</span>,bytesRead:<span class="subst">$bytesRead</span>,bytesWritten:<span class="subst">$bytesWritten</span>,shuffleReadMetrics:<span class="subst">$shuffleReadMetrics</span>,shuffleWriteMetrics:<span class="subst">$shuffleWriteMetrics</span>"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>第四步代码</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//输入、输出路径</span></span><br><span class="line">        <span class="keyword">val</span> (in,out) = (args(<span class="number">0</span>),args(<span class="number">1</span>))</span><br><span class="line">        <span class="comment">//配置conf</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">            .setAppName(getClass.getSimpleName)</span><br><span class="line">            .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">            <span class="comment">//监听类注册</span></span><br><span class="line">            .set(<span class="string">"spark.extraListeners"</span>, <span class="string">"com.tunan.spark.listener.MySparkListener"</span>)</span><br><span class="line">        <span class="comment">//拿到sc</span></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="comment">//删除输出目录</span></span><br><span class="line">        <span class="type">CheckHDFSOutPath</span>.ifExistsDeletePath(<span class="keyword">new</span> <span class="type">Configuration</span>(),out)</span><br><span class="line">        <span class="comment">//操作算子</span></span><br><span class="line">        <span class="keyword">val</span> result = sc.textFile(in).flatMap(_.split(<span class="string">"\t"</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">        <span class="comment">//保存文件</span></span><br><span class="line">        result.saveAsTextFile(out)</span><br><span class="line">        <span class="comment">//关闭sc</span></span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Spark自定义监控案例"><a href="#Spark自定义监控案例" class="headerlink" title="Spark自定义监控案例"></a>Spark自定义监控案例</h2><p>把监控参数写入到MySQL</p><p>需求：应用程序名字、jobID号、stageID号、taskID号、读取数据量、写入数据量、shuffle读取数据量、shuffle写入数据量。</p><ol><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> wc2mysql(</span><br><span class="line">app_name <span class="built_in">varchar</span>(<span class="number">32</span>),</span><br><span class="line">job_id <span class="built_in">bigint</span>,</span><br><span class="line">stage_id <span class="built_in">bigInt</span>,</span><br><span class="line">task_id <span class="built_in">bigint</span>,</span><br><span class="line">file_read_byte <span class="built_in">bigint</span>,</span><br><span class="line">file_write_byte <span class="built_in">bigint</span>,</span><br><span class="line">shuffle_read_byte <span class="built_in">bigint</span>,</span><br><span class="line">shuffle_write_byte <span class="built_in">bigint</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p>实现监控类</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySparkListenerV2</span>(<span class="params">conf: <span class="type">SparkConf</span></span>) <span class="keyword">extends</span> <span class="title">SparkListener</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义JobID</span></span><br><span class="line">    <span class="keyword">var</span> jobId:<span class="type">Long</span> = _</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onJobStart</span></span>(jobStart: <span class="type">SparkListenerJobStart</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//初始化</span></span><br><span class="line">        jobId = jobStart.jobId</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"============准备插入数据============"</span>)</span><br><span class="line">    <span class="comment">//监听每个Task结束</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskEnd</span></span>(taskEnd: <span class="type">SparkListenerTaskEnd</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//获取应用程序名称</span></span><br><span class="line">        <span class="keyword">val</span> appName = conf.get(<span class="string">"spark.app.name"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取作业的taskMetrics</span></span><br><span class="line">        <span class="keyword">val</span> metrics = taskEnd.taskMetrics</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用对象接收参数</span></span><br><span class="line">        <span class="keyword">val</span> listener = <span class="type">Listener</span>(appName, jobId, taskEnd.stageId, taskEnd.taskInfo.taskId, metrics.inputMetrics.bytesRead, metrics.outputMetrics.bytesWritten, metrics.shuffleReadMetrics.totalBytesRead, metrics.shuffleWriteMetrics.bytesWritten)</span><br><span class="line"></span><br><span class="line"><span class="comment">//结果插入到MySQL</span></span><br><span class="line">        <span class="type">ListenerCURD</span>.insert(listener)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//发送监控邮件</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"true"</span> == conf.get(<span class="string">"spark.send.mail.enabled"</span>))&#123;</span><br><span class="line">            <span class="type">MsgUtils</span>.send(<span class="string">"971118017@qq.com"</span>, <span class="string">"ERROR：数据异常"</span>, <span class="string">s"jobID: <span class="subst">$jobId</span> 数据异常，请马上检查: <span class="subst">$&#123;listener.toString&#125;</span>"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    println(<span class="string">"============成功插入数据============"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>实现监控数据写入MySQL</p><p>使用的是scalikejdbc框架实现的，使用具体方法在我的<a href="http://yerias.github.io/2020/03/16/scala/2/">博客</a></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Listener</span>(<span class="params">app_name: <span class="type">String</span>, job_id: <span class="type">Long</span>, stage_id: <span class="type">Long</span>, task_id: <span class="type">Long</span>, file_read_byte: <span class="type">Long</span>, file_write_byte: <span class="type">Long</span>, shuffle_read_byte: <span class="type">Long</span>, shuffle_write_byte: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">ListenerCURD</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Before</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//初始化配置</span></span><br><span class="line">        <span class="type">DBs</span>.setupAll()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(listener: <span class="type">Listener</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="type">Before</span>()</span><br><span class="line">        <span class="comment">//事物插入</span></span><br><span class="line">        <span class="type">DB</span>.localTx &#123;</span><br><span class="line">            <span class="keyword">implicit</span> session =&gt; &#123;</span><br><span class="line">                <span class="type">SQL</span>(<span class="string">"insert into wc2mysql(app_name,job_id,stage_id,task_id,file_read_byte,file_write_byte,shuffle_read_byte,shuffle_write_byte) values(?,?,?,?,?,?,?,?)"</span>)</span><br><span class="line">                    .bind(listener.app_name,listener.job_id, listener.stage_id, listener.task_id, listener.file_read_byte, listener.file_write_byte, listener.shuffle_read_byte, listener.shuffle_write_byte)</span><br><span class="line">                    .update()</span><br><span class="line">                    .apply()</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">After</span>()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">After</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//关闭资源</span></span><br><span class="line">        <span class="type">DBs</span>.closeAll()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>启动被监控类</p><p>被监控的类还是我们上面的WordCount的类，关键在于在SparkConf()中注册</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//配置conf</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">.setAppName(getClass.getSimpleName)</span><br><span class="line">.setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line"><span class="comment">//监听类注册</span></span><br><span class="line">.set(<span class="string">"spark.extraListeners"</span>, <span class="string">"com.tunan.spark.listener.MySparkListenerV2"</span>)</span><br></pre></td></tr></table></figure></li><li><p>在数据库中查询</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> wc2mysql;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark术语&amp;Spark提交&amp;YARN上的提交模式&amp;窄依赖&amp;宽依赖</title>
      <link href="/2019/10/06/spark/6/"/>
      <url>/2019/10/06/spark/6/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>Spark术语</li><li>Spark提交</li><li>YARN上提交模式</li><li>宽依赖</li><li>窄依赖</li></ol><h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><p>下表总结了关于集群概念的术语:</p><table><thead><tr><th><strong>Term</strong></th><th>Meaning</th></tr></thead><tbody><tr><td>Application</td><td>Spark上的应用程序。由一个<strong>driver program</strong>和集群上的<strong>executors</strong>组成。</td></tr><tr><td>Application jar</td><td>一个包含用户的Spark应用程序的jar包</td></tr><tr><td>Driver program</td><td>运行应用程序main()函数并创建SparkContext的进程</td></tr><tr><td>Cluster manager</td><td>用于获取集群资源的外部服务(例如，standalone manager、Mesos、YARN)</td></tr><tr><td>Deploy mode</td><td>区别driver process在何处运行。在“cluster”模式下，框架启动集群内部的驱动程序。在“client”模式下，提交者启动集群外部的驱动程序。</td></tr><tr><td>Worker node</td><td>可以在集群中运行application的任何节点</td></tr><tr><td>Executor</td><td>在Worker node上被application启动的进程，它运行任务并将数据保存在内存或磁盘存储器中。每个application都有自己的Executor。</td></tr><tr><td>Task</td><td>将被发送给一个执行者的工作单元</td></tr><tr><td>Job</td><td>由多个任务组成的并行计算，这些任务在响应一个Spark操作时产生(如保存、收集)</td></tr><tr><td>Stage</td><td>每个作业被分成更小的任务集，称为阶段，这些阶段相互依赖(类似于MapReduce中的map和reduce阶段)</td></tr></tbody></table><h2 id="Spark提交"><a href="#Spark提交" class="headerlink" title="Spark提交"></a><a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">Spark提交</a></h2><p>注意：在使用Spark提交之前，一定要在环境变量中配置<code>HADOOP_CONF_DIR</code>，否则hadoop的环境引不进来</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HADOOP_CONF_DIR=XXX</span><br></pre></td></tr></table></figure><p>Spark支持的部署模式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class &lt;main-class&gt; \</span><br><span class="line">  --master &lt;master-url&gt; \</span><br><span class="line">  --deploy-mode &lt;deploy-mode&gt; \</span><br><span class="line">  --conf &lt;key&gt;=&lt;value&gt; \</span><br><span class="line">  ... # other options</span><br><span class="line">  &lt;application-jar&gt; \</span><br><span class="line">  [application-arguments]</span><br></pre></td></tr></table></figure><p>一些常用的选项是：</p><ul><li><code>--class</code>：您的应用程序的入口点（例如<code>org.apache.spark.examples.SparkPi</code>）</li><li><code>--master</code>：集群的<a href="http://spark.apache.org/docs/latest/submitting-applications.html#master-urls" target="_blank" rel="noopener">主URL</a>（例如<code>spark://23.195.26.187:7077</code>）</li><li><code>--deploy-mode</code>：将驱动程序部署在工作节点（<code>cluster</code>）上还是作为外部客户端（<code>client</code>）本地部署（默认值：<code>client</code>）</li><li><code>--conf</code>：键值格式的任意Spark配置属性。对于包含空格的值，将“ key = value”用引号引起来（如图所示）。</li><li><code>application-jar</code>：jar包的路径，包括您的应用程序和所有依赖项。URL必须在群集内部全局可见，例如，<code>hdfs://</code>路径或<code>file://</code>所有节点上都存在的路径。</li><li><code>application-arguments</code>：参数传递给您的主类的main方法（如果有）</li></ul><p>其他常用的选项：</p><ul><li><p><code>--num-executors</code>：executors的数量</p></li><li><p><code>--executor-memory</code>：每个executor的内存数量</p></li><li><p><code>--total-executor-cores 100</code>：executor的总的core数</p></li><li><p><code>--jars</code>：指定需要依赖的jar包，多个jar包逗号分隔，application中直接引用</p></li><li><p><code>--files</code>：需要依赖的文件，在application中使用SparkFiles.get(“file”)取出，同时需要放在resources目录下</p></li></ul><p>注意：local模式默认读写HDFS数据 读本地要加<code>file://</code></p><h2 id="提交模式"><a href="#提交模式" class="headerlink" title="提交模式"></a>提交模式</h2><h3 id="cliet模式"><a href="#cliet模式" class="headerlink" title="cliet模式"></a>cliet模式</h3><ol><li>Driver运行在Client</li><li>AM职责就是去YARN上申请资源</li><li>Driver会和请求到的container/executor进行通信</li><li>Driver是不能退出的</li></ol><p><img src="https://yerias.github.io/spark_img/Client%E6%A8%A1%E5%BC%8F.jpg" alt="Client模式"></p><p><strong>Client模式控制台能看到日志</strong></p><h3 id="cluster模式"><a href="#cluster模式" class="headerlink" title="cluster模式"></a>cluster模式</h3><ol><li><p>Driver运行位置在AM</p></li><li><p>Client提交上去了  它退出对整个作业没影响</p></li><li><p>AM(申请资源)+Driver（调度DAG，分发任务）</p><p><img src="https://yerias.github.io/spark_img/Cluster%E6%A8%A1%E5%BC%8F.jpg" alt="Cluster模式"></p></li></ol><p><strong>控制台不能看到日志，不支持Spark-shell(Spark-SQL) ，交互性操作的都不能</strong></p><h2 id="窄依赖"><a href="#窄依赖" class="headerlink" title="窄依赖"></a>窄依赖</h2><ol><li>一个父RDD的分区至多被一个子RDD的某个分区使用一次</li><li>一个父RDD的分区和一个子RDD的分区是唯一映射 典型的map</li><li>多个父RDD的分区和一个子RDD的分区是唯一映射 典型的union</li></ol><p><img src="https://yerias.github.io/spark_img/%E7%AA%84%E4%BE%9D%E8%B5%96.jpg" alt="窄依赖"></p><p>在窄依赖中有个特殊的join是不经过shuffle 的</p><p>这个特殊的join的存在有三个条件：</p><ol><li>RDD1的分区数 = RDD2的分区数</li><li>RDD1的分区数 = Join的分区数</li><li>RDD2的分区数 = Join的分区数 </li></ol><p>我们看一个案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * rdd1、rdd2、join 三者的分区数相同，不经过shuffle</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>((<span class="string">"香蕉"</span>,<span class="number">20</span>), (<span class="string">"苹果"</span>,<span class="number">50</span>), (<span class="string">"菠萝"</span>,<span class="number">30</span>), (<span class="string">"猕猴桃"</span>, <span class="number">50</span>)),<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">List</span>((<span class="string">"草莓"</span>,<span class="number">90</span>), (<span class="string">"苹果"</span>,<span class="number">25</span>), (<span class="string">"菠萝"</span>,<span class="number">25</span>), (<span class="string">"猕猴桃"</span>, <span class="number">30</span>), (<span class="string">"西瓜"</span>, <span class="number">45</span>)),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.reduceByKey(_ + _)</span><br><span class="line"><span class="keyword">val</span> rdd4 = rdd2.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> joinRDD = rdd3.join(rdd4,<span class="number">2</span>)</span><br><span class="line">joinRDD.collect()</span><br></pre></td></tr></table></figure><p>再看Application的DAG图，从两个 reduceByKey 到 join 是一个 stage 中的，说明没有产生 shuffle</p><p><img src="https://yerias.github.io/spark_img/%E7%89%B9%E6%AE%8A%E7%9A%84Jion.jpg" alt="特殊的Jion"></p><h2 id="宽依赖"><a href="#宽依赖" class="headerlink" title="宽依赖"></a>宽依赖</h2><ol><li>一个父RDD的分区会被子RDD的分区使用多次</li></ol><p><img src="https://yerias.github.io/spark_img/%E5%AE%BD%E4%BE%9D%E8%B5%96.jpg" alt="宽依赖"></p><p>除了前面那种是三个条件满足的，其他的 join 都是宽依赖</p><p>我们使RDD1的分区数和RDD2的分区数相等，但是 join的分区数不相等</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * rdd1、rdd2的分区数不同，但是和join的分区数不同，会经过shuffle</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>((<span class="string">"香蕉"</span>,<span class="number">20</span>), (<span class="string">"苹果"</span>,<span class="number">50</span>), (<span class="string">"菠萝"</span>,<span class="number">30</span>), (<span class="string">"猕猴桃"</span>, <span class="number">50</span>)),<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">List</span>((<span class="string">"草莓"</span>,<span class="number">90</span>), (<span class="string">"苹果"</span>,<span class="number">25</span>), (<span class="string">"菠萝"</span>,<span class="number">25</span>), (<span class="string">"猕猴桃"</span>, <span class="number">30</span>), (<span class="string">"西瓜"</span>, <span class="number">45</span>)),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.reduceByKey(_ + _)</span><br><span class="line"><span class="keyword">val</span> rdd4 = rdd2.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> joinRDD = rdd3.join(rdd4,<span class="number">4</span>)</span><br><span class="line">joinRDD.collect()</span><br></pre></td></tr></table></figure><p>我们看DAG图，产生了stage，也就是经过了shuffle</p><p><img src="https://yerias.github.io/spark_img/%E7%89%B9%E6%AE%8A%E7%9A%84Jion%E5%AE%BD%E4%BE%9D%E8%B5%96.jpg" alt="特殊的Jion宽依赖"></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark使用Yarn模式解决Jar乱飞情况</title>
      <link href="/2019/10/05/spark/5/"/>
      <url>/2019/10/05/spark/5/</url>
      
        <content type="html"><![CDATA[<ol><li><p>在本地创建zip文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在spark的jars目录下</span></span><br><span class="line">zip spark.zip ./*</span><br></pre></td></tr></table></figure></li><li><p>HDFS上创建存放spark jar的目录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -mkdir -p  /spark-yarn/jars</span><br></pre></td></tr></table></figure></li><li><p>将$SPARK_HOME/jars下的spark.zip包上传至刚建的HDFS路径</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop jars]$ hadoop fs -put ./spark.zip /spark-yarn/jars/</span><br></pre></td></tr></table></figure></li><li><p>在 spark-defaults.conf中添加(也可以在启动的时候–conf指定)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark.yarn.archive=hdfs://hadoop:9000/spark-yarn/jars/spark.zip</span><br></pre></td></tr></table></figure></li><li><p>查看Spark log</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn log -applicationID xxx</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark之排序模块</title>
      <link href="/2019/10/04/spark/4/"/>
      <url>/2019/10/04/spark/4/</url>
      
        <content type="html"><![CDATA[<p>目录</p><ol><li>算子排序</li><li>面向对象排序</li><li>隐式转换排序</li><li>Ordering.on排序</li></ol><p>Spark中的排序模块，顾名思义，这篇文章都是说如何排序</p><h2 id="算子排序"><a href="#算子排序" class="headerlink" title="算子排序"></a>算子排序</h2><p>的确，在Spark中有很多算子可以排序，可以给数组排序，可以给键值对排序，我们会使用算子引入排序，然后再重点介绍如何使用隐式转换达到排序的效果。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">(<span class="string">"西瓜 20 100"</span>, <span class="string">"苹果 10 500"</span>, <span class="string">"香蕉 10 30"</span>, <span class="string">"菠萝 30 200"</span>)</span><br></pre></td></tr></table></figure><p>现在我们有一行数据，我们如何使它按价格降序排序？</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//TODO List中每个东西的含义：名称name、价格price、库存amount</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="string">"西瓜 20 100"</span>, <span class="string">"苹果 10 500"</span>, <span class="string">"香蕉 10 30"</span>, <span class="string">"菠萝 30 200"</span>))</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//拆分map拿出来 ==&gt; 返回tuple(productData) ==&gt; 排序sortBy</span></span><br><span class="line"><span class="keyword">val</span> mapRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    (name, price, amount)</span><br><span class="line">&#125;)</span><br><span class="line">mapRDD.sortBy(-_._2).collect().foreach(println);</span><br></pre></td></tr></table></figure><p>也许你会认为很简单，那么现在要求按价格降序排序，价格相同库存降序排序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//拆分 ==&gt; 返回tuple() ==&gt; 排序sortBy(x =&gt; (-x._2,-x._3))</span></span><br><span class="line"><span class="keyword">val</span> mapRDD2 = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    (name, price, amount)</span><br><span class="line">&#125;)</span><br><span class="line">mapRDD2.sortBy(x=&gt;(-x._2,-x._3)).collect().foreach(println);</span><br></pre></td></tr></table></figure><p>的确，使用算子排序很简单，但是简洁的代码，能让你回想起几个月前的这几行是什么意思吗？</p><h2 id="面向对象排序"><a href="#面向对象排序" class="headerlink" title="面向对象排序"></a>面向对象排序</h2><p>现在我们引入面向对象的方式来排序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//TODO 面向对象的方式实现</span></span><br><span class="line"><span class="keyword">val</span> productRDD: <span class="type">RDD</span>[<span class="type">ProductCaseClass</span>] = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Products</span>(name, price, amount)</span><br><span class="line">&#125;)</span><br><span class="line">productRDD.sortBy(x=&gt;x).collect().foreach(println)</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Products</span>(<span class="params">val name: <span class="type">String</span>, val price: <span class="type">Double</span>, val amount: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">Ordered</span>[<span class="type">Products</span>]</span></span><br><span class="line"><span class="class">    <span class="keyword">with</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = <span class="string">s"<span class="subst">$name</span> | <span class="subst">$price</span> | <span class="subst">$amount</span>"</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">Products</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        that.price.toInt-<span class="keyword">this</span>.price.toInt</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>你是不是傻？case class 自己实现了序列化，实现了toString、equals、hashCode，用起来还不需要new，创建class干啥？</p><p>于是再次改造</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> productRDD: <span class="type">RDD</span>[<span class="type">ProductCaseClass</span>] = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    <span class="type">ProductCaseClass</span>(name, price, amount)</span><br><span class="line">&#125;)</span><br><span class="line">productRDD.sortBy(x=&gt;x).collect().foreach(println)</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ProductCaseClass</span>(<span class="params">name:<span class="type">String</span>,price:<span class="type">Double</span>,amount:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Ordered</span>[<span class="type">ProductCaseClass</span>]</span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> =  <span class="string">s"case class:  <span class="subst">$name</span> | <span class="subst">$price</span> | <span class="subst">$amount</span>"</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">ProductCaseClass</span>): <span class="type">Int</span> = that.price.toInt - <span class="keyword">this</span>.price.toInt</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="隐式转换排序"><a href="#隐式转换排序" class="headerlink" title="隐式转换排序"></a>隐式转换排序</h2><p>于是呼~ 不爽，现在只给你一个最普通的类，给我增强出带排序功能的类。这不是隐式转换吗？</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//一个最普通的类,不实现ordered排序的功能</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductsInfo</span>(<span class="params">val name: <span class="type">String</span>, val price: <span class="type">Double</span>, val amount: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = <span class="string">s"ProductsInfo:  <span class="subst">$name</span> | <span class="subst">$price</span> | <span class="subst">$amount</span>"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>那么我们该如何改造它？</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> productRDD: <span class="type">RDD</span>[<span class="type">ProductsInfo</span>] = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ProductsInfo</span>(name, price, amount)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>我就把数据切分出来，返回一个<code>ProductsInfo</code>，其他啥都不干</p><p>现在最重要的就是实现隐式转换，将ProductsInfo增加排序的功能，排序的规则还要自定义</p><ol><li><p>隐式方法/转换 </p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">ProductsInfo2Orderding</span></span>(productsInfo:<span class="type">ProductsInfo</span>):<span class="type">Ordered</span>[<span class="type">ProductsInfo</span>] = <span class="keyword">new</span> <span class="type">Ordered</span>[<span class="type">ProductsInfo</span>] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">ProductsInfo</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span> (that.price-productsInfo.price&gt;<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="number">1</span></span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (that.price-productsInfo.price==<span class="number">0</span> &amp;&amp; that.amount-productsInfo.amount &gt;<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="number">1</span></span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="number">-1</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>隐式变量</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> <span class="type">ProductsInfo2Orderding</span>:<span class="type">Ordering</span>[<span class="type">ProductsInfo</span>] = <span class="keyword">new</span> <span class="type">Ordering</span>[<span class="type">ProductsInfo</span>] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">ProductsInfo</span>, y: <span class="type">ProductsInfo</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        y.amount - x.amount</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>隐式对象</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">ProductsInfo22Orderding</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[<span class="type">ProductsInfo</span>]</span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">ProductsInfo</span>, y: <span class="type">ProductsInfo</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        y.amount - x.amount</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>这三种方式都可以达到增强<code>ProductsInfo</code>的功能，他们都离不开一个公式：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">x2y</span></span>(普通的x):牛逼的y = <span class="keyword">new</span> 牛逼的y(普通的x)</span><br></pre></td></tr></table></figure><p>上面的隐式转换就是这个公式的直接应用</p><h2 id="Ordering-on排序"><a href="#Ordering-on排序" class="headerlink" title="Ordering.on排序"></a>Ordering.on排序</h2><p>你也许会说，够了吧，这么多种方式排序了，累都累死人了，那么我告诉有一种方法，不需要case class、不需要class、不需要隐式转换，只有一行代码就能排序，学不学？</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> productRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    (name, price, amount)<span class="comment">//请注意，我们这里productRDD返回的是一个tuple</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>实现排序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * (Double,Int) : 定义排序规则的返回值类型,可以是class</span></span><br><span class="line"><span class="comment"> * (String,Double,Int) : 进来数据的类型</span></span><br><span class="line"><span class="comment"> * (x =&gt; (-x._2,-x._3)) : 定义排序的规则</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> order = <span class="type">Ordering</span>[(<span class="type">Double</span>,<span class="type">Int</span>)].on[(<span class="type">String</span>,<span class="type">Double</span>,<span class="type">Int</span>)](x =&gt; (-x._2,-x._3))</span><br></pre></td></tr></table></figure><p>是不是看懵逼了？使用<code>productRDD</code>调用<code>sortBy()</code>就输出了排序后的结果</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">productRDD.sortBy(x =&gt; x).print()</span><br></pre></td></tr></table></figure><p>结果:</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">(菠萝,<span class="number">30.0</span>,<span class="number">200</span>)</span><br><span class="line">(西瓜,<span class="number">20.0</span>,<span class="number">100</span>)</span><br><span class="line">(苹果,<span class="number">10.0</span>,<span class="number">500</span>)</span><br><span class="line">(香蕉,<span class="number">10.0</span>,<span class="number">30</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark之WC产生多少个RDD</title>
      <link href="/2019/10/03/spark/3/"/>
      <url>/2019/10/03/spark/3/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>WC产生多少个RDD</li></ol><h2 id="WC产生多少个RDD"><a href="#WC产生多少个RDD" class="headerlink" title="WC产生多少个RDD"></a>WC产生多少个RDD</h2><p>一句标准的WC产生了多少个RDD？</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> result = sc.textFile(<span class="string">"E:\\Java\\spark\\tunan-spark\\tunan-spark-core\\data\\wc.txt"</span>).flatMap(_.split(<span class="string">"\t"</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">result.saveAsTextFile(<span class="string">"out"</span>)</span><br></pre></td></tr></table></figure><ol><li><p>使用<code>toDebugString</code>方法查看RDD的数量</p><p>result.toDebugString(不包括<code>saveAsTextFile</code>方法)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">(<span class="number">2</span>) <span class="type">ShuffledRDD</span>[<span class="number">4</span>] at reduceByKey at wordcount.scala:<span class="number">11</span> []</span><br><span class="line"> +-(<span class="number">2</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at wordcount.scala:<span class="number">11</span> []</span><br><span class="line">    |  <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at flatMap at wordcount.scala:<span class="number">11</span> []</span><br><span class="line">    |  <span class="type">E</span>:\<span class="type">Java</span>\spark\tunan-spark\tunan-spark-core\data\wc.txt <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at wordcount.scala:<span class="number">11</span> []</span><br><span class="line">    |  <span class="type">E</span>:\<span class="type">Java</span>\spark\tunan-spark\tunan-spark-core\data\wc.txt <span class="type">HadoopRDD</span>[<span class="number">0</span>] at textFile at wordcount.scala:<span class="number">11</span> []</span><br></pre></td></tr></table></figure><p>上面的方法中：textFile算子中有HadoopRDD和MapPartitionsRDD；flatMap方法有MapPartitionsRDD；map方法有MapPartitionsRDD；reduceByKey方法有ShuffledRDD。</p><p>这里一共是5个RDD，如果加上saveAsTextFile方法中的一个MapPartitionsRDD，则一共是6个RDD，如果加上sort方法也是一样的算法。</p></li><li><p>查看源码的方式计算RDD的数量</p><p><code>sc.textFile(&quot;...&quot;)</code></p><p>textFile的作用是从HDFS、本地文件系统(在所有节点上可用)或任何hadoop支持的文件系统URI读取文本文件，并将其作为字符串的RDD返回。</p><p>path：受支持的文件系统上的文本文件的路径</p><p>minPartitions：建议的结果RDD的最小分区数，默认值是2</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(</span><br><span class="line">    path: <span class="type">String</span>,</span><br><span class="line">    minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[<span class="type">String</span>] = withScope &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    hadoopFile(path, classOf[<span class="type">TextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>],</span><br><span class="line">               minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在textFile值调用了hadoopFile方法，该方法传入了<code>path</code>，<code>TextInputFormat</code>(也就是mapreduce中的FileInputFormat方法，特点是按行读取)，<code>LongWritable</code>(mapreduce计算中的key，记录的是offset)，<code>Text</code>(mapreduce计算中的key，记录的是每行的内容)，<code>minPartitions</code>(分区数)，然后返回一个tuple，tuple记录的是key和value，我们这里做了一个处理，<code>.map(pair =&gt; pair._2.toString)</code>方法让结果只有内容，而忽略掉了offset。</p><p>继续看hadoopFile的源码</p><p>使用任意的InputFormat获取Hadoop文件的RDD，返回一个RDD类型的包含(offset，value)的元组</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hadoopFile</span></span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">      path: <span class="type">String</span>,<span class="comment">//目录下的输入数据文件，路径可以用逗号分隔路径作为输入列表</span></span><br><span class="line">      inputFormatClass: <span class="type">Class</span>[_ &lt;: <span class="type">InputFormat</span>[<span class="type">K</span>, <span class="type">V</span>]],<span class="comment">//要读取的数据的存储格式</span></span><br><span class="line">      keyClass: <span class="type">Class</span>[<span class="type">K</span>],<span class="comment">//key的类型</span></span><br><span class="line">      valueClass: <span class="type">Class</span>[<span class="type">V</span>],<span class="comment">//value的类型</span></span><br><span class="line">      minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]<span class="comment">//分区数，默认值是2</span></span><br><span class="line">= withScope &#123;assertNotStopped()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这是一种强制加载hdfs-site.xml的方法</span></span><br><span class="line">    <span class="type">FileSystem</span>.getLocal(hadoopConfiguration)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//一个Hadoop的配置文件大概10 KB,这是相当大的,所以广播它</span></span><br><span class="line">    <span class="keyword">val</span> confBroadcast = broadcast(<span class="keyword">new</span> <span class="type">SerializableConfiguration</span>(hadoopConfiguration))</span><br><span class="line">    <span class="keyword">val</span> setInputPathsFunc = (jobConf: <span class="type">JobConf</span>) =&gt; <span class="type">FileInputFormat</span>.setInputPaths(jobConf, path)<span class="comment">//设置作业环境</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">HadoopRDD</span>(<span class="comment">//创建一个HadoopRDD</span></span><br><span class="line">      <span class="keyword">this</span>,</span><br><span class="line">      confBroadcast,<span class="comment">//广播配置</span></span><br><span class="line">      <span class="type">Some</span>(setInputPathsFunc),<span class="comment">//作业环境也许可能出错，所以使用Some()</span></span><br><span class="line">      inputFormatClass,<span class="comment">//读取文件的格式化类</span></span><br><span class="line">      keyClass,<span class="comment">//key类型</span></span><br><span class="line">      valueClass,<span class="comment">//value类型</span></span><br><span class="line">      minPartitions).setName(path)<span class="comment">//分片数</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>因为Hadoop的RecordReader类为每条记录使用相同的Writable对象，直接保存或者直接使用aggregation或者shuffle将会产生很多对同一个对象的引用，所以我们保存、排序或者聚合操作writable对象前，要使用map方法做一个映射。</p><p>回到上一步的textFile方法中，Hadoop的返回值是一个包含offset和value的元组，我们只需要内容，所以使用map方法做一个映射，只拿元祖中的value即可</p><p><code>.map(pair =&gt; pair._2.toString)</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)<span class="comment">//初始化检查</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.map(cleanF))==&gt;(context, pid, iter) ==&gt; hadoopRDD, 分区<span class="type">ID</span>, 迭代器</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该map方法又创建一个MapPartitionsRDD，将map算子应用于每个分区的子RDD。这应用到了RDD五大特性之一的，对每个RDD做计算，实际上是对每个RDD的partition或者split做计算。由于MapPartitionsRDD较为复杂，暂不解析。</p><p><strong><em>到此，textFile产生了两个RDD，分别是HadoopRDD和MapPartitionsRDD。共两个RDD</em></strong></p></li></ol><p>   <code>.flatMap(_.split(&quot;\t&quot;))</code></p><p>   flatMap首先作用在每一个元素上，然后将结果扁平化，最后返回一个新的RDD</p>   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.flatMap(cleanF))<span class="comment">//对RDD的所有分区做计算</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>   <strong><em>到此，flatMap产生了一个RDD，是MapPartitionsRDD。共三个RDD</em></strong></p><p>   <code>.map((_, 1))</code></p><p>   将map作用在每一个元素上，然后返回一个新的RDD</p>   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.map(cleanF))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>   <strong><em>到此，map产生了一个RDD，是MapPartitionsRDD。共四个RDD</em></strong></p><p>   <code>.reduceByKey(_ + _)</code></p><p>   使用联合和交换reduce函数合并每个键的值。<strong>在将结果发送到reduce之前，这也将在每个mapper上本地执行合并，类似于MapReduce中的“combiner”。</strong>输出将使用现有分区器/并行度级别进行哈希分区。</p>   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">    reduceByKey(defaultPartitioner(self), func)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>   继续查看<code>reduceByKey(defaultPartitioner(self), func)</code>方法</p>   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(partitioner: <span class="type">Partitioner</span>, func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">    combineByKeyWithClassTag[<span class="type">V</span>]((v: <span class="type">V</span>) =&gt; v, func, func, partitioner)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>   继续查看<code>combineByKeyWithClassTag[V]((v: V) =&gt; v, func, func, partitioner)</code>方法</p><p>   这是一个具体干活的方法，它使用一组自定义聚合函数组合每个键的元素。将RDD[(K, V)]转换为RDD[(K, C)]类型的结果，用于“组合类型”C。这是一个复杂的方法，暂不做解析。</p>   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKeyWithClassTag</span></span>[<span class="type">C</span>](</span><br><span class="line">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    partitioner: <span class="type">Partitioner</span>,</span><br><span class="line">    mapSideCombine: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">    serializer: <span class="type">Serializer</span> = <span class="literal">null</span>)(<span class="keyword">implicit</span> ct: <span class="type">ClassTag</span>[<span class="type">C</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = self.withScope &#123;</span><br><span class="line">    require(mergeCombiners != <span class="literal">null</span>, <span class="string">"mergeCombiners must be defined"</span>) <span class="comment">// required as of Spark 0.9.0</span></span><br><span class="line">    <span class="keyword">if</span> (keyClass.isArray) &#123;</span><br><span class="line">        <span class="keyword">if</span> (mapSideCombine) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Cannot use map-side combining with array keys."</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (partitioner.isInstanceOf[<span class="type">HashPartitioner</span>]) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"HashPartitioner cannot partition array keys."</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> aggregator = <span class="keyword">new</span> <span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">        self.context.clean(createCombiner),</span><br><span class="line">        self.context.clean(mergeValue),</span><br><span class="line">        self.context.clean(mergeCombiners))</span><br><span class="line">    <span class="keyword">if</span> (self.partitioner == <span class="type">Some</span>(partitioner)) &#123;</span><br><span class="line">        self.mapPartitions(iter =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> context = <span class="type">TaskContext</span>.get()</span><br><span class="line">            <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context, aggregator.combineValuesByKey(iter, context))</span><br><span class="line">        &#125;, preservesPartitioning = <span class="literal">true</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](self, partitioner)<span class="comment">//创建ShuffledRDD</span></span><br><span class="line">        .setSerializer(serializer)</span><br><span class="line">        .setAggregator(aggregator)</span><br><span class="line">        .setMapSideCombine(mapSideCombine)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>   <strong><em>到此，reduceByKey产生了一个RDD，是ShuffledRDD。共五个RDD</em></strong></p><p>   <code>.saveAsTextFile(&quot;...&quot;)</code></p><p>   将每个元素使用字符串表示形式，将此RDD保存为文本文件。</p>   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsTextFile</span></span>(path: <span class="type">String</span>): <span class="type">Unit</span> = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> nullWritableClassTag = implicitly[<span class="type">ClassTag</span>[<span class="type">NullWritable</span>]]</span><br><span class="line">    <span class="keyword">val</span> textClassTag = implicitly[<span class="type">ClassTag</span>[<span class="type">Text</span>]]</span><br><span class="line">    <span class="keyword">val</span> r = <span class="keyword">this</span>.mapPartitions &#123; iter =&gt;<span class="comment">//注意这里</span></span><br><span class="line">        <span class="keyword">val</span> text = <span class="keyword">new</span> <span class="type">Text</span>()</span><br><span class="line">        iter.map &#123; x =&gt;</span><br><span class="line">            text.set(x.toString)</span><br><span class="line">            (<span class="type">NullWritable</span>.get(), text)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">RDD</span>.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, <span class="literal">null</span>)</span><br><span class="line">    .saveAsHadoopFile[<span class="type">TextOutputFormat</span>[<span class="type">NullWritable</span>, <span class="type">Text</span>]](path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>   通过每次将一个分区的数据以流的方式传入到HDFS中再关闭流</p>   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanedF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>(<span class="comment">//创建MapPartitionsRDD</span></span><br><span class="line">    <span class="keyword">this</span>,</span><br><span class="line">    (context: <span class="type">TaskContext</span>, index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedF(iter),</span><br><span class="line">    preservesPartitioning)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>   <del><strong><em>到此，saveAsTextFile产生了一个RDD，是MapPartitionsRDD。共六个RDD</em></strong></del></p><p>   <em>20200325更新：</em></p><p>   在最后的saveAsTextFile()算子中，我们忽略了一个RDD，它就是是<code>PairRDDFunctions</code>，这个RDD是通过RDD隐式转换过来的</p>   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">rddToPairRDDFunctions</span></span>[<span class="type">K</span>, <span class="type">V</span>](rdd: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)])</span><br><span class="line">(<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>], vt: <span class="type">ClassTag</span>[<span class="type">V</span>], ord: <span class="type">Ordering</span>[<span class="type">K</span>] = <span class="literal">null</span>): <span class="type">PairRDDFunctions</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">PairRDDFunctions</span>(rdd)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>   <strong><em>修正后的 RDD数量是 7个</em></strong></p><ol start="3"><li>总结：使用toDebugString方法，简单的看到了生成了多少个RDD，通过阅读源码的方式，详细了解到了生成了多少个RDD，他们分别做了什么事情。我们这个流程生成了<del>6</del>7个RDD，如果对结果进行排序，也是相同的方法可以看到答案。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark之Transformations&amp;Action</title>
      <link href="/2019/10/02/spark/2/"/>
      <url>/2019/10/02/spark/2/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>Transformations</li><li>Action</li></ol><h2 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h2><p>Transformations的特点是lazy的，和Scala中的lazy该念一致：延迟/懒加载，也就是不会立刻执行，只有等待遇到第一个action才会去提交作业到Spark上</p><h3 id="转换算子"><a href="#转换算子" class="headerlink" title="转换算子"></a>转换算子</h3><p><strong>map</strong> 作用到每一个元素</p><p>输入:任意类型的函数，输出:泛型U类型的函数，返回RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure><p><strong>mapPartitions</strong> 作用到每一个分区</p><p>输入:一个可迭代的类型T，输出:一个可迭代的类型U，返回RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure><p><strong>mapPartitionsWithIndex</strong> 作用到每一个分区并打印分区数</p><p>输入:分区索引，可迭代的类型T，输出:可迭代的类型U，返回RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    f: (<span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure><p><strong>glom()</strong> 按分区返回数组</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">glom</span></span>(): <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">T</span>]]</span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD.glom().collect().foreach(f=&gt;f.foreach(x =&gt; println(_)))</span><br></pre></td></tr></table></figure><p><strong>filter()</strong> 过滤</p><p>输入:输入一个函数T，输出:一个布尔值，返回一个RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Boolean</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p><strong>sample()</strong> 取样</p><p>输入:是否放回的布尔值，抽出来的概率，返回一个RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(</span><br><span class="line">      withReplacement: <span class="type">Boolean</span>,</span><br><span class="line">      fraction: <span class="type">Double</span>,</span><br><span class="line">      seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p><strong>distinct(x)</strong> 去重 ==&gt; numPartitions可指定分区</p><p>输入的必须是RDD，返回的也是一个RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dist: <span class="type">RDD</span>[<span class="type">Int</span>] = listRDD3.distinct()</span><br></pre></td></tr></table></figure><p><strong>coalesce(x)</strong> 重点(小文件相关场景大量使用):   ==&gt; reduce数量决定最终输出的文件数，coalesce的作用是减少到指定分区数(x)，减少分区是窄依赖<br>==&gt; Spark作业遇到shuffle 会切分stage<br>输入一个分区数，返回一个重分区后的RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coalesce</span></span>(numPartitions: <span class="type">Int</span>, shuffle: <span class="type">Boolean</span> = <span class="literal">false</span>,</span><br><span class="line">               partitionCoalescer: <span class="type">Option</span>[<span class="type">PartitionCoalescer</span>] = <span class="type">Option</span>.empty)</span><br><span class="line">              (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>)</span><br><span class="line">      : <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD2.coalesce(<span class="number">1</span>).getNumPartitions</span><br></pre></td></tr></table></figure><h3 id="双value算子"><a href="#双value算子" class="headerlink" title="双value算子"></a>双value算子</h3><p><strong>zip()</strong> 拉链 ==&gt; 不同分区和不同元素都不能用</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zip</span></span>[<span class="type">A1</span> &gt;: <span class="type">A</span>, <span class="type">B</span>, <span class="type">That</span>](that: <span class="type">GenIterable</span>[<span class="type">B</span>])(<span class="keyword">implicit</span> bf: <span class="type">CanBuildFrom</span>[<span class="type">Repr</span>, (<span class="type">A1</span>, <span class="type">B</span>), <span class="type">That</span>]): <span class="type">That</span></span><br></pre></td></tr></table></figure><p><strong>zipWithIndex()</strong> 打印拉链所在的分区</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zipWithIndex</span></span>[<span class="type">A1</span> &gt;: <span class="type">A</span>, <span class="type">That</span>](<span class="keyword">implicit</span> bf: <span class="type">CanBuildFrom</span>[<span class="type">Repr</span>, (<span class="type">A1</span>, <span class="type">Int</span>), <span class="type">That</span>]): <span class="type">That</span></span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> name = <span class="type">List</span>(<span class="string">"张三"</span>, <span class="string">"李四"</span>, <span class="string">"王五"</span>)</span><br><span class="line"><span class="keyword">val</span> age = <span class="type">List</span>(<span class="number">19</span>, <span class="number">26</span>, <span class="number">38</span>)</span><br><span class="line"><span class="keyword">val</span> zipRDD: <span class="type">List</span>[((<span class="type">String</span>, <span class="type">Int</span>), <span class="type">Int</span>)] = name.zip(age).zipWithIndex</span><br></pre></td></tr></table></figure><p><strong>union()</strong> 并集 ==&gt; 分区数相加</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">union</span></span>[<span class="type">B</span> &gt;: <span class="type">A</span>, <span class="type">That</span>](that: <span class="type">GenSeq</span>[<span class="type">B</span>])(<span class="keyword">implicit</span> bf: <span class="type">CanBuildFrom</span>[<span class="type">Repr</span>, <span class="type">B</span>, <span class="type">That</span>]): <span class="type">That</span></span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line"><span class="keyword">val</span> list2 = <span class="type">List</span>(<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">8</span>)</span><br><span class="line"><span class="keyword">val</span> ints: <span class="type">List</span>[<span class="type">Int</span>] = list1.union(list2)</span><br></pre></td></tr></table></figure><p><strong>intersection()</strong> 交集</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">intersect</span></span>[<span class="type">B</span> &gt;: <span class="type">A</span>](that: <span class="type">GenSeq</span>[<span class="type">B</span>]): <span class="type">Repr</span></span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> inter: <span class="type">List</span>[<span class="type">Int</span>] = list1.intersect(list2)</span><br></pre></td></tr></table></figure><p><strong>subtract()</strong> 差集</p><p>输入的必须是RDD，返回的也是一个RDD</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def subtract(other: RDD[T]): RDD[T]</span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sub: <span class="type">RDD</span>[<span class="type">Int</span>] = listRDD2.subtract(listRDD3)</span><br></pre></td></tr></table></figure><p><strong>cartesian()</strong> 笛卡尔积</p><p>输入的必须是RDD，返回的也是一个RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cartesian</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](other: <span class="type">RDD</span>[<span class="type">U</span>]): <span class="type">RDD</span>[(<span class="type">T</span>, <span class="type">U</span>)]</span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> car = listRDD2.cartesian(listRDD3)</span><br></pre></td></tr></table></figure><h3 id="kv算子"><a href="#kv算子" class="headerlink" title="kv算子"></a>kv算子</h3><p><strong>mapValues</strong> 得到所有ky的函数</p><p>输入:一个函数V，输出:一个值U，返回key为K，value为U的键函数对RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapValues</span></span>[<span class="type">U</span>](f: <span class="type">V</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">U</span>)]</span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapRDD.groupByKey().mapValues(_.sum).print()</span><br></pre></td></tr></table></figure><p><strong>sortBy(x)</strong> 降序指定-x，指定任意参数</p><p>输入键值对，指定排序的值，默认升序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortBy</span></span>[<span class="type">K</span>](</span><br><span class="line">      f: (<span class="type">T</span>) =&gt; <span class="type">K</span>,</span><br><span class="line">      ascending: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">      numPartitions: <span class="type">Int</span> = <span class="keyword">this</span>.partitions.length)</span><br><span class="line">      (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">K</span>], ctag: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p><strong>sortByKey(true|false)</strong> 只能根据key排序</p><p>默认升序为true，可指定降序为false</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortByKey</span></span>(ascending: <span class="type">Boolean</span> = <span class="literal">true</span>, numPartitions: <span class="type">Int</span> = self.partitions.length)</span><br><span class="line">      : <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapRDD.map(x=&gt;(x._2,x._1)).sortByKey(<span class="literal">false</span>).map(x=&gt;(x._2,x._1)).print()</span><br></pre></td></tr></table></figure><p><strong>groupByKey</strong> 返回的kv对中的函数可迭代<br>    ==&gt;每个数据都经过shuffle，到reduce聚合，数据量大</p><p>可指定分区数，返回一个PariRDD，包含一个Key和一个可迭代的Value</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapRDD.groupByKey().mapValues(_.sum).print()</span><br></pre></td></tr></table></figure><p><strong>reduceByKey()</strong> 对value做指定的操作，直接返回函数<br>    ==&gt;map端有Combiner先进行了一次预聚合操作，减少了网络IO传输的数据量，所以比groupByKey快<br>    ==&gt;groupByKey的shuffle数据量明显多于reduceByKey，所以建议使用reduceByKey</p><p>输入两个值，输出一个值，返回一个PariRDD，包含一个明确的Key和一个明确的Value</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure><p><strong>join()</strong></p><p>两个RDDjoin，返回一个PariRDD包含一个key，两个Value</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> mapRDD2 = sc.parallelize(<span class="type">List</span>((<span class="string">"zhaoliu"</span>, <span class="number">18</span>), (<span class="string">"zhangsan"</span>, <span class="number">22</span>), (<span class="string">"list"</span>, <span class="number">21</span>), (<span class="string">"wangwu"</span>, <span class="number">26</span>)))</span><br><span class="line"><span class="keyword">val</span> mapRDD3 = sc.parallelize(<span class="type">List</span>((<span class="string">"hongqi"</span>, <span class="string">"男"</span>), (<span class="string">"zhangsan"</span>, <span class="string">"男"</span>), (<span class="string">"list"</span>, <span class="string">"女"</span>), (<span class="string">"wangwu"</span>, <span class="string">"男"</span>)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))]</span><br></pre></td></tr></table></figure><p><strong>leftOuterJoin</strong></p><p>两个RDDjoin，返回一个PariRDD包含一个key，一个确定的左表Value值，一个Option类型的右表Value值，即可能为空</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leftOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">Option</span>[<span class="type">W</span>]))]</span><br></pre></td></tr></table></figure><p><strong>rightOuterJoin</strong></p><p>两个RDDjoin，返回一个PariRDD包含一个key，一个确定的右表Value值，一个Option类型的左表Value值，即可能为空</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rightOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Option</span>[<span class="type">V</span>], <span class="type">W</span>))]</span><br></pre></td></tr></table></figure><p><strong>fullOuterJoin</strong></p><p>两个RDDjoin，返回一个PariRDD包含一个key，一个Option类型的右表Value值，一个Option类型的左表Value值，即都可能为空</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fullOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Option</span>[<span class="type">V</span>], <span class="type">Option</span>[<span class="type">W</span>]))]</span><br></pre></td></tr></table></figure><p><strong>cogroup</strong> </p><p>作用和join类似，不同的是返回的结果是可迭代的，而join返回的是值，原因是join底层调用了cogroup</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W</span>]))]</span><br></pre></td></tr></table></figure><p>面试题：Spark Core 不使用distinct去重</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> listRDD3 = sc.parallelize(<span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">listRDD3.map(x=&gt;(x,<span class="literal">null</span>)).reduceByKey((x,y)=&gt;x).map(_._1).print()</span><br></pre></td></tr></table></figure><h2 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h2><p><strong>first()</strong></p><p>返回第一个元素，等于take(1)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">first</span></span>(): <span class="type">T</span></span><br></pre></td></tr></table></figure><p><strong>take()</strong>    </p><p>拿出指定的前N个元素,返回一个数组，结果为原始顺序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">take</span></span>(num: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p><strong>count()</strong></p><p>返回元素数量，是个Long型</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span> = sc.runJob(<span class="keyword">this</span>, <span class="type">Utils</span>.getIteratorSize _).sum</span><br></pre></td></tr></table></figure><p><strong>sum</strong> </p><p>求和，返回一个Double型</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(): <span class="type">Double</span></span><br></pre></td></tr></table></figure><p><strong>max</strong> </p><p>返回最大值，结果通过隐式转换排序过</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max</span></span>()(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">T</span></span><br></pre></td></tr></table></figure><p><strong>min</strong></p><p>返回最小值，结果通过隐式转换排序过</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">min</span></span>()(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">T</span></span><br></pre></td></tr></table></figure><p><strong>top()</strong></p><p>先排降序再返回前N个元素组成的<strong>数组</strong>，字典序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p>案例：升序排序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD.top(<span class="number">3</span>)(<span class="type">Ordering</span>.by(x =&gt; -x)).foreach(println)</span><br></pre></td></tr></table></figure><p><strong>takeOrdered</strong></p><p>先排降序再返回N个元素组成的<strong>数组</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeOrdered</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p>案例：升序排序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD.takeOrdered(<span class="number">3</span>)(<span class="type">Ordering</span>.by(x =&gt; x)).foreach(println)</span><br></pre></td></tr></table></figure><p><strong>reduce</strong></p><p>聚合，输入两个元素输出一个元素，类型相同</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(f: (<span class="type">T</span>, <span class="type">T</span>) =&gt; <span class="type">T</span>): <span class="type">T</span></span><br></pre></td></tr></table></figure><p><strong>foreach</strong></p><p>循环输出</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def foreach(f: T =&gt; Unit): Unit</span><br></pre></td></tr></table></figure><p><strong>foreachPartition</strong></p><p>分区循环输出</p><p>输入的是一个可迭代的类型T，输出Unit</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreachPartition</span></span>(f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Unit</span>): <span class="type">Unit</span></span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD.foreachPartition(x=&gt;x.foreach(println))</span><br></pre></td></tr></table></figure><p><strong>countByKey</strong></p><p>根据key统计个数，用作<strong>检测数据倾斜</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countByKey</span></span>(): <span class="type">Map</span>[<span class="type">K</span>, <span class="type">Long</span>]</span><br></pre></td></tr></table></figure><p><strong>lookup</strong></p><p>根据map中的键来取出相应的值的，</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lookup</span></span>(key: <span class="type">K</span>): <span class="type">Seq</span>[<span class="type">V</span>]</span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapRDD.lookup(<span class="string">"zhangsan"</span>).foreach(println)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>编译Spark&amp;Idea配置Spark环境&amp;RDD五大特点&amp;Spark参数管理&amp;数据的读写</title>
      <link href="/2019/10/01/spark/1/"/>
      <url>/2019/10/01/spark/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>编译Spark</li><li>Idea配置Spark环境</li><li>RDD五大特点</li><li>Spark参数管理</li><li>数据的读写</li></ol><h2 id="编译Spark"><a href="#编译Spark" class="headerlink" title="编译Spark"></a>编译Spark</h2><p>作为一个Spark玩的6的攻城狮，第一步就是要学会如何编译Spark</p><ol><li><p>下载spark源码: <a href="http://spark.apache.org/" target="_blank" rel="noopener">官网</a>或者<a href="https://github.com/apache/spark" target="_blank" rel="noopener">github</a></p></li><li><p>查看官网<a href="看官方文档http://spark.apache.org/docs/latest/building-spark.html">编译文档</a>，切记注意版本号，不同版本号编译方式区别很大</p></li><li><p>修改相关配置</p><ol><li><p>注释掉make-distribution.sh脚本中的128行左右一下，使用固定的版本替代</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">VERSION=2.4.5</span><br><span class="line">SCALA_VERSION=2.12.10</span><br><span class="line">SPARK_HADOOP_VERSION=2.6.0-cdh5.16.2</span><br><span class="line">SPARK_HIVE=1</span><br></pre></td></tr></table></figure><p>替代==&gt;</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">#VERSION=$("$MVN" help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v "INFO"\</span><br><span class="line">#    | grep -v "WARNING"\</span><br><span class="line">#    | tail -n 1)</span><br><span class="line">#SCALA_VERSION=$("$MVN" help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v "INFO"\</span><br><span class="line">#    | grep -v "WARNING"\</span><br><span class="line">#    | tail -n 1)</span><br><span class="line">#SPARK_HADOOP_VERSION=$("$MVN" help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v "INFO"\</span><br><span class="line">#    | grep -v "WARNING"\</span><br><span class="line">#    | tail -n 1)</span><br><span class="line">#140 #SPARK_HIVE=$("$MVN" help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v "INFO"\</span><br><span class="line">#    | grep -v "WARNING"\</span><br><span class="line">#    | fgrep --count "<span class="tag">&lt;<span class="name">id</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">id</span>&gt;</span>";\</span><br><span class="line">#    # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\</span><br><span class="line">#    # because we use "set -o pipefail"</span><br><span class="line">#    echo -n)</span><br></pre></td></tr></table></figure></li><li><p>修改maven仓库地址，在253行左右，pom.xml文件</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">Maven Repository</span><br><span class="line"><span class="comment">&lt;!--&lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt; --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"></span><br><span class="line">加上cdh的下载地址</span><br><span class="line"><span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">releases</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">releases</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>false<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol></li><li><p>开始编译</p><p>maven编译前执行</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">export MAVEN_OPTS=<span class="string">"-Xmx2g -XX:ReservedCodeCacheSize=1g"</span></span><br></pre></td></tr></table></figure><p>make-distribution.sh编译不需要执行，我们这里使用make-distribution.sh编译，它的脚本自动执行了这句</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">./dev/change-scala-version.sh <span class="number">2.12</span></span><br><span class="line">./dev/make-distribution.sh --name <span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.16</span><span class="number">.2</span> --tgz -Pyarn -Phive -Phive-thriftserver -Pscala-<span class="number">2.12</span> -Phadoop-<span class="number">2.6</span> -Dhadoop.version=<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.16</span><span class="number">.2</span></span><br></pre></td></tr></table></figure></li><li><p>查看生成的jar包</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">/home/hadoop/app/spark-<span class="number">2.4</span><span class="number">.5</span>/spark-<span class="number">2.4</span><span class="number">.5</span>-bin-<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.16</span><span class="number">.2</span>.tgz</span><br></pre></td></tr></table></figure></li></ol><h2 id="Idea配置Spark环境"><a href="#Idea配置Spark环境" class="headerlink" title="Idea配置Spark环境"></a>Idea配置Spark环境</h2><ol><li><p>idae引入依赖</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.source</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.target</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>2.6.0-cdh5.16.2<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.12.10<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.tools.version</span>&gt;</span>2.12<span class="tag">&lt;/<span class="name">scala.tools.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">spark.version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_$&#123;scala.tools.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>配置阿里云和cdh的仓库地址</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>false<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>central<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>aliyun maven<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">layout</span>&gt;</span>default<span class="tag">&lt;/<span class="name">layout</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 是否开启发布版构件下载 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">releases</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">releases</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 是否开启快照版构件下载 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>false<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="RDD五大特点"><a href="#RDD五大特点" class="headerlink" title="RDD五大特点"></a>RDD五大特点</h2><p>先看官方介绍:</p><p>弹性分布式数据集(RDD)，是Spark的基本抽象。表示可以并行操作的不可变的、分区的元素集合。这个类包含所有RDDS上可用的基本操作，如<code>map</code>、<code>filter</code>和<code>filter</code>。</p><p><code>[[org.apache.spark.rdd.PairRDDFunctions]]</code>，包含仅在键值对的RDDs上可用的操作，如<code>groupByKey</code> 和<code>join</code> ; </p><p><code>[[org.apache.spark.rdd.DoubleRDDFunctions]]</code> ，包含仅在双精度的RDDs上可用的操作;</p><p><code>[[org.apache.spark.rdd.SequenceFileRDDFunctions]]</code>，包含可在RDDs上使用的操作，这些操作可以保存为序列文件。</p><p>所有的操作都可以通过隐式转换的方式在任何正确类型的RDD上自动使用(例如RDD[(Int, Int)]);</p><p>RDD: resilient distributed dataset(弹性分布式数据集)，</p><ol><li><p>弹性表示容错</p></li><li><p>分布式表示分区</p></li><li><p>数据集表示集</p></li></ol><p><strong>每个RDD有五个主要特征:</strong></p><ol><li>一个RDD由很多partition构成(block块对应partition)，在spark中，有多少partition就对应有多少个task来执行。</li><li>对RDD做计算，相当于对RDD的每个partition或split做计算</li><li>RDD之间有依赖关系，可溯源，容错机制</li><li>如果RDD里面存的数据是key-value形式，则可以进行重新分区</li><li>最优位置计算，也就是数据的本地性，移动计算而不是移动数据。</li></ol><p>RDD是一个顶级的抽象类，它有五个抽象方法，分别实现了这五个特性</p><ol><li><p>分区</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Implemented by subclasses to return the set of partitions in this RDD. This method will only</span></span><br><span class="line"><span class="comment"> * be called once, so it is safe to implement a time-consuming computation in it.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * The partitions in this array must satisfy the following property:</span></span><br><span class="line"><span class="comment"> *   `rdd.partitions.zipWithIndex.forall &#123; case (partition, index) =&gt; partition.index == index &#125;`</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>]</span><br></pre></td></tr></table></figure></li><li><p>计算</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * :: DeveloperApi ::</span></span><br><span class="line"><span class="comment">  * Implemented by subclasses to compute a given partition.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li><li><p>依赖</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only</span></span><br><span class="line"><span class="comment">  * be called once, so it is safe to implement a time-consuming computation in it.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = deps</span><br></pre></td></tr></table></figure></li><li><p>重新分区</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/** Optionally overridden by subclasses to specify how they are partitioned. */</span></span><br><span class="line"><span class="meta">@transient</span> <span class="keyword">val</span> partitioner: <span class="type">Option</span>[<span class="type">Partitioner</span>] = <span class="type">None</span></span><br></pre></td></tr></table></figure></li><li><p>最优位置</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Optionally overridden by subclasses to specify placement preferences.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPreferredLocations</span></span>(split: <span class="type">Partition</span>): <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Nil</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="Spark参数管理"><a href="#Spark参数管理" class="headerlink" title="Spark参数管理"></a>Spark参数管理</h2><ol><li><p>如果想要定义自己的参数传递到spark中去，一定要以<code>spark.</code>开头</p></li><li><p>如果想要获取spark中的参数的值，使用<code>sc.getConf.get(key)</code></p></li></ol><h2 id="数据的读写"><a href="#数据的读写" class="headerlink" title="数据的读写"></a>数据的读写</h2><ol><li><p>读本地数据：<br><code>sc.textFile(&quot;file://&quot;)</code> 需要添加<code>file://</code></p></li><li><p>读hdfs数据：<br><code>sc.textFile(&quot;&quot;)</code>  默认读hdfs 不需要加前缀<br>注意：textFile() 可以使用通配符匹配目录、指定文件、指定文件夹</p></li><li><p>wholeTextFiles()</p><p>如果使用的是wholeTextFiles() ，它会返回路径+内容</p></li><li><p>写本地</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">rdd.saveAsTestFile(<span class="string">"path"</span>)</span><br></pre></td></tr></table></figure></li><li><p>写HDFS需要配置</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"hadoop"</span>);</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://192.168.91.10:9000"</span>);</span><br><span class="line">conf.set(<span class="string">"dfs.client.use.datanode.hostname"</span>, <span class="string">"true"</span>);</span><br></pre></td></tr></table></figure></li><li><p>压缩写</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">saveAsTestFile(out,classOf[<span class="type">BZip2Codec</span>])</span><br></pre></td></tr></table></figure></li><li><p>对象写</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span>(<span class="params">name:<span class="type">String</span>,age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">p1</span> </span>= (<span class="string">"张三"</span>,<span class="number">18</span>)</span><br><span class="line"><span class="keyword">val</span> p2 = (<span class="string">"李四"</span>,<span class="number">21</span>)</span><br><span class="line">parallelize(<span class="type">List</span>(p1,p2)).saveAsObjectFile(<span class="string">"out"</span>)</span><br></pre></td></tr></table></figure></li><li><p>对象读</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc.objectFile[<span class="type">Persion</span>](<span class="string">"out"</span>).collect().foreach(println)</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CentOS7安装CDH 第九章：CDH中安装Kafka</title>
      <link href="/2019/04/09/cdh/9/"/>
      <url>/2019/04/09/cdh/9/</url>
      
        <content type="html"><![CDATA[<h2 id="CDH官网Kafka的安装教程网址"><a href="#CDH官网Kafka的安装教程网址" class="headerlink" title="CDH官网Kafka的安装教程网址"></a>CDH官网Kafka的安装教程网址</h2><p><a href="https://www.cloudera.com/documentation/kafka/latest/topics/kafka_installing.html#concept_m2t_d45_4r" target="_blank" rel="noopener">点击进入官网</a></p><p><img src="https://yerias.github.io/cdh/kafka-1.png" alt=""></p><h2 id="下载对应的Kafka版本"><a href="#下载对应的Kafka版本" class="headerlink" title="下载对应的Kafka版本"></a>下载对应的Kafka版本</h2><ol><li><p>查看CDH和Kafka的版本对应列表：</p><p><a href="https://www.cloudera.com/documentation/enterprise/release-notes/topics/rn_consolidated_pcm.html#pcm_kafka" target="_blank" rel="noopener">点击进入官网</a></p></li></ol><p><img src="https://yerias.github.io/cdh/kafka-2.png" alt=""></p><ol start="2"><li><p>因为安装的CDH版本为5.10或5.12，故选择的Kafka版本为2.2.x和0.10.2，此时去网站找到对应的Kafka版本：</p><p><a href="https://www.cloudera.com/documentation/kafka/latest/topics/kafka_packaging.html#concept_fzg_phl_br" target="_blank" rel="noopener">点击进入官网</a></p></li></ol><p><img src="https://yerias.github.io/cdh/kafka-3.png" alt=""></p><ol start="3"><li>点击对应的下载地址，下载该Kafka的parcel包（需更改sha1的后缀名）：</li></ol><p><img src="https://yerias.github.io/cdh/kafka-4.png" alt=""></p><ol start="4"><li>最终的主节点上的kafka_parcel包(包的位置任意，最终都要移动到/var/www/html/kafka_parcel目录下)</li></ol><p><img src="https://yerias.github.io/cdh/kafka-4-2.jpg" alt=""></p><h2 id="安装Kafka服务"><a href="#安装Kafka服务" class="headerlink" title="安装Kafka服务"></a>安装Kafka服务</h2><ol><li><p>将主节点上Kafka的parcel包（3个文件）上传到/var/www/html/kafka_parcel目录下，需配置好https服务，请参考上述CDH安装时的方法配置，在浏览器上能访问到如下场景即可：</p><p>安装httpd: <code>yum install -y httpd</code></p></li></ol><p><img src="https://yerias.github.io/cdh/kafka-5.png" alt=""></p><ol start="2"><li>点击CDH主页面中的主机下面的Parcel按钮：</li></ol><p><img src="https://yerias.github.io/cdh/kafka-6.png" alt=""></p><ol start="3"><li>点击Parcel界面的配置按钮，配置Kafka的地址，该地址默认是官网地址，但在CDH的离线安装时已将所有的在线地址删除，所以在这加上Kafka的Parcel包的离线地址即可：</li></ol><p><img src="https://yerias.github.io/cdh/kafka-7.png" alt=""></p><ol start="4"><li>在Parcel界面，点击Kafka的下载按钮：</li></ol><p><img src="https://yerias.github.io/cdh/kafka-8.png" alt=""></p><ol start="5"><li>依次执行Kafka的分配和激活：</li></ol><p><img src="https://yerias.github.io/cdh/kafka-9.png" alt=""></p><p><img src="https://yerias.github.io/cdh/kafka-9-2.png" alt="kafka-9-2"></p><h2 id="将Kafka服务添加到CDH中"><a href="#将Kafka服务添加到CDH中" class="headerlink" title="将Kafka服务添加到CDH中"></a>将Kafka服务添加到CDH中</h2><ol><li>在CDH的主界面点击添加服务按钮，并选择Kafka服务：</li></ol><p><img src="https://yerias.github.io/cdh/kafka-10.png" alt=""></p><ol start="2"><li>给Kafka分配节点（Kafka后面2个服务一般情况下不选）：</li></ol><p><img src="https://yerias.github.io/cdh/kafka-11.png" alt=""></p><p><img src="https://yerias.github.io/cdh/kafka-11-2.png" alt=""></p><ol start="3"><li>Kafka的配置文件进行配置：</li></ol><p>配置Kafka的文件存放目录，因为Kafka是依赖Zookeeper的，所以Kafka的文件也是存放在Zookeeper的目录中，如果要卸载Kafka时，需要将这些Kafka的文件也删除，所以可以把Kafka的文件存放在一个目录中：</p><p>Kafka的文件存放目录：</p><p><img src="https://yerias.github.io/cdh/kafka-12.png" alt=""></p><p>进入Zookeeper的文件管理界面（命令行）：</p><p><img src="https://yerias.github.io/cdh/kafka-13.png" alt=""></p><p><img src="https://yerias.github.io/cdh/kafka-14.png" alt=""></p><p>因为Kafka是一个消息中间键，有将生产者生产的信息进行缓存的操作，所以在配置Kafka的数据存储目录时需要注意，将数据存放到一个比较大的磁盘中，该数据存放的目录如下配置所示：</p><p><img src="https://yerias.github.io/cdh/kafka-15.png" alt=""></p><p>在卸载重装Kafka时，需要将Zookeeper目录下的Kafka文件，以及Kafka数据存放的目录都清空，请注意是每个节点都要清空，否则不能重装。</p><ol start="5"><li>启动Kafka服务，会发现Kafka服务不能成功启动，报错如下：</li></ol><p><img src="https://yerias.github.io/cdh/kafka-16.png" alt=""></p><p>查看日志</p><p><img src="https://yerias.github.io/cdh/kafka-17.png" alt=""></p><p>此时为主机的内存不足，返回Kafka配置文件界面，修改memory中的Java Heap Size of Broker值为512M（如果机器内存充足，可以再大一些），如下：</p><p><img src="https://yerias.github.io/cdh/kafka-18.png" alt=""></p><p>修改之后去CDH的主界面重启Kafka，启动成功，如下所示：</p><p><img src="https://yerias.github.io/cdh/kafka-19.png" alt=""></p><h2 id="测试Kafka"><a href="#测试Kafka" class="headerlink" title="测试Kafka"></a>测试Kafka</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建topic</span></span><br><span class="line">./kafka-topics.sh \</span><br><span class="line">--create \</span><br><span class="line">--zookeeper cdh001:2181,cdh002:2181,cdh003:2181/kafka \</span><br><span class="line">--replication-factor 2 --partitions 3 --topic cloudera</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动生产者</span></span><br><span class="line">./kafka-console-producer.sh \</span><br><span class="line">--broker-list cdh001:9092 ,cdh002:9092 ,cdh003:9092  \</span><br><span class="line">--topic cloudera</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动消费者</span></span><br><span class="line">./kafka-console-consumer.sh \</span><br><span class="line">--bootstrap-server cdh001:9092 ,cdh002:9092 ,cdh003:9092  \</span><br><span class="line">--from-beginning \</span><br><span class="line">--topic cloudera</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除topic</span></span><br><span class="line">./kafka-topics.sh  \</span><br><span class="line">--delete \</span><br><span class="line">--zookeeper  cdh001:2181,cdh002:2181,cdh003:2181/kafka  \</span><br><span class="line">--topic cloudera</span><br></pre></td></tr></table></figure><h2 id="重装Kafka"><a href="#重装Kafka" class="headerlink" title="重装Kafka"></a>重装Kafka</h2><p>除了上文提到的要删除zookeeper中的kafka目录，还要删除Kafka的数据存储目录，即<code>/var/local/kafka/data</code></p>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GIT的常用操作&amp;GITHUB的常用操作&amp;在IDEA中使用GIT操作GITHUB</title>
      <link href="/2019/02/01/git/1/"/>
      <url>/2019/02/01/git/1/</url>
      
        <content type="html"><![CDATA[<h2 id="GIT实战操作"><a href="#GIT实战操作" class="headerlink" title="GIT实战操作"></a>GIT实战操作</h2><ol><li><p>创建版本库</p><p>在项目文件夹内，执行: git init</p></li><li><p>提交文件</p><p>新建文件后，通过git status 进行查看文件状态(可选)</p><p>将文件添加到暂存区  git add 文件名</p><p>或者也可以git commit –m “注释内容”, 直接带注释提交</p></li><li><p>查看文件提交记录</p><p>git log –pretty=oneline 文件名   进行查看历史记录</p></li><li><p>回退历史</p><p>git reset –hard HEAD~n 回退n次操作</p></li><li><p>版本穿越</p><p>进行查看历史记录的版本号，执行 git reflog 文件名</p><p>执行 git reset –hard 版本号</p></li><li><p>还原文件</p><p>git checkout – 文件名 </p></li><li><p>删除某个文件</p><p>先删除文件 git rm 文件名</p><p>再git add 再提交</p></li><li><p>创建分支</p><p>git branch &lt;分支名&gt;</p><p>git branch –v 查看分支</p></li><li><p>切换分支</p><p>git checkout –b &lt;分支名&gt;</p></li><li><p>合并分支</p><p>先切换到主干  git checkout master</p><p>git merge &lt;分支名&gt;</p></li><li><p>合并时冲突</p><p>程序合并时发生冲突系统会提示CONFLICT关键字，命令行后缀会进入MERGING状态，表示此时是解决冲突的状态。</p><p>然后修改冲突文件的内容，再次git add <file> 和git commit 提交后，后缀MERGING消失，说明冲突解决完成。</p></li></ol><h2 id="GITHUB实战操作"><a href="#GITHUB实战操作" class="headerlink" title="GITHUB实战操作"></a>GITHUB实战操作</h2><ol><li><p>搭建代码库</p><ul><li><p>git init</p></li><li><p>git config </p><ul><li>git config –global(全局) user.email “<a href="mailto:you@example.com" target="_blank" rel="noopener">you@example.com</a>”</li><li>git config –global(全局) user.name “Your Name”</li></ul></li></ul></li><li><p>提交代码到本地仓库</p><ul><li><p>git add 文件名</p></li><li><p>git commit –m “注释内容”</p></li></ul></li><li><p>GitHub准备工作：</p><ul><li><p>注册GitHub账号</p></li><li><p>在GitHub搭建项目</p></li></ul></li><li><p>推送代码到远端</p><ul><li><p>git remote add origin <url>(仓库地址)</p></li><li><p>git push origin master</p></li></ul></li><li><p>其他用户克隆</p><p>git clone <url></p></li><li><p>其他用户提交代码到本地仓库</p><ul><li><p>git add 文件名</p></li><li><p>git commit –m “注释内容”</p></li></ul></li><li><p>其他用户推送到远端仓库</p><ul><li>git push origin master</li></ul></li><li><p>其他用户拉取代码</p><ul><li>git pull origin master</li></ul></li><li><p>增加远程地址</p><ul><li>git remote add &lt;远端代号(origin)&gt;  &lt;远端地址&gt;</li></ul></li><li><p>推送到远程库</p><ul><li>git push  &lt;远端代号&gt;  &lt;本地分支名称&gt;</li></ul></li><li><p>合作开发权限</p><p><img src="https://yerias.github.io/git/%E6%B7%BB%E5%8A%A0%E5%90%88%E4%BD%9C%E4%BC%99%E4%BC%B41.jpg" alt="添加合作伙伴1"></p><p><img src="https://yerias.github.io/git/%E6%B7%BB%E5%8A%A0%E5%90%88%E4%BD%9C%E4%BC%99%E4%BC%B42.jpg" alt="添加合作伙伴2"></p></li><li><p>协作冲突</p><p>在上传或同步代码时，由于你和他人都改了同一文件的同一位置的代码，版本管理软件无法判断究竟以谁为准，就会报告冲突,需要程序员手工解决。</p><ul><li><p>修改合并</p></li><li><p>git add 文件名</p></li><li><p>git commit –m “注释内容”</p></li><li><p>git push origin master</p></li></ul></li></ol><h2 id="在IDEA中使用GIT"><a href="#在IDEA中使用GIT" class="headerlink" title="在IDEA中使用GIT"></a>在IDEA中使用GIT</h2><ol><li><p>配置</p><p>setting配置GIT</p><p><img src="https://yerias.github.io/git/%E9%85%8D%E7%BD%AEgit%E6%89%A7%E8%A1%8C%E7%A8%8B%E5%BA%8F.jpg" alt="配置git执行程序"></p></li><li><p>创建仓库</p><p>VCS配置账户密码创建仓库</p><p><img src="https://yerias.github.io/git/%E5%88%9B%E5%BB%BAgithub%E4%BB%93%E5%BA%93.jpg" alt="创建github仓库"></p></li><li><p>提交代码</p><p><img src="https://yerias.github.io/git/%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%811.jpg" alt="提交代码1"></p><p><img src="https://yerias.github.io/git/%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%812.jpg" alt="提交代码2"></p><p><img src="https://yerias.github.io/git/%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%813.jpg" alt="提交代码3"></p></li><li><p>同步代码</p><p><img src="https://yerias.github.io/git/%E5%90%8C%E6%AD%A5%E4%BB%A3%E7%A0%81.jpg" alt="同步代码"></p></li><li><p>克隆项目</p><p><img src="https://yerias.github.io/git/%E5%85%8B%E9%9A%86%E4%BB%A3%E7%A0%811.jpg" alt="克隆代码1"></p><p><img src="https://yerias.github.io/git/%E5%85%8B%E9%9A%86%E4%BB%A3%E7%A0%812.jpg" alt="克隆代码2"></p></li><li><p>解决版本冲突</p><p>代码添加到公共区间再次提交</p><p><img src="https://yerias.github.io/git/%E7%89%88%E6%9C%AC%E5%86%B2%E7%AA%81.jpg" alt="版本冲突"></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IO流&amp;比较器&amp;内部类&amp;Random</title>
      <link href="/2019/01/08/java/8/"/>
      <url>/2019/01/08/java/8/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>IO流</li><li>比较器</li><li>内部类</li><li>Random</li></ol><h2 id="IO流"><a href="#IO流" class="headerlink" title="IO流"></a>IO流</h2><p>Java中的流根据传输方向分为输入输出流，根据操作数据的不同又可以分为字节流和字符流</p><h3 id="字节流"><a href="#字节流" class="headerlink" title="字节流"></a>字节流</h3><p>所有的字节流都继承自InputStream接口和OutputStream接口</p><p>用于文件传输的是FileInputStream类和FileOutputStream类，传输的是字节，使用FileInputStream读取文件时，可以使用byte字节数组建立一个字节数组缓冲区，读取数据时read方法中传入一个字节数组，每次读取一个字节数组的数据，即可实现缓冲区读取数据</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">byte</span>[] buff = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</span><br><span class="line">in.read(buff)  <span class="comment">//数据读进buff中</span></span><br><span class="line">out.write(buff) <span class="comment">//write中传入buff写出数据</span></span><br></pre></td></tr></table></figure><p>在字节流的IO包中提供了两个带缓冲的字节流，分别是BufferedInputStream和BufferedOutputStream，他们的构造方法中分别接受InputStream和OutputStream，类型的参数作为对象，这两</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">new</span> BufferedInputStream(<span class="keyword">new</span> InputStream(filePath));</span><br></pre></td></tr></table></figure><h3 id="字符流"><a href="#字符流" class="headerlink" title="字符流"></a>字符流</h3><h2 id="比较器"><a href="#比较器" class="headerlink" title="比较器"></a>比较器</h2><h2 id="内部类"><a href="#内部类" class="headerlink" title="内部类"></a>内部类</h2><h2 id="Random"><a href="#Random" class="headerlink" title="Random"></a>Random</h2>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA.UTIL包下的TreeSet和迭代器快速失败的的源码解析以及多线程下故障现象、导致原因、以及解决办法和优化建议</title>
      <link href="/2019/01/05/java/6/"/>
      <url>/2019/01/05/java/6/</url>
      
        <content type="html"><![CDATA[<hr><p>该篇博客不适合小白，只做针对性的api源码解析，以及适合我自身的案例研究</p><hr><ol><li><p>使用多个线程往<code>ArrayList</code>中添加元素</p></li><li><p>故障现象</p></li><li><p>故障原因</p></li><li><p>解决方法</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HashMap1.8源码分析</title>
      <link href="/2019/01/04/java/5/"/>
      <url>/2019/01/04/java/5/</url>
      
        <content type="html"><![CDATA[<hr><p>该篇博客不适合小白，只做针对性的api源码解析，以及适合我自身的案例研究</p><hr><h3 id="HashMap构造函数"><a href="#HashMap构造函数" class="headerlink" title="HashMap构造函数"></a>HashMap构造函数</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashMap</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">AbstractMap</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt;</span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">Map</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt;, <span class="title">Cloneable</span>, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始容量</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_INITIAL_CAPACITY = <span class="number">1</span> &lt;&lt; <span class="number">4</span>; <span class="comment">// aka 16</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 最大容量2的30次方</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MAXIMUM_CAPACITY = <span class="number">1</span> &lt;&lt; <span class="number">30</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 负载因子</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">float</span> DEFAULT_LOAD_FACTOR = <span class="number">0.75f</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 链表转树的阈值</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TREEIFY_THRESHOLD = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 树转链表的阈值</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> UNTREEIFY_THRESHOLD = <span class="number">6</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 树最小的子节点数</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MIN_TREEIFY_CAPACITY = <span class="number">64</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 默认初始容量是16，负载因子是0.75</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">HashMap</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.loadFactor = DEFAULT_LOAD_FACTOR; <span class="comment">// all other fields defaulted</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 只传入初始容量</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">HashMap</span><span class="params">(<span class="keyword">int</span> initialCapacity)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>(initialCapacity, DEFAULT_LOAD_FACTOR);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 传入初始容量和负载因子</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HashMap</span><span class="params">(<span class="keyword">int</span> initialCapacity, <span class="keyword">float</span> loadFactor)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (initialCapacity &lt; <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Illegal initial capacity: "</span> +</span><br><span class="line">                                               initialCapacity);</span><br><span class="line">        <span class="keyword">if</span> (initialCapacity &gt; MAXIMUM_CAPACITY)</span><br><span class="line">            initialCapacity = MAXIMUM_CAPACITY;</span><br><span class="line">        <span class="keyword">if</span> (loadFactor &lt;= <span class="number">0</span> || Float.isNaN(loadFactor))</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Illegal load factor: "</span> +</span><br><span class="line">                                               loadFactor);</span><br><span class="line">        <span class="keyword">this</span>.loadFactor = loadFactor;</span><br><span class="line">        <span class="keyword">this</span>.threshold = tableSizeFor(initialCapacity);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 传入一个map集合</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HashMap</span><span class="params">(Map&lt;? extends K, ? extends V&gt; m)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.loadFactor = DEFAULT_LOAD_FACTOR;</span><br><span class="line">        putMapEntries(m, <span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="HashMap的数据结构"><a href="#HashMap的数据结构" class="headerlink" title="HashMap的数据结构"></a>HashMap的数据结构</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">Map</span>.<span class="title">Entry</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// hash值</span></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> hash;</span><br><span class="line">    <span class="comment">// key值</span></span><br><span class="line">    <span class="keyword">final</span> K key;</span><br><span class="line">    <span class="comment">// value值</span></span><br><span class="line">    V value;</span><br><span class="line">    <span class="comment">// 下一个Node的指针</span></span><br><span class="line">    Node&lt;K,V&gt; next;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 构造方法</span></span><br><span class="line">    Node(<span class="keyword">int</span> hash, K key, V value, Node&lt;K,V&gt; next) &#123;</span><br><span class="line">        <span class="keyword">this</span>.hash = hash;</span><br><span class="line">        <span class="keyword">this</span>.key = key;</span><br><span class="line">        <span class="keyword">this</span>.value = value;</span><br><span class="line">        <span class="keyword">this</span>.next = next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 得到key值</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> K <span class="title">getKey</span><span class="params">()</span>        </span>&#123; <span class="keyword">return</span> key; &#125;</span><br><span class="line">    <span class="comment">// 得到vaue值</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> V <span class="title">getValue</span><span class="params">()</span>      </span>&#123; <span class="keyword">return</span> value; &#125;</span><br><span class="line">    <span class="comment">// 重写toString</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> key + <span class="string">"="</span> + value; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// hashCode值等于key和value的hash值取反</span></span><br><span class="line">        <span class="keyword">return</span> Objects.hashCode(key) ^ Objects.hashCode(value);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 修改value</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> V <span class="title">setValue</span><span class="params">(V newValue)</span> </span>&#123;</span><br><span class="line">        V oldValue = value;</span><br><span class="line">        value = newValue;</span><br><span class="line">        <span class="keyword">return</span> oldValue;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断内容相等</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (o == <span class="keyword">this</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        <span class="keyword">if</span> (o <span class="keyword">instanceof</span> Map.Entry) &#123;</span><br><span class="line">            Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o;</span><br><span class="line">            <span class="keyword">if</span> (Objects.equals(key, e.getKey()) &amp;&amp;</span><br><span class="line">                Objects.equals(value, e.getValue()))</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="重温jdk1-7中如何触发死循环的"><a href="#重温jdk1-7中如何触发死循环的" class="headerlink" title="重温jdk1.7中如何触发死循环的"></a>重温jdk1.7中如何触发死循环的</h3><p>单线程情况下，rehash无问题。下图演示了单线程条件下的rehash过程</p><p><img src="https://yerias.github.io/java_img/java_1.png" alt="单线程rehash"></p><p>多线程并发下的rehash</p><p>这里假设有两个线程同时执行了put操作并引发了rehash，执行了transfer方法，并假设线程一进入transfer方法并执行完next = e.next后，因为线程调度所分配时间片用完而“暂停”，此时线程二完成了transfer方法的执行。此时状态如下。</p><p><img src="https://yerias.github.io/java_img/java_2.png" alt="多线程rehash"></p><p>接着线程1被唤醒，继续执行第一轮循环的剩余部分</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">e.next = newTable[<span class="number">1</span>] = <span class="keyword">null</span></span><br><span class="line">newTable[<span class="number">1</span>] = e = key(<span class="number">5</span>)</span><br><span class="line">e = next = key(<span class="number">9</span>)</span><br></pre></td></tr></table></figure><p>结果如下图所示</p><p><img src="https://yerias.github.io/java_img/java_3.png" alt="多线程rehash"></p><p>接着线程1被唤醒，继续执行第一轮循环的剩余部分</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">e.next = newTable[<span class="number">1</span>] = <span class="keyword">null</span></span><br><span class="line">newTable[<span class="number">1</span>] = e = key(<span class="number">5</span>)</span><br><span class="line">e = next = key(<span class="number">9</span>)</span><br></pre></td></tr></table></figure><p>结果如下图所示</p><p><img src="https://yerias.github.io/java_img/java_5.png" alt="多线程rehash"></p><p>接着执行下一轮循环，结果状态图如下所示</p><p><img src="https://yerias.github.io/java_img/java_4.png" alt="多线程rehash"></p><p>此时循环链表形成，并且key(11)无法加入到线程1的新数组。在下一次访问该链表时会出现死循环。</p><h3 id="resize"><a href="#resize" class="headerlink" title="resize()"></a>resize()</h3><p>初始化容量16，负载因子0.75，尾插法，扩容2倍，8个节点转树，6个节点转链表，不会产生死循环</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> Node&lt;K,V&gt;[] resize() &#123;</span><br><span class="line"><span class="comment">// 理解为node/hashmap</span></span><br><span class="line">    Node&lt;K,V&gt;[] oldTab = table;</span><br><span class="line"><span class="comment">// 拿到旧的hashmapnode的长度</span></span><br><span class="line">    <span class="keyword">int</span> oldCap = (oldTab == <span class="keyword">null</span>) ? <span class="number">0</span> : oldTab.length;</span><br><span class="line"><span class="comment">// 旧hashmap的扩容阈值</span></span><br><span class="line">    <span class="keyword">int</span> oldThr = threshold;</span><br><span class="line"><span class="comment">// 新hashmap的容量和阈值初始化为0</span></span><br><span class="line">    <span class="keyword">int</span> newCap, newThr = <span class="number">0</span>;</span><br><span class="line"><span class="comment">// 拿到新hashmap的容量</span></span><br><span class="line">    <span class="keyword">if</span> (oldCap &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">// 旧的hashmap容量大于最大值，则直接返回</span></span><br><span class="line"><span class="keyword">if</span> (oldCap &gt;= MAXIMUM_CAPACITY) &#123;</span><br><span class="line">            threshold = Integer.MAX_VALUE;</span><br><span class="line">            <span class="keyword">return</span> oldTab;</span><br><span class="line">        &#125;</span><br><span class="line"><span class="comment">// 旧的hashmap扩容为原来的两倍</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> ((newCap = oldCap &lt;&lt; <span class="number">1</span>) &lt; MAXIMUM_CAPACITY &amp;&amp;</span><br><span class="line">                 oldCap &gt;= DEFAULT_INITIAL_CAPACITY)</span><br><span class="line">            newThr = oldThr &lt;&lt; <span class="number">1</span>; </span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// 旧的hashmap长度等于0，但是阈值大于0，则把阈值赋值为hashmap的长度</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (oldThr &gt; <span class="number">0</span>) </span><br><span class="line">        newCap = oldThr;</span><br><span class="line"><span class="comment">// 如果旧的hashmap的长度和阈值都为0，则赋初始值</span></span><br><span class="line">    <span class="keyword">else</span> &#123;               </span><br><span class="line">        newCap = DEFAULT_INITIAL_CAPACITY;</span><br><span class="line">        newThr = (<span class="keyword">int</span>)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// 拿到新阈值的值 =&gt; 新容量*0.75</span></span><br><span class="line">    <span class="keyword">if</span> (newThr == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">float</span> ft = (<span class="keyword">float</span>)newCap * loadFactor;</span><br><span class="line">        newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (<span class="keyword">float</span>)MAXIMUM_CAPACITY ?</span><br><span class="line">                  (<span class="keyword">int</span>)ft : Integer.MAX_VALUE);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// 赋值给threshold</span></span><br><span class="line">    threshold = newThr;</span><br><span class="line">    <span class="meta">@SuppressWarnings</span>(&#123;<span class="string">"rawtypes"</span>,<span class="string">"unchecked"</span>&#125;)</span><br><span class="line"><span class="comment">// 创建一个新的容量的Node</span></span><br><span class="line">    Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])<span class="keyword">new</span> Node[newCap];</span><br><span class="line"><span class="comment">// 赋值给table</span></span><br><span class="line">    table = newTab;</span><br><span class="line"><span class="comment">// 尾插法，将旧的table的数据查询出来再插入新的table</span></span><br><span class="line">    <span class="keyword">if</span> (oldTab != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; oldCap; ++j) &#123;</span><br><span class="line">            Node&lt;K,V&gt; e;</span><br><span class="line">            <span class="keyword">if</span> ((e = oldTab[j]) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                oldTab[j] = <span class="keyword">null</span>;</span><br><span class="line">                <span class="keyword">if</span> (e.next == <span class="keyword">null</span>)</span><br><span class="line">                    newTab[e.hash &amp; (newCap - <span class="number">1</span>)] = e;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (e <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line">                    ((TreeNode&lt;K,V&gt;)e).split(<span class="keyword">this</span>, newTab, j, oldCap);</span><br><span class="line">                <span class="keyword">else</span> &#123; <span class="comment">// preserve order</span></span><br><span class="line">                    Node&lt;K,V&gt; loHead = <span class="keyword">null</span>, loTail = <span class="keyword">null</span>;</span><br><span class="line">                    Node&lt;K,V&gt; hiHead = <span class="keyword">null</span>, hiTail = <span class="keyword">null</span>;</span><br><span class="line">                    Node&lt;K,V&gt; next;</span><br><span class="line">                    <span class="keyword">do</span> &#123;</span><br><span class="line">                        next = e.next;</span><br><span class="line">                        <span class="keyword">if</span> ((e.hash &amp; oldCap) == <span class="number">0</span>) &#123;</span><br><span class="line">                            <span class="keyword">if</span> (loTail == <span class="keyword">null</span>)</span><br><span class="line">                                loHead = e;</span><br><span class="line">                            <span class="keyword">else</span></span><br><span class="line">                                loTail.next = e;</span><br><span class="line">                            loTail = e;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">else</span> &#123;</span><br><span class="line">                            <span class="keyword">if</span> (hiTail == <span class="keyword">null</span>)</span><br><span class="line">                                hiHead = e;</span><br><span class="line">                            <span class="keyword">else</span></span><br><span class="line">                                hiTail.next = e;</span><br><span class="line">                            hiTail = e;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125; <span class="keyword">while</span> ((e = next) != <span class="keyword">null</span>);</span><br><span class="line">                    <span class="keyword">if</span> (loTail != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        loTail.next = <span class="keyword">null</span>;</span><br><span class="line">                        newTab[j] = loHead;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span> (hiTail != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        hiTail.next = <span class="keyword">null</span>;</span><br><span class="line">                        newTab[j + oldCap] = hiHead;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// 返回新的Node</span></span><br><span class="line">    <span class="keyword">return</span> newTab;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="hash"><a href="#hash" class="headerlink" title="hash()"></a>hash()</h3><p>使用时异或求hash值，比取余更快更均匀</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">hash</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> h;</span><br><span class="line">    <span class="comment">// '^' 异或操作，相同则为0，不同则为1</span></span><br><span class="line">    <span class="keyword">return</span> (key == <span class="keyword">null</span>) ? <span class="number">0</span> : (h = key.hashCode()) ^ (h &gt;&gt;&gt; <span class="number">16</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="put"><a href="#put" class="headerlink" title="put()"></a>put()</h3><p>如果插入的位置为空则直接插入，如果有值但是key的hash或者内容相等，则覆盖，如果能转树则转树。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">put</span><span class="params">(K key, V value)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 调用putVal方法</span></span><br><span class="line">    <span class="keyword">return</span> putVal(hash(key), key, value, <span class="keyword">false</span>, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">final</span> V <span class="title">putVal</span><span class="params">(<span class="keyword">int</span> hash, K key, V value, <span class="keyword">boolean</span> onlyIfAbsent,</span></span></span><br><span class="line"><span class="function"><span class="params">               <span class="keyword">boolean</span> evict)</span> </span>&#123;</span><br><span class="line">    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; <span class="keyword">int</span> n, i;</span><br><span class="line">    <span class="comment">// 如果当前数组table为null，进行resize()初始化，n = resize的长度</span></span><br><span class="line">    <span class="keyword">if</span> ((tab = table) == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span>)</span><br><span class="line">        n = (tab = resize()).length;</span><br><span class="line">    <span class="comment">// 如果table[i]为空，那就把这个键值对放在table[i]</span></span><br><span class="line">    <span class="keyword">if</span> ((p = tab[i = (n - <span class="number">1</span>) &amp; hash]) == <span class="keyword">null</span>)</span><br><span class="line">        tab[i] = newNode(hash, key, value, <span class="keyword">null</span>);</span><br><span class="line">    <span class="comment">// 当另一个key的hash值已经存在时</span></span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        Node&lt;K,V&gt; e; K k;</span><br><span class="line">        <span class="comment">// 如果节点的key的hash值和容量都相同，则覆盖</span></span><br><span class="line">        <span class="keyword">if</span> (p.hash == hash &amp;&amp;</span><br><span class="line">            ((k = p.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line">            e = p;</span><br><span class="line">        <span class="comment">// Node转成树</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (p <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line">            e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(<span class="keyword">this</span>, tab, hash, key, value);</span><br><span class="line">        <span class="comment">// 遍历table[i]所对应的链表，直到最后一个节点的next为null或者有重复的key值</span></span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> binCount = <span class="number">0</span>; ; ++binCount) &#123;</span><br><span class="line">                <span class="keyword">if</span> ((e = p.next) == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    p.next = newNode(hash, key, value, <span class="keyword">null</span>);</span><br><span class="line">                    <span class="keyword">if</span> (binCount &gt;= TREEIFY_THRESHOLD - <span class="number">1</span>)</span><br><span class="line">                        treeifyBin(tab, hash);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (e.hash == hash &amp;&amp;</span><br><span class="line">                    ((k = e.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                p = e;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// key重复，替换value</span></span><br><span class="line">        <span class="keyword">if</span> (e != <span class="keyword">null</span>) &#123;</span><br><span class="line">            V oldValue = e.value;</span><br><span class="line">            <span class="keyword">if</span> (!onlyIfAbsent || oldValue == <span class="keyword">null</span>)</span><br><span class="line">                e.value = value;</span><br><span class="line">            afterNodeAccess(e);</span><br><span class="line">            <span class="keyword">return</span> oldValue;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    ++modCount;</span><br><span class="line">    <span class="comment">// 触发扩容</span></span><br><span class="line">    <span class="keyword">if</span> (++size &gt; threshold)</span><br><span class="line">        resize();</span><br><span class="line">    afterNodeInsertion(evict);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="get"><a href="#get" class="headerlink" title="get()"></a>get()</h3><p>首先通过<code>hash</code>函数找到索引，然后判断map为null，再判断table[i]是否等于key，然后在找与table相连的链表的key是否相等。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">get</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">Node&lt;K,V&gt; e;</span><br><span class="line"><span class="comment">// 如果拿出来的不为null，就返回value</span></span><br><span class="line"><span class="keyword">return</span> (e = getNode(hash(key), key)) == <span class="keyword">null</span> ? <span class="keyword">null</span> : e.value;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">final</span> Node&lt;K,V&gt; <span class="title">getNode</span><span class="params">(<span class="keyword">int</span> hash, Object key)</span> </span>&#123;</span><br><span class="line">Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; <span class="keyword">int</span> n; K k;</span><br><span class="line"><span class="keyword">if</span> ((tab = table) != <span class="keyword">null</span> &amp;&amp; (n = tab.length) &gt; <span class="number">0</span> &amp;&amp;</span><br><span class="line">(first = tab[(n - <span class="number">1</span>) &amp; hash]) != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="comment">// 如果是第一个元素就返回</span></span><br><span class="line"><span class="keyword">if</span> (first.hash == hash &amp;&amp; <span class="comment">// always check first node</span></span><br><span class="line">((k = first.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line"><span class="keyword">return</span> first;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ((e = first.next) != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="comment">//// 从树中拿</span></span><br><span class="line"><span class="keyword">if</span> (first <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line"><span class="keyword">return</span> ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key);</span><br><span class="line"><span class="keyword">do</span> &#123;</span><br><span class="line"><span class="comment">// 循环链表，key的hash值或者key的内容相同，则返回</span></span><br><span class="line"><span class="keyword">if</span> (e.hash == hash &amp;&amp;</span><br><span class="line">((k = e.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line"><span class="keyword">return</span> e;</span><br><span class="line">&#125; <span class="keyword">while</span> ((e = e.next) != <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 没找到 返回null</span></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>面试题：如果new HashMap(19)，bucket数组多大？</p><ul><li>HashMap的bucket 数组大小一定是2的幂，如果new的时候指定了容量且不是2的幂，实际容量会是最接近(大于)指定容量的2的幂，比如 new HashMap&lt;&gt;(19)，比19大且最接近的2的幂是32，实际容量就是32。</li></ul><p>基础知识</p><p><img src="https://yerias.github.io/java_img/%E4%BD%8D%E8%BF%90%E7%AE%97.png" alt="位运算"></p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HashMap多线程问题</title>
      <link href="/2019/01/04/java/4/"/>
      <url>/2019/01/04/java/4/</url>
      
        <content type="html"><![CDATA[<hr><p>该篇博客不适合小白，只做针对性的api源码解析，以及适合我自身的案例研究</p><h2 id="jdk1-7中的HashMap"><a href="#jdk1-7中的HashMap" class="headerlink" title="jdk1.7中的HashMap"></a>jdk1.7中的HashMap</h2><p>在jdk1.8中对HashMap做了很多优化，这里先分析在jdk1.7中的问题，相信大家都知道在jdk1.7多线程环境下HashMap容易出现死循环，这里我们先用代码来模拟出现死循环的情况：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="number">1</span> <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashMapTest</span> </span>&#123;</span><br><span class="line"> <span class="number">2</span></span><br><span class="line"> <span class="number">3</span>     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"> <span class="number">4</span>         HashMapThread thread0 = <span class="keyword">new</span> HashMapThread();</span><br><span class="line"> <span class="number">5</span>         HashMapThread thread1 = <span class="keyword">new</span> HashMapThread();</span><br><span class="line"> <span class="number">6</span>         HashMapThread thread2 = <span class="keyword">new</span> HashMapThread();</span><br><span class="line"> <span class="number">7</span>         HashMapThread thread3 = <span class="keyword">new</span> HashMapThread();</span><br><span class="line"> <span class="number">8</span>         HashMapThread thread4 = <span class="keyword">new</span> HashMapThread();</span><br><span class="line"> <span class="number">9</span>         thread0.start();</span><br><span class="line"><span class="number">10</span>         thread1.start();</span><br><span class="line"><span class="number">11</span>         thread2.start();</span><br><span class="line"><span class="number">12</span>         thread3.start();</span><br><span class="line"><span class="number">13</span>         thread4.start();</span><br><span class="line"><span class="number">14</span>     &#125;</span><br><span class="line"><span class="number">15</span> &#125;</span><br><span class="line"><span class="number">16</span></span><br><span class="line"><span class="number">17</span> <span class="class"><span class="keyword">class</span> <span class="title">HashMapThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line"><span class="number">18</span>     <span class="keyword">private</span> <span class="keyword">static</span> AtomicInteger ai = <span class="keyword">new</span> AtomicInteger();</span><br><span class="line"><span class="number">19</span>     <span class="keyword">private</span> <span class="keyword">static</span> Map&lt;Integer, Integer&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="number">21</span>     <span class="meta">@Override</span></span><br><span class="line"><span class="number">22</span>     <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="number">23</span>         <span class="keyword">while</span> (ai.get() &lt; <span class="number">1000000</span>) &#123;</span><br><span class="line"><span class="number">24</span>             map.put(ai.get(), ai.get());</span><br><span class="line"><span class="number">25</span>             ai.incrementAndGet();</span><br><span class="line"><span class="number">26</span>         &#125;</span><br><span class="line"><span class="number">27</span>     &#125;</span><br><span class="line"><span class="number">28</span> &#125;</span><br></pre></td></tr></table></figure><p>上述代码比较简单，就是开多个线程不断进行put操作，并且HashMap与AtomicInteger都是全局共享的。在多运行几次该代码后，出现如下死循环情形：</p><p><img src="https://yerias.github.io/java_img/hashmap_1.jpg" alt=""></p><p>其中有几次还会出现数组越界的情况：</p><p><img src="https://yerias.github.io/java_img/hashmap_2.jpg" alt=""></p><p>这里我们着重分析为什么会出现死循环的情况，通过jps和jstack命名查看死循环情况，结果如下：</p><p><img src="https://yerias.github.io/java_img/hashmap_3.jpg" alt=""></p><p>从堆栈信息中可以看到出现死循环的位置，通过该信息可明确知道死循环发生在HashMap的扩容函数中，根源在<strong>transfer函数</strong>中，jdk1.7中HashMap的transfer函数如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="number">1</span>    <span class="function"><span class="keyword">void</span> <span class="title">transfer</span><span class="params">(Entry[] newTable, <span class="keyword">boolean</span> rehash)</span> </span>&#123;</span><br><span class="line"> <span class="number">2</span>         <span class="keyword">int</span> newCapacity = newTable.length;</span><br><span class="line"> <span class="number">3</span>         <span class="keyword">for</span> (Entry&lt;K,V&gt; e : table) &#123;</span><br><span class="line"> <span class="number">4</span>             <span class="keyword">while</span>(<span class="keyword">null</span> != e) &#123;</span><br><span class="line"> <span class="number">5</span>                 Entry&lt;K,V&gt; next = e.next;</span><br><span class="line"> <span class="number">6</span>                 <span class="keyword">if</span> (rehash) &#123;</span><br><span class="line"> <span class="number">7</span>                     e.hash = <span class="keyword">null</span> == e.key ? <span class="number">0</span> : hash(e.key);</span><br><span class="line"> <span class="number">8</span>                 &#125;</span><br><span class="line"> <span class="number">9</span>                 <span class="keyword">int</span> i = indexFor(e.hash, newCapacity);</span><br><span class="line"><span class="number">10</span>                 e.next = newTable[i];</span><br><span class="line"><span class="number">11</span>                 newTable[i] = e;</span><br><span class="line"><span class="number">12</span>                 e = next;</span><br><span class="line"><span class="number">13</span>             &#125;</span><br><span class="line"><span class="number">14</span>         &#125;</span><br><span class="line"><span class="number">15</span>     &#125;</span><br></pre></td></tr></table></figure><p>总结下该函数的主要作用：</p><p>在对table进行扩容到newTable后，需要将原来数据转移到newTable中，注意10-12行代码，这里可以看出在转移元素的过程中，使用的是头插法，也就是<strong>链表的顺序会翻转</strong>，这里也是形成死循环的关键点。下面进行详细分析。</p><h2 id="扩容造成死循环分析过程"><a href="#扩容造成死循环分析过程" class="headerlink" title="扩容造成死循环分析过程"></a>扩容造成死循环分析过程</h2><p>这里假设</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">#1.hash算法为简单的用key mod链表的大小。</span><br><span class="line"></span><br><span class="line">#2.最开始hash表size=2，key=3,7,5，则都在table[1]中。</span><br><span class="line"></span><br><span class="line">#3.然后进行resize，使size变成4。</span><br></pre></td></tr></table></figure><p>未resize前的数据结构如下：</p><p><img src="https://yerias.github.io/java_img/hashmap_4.jpg" alt=""></p><p>如果在单线程环境下，最后的结果如下：</p><p><img src="https://yerias.github.io/java_img/hashmap_5.jpg" alt=""></p><p>这里的转移过程，不再进行详述，只要理解transfer函数在做什么，其转移过程以及如何对链表进行反转应该不难。</p><p>然后在多线程环境下，假设有两个线程A和B都在进行put操作。线程A在执行到transfer函数中第11行代码处挂起，因为该函数在这里分析的地位非常重要，因此再次贴出来。</p><p><img src="https://yerias.github.io/java_img/hashmap_6.jpg" alt=""></p><p>此时线程A中运行结果如下：</p><p><img src="https://yerias.github.io/java_img/hashmap_7.jpg" alt=""></p><p>线程A挂起后，此时线程B正常执行，并完成resize操作，结果如下：</p><p><img src="https://yerias.github.io/java_img/hashmap_8.jpg" alt=""></p><p><strong>这里需要特别注意的点：由于线程B已经执行完毕，根据Java内存模型，现在newTable和table中的Entry都是主存中最新值：7.next=3，3.next=null。</strong></p><p>此时切换到线程A上，在线程A挂起时内存中值如下：e=3，next=7，newTable[3]=null，代码执行过程如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">newTable[<span class="number">3</span>]=e ----&gt; newTable[<span class="number">3</span>]=<span class="number">3</span></span><br><span class="line">e=next ----&gt; e=<span class="number">7</span></span><br></pre></td></tr></table></figure><p>此时结果如下：</p><p><img src="https://yerias.github.io/java_img/hashmap_9.jpg" alt=""></p><p>继续循环：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">e=<span class="number">7</span></span><br><span class="line">next=e.next ----&gt; next=<span class="number">3</span>【从主存中取值】</span><br><span class="line">e.next=newTable[<span class="number">3</span>] ----&gt; e.next=<span class="number">3</span>【从主存中取值】</span><br><span class="line">newTable[<span class="number">3</span>]=e ----&gt; newTable[<span class="number">3</span>]=<span class="number">7</span></span><br><span class="line">e=next ----&gt; e=<span class="number">3</span></span><br></pre></td></tr></table></figure><p>结果如下：</p><p><img src="https://yerias.github.io/java_img/hashmap_10.jpg" alt=""></p><p>再次进行循环：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">e=<span class="number">3</span></span><br><span class="line">next=e.next ----&gt; next=<span class="keyword">null</span></span><br><span class="line">e.next=newTable[<span class="number">3</span>] ----&gt; e.next=<span class="number">7</span> 即：<span class="number">3</span>.next=<span class="number">7</span></span><br><span class="line">newTable[<span class="number">3</span>]=e ----&gt; newTable[<span class="number">3</span>]=<span class="number">3</span></span><br><span class="line">e=next ----&gt; e=<span class="keyword">null</span></span><br></pre></td></tr></table></figure><p>注意此次循环：e.next=7，而在上次循环中7.next=3，出现环形链表，并且此时e=null循环结束。</p><p>结果如下：</p><p><img src="https://yerias.github.io/java_img/hashmap_11.jpg" alt=""></p><p>在后续操作中只要涉及轮询hashmap的数据结构，就会在这里发生死循环，造成悲剧。</p><h2 id="扩容造成数据丢失分析过程"><a href="#扩容造成数据丢失分析过程" class="headerlink" title="扩容造成数据丢失分析过程"></a>扩容造成数据丢失分析过程</h2><p>遵照上述分析过程，初始时：</p><p><img src="https://yerias.github.io/java_img/hashmap_12.jpg" alt=""></p><p>线程A和线程B进行put操作，同样线程A挂起：</p><p><img src="https://yerias.github.io/java_img/hashmap_13.jpg" alt=""></p><p>此时线程A的运行结果如下：</p><p><img src="https://yerias.github.io/java_img/hashmap_14.jpg" alt=""></p><p>此时线程B已获得CPU时间片，并完成resize操作：</p><p><img src="https://yerias.github.io/java_img/hashmap_15.jpg" alt=""></p><p>同样注意由于线程B执行完成，newTable和table都为最新值：<strong>5.next=null</strong>。</p><p>此时切换到线程A，在线程A挂起时：<strong>e=7，next=5，newTable[3]=null。</strong></p><p>执行newtable[i]=e，就将<strong>7放在了table[3]</strong>的位置，此时next=5。接着进行下一次循环：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">e=<span class="number">5</span></span><br><span class="line">next=e.next ----&gt; next=<span class="keyword">null</span>，从主存中取值</span><br><span class="line">e.next=newTable[<span class="number">1</span>] ----&gt; e.next=<span class="number">5</span>，从主存中取值</span><br><span class="line">newTable[<span class="number">1</span>]=e ----&gt; newTable[<span class="number">1</span>]=<span class="number">5</span></span><br><span class="line">e=next ----&gt; e=<span class="keyword">null</span></span><br></pre></td></tr></table></figure><p>将5放置在table[1]位置，此时e=null循环结束，<strong>3元素丢失</strong>，并形成<strong>环形链表</strong>。并在后续操作hashmap时造成死循环。</p><p><img src="https://yerias.github.io/java_img/hashmap_16.jpg" alt=""></p><h2 id="jdk1-8中HashMap"><a href="#jdk1-8中HashMap" class="headerlink" title="jdk1.8中HashMap"></a>jdk1.8中HashMap</h2><p>在jdk1.8中对HashMap进行了优化，在发生hash碰撞，不再采用头插法方式，而是直接插入链表尾部，因此不会出现环形链表的情况，但是在多线程的情况下仍然不安全，这里我们看jdk1.8中HashMap的put操作源码：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span>  <span class="function"><span class="keyword">final</span> V <span class="title">putVal</span><span class="params">(<span class="keyword">int</span> hash, K key, V value, <span class="keyword">boolean</span> onlyIfAbsent,</span></span></span><br><span class="line"><span class="function"><span class="params"> <span class="number">2</span>                    <span class="keyword">boolean</span> evict)</span> </span>&#123;</span><br><span class="line"> <span class="number">3</span>         Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; <span class="keyword">int</span> n, i;</span><br><span class="line"> <span class="number">4</span>         <span class="keyword">if</span> ((tab = table) == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span>)</span><br><span class="line"> <span class="number">5</span>             n = (tab = resize()).length;</span><br><span class="line"> <span class="number">6</span>         <span class="keyword">if</span> ((p = tab[i = (n - <span class="number">1</span>) &amp; hash]) == <span class="keyword">null</span>) <span class="comment">// 如果没有hash碰撞则直接插入元素</span></span><br><span class="line"> <span class="number">7</span>             tab[i] = newNode(hash, key, value, <span class="keyword">null</span>);</span><br><span class="line"> <span class="number">8</span>         <span class="keyword">else</span> &#123;</span><br><span class="line"> <span class="number">9</span>             Node&lt;K,V&gt; e; K k;</span><br><span class="line"><span class="number">10</span>             <span class="keyword">if</span> (p.hash == hash &amp;&amp;</span><br><span class="line"><span class="number">11</span>                 ((k = p.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line"><span class="number">12</span>                 e = p;</span><br><span class="line"><span class="number">13</span>             <span class="keyword">else</span> <span class="keyword">if</span> (p <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line"><span class="number">14</span>                 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(<span class="keyword">this</span>, tab, hash, key, value);</span><br><span class="line"><span class="number">15</span>             <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="number">16</span>                 <span class="keyword">for</span> (<span class="keyword">int</span> binCount = <span class="number">0</span>; ; ++binCount) &#123;</span><br><span class="line"><span class="number">17</span>                     <span class="keyword">if</span> ((e = p.next) == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="number">18</span>                         p.next = newNode(hash, key, value, <span class="keyword">null</span>);</span><br><span class="line"><span class="number">19</span>                         <span class="keyword">if</span> (binCount &gt;= TREEIFY_THRESHOLD - <span class="number">1</span>) <span class="comment">// -1 for 1st</span></span><br><span class="line"><span class="number">20</span>                             treeifyBin(tab, hash);</span><br><span class="line"><span class="number">21</span>                         <span class="keyword">break</span>;</span><br><span class="line"><span class="number">22</span>                     &#125;</span><br><span class="line"><span class="number">23</span>                     <span class="keyword">if</span> (e.hash == hash &amp;&amp;</span><br><span class="line"><span class="number">24</span>                         ((k = e.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line"><span class="number">25</span>                         <span class="keyword">break</span>;</span><br><span class="line"><span class="number">26</span>                     p = e;</span><br><span class="line"><span class="number">27</span>                 &#125;</span><br><span class="line"><span class="number">28</span>             &#125;</span><br><span class="line"><span class="number">29</span>             <span class="keyword">if</span> (e != <span class="keyword">null</span>) &#123; <span class="comment">// existing mapping for key</span></span><br><span class="line"><span class="number">30</span>                 V oldValue = e.value;</span><br><span class="line"><span class="number">31</span>                 <span class="keyword">if</span> (!onlyIfAbsent || oldValue == <span class="keyword">null</span>)</span><br><span class="line"><span class="number">32</span>                     e.value = value;</span><br><span class="line"><span class="number">33</span>                 afterNodeAccess(e);</span><br><span class="line"><span class="number">34</span>                 <span class="keyword">return</span> oldValue;</span><br><span class="line"><span class="number">35</span>             &#125;</span><br><span class="line"><span class="number">36</span>         &#125;</span><br><span class="line"><span class="number">37</span>         ++modCount;</span><br><span class="line"><span class="number">38</span>         <span class="keyword">if</span> (++size &gt; threshold)</span><br><span class="line"><span class="number">39</span>             resize();</span><br><span class="line"><span class="number">40</span>         afterNodeInsertion(evict);</span><br><span class="line"><span class="number">41</span>         <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line"><span class="number">42</span>     &#125;</span><br></pre></td></tr></table></figure><p>这是jdk1.8中HashMap中put操作的主函数， 注意第6行代码，如果没有hash碰撞则会直接插入元素。如果线程A和线程B同时进行put操作，刚好这两条不同的数据hash值一样，并且该位置数据为null，所以这线程A、B都会进入第6行代码中。假设一种情况，线程A进入后还未进行数据插入时挂起，而线程B正常执行，从而正常插入数据，然后线程A获取CPU时间片，此时线程A不用再进行hash判断了，问题出现：线程A会把线程B插入的数据给<strong>覆盖</strong>，发生线程不安全。</p><p>这里只是简要分析下jdk1.8中HashMap出现的线程不安全问题的体现，后续将会对java的集合框架进行总结，到时再进行具体分析。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>首先HashMap是不安全的，其主要体现:</p><ol><li>在jdk1.7中，多线程环境下，扩容时会造成环形链表或丢失数据</li><li>在jdk18中，多线程环境下，会发生数据覆盖的情况</li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA.UTIL包下的LINKEDLIST和迭代器快速失败的源码解析以及多线程下故障现象、导致原因、以及解决办法和优化建议</title>
      <link href="/2019/01/03/java/3/"/>
      <url>/2019/01/03/java/3/</url>
      
        <content type="html"><![CDATA[<hr><p>该篇博客不适合小白，只做针对性的api源码解析，以及适合我自身的案例研究</p><hr><ol><li><p>使用多个线程往<code>LinkedList</code>中添加元素</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">50</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">        list.add(UUID.randomUUID().toString().substring(<span class="number">0</span>,<span class="number">8</span>));</span><br><span class="line">        System.out.println(list);</span><br><span class="line">    &#125;,String.valueOf(i)).start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>故障现象</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Exception in thread <span class="string">"3"</span> java.util.ConcurrentModificationException</span><br></pre></td></tr></table></figure><p>我们知道<code>LinkedList</code>是线程不安全的，当多线程操作时，线程操作迭代器的同时其他线程改变了元素的值就会产生<code>ConcurrentModificationException</code>异常，<code>ConcurrentModificationException</code>是在操作<code>Iterator</code>时抛出的异常</p></li><li><p>故障原因</p><p>从前一篇的<code>ArrayList</code>中的解析中得出，我们最终都会经过迭代器中的代码检查<code>modCount</code> 和 <code>expectedModCount</code>的值相不相等</p><p>所以我们这里就不追踪栈异常信息，而直接看看源码中的操作是如何让<code>modCount</code> 和<code>expectedModCount</code>不相等的</p><ol><li><p>首先找出问题的关键代码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">      <span class="keyword">private</span> Node&lt;E&gt; lastReturned;<span class="comment">//最近一次返回的节点，也是当前持有的节点</span></span><br><span class="line">      <span class="keyword">private</span> Node&lt;E&gt; next;<span class="comment">//对下一个元素的引用</span></span><br><span class="line">      <span class="keyword">private</span> <span class="keyword">int</span> nextIndex;<span class="comment">//下一个节点的索引</span></span><br><span class="line">      <span class="keyword">private</span> <span class="keyword">int</span> expectedModCount = modCount;</span><br><span class="line">      </span><br><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">          checkForComodification();<span class="comment">//每次添加元素前，都检查一次modCount和expectedModCount是否相等，如果不相等就直接返回ConcurrentModificationException异常，产生快速失败,多线程环境下这里通不过</span></span><br><span class="line">          <span class="keyword">if</span> (!hasNext())</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> NoSuchElementException();</span><br><span class="line">      </span><br><span class="line">          lastReturned = next;<span class="comment">//当前节点--&gt;下一个节点</span></span><br><span class="line">          next = next.next;<span class="comment">//下下个节点的指正往前移一个节点</span></span><br><span class="line">          nextIndex++;<span class="comment">//index++</span></span><br><span class="line">          <span class="keyword">return</span> lastReturned.item; <span class="comment">//返回移动后的节点的数据</span></span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">checkForComodification</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (modCount != expectedModCount)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>在解析如何修改<code>modCount</code> 值之前我们应该弄明白，<code>LinkedList</code>中的<code>Node</code>节点是如何实现的</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="comment">//私有节点类Node，在1.8版本之前叫做Entry</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span>&lt;<span class="title">E</span>&gt; </span>&#123;</span><br><span class="line">       E item;<span class="comment">//存储的元素</span></span><br><span class="line">       Node&lt;E&gt; next;<span class="comment">//后继结点</span></span><br><span class="line">       Node&lt;E&gt; prev;<span class="comment">//前驱结点</span></span><br><span class="line">      </span><br><span class="line">       <span class="comment">// 前驱结点、存储的元素和后继结点作为参数的构造方法</span></span><br><span class="line">       Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123;</span><br><span class="line">           <span class="keyword">this</span>.item = element;</span><br><span class="line">           <span class="keyword">this</span>.next = next;</span><br><span class="line">           <span class="keyword">this</span>.prev = prev;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></li><li><p>源码中是如何修改<code>modCount</code> 的值的(不包括作为队列和双端队列的方法)(开始怼源码)</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">transient</span> <span class="keyword">int</span> size = <span class="number">0</span>;  <span class="comment">//元素数量</span></span><br><span class="line"><span class="keyword">transient</span> Node&lt;E&gt; first;  <span class="comment">//首节点引用，为null</span></span><br><span class="line"><span class="keyword">transient</span> Node&lt;E&gt; last;  <span class="comment">//尾结点引用，为null</span></span><br><span class="line">      </span><br><span class="line"><span class="comment">//追加一个元素到列表的末尾</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">add</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//把元素存放到链表的末尾</span></span><br><span class="line">       linkLast(e);</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//尾部添加元素</span></span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">linkLast</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//获取当前尾节点的引用</span></span><br><span class="line">       <span class="keyword">final</span> Node&lt;E&gt; l = last;</span><br><span class="line">       <span class="comment">//构建一个新节点，prev的值为l(尾节点)、节点数据为e、next的值为null</span></span><br><span class="line">       <span class="keyword">final</span> Node&lt;E&gt; newNode = <span class="keyword">new</span> Node&lt;&gt;(l, e, <span class="keyword">null</span>);</span><br><span class="line">       <span class="comment">//新节点作为尾结点</span></span><br><span class="line">       last = newNode;</span><br><span class="line">       <span class="comment">//如果原尾节点为null</span></span><br><span class="line">       <span class="keyword">if</span> (l == <span class="keyword">null</span>)</span><br><span class="line">           <span class="comment">//即原链表为null，链表的首节点也是newNode</span></span><br><span class="line">           first = newNode;</span><br><span class="line">       <span class="keyword">else</span></span><br><span class="line">           <span class="comment">//否则，原节点的next设置为newNode</span></span><br><span class="line">           l.next = newNode;</span><br><span class="line">       <span class="comment">//数量+1</span></span><br><span class="line">       size++;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//将指定元素插入到列表中的指定位置。</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> index, E element)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查index是否在有效范围内</span></span><br><span class="line">       checkPositionIndex(index);</span><br><span class="line"><span class="comment">//如果index==size，说明添加的位置是末尾</span></span><br><span class="line">       <span class="keyword">if</span> (index == size)</span><br><span class="line">           <span class="comment">//在末尾添加元素</span></span><br><span class="line">           linkLast(element);</span><br><span class="line">       <span class="keyword">else</span></span><br><span class="line">           <span class="comment">//把element元素插入到index指定的节点位置</span></span><br><span class="line">           linkBefore(element, node(index));</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//检查index是否在有效范围内</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">checkPositionIndex</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (!isPositionIndex(index))</span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> IndexOutOfBoundsException(outOfBoundsMsg(index));</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">isPositionIndex</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">return</span> index &gt;= <span class="number">0</span> &amp;&amp; index &lt;= size;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//返回指定元素索引处的(非空)节点。(注意这里有技巧)</span></span><br><span class="line">   <span class="function">Node&lt;E&gt; <span class="title">node</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//如果index小于size的1/2长度</span></span><br><span class="line">       <span class="keyword">if</span> (index &lt; (size &gt;&gt; <span class="number">1</span>)) &#123;</span><br><span class="line">           <span class="comment">//获取头结点的引用</span></span><br><span class="line">           Node&lt;E&gt; x = first;</span><br><span class="line">           <span class="comment">//从前到后遍历index个节点，最后返回</span></span><br><span class="line">           <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; index; i++)</span><br><span class="line">               x = x.next;</span><br><span class="line">           <span class="keyword">return</span> x;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="comment">//否则获取最后一个节点的引用</span></span><br><span class="line">           Node&lt;E&gt; x = last;</span><br><span class="line">           <span class="comment">//从后到前遍历index个节点，最后返回</span></span><br><span class="line">           <span class="keyword">for</span> (<span class="keyword">int</span> i = size - <span class="number">1</span>; i &gt; index; i--)</span><br><span class="line">               x = x.prev;</span><br><span class="line">           <span class="keyword">return</span> x;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//在非空节点succ之前插入元素e。</span></span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">linkBefore</span><span class="params">(E e, Node&lt;E&gt; succ)</span> </span>&#123;</span><br><span class="line"><span class="comment">//获取succ节点(index)的前一个节点的引用</span></span><br><span class="line">       <span class="keyword">final</span> Node&lt;E&gt; pred = succ.prev;</span><br><span class="line">       <span class="comment">//创建一个新的节点，维护的新节点的前一个节点是pred、元素本身、和元素的下一个节点succ</span></span><br><span class="line">       <span class="keyword">final</span> Node&lt;E&gt; newNode = <span class="keyword">new</span> Node&lt;&gt;(pred, e, succ);</span><br><span class="line">       <span class="comment">//succ节点的上一个节点指向newNode</span></span><br><span class="line">       succ.prev = newNode;</span><br><span class="line">       <span class="keyword">if</span> (pred == <span class="keyword">null</span>)</span><br><span class="line">           <span class="comment">//如果pred节点为null，则说明没有值，则newNode就是第一个节点</span></span><br><span class="line">           first = newNode;</span><br><span class="line">       <span class="keyword">else</span></span><br><span class="line">           <span class="comment">//否则pred的下一个节点指向newNode</span></span><br><span class="line">           pred.next = newNode;</span><br><span class="line">       <span class="comment">//数量+1</span></span><br><span class="line">       size++;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//将指定集合中的所有元素追加到这个列表</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">addAll</span><span class="params">(Collection&lt;? extends E&gt; c)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//调用了插入集合元素的方法，指定了index=size</span></span><br><span class="line">       <span class="keyword">return</span> addAll(size, c);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//将指定集合中的所有元素插入其中列表</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">addAll</span><span class="params">(<span class="keyword">int</span> index, Collection&lt;? extends E&gt; c)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查index是否有效</span></span><br><span class="line">       checkPositionIndex(index);</span><br><span class="line"><span class="comment">//先把集合转换成数组，这也就限定了集合必须是List下的实现类</span></span><br><span class="line">       Object[] a = c.toArray();</span><br><span class="line">       <span class="comment">//获取集合的长度</span></span><br><span class="line">       <span class="keyword">int</span> numNew = a.length;</span><br><span class="line">       <span class="comment">//如果集合的长度为0，即为空</span></span><br><span class="line">       <span class="keyword">if</span> (numNew == <span class="number">0</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">       </span><br><span class="line"><span class="comment">//定义两个节点，succ指向当前需要插入节点的位置，pred指向其前一个节点</span></span><br><span class="line">    Node&lt;E&gt; pred, succ;</span><br><span class="line">       <span class="comment">//如果在尾部插入元素</span></span><br><span class="line">       <span class="keyword">if</span> (index == size) &#123;</span><br><span class="line">           <span class="comment">//当前节点为空</span></span><br><span class="line">           succ = <span class="keyword">null</span>;</span><br><span class="line">           <span class="comment">//当前节点的上一个节点就是最后一个节点</span></span><br><span class="line">           pred = last;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="comment">//否则获取index位置的节点指向succ</span></span><br><span class="line">           succ = node(index);</span><br><span class="line">           <span class="comment">//index位置节点的前一个引用指向pred</span></span><br><span class="line">           pred = succ.prev;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="comment">//遍历集合，每一个节点都做重复创建，添加引用</span></span><br><span class="line">       <span class="keyword">for</span> (Object o : a) &#123;</span><br><span class="line">           <span class="meta">@SuppressWarnings</span>(<span class="string">"unchecked"</span>) E e = (E) o;</span><br><span class="line">           <span class="comment">//创建节点，该节点的pred指向pred节点(最初的index节点的前一个节点)，元素本身e，下一个节点指向null。</span></span><br><span class="line">           Node&lt;E&gt; newNode = <span class="keyword">new</span> Node&lt;&gt;(pred, e, <span class="keyword">null</span>);</span><br><span class="line">           <span class="comment">//判断是否为链表头</span></span><br><span class="line">           <span class="keyword">if</span> (pred == <span class="keyword">null</span>)</span><br><span class="line">               first = newNode;</span><br><span class="line">           <span class="keyword">else</span></span><br><span class="line">               <span class="comment">//pred节点的next也指向新节点</span></span><br><span class="line">               pred.next = newNode;</span><br><span class="line">           <span class="comment">//新节点又继续作为pred节点存在</span></span><br><span class="line">           pred = newNode;</span><br><span class="line">       &#125;</span><br><span class="line"><span class="comment">//集合遍历完成后，如果当前节点为空节点，即在链表的末尾添加元素，就把pred指向尾结点，succ节点不存在</span></span><br><span class="line">       <span class="keyword">if</span> (succ == <span class="keyword">null</span>) &#123;</span><br><span class="line">           last = pred;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="comment">//如果是在链表中间插入的集合，当前节点succ是作位pred的next元素存在</span></span><br><span class="line">           pred.next = succ;</span><br><span class="line">           <span class="comment">//同时pred指向succ的上一个节点引用，相互引用</span></span><br><span class="line">           succ.prev = pred;</span><br><span class="line">       &#125;</span><br><span class="line"><span class="comment">//size+numNew个数</span></span><br><span class="line">       size += numNew;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除列表中指定位置的元素。</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> E <span class="title">remove</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查index是否有效</span></span><br><span class="line">       checkElementIndex(index);</span><br><span class="line">       <span class="comment">//删除index节点的元素</span></span><br><span class="line">       <span class="keyword">return</span> unlink(node(index));</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//删除节点</span></span><br><span class="line">   <span class="function">E <span class="title">unlink</span><span class="params">(Node&lt;E&gt; x)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//获取节点的元素</span></span><br><span class="line">       <span class="keyword">final</span> E element = x.item;</span><br><span class="line">       <span class="comment">//获取元素的下一个节点的引用</span></span><br><span class="line">       <span class="keyword">final</span> Node&lt;E&gt; next = x.next;</span><br><span class="line">       <span class="comment">//获取元素的上一个节点的引用</span></span><br><span class="line">       <span class="keyword">final</span> Node&lt;E&gt; prev = x.prev;</span><br><span class="line">      </span><br><span class="line">       <span class="comment">//如果上一个节点等于null，则说明前面没有节点，把next节点的下一个节点引用为第一个节点</span></span><br><span class="line">       <span class="keyword">if</span> (prev == <span class="keyword">null</span>) &#123;</span><br><span class="line">           first = next;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="comment">//否则把next节点引用为prev节点的next指向的节点</span></span><br><span class="line">           prev.next = next;</span><br><span class="line">           <span class="comment">//x节点的上一个节点置空</span></span><br><span class="line">           x.prev = <span class="keyword">null</span>;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="comment">//如果next节点等于null，为空值，就直接把prev赋值为last节点</span></span><br><span class="line">       <span class="keyword">if</span> (next == <span class="keyword">null</span>) &#123;</span><br><span class="line">           last = prev;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="comment">//否则把next节点的上一个节点指向prev</span></span><br><span class="line">           next.prev = prev;</span><br><span class="line">           <span class="comment">//x节点的下一个节点置空</span></span><br><span class="line">           x.next = <span class="keyword">null</span>;</span><br><span class="line">       &#125;</span><br><span class="line"><span class="comment">//最后把x的数据置空</span></span><br><span class="line">       x.item = <span class="keyword">null</span>;</span><br><span class="line">       size--;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">       <span class="comment">//返回已经删除的元素值</span></span><br><span class="line">       <span class="keyword">return</span> element;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//从列表中删除指定元素的第一个匹配项</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">remove</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//如果对象o等于null(这里说明linkedList允许存放null值)</span></span><br><span class="line">       <span class="keyword">if</span> (o == <span class="keyword">null</span>) &#123;</span><br><span class="line">           <span class="comment">//从头到尾遍历找出第一个符合数据为null的节点</span></span><br><span class="line">           <span class="keyword">for</span> (Node&lt;E&gt; x = first; x != <span class="keyword">null</span>; x = x.next) &#123;</span><br><span class="line">               <span class="keyword">if</span> (x.item == <span class="keyword">null</span>) &#123;</span><br><span class="line">                   <span class="comment">//删除节点</span></span><br><span class="line">                   unlink(x);</span><br><span class="line">                   <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="comment">//否则 从头到尾遍历找出第一个符合数据为null的节点</span></span><br><span class="line">           <span class="keyword">for</span> (Node&lt;E&gt; x = first; x != <span class="keyword">null</span>; x = x.next) &#123;</span><br><span class="line">               <span class="comment">//找出符合qeuals条件的对象</span></span><br><span class="line">               <span class="keyword">if</span> (o.equals(x.item)) &#123;</span><br><span class="line">                   <span class="comment">//删除</span></span><br><span class="line">                   unlink(x);</span><br><span class="line">                   <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//从列表中删除所有元素。</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">//注意使用的是for循环遍历全部节点</span></span><br><span class="line">           <span class="keyword">for</span> (Node&lt;E&gt; x = first; x != <span class="keyword">null</span>; ) &#123;</span><br><span class="line">               <span class="comment">//每个节点的所有信息全部置空</span></span><br><span class="line">               Node&lt;E&gt; next = x.next;</span><br><span class="line">               x.item = <span class="keyword">null</span>;</span><br><span class="line">               x.next = <span class="keyword">null</span>;</span><br><span class="line">               x.prev = <span class="keyword">null</span>;</span><br><span class="line">               <span class="comment">//循环赋值</span></span><br><span class="line">               x = next;</span><br><span class="line">           &#125;</span><br><span class="line">      <span class="comment">//首节点和尾结点赋值为null</span></span><br><span class="line">           first = last = <span class="keyword">null</span>;</span><br><span class="line">           size = <span class="number">0</span>;</span><br><span class="line">           modCount++;</span><br><span class="line">       &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//获取指定位置节点元素</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> E <span class="title">get</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查index是否有效</span></span><br><span class="line">       checkElementIndex(index);</span><br><span class="line">       <span class="comment">//返回index节点的数据</span></span><br><span class="line">       <span class="keyword">return</span> node(index).item;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//将列表中指定位置的元素替换为指定元素。</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> E <span class="title">set</span><span class="params">(<span class="keyword">int</span> index, E element)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查index是否有效</span></span><br><span class="line">       checkElementIndex(index);</span><br><span class="line">       <span class="comment">//获取index的节点</span></span><br><span class="line">       Node&lt;E&gt; x = node(index);</span><br><span class="line">       <span class="comment">//获取节点的item</span></span><br><span class="line">       E oldVal = x.item;</span><br><span class="line"><span class="comment">//新的元素替换旧的元素</span></span><br><span class="line">       x.item = element;</span><br><span class="line">       <span class="comment">//返回旧元素</span></span><br><span class="line">       <span class="keyword">return</span> oldVal;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//将双向链表转换成数组</span></span><br><span class="line">   <span class="keyword">public</span> Object[] toArray() &#123;</span><br><span class="line">       <span class="comment">//创建一个Size大小的数组</span></span><br><span class="line">       Object[] result = <span class="keyword">new</span> Object[size];</span><br><span class="line">       <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">       <span class="comment">//循环遍历所有节点</span></span><br><span class="line">       <span class="keyword">for</span> (Node&lt;E&gt; x = first; x != <span class="keyword">null</span>; x = x.next)</span><br><span class="line">           <span class="comment">//将节点数据存入数组</span></span><br><span class="line">           result[i++] = x.item;</span><br><span class="line">       <span class="comment">//返回数组</span></span><br><span class="line">       <span class="keyword">return</span> result;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>和<code>ArrayList</code>一样，无论<code>add()</code>、<code>remove()</code>，还是<code>clear()</code>，只要涉及到修改集合中的元素个数时，都会改变<code>modCount</code>(全局)的值。由此回到<code>next()</code>方法中我们发现当 ‘A’ 线程正在做迭代器遍历操作时，<code>modCount</code>赋值给了<code>expectedModCount</code>，每次调用<code>next()</code>方法都会做一次<code>modCount != expectedModCount</code>的校验，此时线程 ’B‘ 进来了调用了add方法修改了<code>modCount</code>的值，此时<code>modCount</code>变成了N+1，判断为false，抛出<code>ConcurrentModificationException</code>异常，产生fail-fast事件 。</p></li></ol></li><li><p>fail-fast`事件</p><p>当多个线程对同一个集合进行操作的时候，某线程访问集合的过程中，该集合的内容被其他线程所改变(即其它线程通过<code>add</code>、<code>remove</code>、<code>clear</code>等方法，改变了<code>modCount</code>的值)；这时，就会抛出<code>ConcurrentModificationException</code>异常。与此对应的安全失败，在后面再解析。</p></li><li><p>解决方法</p><p>使用<code>Collections.synchronizedList()</code>方法包装，但是效率低下，这种方法实际上只是将原来非线程安全的<code>LinkedList</code>中的方法加上一个<code>synchronized</code>同步代码块 (哭了。。。)</p></li><li><p>优化建议</p><p>在多线程下如果有按数据索引访问元素的情形，采用<code>Collections.synchronizedList(new LinkedList&lt;&gt;())</code>方法</p></li><li><p>LinkedList的常用API</p><table><thead><tr><th>方法</th><th>描述</th></tr></thead><tbody><tr><td><code>add(E e)</code></td><td>将指定的元素添加到列表的末尾。</td></tr><tr><td><code>addFirst(E e)</code></td><td>在此列表的开始处插入指定的元素。</td></tr><tr><td><code>addLast(E e)</code></td><td>将指定的元素列表的结束。</td></tr><tr><td><code>get(int index)</code></td><td>返回此列表中指定位置的元素。</td></tr><tr><td><code>remove(int index)</code></td><td>移除此列表中指定位置的元素。</td></tr><tr><td><code>size()</code></td><td>返回此列表中元素的数目。</td></tr><tr><td><code>toArray()</code></td><td>返回一个数组，包含在这个列表中的所有元素在适当的顺序（从第一个到最后一个元素）。</td></tr><tr><td><code>clear()</code></td><td>从这个列表中移除所有的元素。</td></tr><tr><td><code>indexOf(Object o)</code></td><td>返回此列表中指定元素的第一个出现的索引</td></tr><tr><td><code>peek()</code></td><td>检索，但不删除，此列表的头（第一个元素）。</td></tr><tr><td><code>poll()</code></td><td>检索并删除此列表的头（第一个元素）。</td></tr><tr><td><code>pop()</code></td><td>从这个列表所表示的堆栈中弹出一个元素。</td></tr><tr><td><code>push(E e)</code></td><td>将一个元素推到由该列表所表示的堆栈上。</td></tr><tr><td><code>offer(E e)</code></td><td>将指定的元素添加到列表的尾部（最后一个元素）。</td></tr></tbody></table></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA.UTIL包下的ARRAYLIST和迭代器快速失败的源码解析以及多线程下故障现象、导致原因、以及解决办法和优化建议</title>
      <link href="/2019/01/02/java/2/"/>
      <url>/2019/01/02/java/2/</url>
      
        <content type="html"><![CDATA[<hr><p>该篇博客不适合小白，只做针对性的api源码解析，以及适合我自身的案例研究</p><hr><ol><li><p>使用多个线程往<code>ArrayList</code>中添加元素</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建资源集合</span></span><br><span class="line">List&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建30个线程添加元素</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">30</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">        list.add(UUID.randomUUID().toString().substring(<span class="number">0</span>,<span class="number">8</span>));</span><br><span class="line">        System.out.println(list);</span><br><span class="line">    &#125;).start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>故障现象</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Exception in thread <span class="string">"2"</span> java.util.ConcurrentModificationException</span><br></pre></td></tr></table></figure><p>我们知道<code>ArrayList</code>是线程不安全的，当多线程操作时，线程操作迭代器的同时其他线程改变了元素的值就会产生<code>ConcurrentModificationException</code>异常，<code>ConcurrentModificationException</code>是在操作<code>Iterator</code>时抛出的异常</p></li><li><p>故障原因</p><p>java中的<code>java.util</code>包下的类全部都是快速失败的，那么为什么在多线程操作ArrayList的时候会出现<code>ConcurrentModificationException</code>异常呢？</p><p>在我们的案例中，每个线程添加一次元素我们就打印一次集合中的元素，通过源码追踪，得出下面的内容</p><ol><li><p>打印内容，经过第二行代码，String调用了valueOf(x)方法</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">println</span><span class="params">(Object x)</span> </span>&#123;</span><br><span class="line">    String s = String.valueOf(x);</span><br><span class="line">    <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">        print(s);</span><br><span class="line">        newLine();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>ValueOf(Object x)方法是把Object类型的对象x转换成String类型，判断不为null，进入obj也就是我们的集合</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">valueOf</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (obj == <span class="keyword">null</span>) ? <span class="string">"null"</span> : obj.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>集合中的toString方法定义在了AbstractCollection类中，我们的错误出现在了 <code>E e = it.next();</code>获取元素这里，继续追踪错误。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Iterator&lt;E&gt; it = iterator();<span class="comment">//创建迭代器对象</span></span><br><span class="line">    <span class="keyword">if</span> (! it.hasNext())<span class="comment">//判断集合是否为空</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"[]"</span>;</span><br><span class="line">      </span><br><span class="line">    StringBuilder sb = <span class="keyword">new</span> StringBuilder();<span class="comment">//创建StringBuilder动态构造字符串</span></span><br><span class="line">    sb.append(<span class="string">'['</span>);</span><br><span class="line">    <span class="keyword">for</span> (;;) &#123;<span class="comment">//死循环</span></span><br><span class="line">        E e = it.next();<span class="comment">//获取元素 E ==&gt; String 是传进来的泛型</span></span><br><span class="line">        sb.append(e == <span class="keyword">this</span> ? <span class="string">"(this Collection)"</span> : e);<span class="comment">//添加元素</span></span><br><span class="line">        <span class="keyword">if</span> (! it.hasNext())<span class="comment">//循环换了就返回集合的字符串类型</span></span><br><span class="line">            <span class="keyword">return</span> sb.append(<span class="string">']'</span>).toString();</span><br><span class="line">        sb.append(<span class="string">','</span>).append(<span class="string">' '</span>); <span class="comment">//这里是没有return之前会进来，添加分隔符</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>下面是迭代器中的next()方法</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="comment">// 用来记录List修改的次数：每修改一次(添加/删除等操作)，将modCount+1</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">transient</span> <span class="keyword">int</span> modCount = <span class="number">0</span>;</span><br><span class="line">      </span><br><span class="line"><span class="keyword">int</span> cursor;       <span class="comment">// 下一个要返回的元素的索引</span></span><br><span class="line">      <span class="keyword">int</span> lastRet = -<span class="number">1</span>; <span class="comment">// 最后一个返回元素的索引;-1:没有</span></span><br><span class="line">      <span class="keyword">int</span> expectedModCount = modCount;<span class="comment">//这是非常关键的一步，把modCount赋值给expectedModCount，用来在迭代器遍历时next()和remove()方法中做校验</span></span><br><span class="line">      </span><br><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">          checkForComodification();<span class="comment">//每次添加元素前，都检查一次modCount和expectedModCount是否相等，如果不相等就直接返回ConcurrentModificationException异常，产生快速失败,多线程环境下这里通不过</span></span><br><span class="line">          <span class="keyword">int</span> i = cursor;</span><br><span class="line">          <span class="keyword">if</span> (i &gt;= size)</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> NoSuchElementException();</span><br><span class="line">          Object[] elementData = ArrayList.<span class="keyword">this</span>.elementData;</span><br><span class="line">          <span class="keyword">if</span> (i &gt;= elementData.length)</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</span><br><span class="line">          cursor = i + <span class="number">1</span>;</span><br><span class="line">          <span class="keyword">return</span> (E) elementData[lastRet = i];</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">checkForComodification</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (modCount != expectedModCount)<span class="comment">//多线程环境下，modCount和expectedModCount不相等</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>那么在多线程环境下是如何让<code>modCount</code> != <code>expectedModCount</code>的呢？</p><p>我们先看源码中是如何修改<code>modCount</code> 的值的(开始怼源码)</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="comment">// list中容量变化时，对应的同步函数</span></span><br><span class="line"><span class="keyword">transient</span> Object[] elementData; <span class="comment">// 保存了添加到ArrayList中的元素</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;<span class="comment">//默认是空元素</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_CAPACITY = <span class="number">10</span>;<span class="comment">//初始容量</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">ensureCapacity</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">int</span> minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA)</span><br><span class="line">           <span class="comment">//元素列表不等于默认列表</span></span><br><span class="line">           ? <span class="number">0</span></span><br><span class="line">           <span class="comment">//元素列表等于默认列表，返回默认初始值10</span></span><br><span class="line">           : DEFAULT_CAPACITY;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">if</span> (minCapacity &gt; minExpand) &#123;<span class="comment">//传入的容量大于最小扩展容量(minExpand)，则调用ensureExplicitCapacity()方法传入minCapacity</span></span><br><span class="line">           ensureExplicitCapacity(minCapacity);</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//判断元素列表是否为初始的元素列表，返回默认容量10和传入的容量中较大的值</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">calculateCapacity</span><span class="params">(Object[] elementData, <span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123;</span><br><span class="line">           <span class="keyword">return</span> Math.max(DEFAULT_CAPACITY, minCapacity);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> minCapacity;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//JDK1.8 中，add、addAll方法会先调用者方法判断是否需要扩容</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">ensureCapacityInternal</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//把元素列表和扩展的容量传入到calculateCapacity方法，做一个判断</span></span><br><span class="line">       ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//list中容量变化时，modCount+1</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">ensureExplicitCapacity</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">      </span><br><span class="line">       <span class="comment">// 如果minCapacity大于elementData的长度，则进行扩容</span></span><br><span class="line">       <span class="keyword">if</span> (minCapacity - elementData.length &gt; <span class="number">0</span>)</span><br><span class="line">           <span class="comment">//调用grow扩容</span></span><br><span class="line">           grow(minCapacity);</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MAX_ARRAY_SIZE = Integer.MAX_VALUE - <span class="number">8</span>;</span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">grow</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//旧容量</span></span><br><span class="line">       <span class="keyword">int</span> oldCapacity = elementData.length;</span><br><span class="line">       <span class="comment">//新容量=旧容量+旧容量的1/2 ==&gt;扩容1.5倍</span></span><br><span class="line">       <span class="keyword">int</span> newCapacity = oldCapacity + (oldCapacity &gt;&gt; <span class="number">1</span>);</span><br><span class="line">       <span class="comment">//如果计算出来的新容量比传进来的容量小，则以传入的容量为准</span></span><br><span class="line">       <span class="keyword">if</span> (newCapacity - minCapacity &lt; <span class="number">0</span>)</span><br><span class="line">           newCapacity = minCapacity;</span><br><span class="line">       <span class="comment">//如果新容量大于MAX_ARRAY_SIZE(Integer.MAX_VALUE - 8)，则把新容量交给hugeCapacity方法</span></span><br><span class="line">       <span class="keyword">if</span> (newCapacity - MAX_ARRAY_SIZE &gt; <span class="number">0</span>)</span><br><span class="line">           newCapacity = hugeCapacity(minCapacity);<span class="comment">//hugeCapacity实际上是做了一次内存溢出的判断，因为MAX_ARRAY_SIZE的容量已经接近溢出的边缘</span></span><br><span class="line">       <span class="comment">//Arrays.copyOf()方法放入elementData的元素，并把容量扩容为newCapacity</span></span><br><span class="line">       elementData = Arrays.copyOf(elementData, newCapacity);</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//主要检查内存是否溢出</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">hugeCapacity</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">       <span class="comment">// 内存溢出</span></span><br><span class="line">       <span class="keyword">if</span> (minCapacity &lt; <span class="number">0</span>) </span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> OutOfMemoryError();</span><br><span class="line">       <span class="keyword">return</span> (minCapacity &gt; MAX_ARRAY_SIZE) ?</span><br><span class="line">           Integer.MAX_VALUE :</span><br><span class="line">           MAX_ARRAY_SIZE;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line">   <span class="comment">// 添加元素到队列最后</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">add</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//判断是否需要扩容和modCount+1,以及判断内存溢出</span></span><br><span class="line">       ensureCapacityInternal(size + <span class="number">1</span>);  </span><br><span class="line">       <span class="comment">//将元素添加到数组末尾</span></span><br><span class="line">       elementData[size++] = e;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">   <span class="comment">// 添加元素到指定的位置</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> index, E element)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查index的范围</span></span><br><span class="line">       rangeCheckForAdd(index);</span><br><span class="line"><span class="comment">//判断是否需要扩容和modCount+1,以及判断内存溢出</span></span><br><span class="line">       ensureCapacityInternal(size + <span class="number">1</span>);  </span><br><span class="line">       <span class="comment">//把原数组的index位置移动到目标数组的index+1的位置,长度=数组长度-插入位置，这样就把index位置空出来了，涉及到内存操作，贼慢</span></span><br><span class="line">       System.arraycopy(elementData, index, elementData, index + <span class="number">1</span>,</span><br><span class="line">                        size - index);</span><br><span class="line">       <span class="comment">//把元素插入到数组中的index位置</span></span><br><span class="line">       elementData[index] = element;</span><br><span class="line">       size++;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//指定位置的范围检查，适用于add和addAll.</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">rangeCheckForAdd</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (index &gt; size || index &lt; <span class="number">0</span>)</span><br><span class="line">           <span class="comment">//throw new 下标越界异常</span></span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> IndexOutOfBoundsException(outOfBoundsMsg(index));</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line">   <span class="comment">// 添加集合</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">addAll</span><span class="params">(Collection&lt;? extends E&gt; c)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//把集合转换成一个Object类型的数组</span></span><br><span class="line">       Object[] a = c.toArray();</span><br><span class="line">       <span class="comment">//获集合的长度，用作扩容和复制</span></span><br><span class="line">       <span class="keyword">int</span> numNew = a.length;</span><br><span class="line">       <span class="comment">//判断是否需要扩容和modCount+1,以及判断内存溢出</span></span><br><span class="line">       ensureCapacityInternal(size + numNew);</span><br><span class="line">       <span class="comment">//把集合从0位置开始移动到目标数组的size位置,长度就等于集合的长度</span></span><br><span class="line">       System.arraycopy(a, <span class="number">0</span>, elementData, size, numNew);</span><br><span class="line">       <span class="comment">//计算size</span></span><br><span class="line">       size += numNew;</span><br><span class="line">       <span class="keyword">return</span> numNew != <span class="number">0</span>;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//类似于addAll，插入的位置变了而已</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">addAll</span><span class="params">(<span class="keyword">int</span> index, Collection&lt;? extends E&gt; c)</span> </span>&#123;&#125;</span><br><span class="line">  </span><br><span class="line">      </span><br><span class="line">   <span class="comment">// 删除指定位置的元素 </span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> E <span class="title">remove</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查范围</span></span><br><span class="line">       rangeCheck(index);</span><br><span class="line"><span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">       <span class="comment">//找出index位置的元素</span></span><br><span class="line">       E oldValue = elementData(index);</span><br><span class="line"><span class="comment">//计算数组从index到size的长度，-1是为了减去index的位置</span></span><br><span class="line">       <span class="keyword">int</span> numMoved = size - index - <span class="number">1</span>;</span><br><span class="line">       <span class="comment">//如果计算后的长度大于0，则使用System.arraycopy复制index后的元素向前移动一位，内存操作，贼慢</span></span><br><span class="line">       <span class="keyword">if</span> (numMoved &gt; <span class="number">0</span>)</span><br><span class="line">           System.arraycopy(elementData, index+<span class="number">1</span>, elementData, index,numMoved);</span><br><span class="line">        <span class="comment">//size--,并赋值为null</span></span><br><span class="line">       elementData[--size] = <span class="keyword">null</span>; <span class="comment">// clear to let GC do its work</span></span><br><span class="line"><span class="comment">//返回删除元素后的数组</span></span><br><span class="line">       <span class="keyword">return</span> oldValue;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//检查索引的范围</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">rangeCheck</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (index &gt;= size)</span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> IndexOutOfBoundsException(outOfBoundsMsg(index));</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">   <span class="comment">// 快速删除指定位置的元素,和remove(int index)类似，省去了范围校验</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">fastRemove</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">       <span class="keyword">int</span> numMoved = size - index - <span class="number">1</span>;</span><br><span class="line">       <span class="keyword">if</span> (numMoved &gt; <span class="number">0</span>)</span><br><span class="line">           System.arraycopy(elementData, index+<span class="number">1</span>, elementData, index,numMoved);</span><br><span class="line">       <span class="comment">//原来这里也会有GC回收</span></span><br><span class="line">       elementData[--size] = <span class="keyword">null</span>;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line">   <span class="comment">// 清空集合</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">      </span><br><span class="line">       <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++)</span><br><span class="line">           <span class="comment">//这里也会有GC回收</span></span><br><span class="line">           elementData[i] = <span class="keyword">null</span>;</span><br><span class="line">      </span><br><span class="line">       size = <span class="number">0</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   ...</span><br></pre></td></tr></table></figure><p>一遍源码读下来，发现无论是<code>add()</code>、<code>remove()</code>，还是<code>clear()</code>，只要涉及到修改集合中的元素个数时，都会改变<code>modCount</code>(全局)的值。</p><p>由此回到<code>next()</code>方法中我们发现当 ‘A’ 线程正在做迭代器遍历操作时，<code>modCount</code>赋值给了<code>expectedModCount</code>，每次调用<code>next()</code>方法都会做一次<code>modCount != expectedModCount</code>的校验，此时线程 ’B‘ 进来了调用了add方法修改了<code>modCount</code>的值，此时<code>modCount</code>变成了N+1，判断为false，抛出<code>ConcurrentModificationException</code>异常，产生fail-fast事件 。</p></li></ol></li><li><p><code>fail-fast</code>事件</p><p>当多个线程对同一个集合进行操作的时候，某线程访问集合的过程中，该集合的内容被其他线程所改变(即其它线程通过<code>add</code>、<code>remove</code>、<code>clear</code>等方法，改变了<code>modCount</code>的值)；这时，就会抛出<code>ConcurrentModificationException</code>异常。与此对应的安全失败，在后面再解析。</p></li><li><p>解决方法</p><p>经过源码解析<code>ConcurrentModificationException</code>异常是因为多个线程同时调用add()方法导致的，解决的办法有一下三种:</p><ol><li><p>使用<code>Vector()</code>替代<code>ArrayList()</code>，但是这种方法效率低下，因为<code>Vector()</code>的几乎所有方法都加上了<code>synchronized</code>修饰符，<code>synchronized</code>保证了在同一时刻最多只有一个线程访问该段代码，虽然jdk1.5引入了自旋锁、锁粗化、轻量级锁和偏向锁，但还是太重，效率很低。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;String&gt; list = <span class="keyword">new</span> Vector&lt;String&gt;());</span><br></pre></td></tr></table></figure></li><li><p>使用<code>Collections.synchronizedList()</code>包装<code>ArrayList</code>，但是这种方法实际上只是将原来非线程安全的<code>ArrayList</code>中的方法加上一个<code>synchronized</code>同步代码块 (哭了。。。)</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;String&gt; list = Collections.synchronizedList(<span class="keyword">new</span> ArrayList&lt;String&gt;());</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> Object mutex;     <span class="comment">// 对象锁</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">get</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line"><span class="keyword">synchronized</span> (mutex) &#123;<span class="comment">//同步代码块</span></span><br><span class="line">        <span class="keyword">return</span> list.get(index);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">set</span><span class="params">(<span class="keyword">int</span> index, E element)</span> </span>&#123;</span><br><span class="line"><span class="keyword">synchronized</span> (mutex) &#123;<span class="comment">//同步代码块</span></span><br><span class="line">        <span class="keyword">return</span> list.set(index, element);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure></li><li><p>第三种也是推介的一种方法就是使用java.util.concurrent包下的CopyOnWriteArrayList解决，俗称写时复制机制，是读写分离的一种实现，这种方法在后面将详细源码解析。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;String&gt; list = <span class="keyword">new</span> CopyOnWriteArrayList&lt;String&gt;();</span><br></pre></td></tr></table></figure></li></ol></li><li><p>优化建议</p><p>在多线程下如果有按数据索引访问元素的情形，采用<code>CopyOnWriteArrayList()</code>方法</p></li><li><p>ArrayList的常用API</p><table><thead><tr><th>方法</th><th>描述</th></tr></thead><tbody><tr><td>add(E e)</td><td>将指定的元素列表的结束。</td></tr><tr><td>addAll(Collection c)</td><td>追加指定集合的所有元素到这个列表的末尾，按他们的指定集合的迭代器返回。</td></tr><tr><td>clear()</td><td>从这个列表中移除所有的元素。</td></tr><tr><td>contains(Object o)</td><td>返回 <code>true</code>如果这个列表包含指定元素。</td></tr><tr><td>get(int index)</td><td>返回此列表中指定位置的元素。</td></tr><tr><td>iterator()</td><td>在这个列表中的元素上返回一个正确的顺序。</td></tr><tr><td>remove(int index)</td><td>移除此列表中指定位置的元素。</td></tr><tr><td>set(int index, E element)</td><td>用指定元素替换此列表中指定位置的元素。</td></tr><tr><td>size()</td><td>返回此列表中元素的数目</td></tr><tr><td>toArray()</td><td>返回一个数组，包含在这个列表中的所有元素在适当的顺序</td></tr></tbody></table></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lock可重入锁与函数式接口Runnable接口的lambda编程方式</title>
      <link href="/2019/01/01/java/1/"/>
      <url>/2019/01/01/java/1/</url>
      
        <content type="html"><![CDATA[<h2 id="多线程企业级Demo"><a href="#多线程企业级Demo" class="headerlink" title="多线程企业级Demo"></a>多线程企业级Demo</h2><ol><li><p>前提</p><p>所有的多线程开开发遵循一个规则:</p><p><code>在高内聚低耦合的前提下，线程--&gt;操作--&gt;资源类</code></p><p>在这个条件下我们写一个卖票的Demo，三个售票员卖出30张票</p></li><li><p>资源类</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//资源类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Ticket</span> </span>&#123;</span><br><span class="line">    <span class="comment">//创建一个可重入的lock锁</span></span><br><span class="line">    Lock lock = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line">    <span class="comment">//总30张票</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> number = <span class="number">30</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sale</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        lock.lock();<span class="comment">//上锁</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (number &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                System.out.println(Thread.currentThread().getName() + <span class="string">"\t卖出第"</span> + (number--) + <span class="string">"\t张票，还剩下"</span> + number);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            lock.unlock();<span class="comment">//解锁</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>线程</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">40</span>; i++) &#123;</span><br><span class="line">            <span class="comment">//待操作的代码</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;,<span class="string">"a"</span>).start();</span><br></pre></td></tr></table></figure><p><code>注意:</code> 我们查看Runnable()接口的源代码发现，这是一个函数式接口，下面提供源码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@FunctionalInterface</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所以我们可以使用lambda表达式简化操作</p></li><li><p>操作</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//new资源类</span></span><br><span class="line">Ticket ticket = <span class="keyword">new</span> Ticket();</span><br><span class="line"><span class="keyword">new</span> Thread(() -&gt; &#123; <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">40</span>; i++) ticket.sale();&#125;,<span class="string">"A卖票员"</span>).start();</span><br><span class="line"><span class="keyword">new</span> Thread(() -&gt; &#123; <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">40</span>; i++) ticket.sale();&#125;,<span class="string">"B卖票员"</span>).start();</span><br><span class="line"><span class="keyword">new</span> Thread(() -&gt; &#123; <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">40</span>; i++) ticket.sale();&#125;,<span class="string">"C卖票员"</span>).start();</span><br></pre></td></tr></table></figure></li><li><p>结果：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">A卖票员卖出第<span class="number">30</span>张票，还剩下<span class="number">29</span></span><br><span class="line">A卖票员卖出第<span class="number">29</span>张票，还剩下<span class="number">28</span></span><br><span class="line">A卖票员卖出第<span class="number">28</span>张票，还剩下<span class="number">27</span></span><br><span class="line">...</span><br><span class="line">A卖票员卖出第<span class="number">1</span>张票，还剩下<span class="number">0</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="Lambda表达式与函数式接口"><a href="#Lambda表达式与函数式接口" class="headerlink" title="Lambda表达式与函数式接口"></a>Lambda表达式与函数式接口</h2><p>在jdk1.8中，引入了函数式接口，函数式接口中只能声明一个抽象方法，lambda表达式可以直接使用这个接口</p><ol><li><p>定义函数式接口</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@FunctionalInterface</span><span class="comment">//显式指定 当接口中只有一条抽象方法时，默认是函数式接口</span></span><br><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">Foo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>使用lambda表达式实现</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="comment">//指定接口的实现</span></span><br><span class="line">    Foo foo = () -&gt; System.out.println(<span class="string">"hello FunctionInterface.."</span>);</span><br><span class="line">    foo.sayHello();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>还需要知道的是在jdk1.8中，引入了default方法修饰接口，并且可以在接口中声明static方法，但是必须实现方法，这里和我们印象中的java接口有点不同</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//@FunctionalInterface  //声明函数式接口 只有一条抽象方法时可以省略</span></span><br><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">Foo</span> </span>&#123;</span><br><span class="line">    <span class="comment">//声明抽象方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//声明默认方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">default</span> <span class="keyword">int</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span>  x+y;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//声明静态方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">dec</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> x-y;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义类，没有实现接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FuncitonInterfaceDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//Foo对象接收实现的接口</span></span><br><span class="line">        Foo foo = () -&gt; System.out.println(<span class="string">"hello FunctionInterface.."</span>);</span><br><span class="line">        foo.sayHello();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//Foo对象的引用调用add方法</span></span><br><span class="line">        System.out.println(foo.add(<span class="number">10</span>, <span class="number">5</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//接口名直接调用static修饰的dec方法</span></span><br><span class="line">        System.out.println(Foo.dec(<span class="number">10</span>, <span class="number">5</span>));</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume的Channel内存调优</title>
      <link href="/2018/12/07/flume/7/"/>
      <url>/2018/12/07/flume/7/</url>
      
        <content type="html"><![CDATA[<p><strong>flume“Space for commit to queue couldn’t be acquired”异常产生分析</strong></p><p>日志截图如下：</p><p><img src="https://yerias.github.io/flume_img/channel%E8%B0%83%E4%BC%98.png" alt=""></p><p>这里说的内容是：<strong>queue空间不足。sink好像没有紧跟source，或者是buffer大小太小。</strong>这里的queue代表什么？sink没有紧跟source的具体含义是什么？buffer又是什么？我分析源代码后，将结果在下面铺开向大家展示。</p><h3 id="memory-channel内部结构"><a href="#memory-channel内部结构" class="headerlink" title="memory channel内部结构"></a>memory channel内部结构</h3><p><img src="https://yerias.github.io/flume_img/channel%E9%98%9F%E5%88%97.png" alt=""></p><p>memory channel内部有三个队列，分别是图中的putList，queue，takeList。有两个参数来控制他们的大小，默认值都是100，分别是：</p><h3 id="channel是如何被使用的"><a href="#channel是如何被使用的" class="headerlink" title="channel是如何被使用的"></a>channel是如何被使用的</h3><p>channel之上有一把锁，当source主动向channel放数据或者sink主动从channel取数据时，会抢锁，谁取到锁，谁就可以操作channel。</p><p>每次使用时会首先调用tx.begin()开始事务，也就是获取锁。然后调用tx.commit()提交数据或者调用tx.rollback()取消操作。</p><h3 id="source往channel放数据"><a href="#source往channel放数据" class="headerlink" title="source往channel放数据"></a>source往channel放数据</h3><p>就是一个死循环，source一直试图获取channel锁，然后从kafka获取数据，放入channel中，那每次放入多少个数据呢？在KafkaSource.java中，代码是这样的：</p><p><img src="https://yerias.github.io/flume_img/channel3.png" alt=""></p><p>含义就是：每次最多放batchUpperLimit或最多等待batchEndTime的时间，就结束向channel放数据。这两个参数的默认值分别是1000个和1s。分别由batchSize和batchDurationMillis设置。</p><p>当获取了足够的数据，首先放入putList中，然后就会调用tx.commit()将putList的全部数据放入queue中。</p><h3 id="sink从channel取数据"><a href="#sink从channel取数据" class="headerlink" title="sink从channel取数据"></a>sink从channel取数据</h3><p>也是一个死循环，sink一直试图获取channel锁，然后从channel取一批数据，放入sink和takeList（仅仅用于回滚，在调用rollback时takeList的数据会回滚道queue中）。每次取多少个event呢？这在HdfsSink中，代码如下：</p><p><img src="https://yerias.github.io/flume_img/channel4.png" alt=""></p><p>batchSize的大小默认是100，由hdfs.batchSize控制，取够了，再调用tx.commit()，将putList的所有数据放入queue。</p><p><strong>Space for commit to queue couldn’t be acquired异常如何发生的</strong></p><p>经过上面的一系列介绍，已经知道了kafka source、memory channel、hdfs sink协同工作的过程。因为“source往putList放数据，然后提交到queue中”与“sink从channel中取数据到sink和takeList，然后再从putList取数据到queue中”这两部分是分开来，任他们自由抢锁，所以，当前者多次抢到锁，后者没有抢到锁，同时queue的大小又太小，撑不住多次往里放数据，就会导致触发这个异常。</p><p>解决这个问题最直接的办法就是增大queue的大小，增大capacity和transacCapacity之间的差距，queue能撑住多次往里面放数据即可。</p><h3 id="失败后flume是如何处理的"><a href="#失败后flume是如何处理的" class="headerlink" title="失败后flume是如何处理的"></a>失败后flume是如何处理的</h3><p>flume会暂停source向channel放数据，等待几秒钟，这期间sink应该会消费channel中的数据，当source再次开始想channel放数据时channel就有足够的空间了。</p><h3 id="flume调优避免内存不"><a href="#flume调优避免内存不" class="headerlink" title="flume调优避免内存不"></a>flume调优避免内存不</h3><ol><li><p>增加 JVM 内存</p><p><img src="https://yerias.github.io/flume_img/channel5.png" alt=""></p></li><li><p>增加capacity和transactionCapacity的容量</p><p><img src="https://yerias.github.io/flume_img/channel6.png" alt=""></p><table><thead><tr><th>type</th><th>-</th><th>组件类型名称必须是<code>memory</code></th></tr></thead><tbody><tr><td>capacity</td><td>100</td><td>存储在 Channel 当中的最大 events 数</td></tr><tr><td>transactionCapacity</td><td>100</td><td>同时刻从Source 获取，或发送到 Sink 的最大 events 数</td></tr><tr><td>keep-alive</td><td>3</td><td>添加或删除一个 event 超时的秒数</td></tr></tbody></table><p>如果每分钟开头量比较大,keep-alive需要调大 </p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>修改Flume源码使taildir source支持递归（可配置）</title>
      <link href="/2018/12/06/flume/6/"/>
      <url>/2018/12/06/flume/6/</url>
      
        <content type="html"><![CDATA[<p>Flume的source选哪个？<br>taildir source首选！</p><ol><li>断点还原 <code>positionFile</code>可以记录偏移量</li><li>可配置文件组，里面使用正则表达式配置多个要监控的文件</li></ol><p>这么好的taildir source有一点不完美，不能支持递归监控文件夹。</p><p>所以就只能修改源代码了，需要注意的是无论是Apache版本的还是CDH的都能够兼容使用，我这里使用的版本是flume-ng-1.6.0-cdh5.16.2，但是即使你使用Apache的版本编译源码，也是没问题的。</p><h2 id="改源码，先读源码"><a href="#改源码，先读源码" class="headerlink" title="改源码，先读源码"></a>改源码，先读源码</h2><p>Flume的taildir source启动会调用<code>start()</code>方法作初始化，里面创建一个<code>ReliableTaildirEventReader</code>,这里用到了建造者模式</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    logger.info(<span class="string">"&#123;&#125; TaildirSource source starting with directory: &#123;&#125;"</span>, getName(), filePaths);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        reader = <span class="keyword">new</span> ReliableTaildirEventReader.Builder()</span><br><span class="line">                .filePaths(filePaths)</span><br><span class="line">                .headerTable(headerTable)</span><br><span class="line">                .positionFilePath(positionFilePath)</span><br><span class="line">                .skipToEnd(skipToEnd)</span><br><span class="line">                .addByteOffset(byteOffsetHeader)</span><br><span class="line">                .cachePatternMatching(cachePatternMatching)</span><br><span class="line">                .recursive(isRecursive)</span><br><span class="line">                .annotateFileName(fileHeader)</span><br><span class="line">                .fileNameHeader(fileHeaderKey)</span><br><span class="line">                .build();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> FlumeException(<span class="string">"Error instantiating ReliableTaildirEventReader"</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line">    idleFileChecker = Executors.newSingleThreadScheduledExecutor(</span><br><span class="line">            <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"idleFileChecker"</span>).build());</span><br><span class="line">    idleFileChecker.scheduleWithFixedDelay(<span class="keyword">new</span> idleFileCheckerRunnable(),</span><br><span class="line">            idleTimeout, checkIdleInterval, TimeUnit.MILLISECONDS);</span><br><span class="line"></span><br><span class="line">    positionWriter = Executors.newSingleThreadScheduledExecutor(</span><br><span class="line">            <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"positionWriter"</span>).build());</span><br><span class="line">    positionWriter.scheduleWithFixedDelay(<span class="keyword">new</span> PositionWriterRunnable(),</span><br><span class="line">            writePosInitDelay, writePosInterval, TimeUnit.MILLISECONDS);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">super</span>.start();</span><br><span class="line">    logger.debug(<span class="string">"TaildirSource started"</span>);</span><br><span class="line">    sourceCounter.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>taildir source属于<code>PollableSource</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A &#123;<span class="doctag">@link</span> Source&#125; that requires an external driver to poll to determine</span></span><br><span class="line"><span class="comment"> * whether there are &#123;<span class="doctag">@linkplain</span> Event events&#125; that are available to ingest</span></span><br><span class="line"><span class="comment"> * from the source.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@see</span> org.apache.flume.source.EventDrivenSourceRunner</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">PollableSource</span> <span class="keyword">extends</span> <span class="title">Source</span> </span>&#123;</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>这段注释的意思是<code>PollableSource</code>是需要一个外部驱动去查看有没有需要消费的事件，从而拉取事件，讲白了就是<strong>定时拉取</strong>。所以flume也不一定是真正实时的，只是隔一会儿不停地来查看事件而已。(与之相应的是另一种<code>EventDrivenSourceRunner</code>)<br>那么taildir source在定时拉取事件的时候是调用的<code>process</code>方法</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Status status = Status.READY;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        existingInodes.clear();</span><br><span class="line">        existingInodes.addAll(reader.updateTailFiles());</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">long</span> inode : existingInodes) &#123;</span><br><span class="line">            TailFile tf = reader.getTailFiles().get(inode);</span><br><span class="line">            <span class="keyword">if</span> (tf.needTail()) &#123;</span><br><span class="line">                tailFileProcess(tf, <span class="keyword">true</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        closeTailFiles();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            TimeUnit.MILLISECONDS.sleep(retryInterval);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            logger.info(<span class="string">"Interrupted while sleeping"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">        logger.error(<span class="string">"Unable to tail files"</span>, t);</span><br><span class="line">        status = Status.BACKOFF;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> status;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重点就是下面这几行</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">existingInodes.addAll(reader.updateTailFiles());</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">long</span> inode : existingInodes) &#123;</span><br><span class="line">    TailFile tf = reader.getTailFiles().get(inode);</span><br><span class="line">    <span class="keyword">if</span> (tf.needTail()) &#123;</span><br><span class="line">        tailFileProcess(tf, <span class="keyword">true</span>);</span><br><span class="line">&#125; &#125;</span><br></pre></td></tr></table></figure><p>从<code>reader.updateTailFiles()</code>获取需要监控的文件，然后对每一个进行处理，查看最后修改时间，判定是否需要<code>tail</code>，需要<code>tail</code>就<code>tail</code><br>那么进入<code>reader.updateTailFiles()</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (TaildirMatcher taildir : taildirCache) &#123;</span><br><span class="line">    Map&lt;String, String&gt; headers = headerTable.row(taildir.getFileGroup());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (File f : taildir.getMatchingFiles()) &#123;</span><br><span class="line">        <span class="keyword">long</span> inode = getInode(f);</span><br><span class="line">        TailFile tf = tailFiles.get(inode);</span><br><span class="line">        <span class="keyword">if</span> (tf == <span class="keyword">null</span> || !tf.getPath().equals(f.getAbsolutePath())) &#123;</span><br><span class="line">            <span class="keyword">long</span> startPos = skipToEnd ? f.length() : <span class="number">0</span>;</span><br><span class="line">            tf = openFile(f, headers, inode, startPos);</span><br></pre></td></tr></table></figure><p>遍历每一个正则表达式匹配对应的匹配器，每个匹配器去获取匹配的文件！<code>taildir.getMatchingFiles()</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">List&lt;File&gt; <span class="title">getMatchingFiles</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> now = TimeUnit.SECONDS.toMillis(</span><br><span class="line">            TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()));</span><br><span class="line">    <span class="keyword">long</span> currentParentDirMTime = parentDir.lastModified();</span><br><span class="line">    List&lt;File&gt; result;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// calculate matched files if</span></span><br><span class="line">    <span class="comment">// - we don't want to use cache (recalculate every time) OR</span></span><br><span class="line">    <span class="comment">// - directory was clearly updated after the last check OR</span></span><br><span class="line">    <span class="comment">// - last mtime change wasn't already checked for sure</span></span><br><span class="line">    <span class="comment">//   (system clock hasn't passed that second yet)</span></span><br><span class="line">    <span class="keyword">if</span> (!cachePatternMatching ||</span><br><span class="line">            lastSeenParentDirMTime &lt; currentParentDirMTime ||</span><br><span class="line">            !(currentParentDirMTime &lt; lastCheckedTime)) &#123;</span><br><span class="line">        lastMatchedFiles = sortByLastModifiedTime(getMatchingFilesNoCache(isRecursive));</span><br><span class="line">        lastSeenParentDirMTime = currentParentDirMTime;</span><br><span class="line">        lastCheckedTime = now;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> lastMatchedFiles;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到<code>getMatchingFilesNoCache(isRecursive)</code>就是获取匹配的文件的方法，也就是需要修改的方法了！<br>ps：这里的<code>isRecursive</code>是我加的~<br>点进去：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> List&lt;File&gt; <span class="title">getMatchingFilesNoCache</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    List&lt;File&gt; result = Lists.newArrayList();</span><br><span class="line">    <span class="keyword">try</span> (DirectoryStream&lt;Path&gt; stream = Files.newDirectoryStream(parentDir.toPath(), fileFilter)) &#123;</span><br><span class="line">        <span class="keyword">for</span> (Path entry : stream) &#123;</span><br><span class="line">            result.add(entry.toFile());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        logger.error(<span class="string">"I/O exception occurred while listing parent directory. "</span> +</span><br><span class="line">                <span class="string">"Files already matched will be returned. "</span> + parentDir.toPath(), e);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>源码是用了<code>Files.newDirectoryStream(parentDir.toPath(), fileFilter))</code>，将父目录下符合正则表达式的文件都添加到一个迭代器里。（这里还用了<code>try (...)</code>的语法糖）</p><h2 id="找到地方了，开始改！"><a href="#找到地方了，开始改！" class="headerlink" title="找到地方了，开始改！"></a>找到地方了，开始改！</h2><p>我在这个<code>getMatchingFilesNoCache()</code>方法下面下了一个重载的方法, 可增加扩展性：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> List&lt;File&gt; <span class="title">getMatchingFilesNoCache</span><span class="params">(<span class="keyword">boolean</span> recursion)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!recursion) &#123;</span><br><span class="line">        <span class="keyword">return</span> getMatchingFilesNoCache();</span><br><span class="line">    &#125;</span><br><span class="line">    List&lt;File&gt; result = Lists.newArrayList();</span><br><span class="line">    <span class="comment">// 使用非递归的方式遍历文件夹</span></span><br><span class="line">    Queue&lt;File&gt; dirs = <span class="keyword">new</span> ArrayBlockingQueue&lt;&gt;(<span class="number">10</span>);</span><br><span class="line">    dirs.offer(parentDir);</span><br><span class="line">    <span class="keyword">while</span> (dirs.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        File dir = dirs.poll();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            DirectoryStream&lt;Path&gt; stream = Files.newDirectoryStream(dir.toPath(), fileFilter);</span><br><span class="line">            stream.forEach(path -&gt; result.add(path.toFile()));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            logger.error(<span class="string">"I/O exception occurred while listing parent directory. "</span> +</span><br><span class="line">                    <span class="string">"Files already matched will be returned. (recursion)"</span> + parentDir.toPath(), e);</span><br><span class="line">        &#125;</span><br><span class="line">        File[] dirList = dir.listFiles();</span><br><span class="line">        <span class="keyword">assert</span> dirList != <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">for</span> (File f : dirList) &#123;</span><br><span class="line">            <span class="keyword">if</span> (f.isDirectory()) &#123;</span><br><span class="line">                dirs.add(f);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我使用了非递归的方式遍历文件夹，就是树到队列的转换。<br>到这里，核心部分就改完了。接下来要处理这个<code>recursion</code>的参数</p><h2 id="华丽的分割线后，顺腾摸瓜！"><a href="#华丽的分割线后，顺腾摸瓜！" class="headerlink" title="华丽的分割线后，顺腾摸瓜！"></a>华丽的分割线后，顺腾摸瓜！</h2><p>一路改构造方法，添加这个参数，最终参数从哪来呢？<br>flume的source启动时会调用<code>configure</code>方法，将<code>Context</code>中的内容配置进<code>reader</code>等对象中。<br><code>isRecursive = context.getBoolean(RECURSIVE, DEFAULT_RECURSIVE);</code><br><code>context</code>从<code>TaildirSourceConfigurationConstants</code>中获取配置名和默认值</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Whether to support recursion. */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String RECURSIVE = <span class="string">"recursive"</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">boolean</span> DEFAULT_RECURSIVE = <span class="keyword">false</span>;</span><br></pre></td></tr></table></figure><p>这里的<code>recursive</code>也就是flume配置文件里配置项了</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Whether to support recusion</span><br><span class="line">a1.sources.r1.recursive = <span class="keyword">true</span></span><br></pre></td></tr></table></figure><h2 id="大功告成，打包试试！"><a href="#大功告成，打包试试！" class="headerlink" title="大功告成，打包试试！"></a>大功告成，打包试试！</h2><p>执行package将其放在flume的lib下，替换原来的<code>flume-taildir-source***.jar</code><br>启动，测试，成功！</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">a1.sources.r1.type</span> = <span class="string">TAILDIR</span></span><br><span class="line"><span class="meta">a1.sources.r1.cachePatternMatching</span> = <span class="string">false</span></span><br><span class="line"><span class="meta">a1.sources.r1.positionFile</span> = <span class="string">/home/hadoop/app/flume/position/taildir_position.json</span></span><br><span class="line"><span class="meta">a1.sources.r1.filegroups</span> = <span class="string">f1</span></span><br><span class="line"><span class="meta">a1.sources.r1.filegroups.f1</span> = <span class="string">/home/hadoop/data/taildir/.*txt</span></span><br><span class="line"><span class="meta">a1.sources.r1.recursive</span> = <span class="string">false</span></span><br></pre></td></tr></table></figure><p>具体代码见GitHub地址：<a href="https://github.com/yerias/recursion-flume-taildir" target="_blank" rel="noopener">https://github.com/yerias/recursion-flume-taildir</a></p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume源代码二次开发Source&amp;Sink&amp;Interceptor&amp;Channel的事物保证</title>
      <link href="/2018/12/05/flume/5/"/>
      <url>/2018/12/05/flume/5/</url>
      
        <content type="html"><![CDATA[<ol><li>Agent架构</li><li>自定义Source</li><li>自定义Sink</li><li>自定义Interceptor</li><li>Channel的事物保证</li></ol><h2 id="Agent架构"><a href="#Agent架构" class="headerlink" title="Agent架构"></a>Agent架构</h2><p><img src="https://yerias.github.io/flume_img/Agent.jpg" alt="Agent"></p><h2 id="自定义Source"><a href="#自定义Source" class="headerlink" title="自定义Source"></a>自定义Source</h2><p>提示：当不会写的时候，看源码是个不错的选择</p><p>在自定义Flume的组件之前， IDEA需要引入Flume的依赖</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Source的目的是从外部客户端接收数据并将其存储到配置的Channels中。一个Source可以获得它自己的ChannelProcessor的一个实例来连续处理一个Event，该Event在一个Channel本地事务中被提交。</p><p>在异常的情况下，所需的Channel将传播异常，所有Channel将回滚它们的Event，但之前在其他Channel上处理的Event将保持提交。</p><p>自定义Source可以在数据源里直接产生数据，产生的数据你可以定制化(前缀、后缀)</p><p>自定义Source类，参考官网<a href="http://flume.apache.org/releases/content/1.9.0/FlumeDeveloperGuide.html#source" target="_blank" rel="noopener">demo/github</a></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> tunan.hadoop.flume.sources;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.EventDeliveryException;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.PollableSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.event.SimpleEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.source.AbstractSource;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySource</span> <span class="keyword">extends</span> <span class="title">AbstractSource</span> <span class="keyword">implements</span> <span class="title">Configurable</span>, <span class="title">PollableSource</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 自定义属性，前缀和后缀</span></span><br><span class="line">    <span class="keyword">private</span> String prefix;</span><br><span class="line">    <span class="keyword">private</span> String suffix;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 处理Event</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>&#123;</span><br><span class="line">        <span class="comment">// 自定义状态属性</span></span><br><span class="line">        Status status = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 模拟产生数据</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            <span class="comment">// 创建Event</span></span><br><span class="line">            SimpleEvent event = <span class="keyword">new</span> SimpleEvent();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 把数据设置到Body中去，注意header为空</span></span><br><span class="line">            event.setBody((prefix + i + suffix).getBytes());</span><br><span class="line">            <span class="comment">// 开始处理Evetn</span></span><br><span class="line">            getChannelProcessor().processEvent(event);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 成功</span></span><br><span class="line">            status = Status.READY;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            <span class="comment">// 失败，回退</span></span><br><span class="line">            status = Status.BACKOFF;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 返回状态的结果</span></span><br><span class="line">        <span class="keyword">return</span> status;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取Agent中传入的参数信息</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.prefix = context.getString(<span class="string">"prefix"</span>,<span class="string">"TUNAN"</span>);</span><br><span class="line">        <span class="keyword">this</span>.suffix = context.getString(<span class="string">"suffix"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将jar包上传到$FLUME_HOME/lib下，并修改配置文件中的Source，修改Type、添加类中自定义的参数</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 我们在类中自定义的Source参数和类的全限定名</span></span><br><span class="line">a1.sources.r1.type = tunan.hadoop.flume.sources.MySource</span><br><span class="line">a1.sources.r1.prefix = tunan:</span><br><span class="line">a1.sources.r1.suffix = -6639</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--conf /home/hadoop/app/flume/conf \</span><br><span class="line">--conf-file /home/hadoop/app/flume/script/MySource.conf \</span><br><span class="line">--name a1 \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>查看结果</p><p><img src="https://yerias.github.io/flume_img/mysource.jpg" alt="mysource"></p><h2 id="自定义Sink"><a href="#自定义Sink" class="headerlink" title="自定义Sink"></a>自定义Sink</h2><p>提示：当不会写的时候，看源码是个不错的选择</p><p>Sink的目的是从Channel中提取Event并将它们转发到流中的下一个Flume Agent或将它们存储在外部存储库中。</p><p>正如在Flume属性文件中配置的那样，一个Sink仅与一个Channel相关联。</p><p>有一个与每个配置的Sink相关联的SinkRunner实例，当Flume框架调用SinkRunner.start()时，会创建一个新线程来驱动Sink(使用SinkRunner)。这个线程管理Sink的生命周期。Sink需要实现start()和stop()方法，它们是生命周期感知接口的一部分。start()方法应该初始化Sink，并使其处于可以将Event转发到下一个目的地的状态。process()方法应该执行从Channel中提取Event并转发Event的核心处理。stop()方法应该做必要的清理工作(例如释放资源)。</p><p>从Channel拿到数据(Event)，把数据输出我们自定义的Sink中去，架构为：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nc source ==&gt; memory channel ==&gt; MySink</span><br></pre></td></tr></table></figure><p>注意： nc可以保证消息有序，telnet不能保证消息有序</p><p>代码实现：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> tunan.hadoop.flume.sink;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.sink.AbstractSink;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySink</span> <span class="keyword">extends</span> <span class="title">AbstractSink</span> <span class="keyword">implements</span> <span class="title">Configurable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 拿到logger</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger logger =  LoggerFactory.getLogger(MySink<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 属性作为参数的前缀和后缀</span></span><br><span class="line">    <span class="keyword">private</span> String prefix;</span><br><span class="line">    <span class="keyword">private</span> String suffix;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从channel中获取数据发送到目的地</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>&#123;</span><br><span class="line">        Status status;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取channel</span></span><br><span class="line">        Channel ch = getChannel();</span><br><span class="line">        <span class="comment">// 获取事物</span></span><br><span class="line">        Transaction txn = ch.getTransaction();</span><br><span class="line">        <span class="comment">// 开启事物</span></span><br><span class="line">        txn.begin();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 拿到event</span></span><br><span class="line">            Event event;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">do</span> &#123;   <span class="comment">// 一直等待，直到拿到不为空的Event</span></span><br><span class="line">                event = ch.take();</span><br><span class="line">            &#125; <span class="keyword">while</span> ((event == <span class="keyword">null</span>));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// body是个字节数组转换成字符串</span></span><br><span class="line">            String body = <span class="keyword">new</span> String(event.getBody());</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 控制台打印</span></span><br><span class="line">            logger.error(prefix + body + suffix);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 提交事务</span></span><br><span class="line">            txn.commit();</span><br><span class="line">            status = Status.READY;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">            <span class="comment">// 回滚事务</span></span><br><span class="line">            txn.rollback();</span><br><span class="line">            status = Status.BACKOFF;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (t <span class="keyword">instanceof</span> Error) &#123;</span><br><span class="line">                <span class="keyword">throw</span> (Error)t;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            txn.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> status;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从Agent中拿到参数</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.prefix = context.getString(<span class="string">"prefix"</span>,<span class="string">"Tunan"</span>);</span><br><span class="line">        <span class="keyword">this</span>.suffix = context.getString(<span class="string">"suffix"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>jar包上传到$FLUME_HOME\lib下，并修改配置文件中的Sink，修改Type、添加类中自定义的参数</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="comment"># 我们在类中自定义的Sink参数和类的全限定名</span></span></span><br><span class="line">a1.sinks.k1.type = tunan.hadoop.flume.sink.MySink</span><br><span class="line">a1.sinks.k1.prefix = tunan-sink:</span><br><span class="line">a1.sinks.k1.suffix = -flie</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>发送消息</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ nc localhost 44444</span><br><span class="line">1</span><br><span class="line">OK</span><br><span class="line">2</span><br><span class="line">OK</span><br><span class="line">3</span><br><span class="line">OK</span><br><span class="line">..</span><br><span class="line">0</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><p>查看结果</p><p><img src="https://yerias.github.io/flume_img/image-20200502115329892.png" alt=""></p><h2 id="自定义Interceptor"><a href="#自定义Interceptor" class="headerlink" title="自定义Interceptor"></a>自定义Interceptor</h2><p>提示：当不会写的时候，看源码是个不错的选择</p><p>Flume具有修改/删除运行中的Event的能力。这是在拦截器的帮助下完成的。拦截器是实现<code>org.apache.flum .interceptor. interceptor</code>接口的类。拦截器可以根据拦截器开发人员选择的任何标准修改甚至删除Event。</p><p>Flume支持链式的拦截器。这是通过在配置中指定拦截器builder类名列表来实现的。截取程序被指定为source配置中的空白分隔列表。指定拦截器的顺序与调用它们的顺序相同。一个拦截器返回的Event列表被传递到链中的下一个拦截器。</p><p>拦截器可以修改或删除Event。如果拦截器需要删除Event，它只会在其返回的列表中不返回该Event。如果它要删除所有Event，那么它只返回一个空列表。</p><p>需求：Flume接进来的数据都在一起，有些业务线的数据比较重要，单独拉出来，这里自定义拦截器，并配合<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#multiplexing-channel-selector" target="_blank" rel="noopener">Multiplexing Channel Selector</a>将body包含<code>gifshow</code>的数据单独拿出来</p><p><img src="https://yerias.github.io/flume_img/Flume%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8B%A6%E6%88%AA%E5%99%A8.jpg" alt="Flume自定义拦截器"></p><p>代码实现自定义拦截器</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> tunan.hadoop.flume.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Event;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.interceptor.Interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyInterceptor</span> <span class="keyword">implements</span> <span class="title">Interceptor</span> </span>&#123;</span><br><span class="line"><span class="comment">// 自定义List用来处理批量Event</span></span><br><span class="line">    <span class="keyword">private</span>  List&lt;Event&gt; newEvents;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化设置</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        newEvents = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 拦截单个Event</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        Map&lt;String, String&gt; headers = event.getHeaders();</span><br><span class="line">        String body = <span class="keyword">new</span> String(event.getBody());</span><br><span class="line">        <span class="keyword">if</span> (body.contains(<span class="string">"gifshow"</span>)) &#123;</span><br><span class="line">            <span class="comment">// 自定义头信息</span></span><br><span class="line">            headers.put(<span class="string">"type"</span>, <span class="string">"gifshow"</span>);</span><br><span class="line">        <span class="comment">// 自定义头信息</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            headers.put(<span class="string">"type"</span>, <span class="string">"other"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 拦截多个Event处理</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Event&gt; <span class="title">intercept</span><span class="params">(List&lt;Event&gt; events)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 每次进来初始化List</span></span><br><span class="line">        newEvents.clear();</span><br><span class="line">        Iterator&lt;Event&gt; iter = events.iterator();</span><br><span class="line">        <span class="keyword">while</span> (iter.hasNext())&#123;</span><br><span class="line">            Event next = iter.next();</span><br><span class="line">            <span class="comment">// event传递给单个处理，并添加到新的List</span></span><br><span class="line">            newEvents.add(intercept(next));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 返回拦截后的Event</span></span><br><span class="line">        <span class="keyword">return</span> newEvents;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 源码在 HostInterceptor，需要添加一个静态内部类，并且名为Builder</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Builder</span> <span class="keyword">implements</span> <span class="title">Interceptor</span>.<span class="title">Builder</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Interceptor <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> MyInterceptor();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>配置第一个Agent</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 自定义拦截器</span></span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = tunan.hadoop.flume.interceptor.MyIntercepto$Builder</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> multiplexing selector</span></span><br><span class="line">a1.sources.r1.selector.type = multiplexing</span><br><span class="line">a1.sources.r1.selector.header = type</span><br><span class="line"><span class="meta">#</span><span class="bash"> 与自定义拦截器中设置的头信息对应</span></span><br><span class="line">a1.sources.r1.selector.mapping.figshow = c1</span><br><span class="line">a1.sources.r1.selector.mapping.other = c2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop</span><br><span class="line">a1.sinks.k1.port = 4441</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = hadoop</span><br><span class="line">a1.sinks.k2.port = 4442</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure><p>配置第二个Agent</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = hadoop</span><br><span class="line">a1.sources.r1.port = 4441</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>配置第三个Agent</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = hadoop</span><br><span class="line">a1.sources.r1.port = 4442</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>启动Agent，先启动Agent2和Agent3，不然会报错</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--conf /home/hadoop/app/flume/conf \</span><br><span class="line">--conf-file /home/hadoop/app/flume/script/Interceptor_3.conf \</span><br><span class="line">--name a1 \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--conf /home/hadoop/app/flume/conf \</span><br><span class="line">--conf-file /home/hadoop/app/flume/script/Interceptor_2.conf \</span><br><span class="line">--name a1 \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--conf /home/hadoop/app/flume/conf \</span><br><span class="line">--conf-file /home/hadoop/app/flume/script/Interceptor_1.conf \</span><br><span class="line">--name a1 \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>发送消息</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop ~]$ nc localhost 44444</span><br><span class="line">gifshow</span><br><span class="line">OK</span><br><span class="line">aaaaa</span><br><span class="line">OK</span><br><span class="line">aaaagifshow</span><br><span class="line">OK</span><br><span class="line">figshow</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><p>查看结果</p><p><img src="https://yerias.github.io/flume_img/other.jpg" alt=""></p><p><img src="https://yerias.github.io/flume_img/gifshow.jpg" alt=""></p><h2 id="Channel的事物保证"><a href="#Channel的事物保证" class="headerlink" title="Channel的事物保证"></a>Channel的事物保证</h2><p>Transaction interface 是Flume可靠性的基础，所有主要组件(即Source、Sink、Channel)必须使用Flume Transaction。</p><p>Transaction 在连接Channel中实现，连接到Channel的每个Source和Sink都必须获得一个Transaction 对象。</p><p>Source使用ChannelProcessor来管理Transaction ，而Sinks 通过其配置的Channel显式地管理Transaction。</p><p>每个阶段的操作Event(将其放入Channel中)或提取Event(将其从Channel中取出)的操作在活动必须在Transaction中完成。</p><p>![Channel Trancastion](<a href="https://yerias.github.io/flume_img/Channel">https://yerias.github.io/flume_img/Channel</a> Trancastion.jpg)</p><p>所有的事物管理由MemoryChannel类来做，具体可以查看源码</p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume源代码二次开发debug</title>
      <link href="/2018/12/04/flume/4/"/>
      <url>/2018/12/04/flume/4/</url>
      
        <content type="html"><![CDATA[<h3 id="1-下载安装Flume，配置flume"><a href="#1-下载安装Flume，配置flume" class="headerlink" title="1.下载安装Flume，配置flume"></a>1.下载安装Flume，配置flume</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop conf]$ cat exec_memory_kafka.conf</span><br><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the custom <span class="built_in">exec</span> <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = com.ruozedata.prewarning.ExecSourceJSON</span><br><span class="line">a1.sources.r1.command = tail -F /home/hadoop/flume-test-log hadoop-cmf-hdfs-NAMENODE-ruozedata001.log.out</span><br><span class="line">a1.sources.r1.hostname = hadoop</span><br><span class="line">a1.sources.r1.servicename = namenode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">a1.sinks.k1.kafka.topic = PREWARNING</span><br><span class="line">a1.sinks.k1.kafka.bootstrap.servers = hadoop:9090,hadoop:9091,hadoop:9092</span><br><span class="line">a1.sinks.k1.kafka.flumeBatchSize = 6000</span><br><span class="line">a1.sinks.k1.kafka.producer.acks = all</span><br><span class="line">a1.sinks.k1.kafka.producer.linger.ms = 1</span><br><span class="line">a1.sinks.ki.kafka.producer.compression.type = snappy</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.keep-alive = 90</span><br><span class="line">a1.channels.c1.capacity = 2000000</span><br><span class="line">a1.channels.c1.transactionCapacity = 6000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h3 id="2-拉取源码拉取源码到idea"><a href="#2-拉取源码拉取源码到idea" class="headerlink" title="2.拉取源码拉取源码到idea"></a>2.拉取源码拉取源码到idea</h3><p>地址：<a href="https://github.com/apache/flume" target="_blank" rel="noopener">https://github.com/apache/flume</a></p><h3 id="3-将自己写的代码放入到源码中"><a href="#3-将自己写的代码放入到源码中" class="headerlink" title="3.将自己写的代码放入到源码中"></a>3.将自己写的代码放入到源码中</h3><p><img src="https://yerias.github.io/flume_img/flume-debug.png" alt="flume-debug"></p><h3 id="4-自己写的代码打包后放到远端服务器的lib目录下"><a href="#4-自己写的代码打包后放到远端服务器的lib目录下" class="headerlink" title="4.自己写的代码打包后放到远端服务器的lib目录下"></a>4.自己写的代码打包后放到远端服务器的lib目录下</h3><p><img src="https://yerias.github.io/flume_img/flume-jar.png" alt="flume-jar"></p><p>注意：除了这种方法，还可以直接修改源码，然后一起打包上传到服务器上</p><h3 id="5-启动flume服务"><a href="#5-启动flume服务" class="headerlink" title="5.启动flume服务"></a>5.启动flume服务</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nohup  /home/hadoop/app/apache-flume-1.7.0-bin/bin/flume-ng agent \</span><br><span class="line">-c /home/hadoop/app/apache-flume-1.7.0-bin/conf \</span><br><span class="line">-f /home/hadoop/MonitoringProject/exec_memory_kafka.properties \</span><br><span class="line">-n a1 \</span><br><span class="line">-Dflume.root.logger=DEBUG,console -Xmx20m -Xdebug -Xrunjdwp:transport=dt_socket,address=8000,server=y,suspend=y</span><br></pre></td></tr></table></figure><h3 id="6-配置启动应用"><a href="#6-配置启动应用" class="headerlink" title="6.配置启动应用"></a>6.配置启动应用</h3><p><img src="https://yerias.github.io/flume_img/%E9%85%8D%E7%BD%AE.png" alt="配置"></p><p>打上断点，启动后即可进入断点</p><p><img src="https://yerias.github.io/flume_img/%E6%9F%A5%E7%9C%8B.png" alt="查看"></p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume的Channel选择器&amp;Flume的Sink选择器&amp;Channel的两种类型</title>
      <link href="/2018/12/03/flume/3/"/>
      <url>/2018/12/03/flume/3/</url>
      
        <content type="html"><![CDATA[<h2 id="Flume的Channel选择器"><a href="#Flume的Channel选择器" class="headerlink" title="Flume的Channel选择器"></a>Flume的Channel选择器</h2><p>Flume的Channel选择器有<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#replicating-channel-selector-default" target="_blank" rel="noopener">Replicating Channel Selector (default)</a>和<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#multiplexing-channel-selector" target="_blank" rel="noopener">Multiplexing Channel Selector</a>，作用分别是复制和多路分发，默认用的复制</p><p>下面我们用两个案例分别实现Replicating Channel Selector和Multiplexing Channel Selector</p><h3 id="Replicating-Channel-Selector"><a href="#Replicating-Channel-Selector" class="headerlink" title="Replicating Channel Selector"></a>Replicating Channel Selector</h3><ol><li><p>需要实现的功能</p><p><img src="https://yerias.github.io/flume_img/ReplicatingChannel.png" alt="ReplicatingChannel"></p></li><li><p>代码实现</p></li></ol><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2#两个sink</span><br><span class="line">a1.channels = c1 c2 #两个channel</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat#nc输入</span><br><span class="line">a1.sources.r1.bind = 0.0.0.0</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k2.type = logger#第一个sink</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = hdfs#第二个sink</span><br><span class="line">a1.sinks.k1.hdfs.path = /flume/%y-%m-%d/%H/%M</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = replicating-</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">a1.sinks.k1.hdfs.fileType=DataStream</span><br><span class="line"></span><br><span class="line">a1.sources.r1.selector.type = replicating#选择器</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory#第一个memory</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type = memory#第二个memory</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1 c2//source连上两个channel</span><br><span class="line">a1.sinks.k1.channel = c1//channel1连上sink1</span><br><span class="line">a1.sinks.k2.channel = c2//channel2连上sink2</span><br></pre></td></tr></table></figure><ol start="3"><li>执行命令</li></ol><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/nc-replicating-logger_and_hdfs.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><ol start="4"><li><p>发送消息</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">telnet localhost 44444</span><br></pre></td></tr></table></figure><p>结果：成功</p></li></ol><h3 id="Multiplexing-Channel-Selector"><a href="#Multiplexing-Channel-Selector" class="headerlink" title="Multiplexing Channel Selector"></a>Multiplexing Channel Selector</h3><ol><li><p>需要实现的功能</p><p><img src="https://yerias.github.io/flume_img/multiplexingChannel.jpg" alt="multiplexingChannel"></p><p>数据从Source到Channel中间会经过一个<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#static-interceptor" target="_blank" rel="noopener">拦截器</a>，拦截器中的Key和Value参数被添加到了所有的Event上，在经过<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#multiplexing-channel-selector" target="_blank" rel="noopener">选择器</a>的时候，会根据拦截器中的Event所带的Value值的不同发送到不同的Sink</p></li><li><p>代码实现</p><p>nc-memory-arvo1.conf</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">a1.sources.r1.port = <span class="number">44441</span></span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun</span><br><span class="line">a1.sinks.k1.port = <span class="number">55555</span></span><br><span class="line"></span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = <span class="keyword">static</span></span><br><span class="line">a1.sources.r1.interceptors.i1.key = state</span><br><span class="line">a1.sources.r1.interceptors.i1.value = UA</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>nc-memory-arvo2.conf</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">a1.sources.r1.port = <span class="number">44442</span></span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun</span><br><span class="line">a1.sinks.k1.port = <span class="number">55555</span></span><br><span class="line"></span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = <span class="keyword">static</span></span><br><span class="line">a1.sources.r1.interceptors.i1.key = state</span><br><span class="line">a1.sources.r1.interceptors.i1.value = UB</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>vim nc-memory-arvo3.conf</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">a1.sources.r1.port = <span class="number">44443</span></span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun</span><br><span class="line">a1.sinks.k1.port = <span class="number">55555</span></span><br><span class="line"></span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = <span class="keyword">static</span></span><br><span class="line">a1.sources.r1.interceptors.i1.key = state</span><br><span class="line">a1.sources.r1.interceptors.i1.value = UC</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>vim avro-memory-multi.conf</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2 k3</span><br><span class="line">a1.channels = c1 c2 c3</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = <span class="number">55555</span></span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = logger</span><br><span class="line"></span><br><span class="line">a1.sinks.k3.type = hdfs</span><br><span class="line">a1.sinks.k3.hdfs.path = /flume/%y-%m-%d/%H/%M</span><br><span class="line">a1.sinks.k3.hdfs.filePrefix = multiplexing-</span><br><span class="line">a1.sinks.k3.hdfs.useLocalTimeStamp = <span class="keyword">true</span></span><br><span class="line">a1.sinks.k3.hdfs.fileType=DataStream</span><br><span class="line">a1.sinks.k3.hdfs.writeFormat=Text</span><br><span class="line"></span><br><span class="line">a1.sources.r1.selector.type = multiplexing</span><br><span class="line">a1.sources.r1.selector.header = state</span><br><span class="line">a1.sources.r1.selector.mapping.UA = c1</span><br><span class="line">a1.sources.r1.selector.mapping.UB = c2</span><br><span class="line">a1.sources.r1.selector.<span class="keyword">default</span> = c3</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c3.type = memory</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1 c2 c3</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br><span class="line">a1.sinks.k3.channel = c3</span><br></pre></td></tr></table></figure></li><li><p>执行命令</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/nc-memory-arvo1.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/nc-memory-arvo2.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/nc-memory-arvo3.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/avro-memory-multi.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li><li><p>发送消息</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">telnet aliyun <span class="number">44441</span></span><br><span class="line">telnet aliyun <span class="number">44442</span></span><br><span class="line">telnet aliyun <span class="number">44443</span></span><br></pre></td></tr></table></figure><p>结果：成功</p></li></ol><h2 id="Flume的Sink选择器"><a href="#Flume的Sink选择器" class="headerlink" title="Flume的Sink选择器"></a>Flume的Sink选择器</h2><p>Flume的Sink选择器常用的有两种：<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#failover-sink-processor" target="_blank" rel="noopener">Failover Sink Processor</a>和<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#load-balancing-sink-processor" target="_blank" rel="noopener">Load balancing Sink Processor</a></p><h3 id="Failover-Sink-Processor"><a href="#Failover-Sink-Processor" class="headerlink" title="Failover Sink Processor"></a>Failover Sink Processor</h3><p>Failover Sink Processor可以在Agent中的Sink端做一个类似于灾备的Sink组，官方文档中介绍Failover Sink Processor维护一个按优先级排序的Sink列表，确保只要有一个可用的Sink，就会处理Event。</p><p>Failover Sink Processor的工作方式是将多个Sink组成Sinks组，他们有一个优先级的顺序关系，优先级大的先被激活。如果Sink在发送Event时失败，则下一个具有最高优先级的Sink将会用于发送Event。例如，优先级为100的接收器在优先级为80的接收器之前被激活。如果没有指定优先级，则根据在配置中指定Sink的顺序确定优先级。</p><table><thead><tr><th align="left">Property Name</th><th align="left">Default</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left"><strong>sinks</strong></td><td align="left">–</td><td align="left">Space-separated list of sinks that are participating in the group</td></tr><tr><td align="left"><strong>processor.type</strong></td><td align="left"><code>default</code></td><td align="left">The component type name, needs to be <code>failover</code></td></tr><tr><td align="left"><strong>processor.priority.</strong></td><td align="left">–</td><td align="left">Priority value. <sinkName> must be one of the sink instances associated with the current sink group A higher priority value Sink gets activated earlier. A larger absolute value indicates higher priority</td></tr><tr><td align="left">processor.maxpenalty</td><td align="left">30000</td><td align="left">The maximum backoff period for the failed Sink (in millis)</td></tr></tbody></table><ol><li><p>需要实现的功能</p><p>​    ![Failover Sink Processor](<a href="https://yerias.github.io/flume_img/Failover">https://yerias.github.io/flume_img/Failover</a> Sink Processor.jpg)</p><p>Agent1发送Event，如果Sink组中的任意一个Sink接收Event失败，其他的Sink激活继续接收</p></li><li><p>代码实现</p><p>nc-memory-avro.conf</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinkgroups.g1.processor.type = failover</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k1 = 5</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k2 = 10</span><br><span class="line">a1.sinkgroups.g1.processor.maxpenalty = 10000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun</span><br><span class="line">a1.sinks.k1.port = 55551</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = aliyun</span><br><span class="line">a1.sinks.k2.port = 55552</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure><p>avro1-memory-logger.conf </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = 55551</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>avro2-memory-logger.conf </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = 55552</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>执行命令</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/failover/nc-memory-avro.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/failover/avro1-memory-logger.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/failover/avro2-memory-logger.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li><li><p>发送消息</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">telnet aliyun 44444</span><br></pre></td></tr></table></figure><p>结果：成功</p></li></ol><h3 id="Load-balancing-Sink-Processor"><a href="#Load-balancing-Sink-Processor" class="headerlink" title="Load balancing Sink Processor"></a>Load balancing Sink Processor</h3><p>Load balancing Sink Processor提供了在多个Sink 上实现负载均衡的能力。它维护一个活动Sink的索引列表，发送的Event必须分布在这个列表上。实现支持通过round_robin或random分配负载。默认为round_robin类型，但是可以通过配置覆盖。自定义选择机制是通过继承AbstractSinkSelector的自定义类来支持的。</p><p>调用时，选择器使用其配置的选择机制选择下一个Sink 调用它。对于round_robin和random，如果选择的Sink 不能传递Event，处理器将通过其配置的选择机制选择下一个可用的Sink 。这种方法不会将失败的Sink加入黑名单，而是继续乐观地尝试每个可用的Sink。如果所有的Sink调用都导致失败，则整个程序运行失败</p><p>如果启用了backoff，Sink处理器将把失败的Sink列入黑名单，超过给定的时间后删除他们。当超时结束时，如果Sink仍然没有响应，超时将以指数方式增加，以避免在没有响应的Sink上陷入长时间的等待。禁用此功能后，在循环中，所有失败的Sink负载将按行传递到下一个Sink，因此不是均匀的。</p><table><thead><tr><th align="left">Property Name</th><th align="left">Default</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left"><strong>processor.sinks</strong></td><td align="left">–</td><td align="left">Space-separated list of sinks that are participating in the group</td></tr><tr><td align="left"><strong>processor.type</strong></td><td align="left"><code>default</code></td><td align="left">The component type name, needs to be <code>load_balance</code></td></tr><tr><td align="left">processor.backoff</td><td align="left">false</td><td align="left">Should failed sinks be backed off exponentially.</td></tr><tr><td align="left">processor.selector</td><td align="left"><code>round_robin</code></td><td align="left">Selection mechanism. Must be either <code>round_robin</code>, <code>random</code> or FQCN of custom class that inherits from <code>AbstractSinkSelector</code></td></tr><tr><td align="left">processor.selector.maxTimeOut</td><td align="left">30000</td><td align="left">Used by backoff selectors to limit exponential backoff (in milliseconds)</td></tr></tbody></table><ol><li><p>需要实现的功能</p><p>![Load balancing Sink Processor](<a href="https://yerias.github.io/flume_img/Load">https://yerias.github.io/flume_img/Load</a> balancing Sink Processor.jpg)</p><p>Agent1发送Event，Sink组中的每个Sink根据配置的选择器机制选择发送到哪一个Sink</p></li><li><p>代码实现</p><p>nc-memory-avro.conf</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">a1.sinkgroups.g1.processor.backoff = true</span><br><span class="line">a1.sinkgroups.g1.processor.selector = random</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun</span><br><span class="line">a1.sinks.k1.port = 55551</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = aliyun</span><br><span class="line">a1.sinks.k2.port = 55552</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure><p>avro1-memory-logger.conf </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = 55551</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>avro2-memory-logger.conf</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = 55552</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>执行命令</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/load_balancing/nc-memory-avro.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/load_balancing/avro1-memory-logger.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/load_balancing/avro2-memory-logger.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li><li><p>发送消息</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">telnet aliyun 44444</span><br></pre></td></tr></table></figure><p>结果：成功</p></li></ol><h2 id="Channel的两种类型"><a href="#Channel的两种类型" class="headerlink" title="Channel的两种类型"></a>Channel的两种类型</h2><p>Channel有File和Memory两种类型，Memory的特点是使用内存，速度快，但是安全性没有保障；File的特点是数据都会写进文件，速度慢，但是安全性高。</p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>案例&amp;Flume单源单出口&amp;Flume单源多出口&amp;Flume多源单出口</title>
      <link href="/2018/12/02/flume/2/"/>
      <url>/2018/12/02/flume/2/</url>
      
        <content type="html"><![CDATA[<h2 id="安装地址"><a href="#安装地址" class="headerlink" title="安装地址"></a>安装地址</h2><ol><li><p>Flume官网地址</p><p><a href="http://flume.apache.org/" target="_blank" rel="noopener">http://flume.apache.org/</a></p></li><li><p>文档查看地址</p><p><a href="http://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="noopener">http://flume.apache.org/FlumeUserGuide.html</a></p></li><li><p>下载地址</p><p><a href="http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.16.2-src.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.16.2-src.tar.gz</a></p></li></ol><h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><ol><li><p>将apache-flume-1.7.0-bin.tar.gz上传到linux的/opt/software目录下</p></li><li><p>解压apache-flume-1.7.0-bin.tar.gz到/opt/module/目录下</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun software]$ tar -zxf apache-flume-1.7.0-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li><li><p>修改apache-flume-1.7.0-bin的名称为flume</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun module]$ mv apache-flume-1.7.0-bin flume</span><br></pre></td></tr></table></figure></li><li><p>将flume/conf下的flume-env.sh.template文件修改为flume-env.sh，并配置flume-env.sh文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun conf]$ mv flume-env.sh.template flume-env.sh</span><br><span class="line">[aliyun@aliyun conf]$ vi flume-env.sh</span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure></li></ol><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><ol><li><p>安装netcat工具</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun software]$ sudo yum install -y nc</span><br></pre></td></tr></table></figure></li><li><p>判断44444端口是否被占用</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume-telnet]$ sudo netstat -tunlp | grep 44444</span><br></pre></td></tr></table></figure></li><li><p>创建Flume Agent配置文件flume-netcat-logger.conf</p><p>在flume目录下创建job文件夹并进入job文件夹。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ mkdir job</span><br><span class="line">[aliyun@aliyun flume]$ cd job/</span><br></pre></td></tr></table></figure></li><li><p>在job文件夹下创建Flume Agent配置文件flume-netcat-logger.conf。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ vim flume-netcat-logger.conf</span><br></pre></td></tr></table></figure></li><li><p>添加内容如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent    #a1表示agent的名称</span><br><span class="line">a1.sources = r1#r1表示a1的输入源</span><br><span class="line">a1.sinks = k1#k1表示a1的输出目的地</span><br><span class="line">a1.channels = c1#c1表示a1的缓冲区</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat#表示a1的输入源类型为netcat端口类型</span><br><span class="line">a1.sources.r1.bind = localhost#表示a1的监听的主机</span><br><span class="line">a1.sources.r1.port = 44444#表示a1的监听的端口号</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger#表示a1的输出目的地是控制台logger类型</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory#表示a1的channel类型是memory内存型</span><br><span class="line">a1.channels.c1.capacity = 1000#表示a1的channel总容量1000个event</span><br><span class="line">a1.channels.c1.transactionCapacity = 10#表示a1的channel传输时收集到了100条event以后再去提交事务</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1#将r1和c1连接起来</span><br><span class="line">a1.sinks.k1.channel = c1#将k1和c1连接起来</span><br></pre></td></tr></table></figure></li><li><p>先开启flume监听端口</p><p>第一种写法：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>第二种写法：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ bin/flume-ng agent -c conf/ -n a1 –f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>参数说明：</p><p>​        <code>--conf conf/：</code>表示配置文件存储在conf/目录</p><p>​        <code>--name a1：</code>表示给agent起名为a1</p><pre><code>`--conf-file job/flume-netcat.conf ：`flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。</code></pre><p>​        <code>--Dflume.root.logger==INFO,console ：</code>-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error。</p></li><li><p>使用netcat工具向本机的44444端口发送内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun ~]$ nc localhost 44444</span><br><span class="line">hello </span><br><span class="line">aliyun</span><br></pre></td></tr></table></figure></li><li><p>在Flume监听页面观察接收数据情况</p></li></ol><h2 id="实时读取本地文件到HDFS案例"><a href="#实时读取本地文件到HDFS案例" class="headerlink" title="实时读取本地文件到HDFS案例"></a>实时读取本地文件到HDFS案例</h2><p>案例需求：实时监控Hive日志，并上传到HDFS中</p><p><img src="https://yerias.github.io/flume_img/%E5%8D%95%E6%BA%90%E5%8D%95%E5%87%BA%E5%8F%A31.jpg" alt="单源单出口1"></p><ol><li><p>给Flume的lib目录下添加aliyun相关的jar包</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">commons-configuration-1.6.jar</span><br><span class="line">aliyun-auth-2.7.2.jar</span><br><span class="line">aliyun-common-2.7.2.jar</span><br><span class="line">aliyun-hdfs-2.7.2.jar</span><br><span class="line">commons-io-2.4.jar</span><br><span class="line">htrace-core-3.1.0-incubating.jar</span><br></pre></td></tr></table></figure></li><li><p>创建flume-file-hdfs.conf文件</p><p>创建文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ touch flume-file-hdfs.conf</span><br></pre></td></tr></table></figure><p>注：要想读取Linux系统中的文件，就得按照Linux命令的规则执行命令。由于Hive日志在Linux系统中所以读取文件的类型选择：exec即execute执行的意思。表示执行Linux命令来读取文件。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ vim flume-file-hdfs.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources = r2#定义source</span><br><span class="line">a2.sinks = k2#定义sink</span><br><span class="line">a2.channels = c2#定义channels</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a2.sources.r2.type = exec#定义source类型为exec可执行命令的</span><br><span class="line">a2.sources.r2.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line">a2.sources.r2.shell = /bin/bash -c#执行shell脚本的绝对路径</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k2.type = hdfs</span><br><span class="line">a2.sinks.k2.hdfs.path = hdfs://aliyun:9000/flume/%Y%m%d/%H</span><br><span class="line">a2.sinks.k2.hdfs.filePrefix = logs-#上传文件的前缀</span><br><span class="line">a2.sinks.k2.hdfs.round = true#是否按照时间滚动文件夹</span><br><span class="line">a2.sinks.k2.hdfs.roundValue = 1#多少时间单位创建一个新的文件夹</span><br><span class="line">a2.sinks.k2.hdfs.roundUnit = hour#重新定义时间单位</span><br><span class="line">a2.sinks.k2.hdfs.useLocalTimeStamp = true#是否使用本地时间戳</span><br><span class="line">a2.sinks.k2.hdfs.batchSize = 1000#积攒多少个Event才flush到HDFS一次</span><br><span class="line">a2.sinks.k2.hdfs.fileType = DataStream#设置文件类型，可支持压缩</span><br><span class="line">a2.sinks.k2.hdfs.rollInterval = 60#多久生成一个新的文件</span><br><span class="line">a2.sinks.k2.hdfs.rollSize = 134217700#设置每个文件的滚动大小</span><br><span class="line">a2.sinks.k2.hdfs.rollCount = 0#文件的滚动与Event数量无关</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a2.channels.c2.type = memory</span><br><span class="line">a2.channels.c2.capacity = 1000</span><br><span class="line">a2.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r2.channels = c2</span><br><span class="line">a2.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：</p><p>对于所有与时间相关的转义序列，Event Header中必须存在以 “timestamp”的key（除非hdfs.useLocalTimeStamp设置为true，此方法会使用TimestampInterceptor自动添加timestamp）。</p></li><li><p>执行监控配置</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/flume-file-hdfs.conf</span><br></pre></td></tr></table></figure></li><li><p>开启aliyun和Hive并操作Hive产生日志</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun aliyun-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[aliyun@aliyun103 aliyun-2.7.2]$ sbin/start-yarn.sh</span><br><span class="line"></span><br><span class="line">[aliyun@aliyun hive]$ bin/hive</span><br><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure></li><li><p>在HDFS上查看文件。</p></li></ol><h2 id="实时读取目录文件到HDFS案例"><a href="#实时读取目录文件到HDFS案例" class="headerlink" title="实时读取目录文件到HDFS案例"></a>实时读取目录文件到HDFS案例</h2><p>案例需求：使用Flume监听整个目录的文件</p><p><img src="https://yerias.github.io/flume_img/%E5%8D%95%E6%BA%90%E5%8D%95%E5%87%BA%E5%8F%A32.jpg" alt="单源单出口2"></p><ol><li><p>创建配置文件flume-dir-hdfs.conf</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ touch flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure></li><li><p>打开文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ vim flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure></li><li><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a3.sources = r3#定义sources</span><br><span class="line">a3.sinks = k3#定义sink</span><br><span class="line">a3.channels = c3#定义channel</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a3.sources.r3.type = spooldir#定义souce类型为目录</span><br><span class="line">a3.sources.r3.spoolDir = /opt/module/flume/upload#定义监控目录</span><br><span class="line">a3.sources.r3.fileSuffix = .COMPLETED#定义文件上传完的后缀</span><br><span class="line">a3.sources.r3.fileHeader = true#是否有文件头</span><br><span class="line">a3.sources.r3.ignorePattern = ([^ ]*\.tmp)#忽略所有以.tmp结尾的文件，不上传</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a3.sinks.k3.type = hdfs#sink类型为hdfs</span><br><span class="line">a3.sinks.k3.hdfs.pat=hdfs://aliyun:9000/flume/upload/%Y%m%d/%H  #文件上传到hdfs的路径</span><br><span class="line"></span><br><span class="line">a3.sinks.k3.hdfs.filePrefix = upload-#上传文件到hdfs的前缀</span><br><span class="line">a3.sinks.k3.hdfs.round = true#是否按照时间滚动文件夹</span><br><span class="line">a3.sinks.k3.hdfs.roundValue = 1#多少时间单位创建一个新的文件夹</span><br><span class="line">a3.sinks.k3.hdfs.roundUnit = hour#重新定义时间单位</span><br><span class="line">a3.sinks.k3.hdfs.useLocalTimeStamp = true#是否使用本地时间戳</span><br><span class="line">a3.sinks.k3.hdfs.batchSize = 100#积攒多少个Event才flush到HDFS一次</span><br><span class="line">a3.sinks.k3.hdfs.fileType = DataStream#设置文件类型，可支持压缩</span><br><span class="line">a3.sinks.k3.hdfs.rollInterval = 60#多久生成一个新的文件</span><br><span class="line">a3.sinks.k3.hdfs.rollSize = 134217700#设置每个文件的滚动大小大概是128M</span><br><span class="line">a3.sinks.k3.hdfs.rollCount = 0#文件的滚动与Event数量无关</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a3.channels.c3.type = memory</span><br><span class="line">a3.channels.c3.capacity = 1000</span><br><span class="line">a3.channels.c3.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a3.sources.r3.channels = c3</span><br><span class="line">a3.sinks.k3.channel = c3</span><br></pre></td></tr></table></figure></li><li><p>启动监控文件夹命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure><p>说明： 在使用Spooling Directory Source时</p><ol><li>不要在监控目录中创建并持续修改文件</li><li>上传完成的文件会以.COMPLETED结尾</li><li>被监控文件夹每500毫秒扫描一次文件变动</li></ol></li><li><p>向upload文件夹中添加文件</p><p>在/opt/module/flume目录下创建upload文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ mkdir upload</span><br></pre></td></tr></table></figure><p>向upload文件夹中添加文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun upload]$ touch aliyun.txt</span><br><span class="line">[aliyun@aliyun upload]$ touch aliyun.tmp</span><br><span class="line">[aliyun@aliyun upload]$ touch aliyun.log</span><br></pre></td></tr></table></figure></li><li><p>查看HDFS上的数据</p></li><li><p>等待1s，再次查询upload文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun upload]$ ll</span><br><span class="line">总用量 0</span><br><span class="line">-rw-rw-r--. 1 aliyun aliyun 0 5月  20 22:31 aliyun.log.COMPLETED</span><br><span class="line">-rw-rw-r--. 1 aliyun aliyun 0 5月  20 22:31 aliyun.tmp</span><br><span class="line">-rw-rw-r--. 1 aliyun aliyun 0 5月  20 22:31 aliyun.txt.COMPLETED</span><br></pre></td></tr></table></figure></li></ol><h2 id="单数据源多出口案例-选择器"><a href="#单数据源多出口案例-选择器" class="headerlink" title="单数据源多出口案例(选择器)"></a>单数据源多出口案例(选择器)</h2><p>案例需求：使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。同时Flume-1将变动内容传递给Flume-3，Flume-3负责输出到Local FileSystem。</p><p><img src="https://yerias.github.io/flume_img/%E5%8D%95%E6%BA%90%E5%A4%9A%E5%87%BA%E5%8F%A31.jpg" alt="单源多出口1"></p><ol><li><p>准备工作</p><p>在/opt/module/flume/job目录下创建group1文件夹</p><p><code>[hadoop@aliyun102 job]$ cd group1/</code></p><p>在/opt/module/datas/目录下创建flume3文件夹</p><p><code>[hadoop@aliyun102 datas]$ mkdir flume3</code></p></li><li><p>创建flume-file-flume.conf</p><p>配置1个接收日志文件的source和两个channel、两个sink，分别输送给flume-flume-hdfs和flume-flume-dir。</p><p>创建配置文件并打开</p><p><code>[hadoop@aliyun102 group1]$ touch flume-file-flume.conf</code></p><p><code>[hadoop@aliyun102 group1]$ vim flume-file-flume.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"></span><br><span class="line"># 将数据流复制给所有channel</span><br><span class="line">a1.sources.r1.selector.type = replicating</span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"> </span><br><span class="line"># Describe the sink</span><br><span class="line"># sink端的avro是一个数据发送者</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun102 </span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = aliyun102</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure><p>注：Avro是由aliyun创始人Doug Cutting创建的一种语言无关的数据序列化和RPC框架。</p><p>注：RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。</p></li><li><p>创建flume-flume-hdfs.conf</p><p>配置上级Flume输出的Source，输出是到HDFS的Sink。</p><p>创建配置文件并打开</p><p><code>[hadoop@aliyun102 group1]$ touch flume-flume-hdfs.conf</code></p><p><code>[hadoop@aliyun102 group1]$ vim flume-flume-hdfs.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line"># source端的avro是一个数据接收服务</span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = aliyun102</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k1.type = hdfs</span><br><span class="line">a2.sinks.k1.hdfs.path = hdfs://aliyun102:9000/flume2/%Y%m%d/%H</span><br><span class="line">#上传文件的前缀</span><br><span class="line">a2.sinks.k1.hdfs.filePrefix = flume2-</span><br><span class="line">#是否按照时间滚动文件夹</span><br><span class="line">a2.sinks.k1.hdfs.round = true</span><br><span class="line">#多少时间单位创建一个新的文件夹</span><br><span class="line">a2.sinks.k1.hdfs.roundValue = 1</span><br><span class="line">#重新定义时间单位</span><br><span class="line">a2.sinks.k1.hdfs.roundUnit = hour</span><br><span class="line">#是否使用本地时间戳</span><br><span class="line">a2.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">#积攒多少个Event才flush到HDFS一次</span><br><span class="line">a2.sinks.k1.hdfs.batchSize = 100</span><br><span class="line">#设置文件类型，可支持压缩</span><br><span class="line">a2.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">#多久生成一个新的文件</span><br><span class="line">a2.sinks.k1.hdfs.rollInterval = 600</span><br><span class="line">#设置每个文件的滚动大小大概是128M</span><br><span class="line">a2.sinks.k1.hdfs.rollSize = 134217700</span><br><span class="line">#文件的滚动与Event数量无关</span><br><span class="line">a2.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>创建flume-flume-dir.conf</p><p>配置上级Flume输出的Source，输出是到本地目录的Sink。</p><p>创建配置文件并打开</p><p><code>[hadoop@aliyun102 group1]$ touch flume-flume-dir.conf</code></p><p><code>[hadoop@aliyun102 group1]$ vim flume-flume-dir.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = aliyun102</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a3.sinks.k1.type = file_roll</span><br><span class="line">a3.sinks.k1.sink.directory = /opt/module/data/flume3</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure><p>提示：输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录。</p></li><li><p>执行配置文件</p><p>分别开启对应配置文件：flume-flume-dir，flume-flume-hdfs，flume-file-flume。</p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group1/flume-flume-dir.conf</code></p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group1/flume-flume-hdfs.conf</code></p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group1/flume-file-flume.conf</code></p></li><li><p>启动aliyun和Hive</p><p><code>[hadoop@aliyun102 aliyun-2.7.2]$ sbin/start-dfs.sh</code></p><p><code>[hadoop@aliyun103 aliyun-2.7.2]$ sbin/start-yarn.sh</code></p><p><code>[hadoop@aliyun102 hive]$ bin/hive</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure></li><li><p>检查HDFS上数据</p></li><li><p>检查/opt/module/datas/flume3目录中数据</p><p><code>[hadoop@aliyun102 flume3]$ ll</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">总用量 8</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 5942 5月 22 00:09 1526918887550-3</span><br></pre></td></tr></table></figure></li></ol><h2 id="单数据源多出口案例-Sink组"><a href="#单数据源多出口案例-Sink组" class="headerlink" title="单数据源多出口案例(Sink组)"></a>单数据源多出口案例(Sink组)</h2><p>案例需求：使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。同时Flume-1将变动内容传递给Flume-3，Flume-3也负责存储到HDFS </p><p><img src="https://yerias.github.io/flume_img/%E5%8D%95%E6%BA%90%E5%A4%9A%E5%87%BA%E5%8F%A32.jpg" alt="单源多出口2"></p><ol><li><p>准备工作</p><p>在/opt/module/flume/job目录下创建group2文件夹</p><p><code>[hadoop@aliyun102 job]$ cd group2/</code></p></li><li><p>创建flume-netcat-flume.conf</p><p>配置1个接收日志文件的source和1个channel、两个sink，分别输送给flume-flume-console1和flume-flume-console2。</p><p>创建配置文件并打开</p><p><code>[hadoop@aliyun102 group2]$ touch flume-netcat-flume.conf</code></p><p><code>[hadoop@aliyun102 group2]$ vim flume-netcat-flume.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">a1.sinkgroups.g1.processor.backoff = true</span><br><span class="line">a1.sinkgroups.g1.processor.selector = round_robin</span><br><span class="line">a1.sinkgroups.g1.processor.selector.maxTimeOut=10000</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun102</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = aliyun102</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure><p>注：Avro是由aliyun创始人Doug Cutting创建的一种语言无关的数据序列化和RPC框架。</p><p>注：RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。</p></li><li><p>创建flume-flume-console1.conf</p><p>配置上级Flume输出的Source，输出是到本地控制台。</p><p>创建配置文件并打开</p><p><code>[hadoop@aliyun102 group2]$ touch flume-flume-console1.conf</code></p><p><code>[hadoop@aliyun102 group2]$ vim flume-flume-console1.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = aliyun102</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>创建flume-flume-console2.conf</p><p>配置上级Flume输出的Source，输出是到本地控制台。</p><p>创建配置文件并打开</p><p><code>[hadoop@aliyun102 group2]$ touch flume-flume-console2.conf</code></p><p><code>[hadoop@aliyun102 group2]$ vim flume-flume-console2.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = aliyun102</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure></li><li><p>执行配置文件</p><p>分别开启对应配置文件：flume-flume-console2，flume-flume-console1，flume-netcat-flume。</p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group2/flume-flume-console2.conf -Dflume.root.logger=INFO,console</code></p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group2/flume-flume-console1.conf -Dflume.root.logger=INFO,console</code></p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group2/flume-netcat-flume.conf</code></p></li><li><p>使用netcat工具向本机的44444端口发送内容</p><p><code>$ nc localhost 44444</code></p></li><li><p>查看Flume2及Flume3的控制台打印</p></li></ol><h2 id="多数据源汇总案例"><a href="#多数据源汇总案例" class="headerlink" title="多数据源汇总案例"></a>多数据源汇总案例</h2><p>案例需求：</p><p>aliyun103上的Flume-1监控文件/opt/module/group.log，</p><p>aliyun102上的Flume-2监控某一个端口的数据流，</p><p>Flume-1与Flume-2将数据发送给aliyun104上的Flume-3，Flume-3将最终数据打印到控制台。</p><p><img src="https://yerias.github.io/flume_img/%E5%A4%9A%E6%BA%90%E5%8D%95%E5%87%BA%E5%8F%A3.jpg" alt="多源单出口"></p><ol><li><p>准备工作</p><p>分发Flume</p><p><code>[hadoop@aliyun102 module]$ xsync flume</code></p><p>在aliyun102、aliyun103以及aliyun104的/opt/module/flume/job目录下创建一个group3文件夹。</p><p><code>[hadoop@aliyun102 job]$ mkdir group3</code></p><p><code>[hadoop@aliyun103 job]$ mkdir group3</code></p><p><code>[hadoop@aliyun104 job]$ mkdir group3</code></p></li><li><p>创建flume1-logger-flume.conf</p><p>配置Source用于监控hive.log文件，配置Sink输出数据到下一级Flume。</p><p>在aliyun103上创建配置文件并打开</p><p><code>[hadoop@aliyun103 group3]$ touch flume1-logger-flume.conf</code></p><p><code>[hadoop@aliyun103 group3]$ vim flume1-logger-flume.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/group.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun104</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>创建flume2-netcat-flume.conf</p><p>配置Source监控端口44444数据流，配置Sink数据到下一级Flume：</p><p>在aliyun102上创建配置文件并打开</p><p><code>[hadoop@aliyun102 group3]$ touch flume2-netcat-flume.conf</code></p><p><code>[hadoop@aliyun102 group3]$ vim flume2-netcat-flume.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a2.sources.r1.type = netcat</span><br><span class="line">a2.sources.r1.bind = aliyun102</span><br><span class="line">a2.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k1.type = avro</span><br><span class="line">a2.sinks.k1.hostname = aliyun104</span><br><span class="line">a2.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>创建flume3-flume-logger.conf</p><p>配置source用于接收flume1与flume2发送过来的数据流，最终合并后sink到控制台。</p><p>在aliyun104上创建配置文件并打开</p><p><code>[hadoop@aliyun104 group3]$ touch flume3-flume-logger.conf</code></p><p><code>[hadoop@aliyun104 group3]$ vim flume3-flume-logger.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = aliyun104</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line"># Describe the sink</span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>执行配置文件</p><p>分别开启对应配置文件：flume3-flume-logger.conf，flume2-netcat-flume.conf，flume1-logger-flume.conf。</p><p><code>[hadoop@aliyun104 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group3/flume3-flume-logger.conf -Dflume.root.logger=INFO,console</code></p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group3/flume2-netcat-flume.conf</code></p><p><code>[hadoop@aliyun103 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group3/flume1-logger-flume.conf</code></p></li><li><p>在aliyun103上向/opt/module目录下的group.log追加内容</p><p><code>[hadoop@aliyun103 module]$ echo &#39;hello&#39; &gt; group.log</code></p></li><li><p>在aliyun102上向44444端口发送数据</p><p><code>[hadoop@aliyun102 flume]$ telnet aliyun102 44444</code></p></li><li><p>检查aliyun104上数据</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SQOOP安装&amp;RDBMS导入HDFS&amp;RDBMS导入HIVE&amp;HDFS导入RDBMS&amp;HIVE导入RDBMS&amp;SQOOP的ETL案例&amp;在SHELL中操作MYSQL</title>
      <link href="/2018/12/01/sqoop/1/"/>
      <url>/2018/12/01/sqoop/1/</url>
      
        <content type="html"><![CDATA[<p>首先抛出两个场景</p><ol><li>数据数据在RDBMS中，你想使用Hive进行处理，怎么做</li><li>使用Hive统计分析好了，数据还在Hive中，如何导到RDBMS中</li></ol><h2 id="Sqoop安装"><a href="#Sqoop安装" class="headerlink" title="Sqoop安装"></a>Sqoop安装</h2><ol><li><p>下载并解压</p><p>下载地址：<a href="http://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.16.2.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.16.2.tar.gz</a></p><p>上传安装包sqoop-1.4.6-cdh5.16.2.tar.gz到主机中</p><p>解压sqoop安装包到指定目录，如：$ tar -zxf sqoop-1.4.6-cdh5.16.2.tar.gz -C /opt/module/</p></li><li><p>修改配置文件</p><p>重命名配置文件</p><p><code>$ mv sqoop-env-template.sh sqoop-env.sh</code></p><p>修改配置文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HADOOP_COMMON_HOME=/opt/module/hadoop</span><br><span class="line">export HADOOP_MAPRED_HOME=/opt/module/hadoop</span><br><span class="line">export HIVE_HOME=/opt/module/hive</span><br><span class="line">export ZOOKEEPER_HOME=/opt/module/zookeeper</span><br><span class="line">export ZOOCFGDIR=/opt/module/zookeeper</span><br><span class="line">export HBASE_HOME=/opt/module/hbase</span><br></pre></td></tr></table></figure></li><li><p>拷贝JDBC驱动</p><p><code>$ cp mysql-connector-java-5.1.27-bin.jar /opt/module/sqoop/lib/</code></p></li><li><p>验证Sqoop</p><p><code>$ bin/sqoop help</code></p><p>出现一些Warning警告（警告信息已省略），并伴随着帮助命令的输出：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Available commands:</span><br><span class="line">  codegen            Generate code to interact with database records</span><br><span class="line">  create-hive-table  Import a table definition into Hive</span><br><span class="line">  eval               Evaluate a SQL statement and display the results</span><br><span class="line">  export             Export an HDFS directory to a database table</span><br><span class="line">  help               List available commands</span><br><span class="line">  import             Import a table from a database to HDFS</span><br><span class="line">  import-all-tables  Import tables from a database to HDFS</span><br><span class="line">  import-mainframe   Import datasets from a mainframe server to HDFS</span><br><span class="line">  job                Work with saved jobs</span><br><span class="line">  list-databases     List available databases on a server</span><br><span class="line">  list-tables        List available tables in a database</span><br><span class="line">  merge              Merge results of incremental imports</span><br><span class="line">  metastore          Run a standalone Sqoop metastore</span><br><span class="line">  version            Display version information</span><br></pre></td></tr></table></figure></li><li><p>测试Sqoop是否能够成功连接数据库</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop \</span><br><span class="line">list-databases \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/ \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root</span></span><br></pre></td></tr></table></figure><p>数据库用到的参数:</p><ul><li>“\“： 代表换行</li><li>“connect”：连接数据库</li><li>username：用户</li><li>password：密码</li></ul><p>显示所有的数据库列表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">information_schema</span><br><span class="line">leyou1</span><br><span class="line">metastore</span><br><span class="line">mypython</span><br><span class="line">mysql</span><br><span class="line">performance_schema</span><br><span class="line">sys</span><br><span class="line">travel</span><br><span class="line">tunan</span><br></pre></td></tr></table></figure></li></ol><h2 id="RDBMS到HDFS"><a href="#RDBMS到HDFS" class="headerlink" title="RDBMS到HDFS"></a>RDBMS到HDFS</h2><h3 id="MySQL准备表和数据"><a href="#MySQL准备表和数据" class="headerlink" title="MySQL准备表和数据"></a>MySQL准备表和数据</h3><ol><li><p>确定mysql服务开启正常</p></li><li><p>在mysql中创建库表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> company;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> company.staff(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">4</span>) primary <span class="keyword">key</span> <span class="keyword">not</span> <span class="literal">null</span> auto_increment, </span><br><span class="line">    <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">255</span>), </span><br><span class="line">    sex <span class="built_in">varchar</span>(<span class="number">255</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure></li><li><p>插入数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> company.staff(<span class="keyword">name</span>, sex) <span class="keyword">values</span>(<span class="string">'Thomas'</span>, <span class="string">'Male'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> company.staff(<span class="keyword">name</span>, sex) <span class="keyword">values</span>(<span class="string">'Catalina'</span>, <span class="string">'FeMale'</span>);</span><br></pre></td></tr></table></figure></li><li><p>查看数据</p><p><code>select * from staff;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">----+----------+--------+</span></span><br><span class="line">| id | name     | sex    |</span><br><span class="line">+<span class="comment">----+----------+--------+</span></span><br><span class="line">|  1 | Thomas   | Male   |</span><br><span class="line">|  2 | Catalina | FeMale |</span><br><span class="line">+<span class="comment">----+----------+--------+</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="将MySQL数据导入到HDFS"><a href="#将MySQL数据导入到HDFS" class="headerlink" title="将MySQL数据导入到HDFS"></a>将MySQL数据导入到HDFS</h3><h4 id="全部导入"><a href="#全部导入" class="headerlink" title="全部导入"></a>全部导入</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://aliyun:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table staff \</span><br><span class="line">--target-dir /user/company \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &quot;\t&quot;</span><br></pre></td></tr></table></figure><p>全部导入用到的参数:</p><ul><li>table：指定被导入的表名</li><li>target-dir：指定导入路径</li><li>delete-target-dir：如果目标目录存在就删除它</li><li>num-mappers：mapper的个数</li><li>fields-terminated-by：指定字段分隔符</li></ul><h4 id="查询导入-query"><a href="#查询导入-query" class="headerlink" title="查询导入(query)"></a>查询导入(query)</h4><p>query参数就可以让用户随意写sql语句来查询了。query和table参数是互斥的。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://aliyun:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--target-dir /user/company \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &quot;\t&quot; \</span><br><span class="line">--query &apos;select name,sex from staff where id &lt;=1 and $CONDITIONS;&apos;</span><br></pre></td></tr></table></figure><ul><li>query：指定查询SQL where条件要有$CONDITIONS</li></ul><p>注意:  <code>must contain &#39;$CONDITIONS&#39; in WHERE clause.</code></p><p>如果query后使用的是双引号，则$CONDITIONS前必须加转移符，防止shell识别为自己的变量。</p><h4 id="导入指定列-columns"><a href="#导入指定列-columns" class="headerlink" title="导入指定列(columns)"></a>导入指定列(columns)</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--target-dir /user/company \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--num-mappers 1 \</span></span><br><span class="line"><span class="comment">--fields-terminated-by "\t" \</span></span><br><span class="line"><span class="comment">--columns id,sex \</span></span><br><span class="line"><span class="comment">--table staff</span></span><br></pre></td></tr></table></figure><ul><li>columns：指定导入的列</li></ul><h4 id="筛选查询导入数据-where"><a href="#筛选查询导入数据-where" class="headerlink" title="筛选查询导入数据(where)"></a>筛选查询导入数据(where)</h4><p>where参数可以进行一些简单的筛选</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--target-dir /user/company \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--num-mappers 1 \</span></span><br><span class="line"><span class="comment">--fields-terminated-by "\t" \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--where "id=1"</span></span><br></pre></td></tr></table></figure><ul><li>where：指定查询过滤条件</li></ul><h4 id="增量导入"><a href="#增量导入" class="headerlink" title="增量导入"></a>增量导入</h4><p>增量导入的一个场景就是昨天导入了一批数据，今天又增加了部分数据，现在要把这部分数据也导入到hdfs中。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://localhost:3306/company \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--target-dir /user/company/ \</span></span><br><span class="line"><span class="comment">--num-mappers 1 \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--null-string "" \</span></span><br><span class="line"><span class="comment">--null-non-string "0" \</span></span><br><span class="line"><span class="comment">--check-column "id" \</span></span><br><span class="line"><span class="comment">--incremental append \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line"><span class="comment">--last-value 0</span></span><br></pre></td></tr></table></figure><ul><li>null-string：字符串为null怎么处理</li><li>null-non-string：其他类型为null怎么处理</li><li>check-column：根据哪一行做增量导入</li><li>last-value：开始增量导入的上个位置</li></ul><h2 id="RDBMS导入到Hive"><a href="#RDBMS导入到Hive" class="headerlink" title="RDBMS导入到Hive"></a>RDBMS导入到Hive</h2><ol><li><p>在导入hive之前先在hive创建一样的表结构</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> staff(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">4</span>) , </span><br><span class="line">    <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">255</span>), </span><br><span class="line">    sex <span class="built_in">varchar</span>(<span class="number">255</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure></li><li><p>使用sqoop导入数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--hive-overwrite \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--null-string "" \</span></span><br><span class="line"><span class="comment">--null-non-string "0" \</span></span><br><span class="line"><span class="comment">--hive-import \</span></span><br><span class="line"><span class="comment">--hive-database default \</span></span><br><span class="line"><span class="comment">--hive-table staff \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line"><span class="comment">--num-mappers 1</span></span><br></pre></td></tr></table></figure><ul><li><p>hive-import：数据从关系数据库中导入到hive表中</p></li><li><p>hive-overwrite：覆盖掉在hive表中已经存在的数据</p></li><li><p>hive-table：后面接hive表,默认使用MySQL的表名</p></li><li><p>如果导入的是分区表，需要指定分区的key和value</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--hive-partition-key key \</span></span><br><span class="line"><span class="comment">--hive-partition-value value \</span></span><br></pre></td></tr></table></figure></li></ul><p>该过程分为两步，第一步将数据导入到HDFS，第二步将导入到HDFS的数据迁移到Hive仓库，第一步默认的临时目录是/user/hadoop/表名</p></li><li><p>查看hive表中的数据:</p><p><code>select * from staff;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">id|name    |sex   |</span><br><span class="line"><span class="comment">--|--------|------|</span></span><br><span class="line"> 1|Thomas  |Male  |</span><br><span class="line"> 2|Catalina|FeMale|</span><br></pre></td></tr></table></figure></li></ol><h2 id="HDFS导出到RDBMS"><a href="#HDFS导出到RDBMS" class="headerlink" title="HDFS导出到RDBMS"></a>HDFS导出到RDBMS</h2><ol><li><p>首先保证MySQL创建了一张和Hive一样表结构的表用来接收数据</p><p>注意表结构和分隔符都要一样</p></li><li><p>写Sqoop代码(批量导入)</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line">-Dsqoop.export.records.per.statement=10 \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--export-dir /user/company/ \</span></span><br><span class="line"><span class="comment">--null-string "" \</span></span><br><span class="line"><span class="comment">--null-non-string "0" \</span></span><br><span class="line"><span class="comment">--columns "id,name" \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure><ul><li><code>Dsqoop.export.records.per.statement</code>：批量更新，每隔10条提交一次 </li><li>export-dir：导出的hdfs目录</li><li>table：导入的表名</li><li>columns：指定导入的列</li></ul><p><code>注意:</code> MySQL中如果表不存在，不会自动创建</p></li></ol><h2 id="Hive导出到RDBMS"><a href="#Hive导出到RDBMS" class="headerlink" title="Hive导出到RDBMS"></a>Hive导出到RDBMS</h2><ol><li><p>首先保证MySQL创建了一张和Hive一样表结构的表用来接收数据</p><p>注意表结构和分隔符都要一样</p></li><li><p>写Sqoop代码</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--num-mappers 1 \</span></span><br><span class="line"><span class="comment">--export-dir /user/hive/warehouse/staff \</span></span><br><span class="line"><span class="comment">--input-fields-terminated-by "\t"</span></span><br></pre></td></tr></table></figure><ul><li><p>export-dir：指定被导出的目录</p></li><li><p>input-fields-terminated-by：导入的分隔符格式，和导入的fields-terminated-by有区别</p></li></ul><p><code>注意:</code> Mysql中如果表不存在，不会自动创建</p></li><li><p>查看MySQL数据库中的数据</p><p><code>select * from staff;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">----+----------+--------+</span></span><br><span class="line">| id | name     | sex    |</span><br><span class="line">+<span class="comment">----+----------+--------+</span></span><br><span class="line">|  1 | Thomas   | Male   |</span><br><span class="line">|  2 | Catalina | FeMale |</span><br><span class="line">|  3 | tunan    | Male   |</span><br><span class="line">+<span class="comment">----+----------+--------+</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="Sqoop的综合操作"><a href="#Sqoop的综合操作" class="headerlink" title="Sqoop的综合操作"></a>Sqoop的综合操作</h2><p>需求：emp和dept表是在MySQL，把MySQL数据抽取到Hive进行统计分析，然后把统计的结果回写到MySQL中</p><ol><li><p>在Hive中创建与MySQL中emp和dept表相对应的表emp_hive，dept_hive</p><p><code>emp_hive表</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> emp_hive(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">job <span class="keyword">string</span>,</span><br><span class="line">mgr <span class="built_in">int</span>,</span><br><span class="line">hiredate <span class="keyword">string</span>,</span><br><span class="line">sal <span class="keyword">double</span>,</span><br><span class="line">comm <span class="keyword">double</span>,</span><br><span class="line">deptno <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p><code>dept_hive表</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> dept_hive(</span><br><span class="line">deptno <span class="built_in">int</span>,</span><br><span class="line">dname <span class="keyword">string</span>,</span><br><span class="line">loc <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure></li><li><p>创建在Hive中的中间结果表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> mid_hive(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">deptno <span class="built_in">int</span>,</span><br><span class="line">dname <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure></li><li><p>将MySQL中的emp表中的数据传到emp_hive中</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/tunan \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--table emp \</span></span><br><span class="line"><span class="comment">--hive-overwrite \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--null-string "-" \</span></span><br><span class="line"><span class="comment">--null-non-string "0" \</span></span><br><span class="line"><span class="comment">--hive-import \</span></span><br><span class="line"><span class="comment">--hive-database default \</span></span><br><span class="line"><span class="comment">--hive-table emp_hive \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure></li><li><p>将MySQL中的dept表中的数据传到dept_hive</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/tunan \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--table dept \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--null-string "-" \</span></span><br><span class="line"><span class="comment">--null-non-string '0' \</span></span><br><span class="line"><span class="comment">--hive-import \</span></span><br><span class="line"><span class="comment">--hive-database default \</span></span><br><span class="line"><span class="comment">--hive-table dept_hive \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure></li><li><p>将Hive中的表进行业务处理</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> </span><br><span class="line">mid_hive</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">e.empno,e.ename,d.deptno,d.dname</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">emp_hive  e</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">dept_hive  d</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">e.deptno=d.deptno</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">*</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">mid_hive;</span><br></pre></td></tr></table></figure></li><li><p>查看中间表的数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> mid_hive;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |deptno|dname     |</span><br><span class="line"><span class="comment">-----|------|------|----------|</span></span><br><span class="line"> 7369|SMITH |    20|RESEARCH  |</span><br><span class="line"> 7499|ALLEN |    30|SALES     |</span><br><span class="line"> 7521|WARD  |    30|SALES     |</span><br><span class="line"> 7566|JONES |    20|RESEARCH  |</span><br><span class="line"> 7654|MARTIN|    30|SALES     |</span><br><span class="line"> 7698|BLAKE |    30|SALES     |</span><br><span class="line"> 7782|CLARK |    10|ACCOUNTING|</span><br><span class="line"> 7788|SCOTT |    20|RESEARCH  |</span><br><span class="line"> 7839|KING  |    10|ACCOUNTING|</span><br><span class="line"> 7844|TURNER|    30|SALES     |</span><br><span class="line"> 7876|ADAMS |    20|RESEARCH  |</span><br><span class="line"> 7900|JAMES |    30|SALES     |</span><br><span class="line"> 7902|FORD  |    20|RESEARCH  |</span><br><span class="line"> 7934|MILLER|    10|ACCOUNTING|</span><br></pre></td></tr></table></figure></li><li><p>在MySQL中创建返回数据的表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="string">`mid`</span>(</span><br><span class="line">empno <span class="built_in">int</span>(<span class="number">11</span>),</span><br><span class="line">ename <span class="built_in">varchar</span>(<span class="number">20</span>),</span><br><span class="line">deptno <span class="built_in">int</span>(<span class="number">11</span>),</span><br><span class="line">dname <span class="built_in">varchar</span>(<span class="number">20</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p>将处理好的数据用Sqoop发回MySQL</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/tunan \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--table mid \</span></span><br><span class="line">-m 1 \</span><br><span class="line"><span class="comment">--export-dir /user/hive/warehouse/mid_hive \</span></span><br><span class="line"><span class="comment">--input-fields-terminated-by '\t'</span></span><br></pre></td></tr></table></figure></li><li><p>查看MySQL中已经发回的数据</p><p><code>select * from mid;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">-------+--------+--------+------------+</span></span><br><span class="line">| empno | ename  | deptno | dname      |</span><br><span class="line">+<span class="comment">-------+--------+--------+------------+</span></span><br><span class="line">|  7369 | SMITH  |     20 | RESEARCH   |</span><br><span class="line">|  7499 | ALLEN  |     30 | SALES      |</span><br><span class="line">|  7521 | WARD   |     30 | SALES      |</span><br><span class="line">|  7566 | JONES  |     20 | RESEARCH   |</span><br><span class="line">|  7654 | MARTIN |     30 | SALES      |</span><br><span class="line">|  7698 | BLAKE  |     30 | SALES      |</span><br><span class="line">|  7782 | CLARK  |     10 | ACCOUNTING |</span><br><span class="line">|  7788 | SCOTT  |     20 | RESEARCH   |</span><br><span class="line">|  7839 | KING   |     10 | ACCOUNTING |</span><br><span class="line">|  7844 | TURNER |     30 | SALES      |</span><br><span class="line">|  7876 | ADAMS  |     20 | RESEARCH   |</span><br><span class="line">|  7900 | JAMES  |     30 | SALES      |</span><br><span class="line">|  7902 | FORD   |     20 | RESEARCH   |</span><br><span class="line">|  7934 | MILLER |     10 | ACCOUNTING |</span><br><span class="line">+<span class="comment">-------+--------+--------+------------+</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="shell操作数据库"><a href="#shell操作数据库" class="headerlink" title="shell操作数据库"></a>shell操作数据库</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql -uroot -pruozedata &lt;&lt;EOF</span><br><span class="line"><span class="keyword">use</span> sqoop;</span><br><span class="line"><span class="keyword">truncate</span> etl_result;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume架构摸排</title>
      <link href="/2018/12/01/flume/1/"/>
      <url>/2018/12/01/flume/1/</url>
      
        <content type="html"><![CDATA[<h2 id="Flume定义"><a href="#Flume定义" class="headerlink" title="Flume定义"></a>Flume定义</h2><p>Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单。</p><h2 id="Flume的优点"><a href="#Flume的优点" class="headerlink" title="Flume的优点"></a>Flume的优点</h2><ol><li><p>可以和任意存储进程集成。</p></li><li><p>输入的的数据速率大于写入目的存储的速率，flume会进行缓冲，减小hdfs的压力。</p></li><li><p>flume中的事务基于channel，使用了两个事务模型（sender + receiver），确保消息被可靠发送。</p></li></ol><p>Flume使用两个独立的事务分别负责从soucrce到channel，以及从channel到sink的事件传递。一旦事务中所有的数据全部成功提交到channel，那么source才认为该数据读取完成。同理，只有成功被sink写出去的数据，才会从channel中移除。</p><h2 id="Flume组成架构"><a href="#Flume组成架构" class="headerlink" title="Flume组成架构"></a>Flume组成架构</h2><p><img src="https://yerias.github.io/flume_img/flume.jpg" alt="flume架构图"></p><ul><li><p>Agent</p><p>Agent是一个JVM进程，它以事件的形式将数据从源头送至目的。</p><p>Agent主要有3个部分组成，Source、Channel、Sink。</p></li><li><p>Source</p><p>Source是负责接收数据到Flume Agent的组件。</p><p>Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。</p></li><li><p>Channel</p><p>Channel是位于Source和Sink之间的缓冲区。因此，Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。</p><p>Flume自带两种Channel：Memory Channel和File Channel。</p><ol><li><p>Memory Channel是内存中的队列。Memory Channel在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么Memory Channel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。</p></li><li><p>File Channel将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。</p></li></ol></li><li><p>Sink</p><p>Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。</p><p>Sink是完全事务性的。在从Channel批量删除数据之前，每个Sink用Channel启动一个事务。批量事件一旦成功写出到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。</p><p>Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。</p></li><li><p>Event</p><p>传输单元，Flume数据传输的基本单元，以事件的形式将数据从源头送至目的地。 Event由可选的header和载有数据的一个byte array 构成。Header是容纳了key-value字符串对的HashMap。</p></li></ul><h2 id="Flume拓扑结构"><a href="#Flume拓扑结构" class="headerlink" title="Flume拓扑结构"></a>Flume拓扑结构</h2><h3 id="Flume-Agent连接"><a href="#Flume-Agent连接" class="headerlink" title="Flume Agent连接"></a>Flume Agent连接</h3><p><img src="https://yerias.github.io/flume_img/flume2.jpg" alt="flume2"></p><p>这种模式是将多个flume给顺序连接起来了，从最初的source开始到最终sink传送的目的存储系统。此模式不建议桥接过多的flume数量， flume数量过多不仅会影响传输速率，而且一旦传输过程中某个节点flume宕机，会影响整个传输系统。</p><h3 id="单source，多channel、sink"><a href="#单source，多channel、sink" class="headerlink" title="单source，多channel、sink"></a>单source，多channel、sink</h3><p><img src="https://yerias.github.io/flume_img/flume3.jpg" alt="flume3"></p><p>Flume支持将事件流向一个或者多个目的地。这种模式将数据源复制到多个channel中，每个channel都有相同的数据，sink可以选择传送的不同的目的地。</p><h3 id="Flume负载均衡"><a href="#Flume负载均衡" class="headerlink" title="Flume负载均衡"></a>Flume负载均衡</h3><p><img src="https://yerias.github.io/flume_img/flume4.jpg" alt="flume4"></p><p>Flume支持使用将多个sink逻辑上分到一个sink组，flume将数据发送到不同的sink，主要解决负载均衡和故障转移问题。</p><h3 id="Flume-Agent聚合"><a href="#Flume-Agent聚合" class="headerlink" title="Flume Agent聚合"></a>Flume Agent聚合</h3><p><img src="https://yerias.github.io/flume_img/flume5.jpg" alt="flume5"></p><p>这种模式是我们最常见的，也非常实用，日常web应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用flume的这种组合方式能很好的解决这一问题，每台服务器部署一个flume采集日志，传送到一个集中收集日志的flume，再由此flume上传到hdfs、hive、hbase、jms等，进行日志分析。</p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive-ORC文件存储格式</title>
      <link href="/2018/11/19/hive/18/"/>
      <url>/2018/11/19/hive/18/</url>
      
        <content type="html"><![CDATA[<h2 id="ORC文件格式"><a href="#ORC文件格式" class="headerlink" title="ORC文件格式"></a>ORC文件格式</h2><p>ORC的全称是(Optimized Record Columnar)，使用ORC文件格式可以提高hive读、写和处理数据的能力。ORC在RCFile的基础上进行了一定的改进，所以与RCFile相比，具有以下一些优势： </p><ol><li>ORC中的特定的序列化与反序列化操作可以使ORC file writer根据数据类型进行写出。 </li><li>提供了多种RCFile中没有的indexes，这些indexes可以使ORC的reader很快的读到需要的数据，并且跳过无用数据，这使得ORC文件中的数据可以很快的得到访问。 </li><li>由于ORC file writer可以根据数据类型进行写出，所以ORC可以支持复杂的数据结构（比如Map等）。 </li><li>除了上面三个理论上就具有的优势之外，ORC的具体实现上还有一些其他的优势，比如ORC的stripe默认大小为256M，为ORC writer提供了一个memory manager来管理内存使用情况。 </li></ol><p>ORC File包含一组组的行数据，称为<strong>stripes</strong>，除此之外，ORC File的<strong>file footer</strong>还包含一些额外的辅助信息。在ORC File文件的最后，有一个被称为<strong>postscript</strong>，它主要是用来存储压缩参数及压缩页脚的大小。</p><p>在默认情况下，一个stripe的大小为256MB。大尺寸的stripes使得从HDFS读数据更高效。</p><p>在file footer里面包含了该ORC File文件中stripes的信息，每个stripe中有多少行，以及每列的数据类型。当然，它里面还包含了列级别的一些聚合的结果，比如：count, min, max, and sum。下图显示出可ORC File文件结构：</p><p><img src="https://yerias.github.io/hive_img/orc.jpg" alt=""></p><h2 id="ORC数据存储方法"><a href="#ORC数据存储方法" class="headerlink" title="ORC数据存储方法"></a>ORC数据存储方法</h2><p>在ORC格式的hive表中，记录首先会被横向的切分为多个<strong>stripes</strong>，每个Stripe都包含index data、row data以及stripe footer。Stripe footer包含流位置的目录；Row data在表扫描的时候会用到。Index data包含每列的最大和最小值以及每列所在的行。行索引里面提供了偏移量，它可以跳到正确的压缩块位置。具有相对频繁的行索引，使得在stripe中快速读取的过程中可以跳过很多行，尽管这个stripe的大小很大。在默认情况下，最大可以跳过10000行。拥有通过过滤谓词而跳过大量的行的能力，你可以在表的 secondary keys 进行排序，从而可以大幅减少执行时间。比如你的表的主分区是交易日期，那么你可以对次分区（state、zip code以及last name）进行排序。</p><p>对于复杂数据类型，比如Map，ORC文件会将一个复杂数据类型字段解析成多个子字段。下表中列举了ORC文件中对于复杂数据类型的解析</p><table><thead><tr><th>Data type</th><th>Chile columns</th></tr></thead><tbody><tr><td>Array</td><td>一个包含所有数组元素的子字段</td></tr><tr><td>Map</td><td>两个子字段，一个key字段，一个value字段</td></tr><tr><td>Struct</td><td>每一个属性对应一个子字段</td></tr><tr><td>Union</td><td>每一个属性对应一个子字段</td></tr></tbody></table><p>当字段类型都被解析后，会由这些字段类型组成一个字段树，只有树的叶子节点才会保存表数据，这些叶子节点中的数据形成一个数据流，如上图中的Data Stream。 </p><p>为了使ORC文件的reader更加高效的读取数据，字段的metadata会保存在Meta Stream中。在字段树中，每一个非叶子节点记录的就是字段的metadata，比如对一个array来说，会记录它的长度。下图根据表的字段类型生成了一个对应的字段树。 </p><p><img src="https://yerias.github.io/hive_img/orc2.jpg" alt=""></p><p>使用ORC文件格式时，用户可以使用HDFS的每一个block存储ORC文件的一个stripe。对于一个ORC文件来说，stripe的大小一般需要设置得比HDFS的block小，如果不这样的话，一个stripe就会分别在HDFS的多个block上，当读取这种数据时就会发生远程读数据的行为。如果设置stripe的只保存在一个block上的话，如果当前block上的剩余空间不足以存储下一个strpie，ORC的writer接下来会将数据打散保存在block剩余的空间上，直到这个block存满为止。这样，下一个stripe又会从下一个block开始存储。 </p><h2 id="Position-Pointers"><a href="#Position-Pointers" class="headerlink" title="Position Pointers"></a>Position Pointers</h2><p>当读取一个ORC文件时，ORC reader需要有两个位置信息才能准确的进行数据读取操作。 </p><ol><li><p>metadata streams和data streams中每个group的开始位置*</p><p>由于每个stripe中有多个group，ORC reader需要知道每个group的metadata streams和data streams的开始位置。图１中右边的虚线代表的就是这种pointer。 </p></li><li><p>stripes的开始位置</p><p>由于一个ORC文件可以包含多个stripes，并且一个HDFS block也能包含多个stripes。为了快速定位指定stripe的位置，需要知道每个stripe的开始位置。这些信息会保存在ORC file的File Footer中。如图1中间位置的虚线所示。</p></li></ol><h2 id="文件压缩"><a href="#文件压缩" class="headerlink" title="文件压缩"></a>文件压缩</h2><p>ORC文件使用两级压缩机制，首先将一个数据流使用流式编码器进行编码，然后使用一个可选的压缩器对数据流进行进一步压缩。 </p><p>一个column可能保存在一个或多个数据流中，可以将数据流划分为以下四种类型： </p><ul><li><p>Byte Stream </p><p>字节流保存一系列的字节数据，不对数据进行编码。 </p></li><li><p>Run Length Byte Stream </p><p>字节长度字节流保存一系列的字节数据，对于相同的字节，保存这个重复值以及该值在字节流中出现的位置。 </p></li><li><p>Integer Stream</p><p>整形数据流保存一系列整形数据。可以对数据量进行字节长度编码以及delta编码。具体使用哪种编码方式需要根据整形流中的子序列模式来确定。 </p></li><li><p>Bit Field Stream</p><p>比特流主要用来保存boolean值组成的序列，一个字节代表一个boolean值，在比特流的底层是用Run Length Byte Stream来实现的。 </p></li></ul><p>接下来会以Integer和String类型的字段举例来说明。 </p><ol><li><p>Integer</p><p>对于一个整形字段，会同时使用一个比特流和整形流。比特流用于标识某个值是否为null，整形流用于保存该整形字段非空记录的整数值。 </p></li><li><p>String</p><p>对于一个String类型字段，ORC writer在开始时会检查该字段值中不同的内容数占非空记录总数的百分比不超过0.8的话，就使用字典编码，字段值会保存在一个比特流，一个字节流及两个整形流中。比特流也是用于标识null值的，字节流用于存储字典值，一个整形流用于存储字典中每个词条的长度，另一个整形流用于记录字段值。 </p><p>如果不能用字典编码，ORC writer会知道这个字段的重复值太少，用字典编码效率不高，ORC writer会使用一个字节流保存String字段的值，然后用一个整形流来保存每个字段的字节长度。 </p><p>在ORC文件中，在各种数据流的底层，用户可以自选ZLIB, Snappy和LZO压缩方式对数据流进行压缩。编码器一般会将一个数据流压缩成一个个小的压缩单元，在目前的实现中，压缩单元的默认大小是256KB。</p></li></ol><h2 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h2><p>当ORC writer写数据时，会将整个stripe保存在内存中。由于stripe的默认值一般比较大，当有多个ORC writer同时写数据时，可能会导致内存不足。为了现在这种并发写时的内存消耗，ORC文件中引入了一个内存管理器。在一个Map或者Reduce任务中内存管理器会设置一个阈值，这个阈值会限制writer使用的总内存大小。当有新的writer需要写出数据时，会向内存管理器注册其大小（一般也就是stripe的大小），当内存管理器接收到的总注册大小超过阈值时，内存管理器会将stripe的实际大小按该writer注册的内存大小与总注册内存大小的比例进行缩小。当有writer关闭时，内存管理器会将其注册的内存从总注册内存中注销。</p><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><table><thead><tr><th>参数名</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td>hive.exec.orc.default.stripe.size</td><td>256<em>1024</em>1024</td><td>stripe的默认大小</td></tr><tr><td>hive.exec.orc.default.block.size</td><td>256<em>1024</em>1024</td><td>orc文件在文件系统中的默认block大小，从hive-0.14开始</td></tr><tr><td>hive.exec.orc.dictionary.key.size.threshold</td><td>0.8</td><td>String类型字段使用字典编码的阈值</td></tr><tr><td>hive.exec.orc.default.row.index.stride</td><td>10000</td><td>stripe中的分组大小</td></tr><tr><td>hive.exec.orc.default.compress</td><td>ZLIB</td><td>ORC文件的默认压缩方式</td></tr><tr><td>hive.exec.orc.skip.corrupt.data</td><td>false</td><td>遇到错误数据的处理方式，false直接抛出异常，true则跳过该记录</td></tr></tbody></table><p>更多参数：<a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-ORCFileFormat" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-ORCFileFormat</a></p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive元数据管理(3)</title>
      <link href="/2018/11/17/hive/17/"/>
      <url>/2018/11/17/hive/17/</url>
      
        <content type="html"><![CDATA[<p>前面掌握了Hive元数据的表结构和如何删除元数据达到删除表的目的</p><p>本节使用一个project说明了如何使用HiveMetaStoreClient类来快速实现UI界面的对Hive库表的增删查改</p><p>githup地址: <a href="https://github.com/yerias/tunan-hive-metastore" target="_blank" rel="noopener">https://github.com/yerias/tunan-hive-metastore</a></p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive元数据管理(2)</title>
      <link href="/2018/11/16/hive/16/"/>
      <url>/2018/11/16/hive/16/</url>
      
        <content type="html"><![CDATA[<p>在上节的内容讲述了Hive的元数据都有哪些重要的表，这些表中都有哪些字段，现在我们将系统的看一下他们的组成结构</p><p><img src="https://yerias.github.io/hive_img/hive%E5%85%83%E6%95%B0%E6%8D%AE.jpg" alt=""></p><p>在老师的某个生产故障背景下: CDH集群版本为5.2，Yarn出现了Bug，这个Bug在CDH5.3的版本下修复了，就没有多想就将Yarn组件升级到5.3，这时候出现了问题，由于5.3和5.2最后的Thrift协议发生了变化，除了删除Hive表以外的其他功能都正常，升级整个集群不可能，只能采取曲线救国的策略，删除指定表在Hive中的元数据，来达到删除表的效果。</p><ol><li><p>根据表名和库名查询得出TBL_ID、SD_ID、PART_ID、CD_ID、SERDE_ID</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">t.TBL_ID,t.SD_ID,p.PART_ID,s.CD_ID,s.SERDE_ID</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">(<span class="keyword">SELECT</span></span><br><span class="line">tt.TBL_ID,tt.SD_ID</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">tbls tt,</span><br><span class="line">dbs d</span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">d.DB_ID = tt.DB_ID</span><br><span class="line"><span class="keyword">AND</span> </span><br><span class="line">d.<span class="string">`NAME`</span> = <span class="string">'DATABASE_NAME'</span></span><br><span class="line"><span class="keyword">AND</span></span><br><span class="line">tt.TBL_NAME = <span class="string">'TABLE_NAME'</span></span><br><span class="line">) t</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> sds s <span class="keyword">ON</span> t.sd_id = s.SD_ID</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> <span class="string">`partitions`</span> p <span class="keyword">on</span> t.TBL_ID=p.TBL_ID</span><br></pre></td></tr></table></figure></li><li><p>按照顺序依次执行删除数据语句（要先删除子表数据）</p><ul><li><p>删除partition相关的（假设有分区）：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 子表partition_params</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> partition_params <span class="keyword">where</span> PART_ID=</span><br><span class="line"><span class="comment"># 子表partition_key_vals</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> partition_key_vals <span class="keyword">where</span> PART_ID=</span><br><span class="line"><span class="comment"># 子表part_privs，part_col_stats，part_col_privs </span></span><br><span class="line"><span class="comment"># 一般没有数据，如有则需要删除，否则因为外键关系无法删除对应数据</span></span><br><span class="line">part_privs，part_col_stats，part_col_privs </span><br><span class="line"><span class="comment"># 主表partitions</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> <span class="keyword">partitions</span> <span class="keyword">where</span> PART_ID=</span><br></pre></td></tr></table></figure></li><li><p>删除tbls相关的</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 子表table_params</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> table_params <span class="keyword">where</span> TBL_ID=<span class="number">16</span></span><br><span class="line"><span class="comment"># 子表partition_keys</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> partition_keys <span class="keyword">where</span> TBL_ID=<span class="number">16</span></span><br><span class="line"><span class="comment"># 一般没有数据，如有则需要删除，否则因为外键关系无法删除对应数据</span></span><br><span class="line">tbl_col_privs，tbl_privs，tab_col_stats，index_params，idxs的原理同上（注意idxs是index_params的主表）</span><br><span class="line"><span class="comment"># 主表tbls</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> tbls <span class="keyword">where</span> TBL_ID=<span class="number">16</span></span><br></pre></td></tr></table></figure></li><li><p>删除sds相关</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 一般没有数据，如有则需要删除，否则因为外键关系无法删除对应数据</span></span><br><span class="line">bucketing_cols，skewed_string_list_values，skewed_col_names，skewed_values</span><br><span class="line"><span class="comment"># 子表 sd_params</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> sd_params <span class="keyword">where</span> SD_ID=<span class="number">16</span></span><br><span class="line"><span class="comment"># 主表</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> sds <span class="keyword">where</span> SD_ID=<span class="number">16</span></span><br></pre></td></tr></table></figure></li><li><p>删除cds相关</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 只有两张表</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> columns_v2 <span class="keyword">where</span> CD_ID=<span class="number">21</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> cds <span class="keyword">where</span> CD_ID=<span class="number">21</span></span><br></pre></td></tr></table></figure></li><li><p>删除serdes相关</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 只有两张表</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> serde_params <span class="keyword">where</span> SERDE_ID=<span class="number">16</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> serdes <span class="keyword">where</span> SERDE_ID=<span class="number">16</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>使用jdbc改写，只需要在外部传入数据库名和表名即可，打入jar包使用</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive元数据管理(1)</title>
      <link href="/2018/11/15/hive/15/"/>
      <url>/2018/11/15/hive/15/</url>
      
        <content type="html"><![CDATA[<p>众所周知，hive表中的数据是HDFS上的文件，可是hive怎么知道这些文件的内容都对应哪个字段，对应哪个分区呢？<br>就是hive的元数据管理着这一切。通常在hive-site.xml中的元数据库配置成MySQL，替换Derby。MySQL比Derby最大的优势在于可以多用户登录</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql:///metastore?createDatabaseIfNotExists=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>下面我们进MySQL看看元数据的表具体有哪些。</p><h2 id="Hive版本的元数据表（VERSION）"><a href="#Hive版本的元数据表（VERSION）" class="headerlink" title="Hive版本的元数据表（VERSION）"></a>Hive版本的元数据表（VERSION）</h2><h3 id="VERSION"><a href="#VERSION" class="headerlink" title="VERSION"></a>VERSION</h3><p>这个存hive版本，有且仅有一条数据</p><p><img src="https://yerias.github.io/hive_img/hive_metastore1.png" alt=""></p><p>该表比较简单，但很重要。</p><table><thead><tr><th><strong>VER_ID</strong></th><th><strong>SCHEMA_VERSION</strong></th><th><strong>VERSION_COMMENT</strong></th></tr></thead><tbody><tr><td>ID主键</td><td>Hive版本</td><td>版本说明</td></tr><tr><td>1</td><td>0.13.0</td><td>Set by MetaStore</td></tr></tbody></table><p>如果该表出现问题，根本进入不了Hive-Cli。</p><p>比如该表不存在，当启动Hive-Cli时候，就会报错”<code>Table ‘hive.version’ doesn’t exist</code>”。</p><p>如果多了，会报错</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Caused by: MetaException(message:<span class="function">Metastore contains multiple <span class="title">versions</span> <span class="params">(<span class="number">2</span>)</span></span></span><br></pre></td></tr></table></figure><h2 id="Hive数据库相关的元数据表"><a href="#Hive数据库相关的元数据表" class="headerlink" title="Hive数据库相关的元数据表"></a>Hive数据库相关的元数据表</h2><p><strong>DBS和DATABASE_PARAMS这两张表通过DB_ID字段关联，dbs和funcs使用DB_ID关联，funcs和func_ru使用FUNC_ID关联</strong></p><p><img src="https://yerias.github.io/hive_img/hive_metastore2.png" alt=""></p><h3 id="DBS"><a href="#DBS" class="headerlink" title="DBS"></a>DBS</h3><p><strong>DBS</strong>是数据库主表，字段名顾名思义，样例数据如下</p><p><img src="https://yerias.github.io/hive_img/hive_metastore3.png" alt=""></p><p>该表存储Hive中所有数据库的基本信息，字段如下：</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>DB_ID</strong></td><td>数据库ID</td><td>2</td></tr><tr><td><strong>DESC</strong></td><td>数据库描述</td><td>测试库</td></tr><tr><td><strong>DB_LOCATION_URI</strong></td><td>数据库HDFS路径</td><td>hdfs://namenode/user/hive/warehouse/lxw1234.db</td></tr><tr><td><strong>NAME</strong></td><td>数据库名</td><td>lxw1234</td></tr><tr><td><strong>OWNER_NAME</strong></td><td>数据库所有者用户名</td><td>lxw1234</td></tr><tr><td><strong>OWNER_TYPE</strong></td><td>所有者角色</td><td>USER</td></tr></tbody></table><h3 id="DATABASE-PARAMS"><a href="#DATABASE-PARAMS" class="headerlink" title="DATABASE_PARAMS"></a>DATABASE_PARAMS</h3><p><strong>DATABASE_PARAMS</strong>是创建数据库 WITH DBPROPERTIES (property_name=property_value, …)指定的参数</p><p><img src="https://yerias.github.io/hive_img/hive_metastore4.png" alt=""></p><p>该表存储数据库的相关参数</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>DB_ID</strong></td><td>数据库ID</td><td>2</td></tr><tr><td><strong>PARAM_KEY</strong></td><td>参数名</td><td>createdby</td></tr><tr><td><strong>PARAM_VALUE</strong></td><td>参数值</td><td>lxw1234</td></tr></tbody></table><h3 id="FUNCS"><a href="#FUNCS" class="headerlink" title="FUNCS"></a>FUNCS</h3><p><strong>FUNCS</strong>是函数表</p><p><img src="https://yerias.github.io/hive_img/hive_metastore5.png" alt=""></p><h3 id="FUNC-RU"><a href="#FUNC-RU" class="headerlink" title="FUNC_RU"></a>FUNC_RU</h3><p><strong>FUNC_RU</strong>是函数在哪个jar包中</p><p><img src="https://yerias.github.io/hive_img/hive_metastore6.png" alt=""></p><h2 id="Hive表和视图相关的元数据表"><a href="#Hive表和视图相关的元数据表" class="headerlink" title="Hive表和视图相关的元数据表"></a>Hive表和视图相关的元数据表</h2><p><strong>主要有TBLS、TABLE_PARAMS、TBL_PRIVS，这三张表通过TBL_ID关联</strong>。</p><p><img src="https://yerias.github.io/hive_img/hive_metastore7.png" alt=""></p><h3 id="TBLS"><a href="#TBLS" class="headerlink" title="TBLS"></a>TBLS</h3><p><strong>TBLS</strong>是表的主表，存放hive所有表的主要信息</p><p><img src="https://yerias.github.io/hive_img/hive_metastore8.png" alt=""></p><p>该表中存储Hive表、视图、索引表的基本信息。</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>TBL_ID</strong></td><td>表ID</td><td>1</td></tr><tr><td><strong>CREATE_TIME</strong></td><td>创建时间</td><td>1436317071</td></tr><tr><td><strong>DB_ID</strong></td><td>数据库ID</td><td>2，对应DBS中的DB_ID</td></tr><tr><td><strong>LAST_ACCESS_TIME</strong></td><td>上次访问时间</td><td>1436317071</td></tr><tr><td><strong>OWNER</strong></td><td>所有者</td><td>liuxiaowen</td></tr><tr><td><strong>RETENTION</strong></td><td>保留字段</td><td>0</td></tr><tr><td><strong>SD_ID</strong></td><td>序列化配置信息</td><td>86，对应SDS表中的SD_ID</td></tr><tr><td><strong>TBL_NAME</strong></td><td>表名</td><td>lxw1234</td></tr><tr><td><strong>TBL_TYPE</strong></td><td>表类型</td><td>MANAGED_TABLE、EXTERNAL_TABLE、INDEX_TABLE、VIRTUAL_VIEW</td></tr><tr><td><strong>VIEW_EXPANDED_TEXT</strong></td><td>视图的详细HQL语句</td><td>select <code>lxw1234</code>.<code>pt</code>, <code>lxw1234</code>.<code>pcid</code> from <code>liuxiaowen</code>.<code>lxw1234</code></td></tr><tr><td><strong>VIEW_ORIGINAL_TEXT</strong></td><td>视图的原始HQL语句</td><td>select * from lxw1234</td></tr></tbody></table><h3 id="TABLE-PARAMS"><a href="#TABLE-PARAMS" class="headerlink" title="TABLE_PARAMS"></a>TABLE_PARAMS</h3><p><strong>TABLE_PARAMS</strong>表存储表/视图的属性信息。</p><p><img src="https://yerias.github.io/hive_img/hive_metastore16.png" alt=""></p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>TBL_ID</strong></td><td>表ID</td><td>1</td></tr><tr><td><strong>PARAM_KEY</strong></td><td>属性名</td><td>totalSize、numRows、EXTERNAL</td></tr><tr><td><strong>PARAM_VALUE</strong></td><td>属性值</td><td>970107336、21231028、TRUE</td></tr></tbody></table><h3 id="TBL-PRIVS"><a href="#TBL-PRIVS" class="headerlink" title="TBL_PRIVS"></a>TBL_PRIVS</h3><p><strong>TBL_PRIVS</strong>表存储表/视图的授权信息</p><p><img src="https://yerias.github.io/hive_img/hive_metastore17.png" alt=""></p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>TBL_GRANT_ID</strong></td><td>授权ID</td><td>1</td></tr><tr><td><strong>CREATE_TIME</strong></td><td>授权时间</td><td>1436320455</td></tr><tr><td><strong>GRANT_OPTION</strong></td><td></td><td>0</td></tr><tr><td><strong>GRANTOR</strong></td><td>授权执行用户</td><td>liuxiaowen</td></tr><tr><td><strong>GRANTOR_TYPE</strong></td><td>授权者类型</td><td>USER</td></tr><tr><td><strong>PRINCIPAL_NAME</strong></td><td>被授权用户</td><td>username</td></tr><tr><td><strong>PRINCIPAL_TYPE</strong></td><td>被授权用户类型</td><td>USER</td></tr><tr><td><strong>TBL_PRIV</strong></td><td>权限</td><td>Select、Alter</td></tr><tr><td><strong>TBL_ID</strong></td><td>表ID</td><td>22，对应TBLS表中的TBL_ID</td></tr></tbody></table><h2 id="Hive文件存储信息相关的元数据表"><a href="#Hive文件存储信息相关的元数据表" class="headerlink" title="Hive文件存储信息相关的元数据表"></a>Hive文件存储信息相关的元数据表</h2><p>主要涉及SDS、SD_PARAMS、SERDES、SERDE_PARAMS</p><p><strong>其中sds、serdes、serde_params使用SERDE_ID关联，sds和sd_params使用SD_ID关联</strong></p><p><img src="https://yerias.github.io/hive_img/hive_metastore11.png" alt=""></p><p>由于HDFS支持的文件格式很多，而建Hive表时候也可以指定各种文件格式，Hive在将HQL解析成MapReduce时候，需要知道去哪里，使用哪种格式去读写HDFS文件，而这些信息就保存在这几张表中。</p><h3 id="SDS"><a href="#SDS" class="headerlink" title="SDS"></a>SDS</h3><p>SDS是存储主表，包含数据文件的输入输出格式，所在HDFS路径，是否压缩等</p><p><img src="https://yerias.github.io/hive_img/hive_metastore12.png" alt=""></p><p>该表保存文件存储的基本信息，如INPUT_FORMAT、OUTPUT_FORMAT、是否压缩等。</p><p>TBLS表中的SD_ID与该表关联，可以获取Hive表的存储信息。</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>SD_ID</strong></td><td>存储信息ID</td><td>1</td></tr><tr><td><strong>CD_ID</strong></td><td>字段信息ID</td><td>21，对应CDS表</td></tr><tr><td><strong>INPUT_FORMAT</strong></td><td>文件输入格式</td><td>org.apache.hadoop.mapred.TextInputFormat</td></tr><tr><td><strong>IS_COMPRESSED</strong></td><td>是否压缩</td><td>0</td></tr><tr><td><strong>IS_STOREDASSUBDIRECTORIES</strong></td><td>是否以子目录存储</td><td>0</td></tr><tr><td><strong>LOCATION</strong></td><td>HDFS路径</td><td>hdfs://namenode/hivedata/warehouse/ut.db/t_lxw</td></tr><tr><td><strong>NUM_BUCKETS</strong></td><td>分桶数量</td><td>5</td></tr><tr><td><strong>OUTPUT_FORMAT</strong></td><td>文件输出格式</td><td>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</td></tr><tr><td><strong>SERDE_ID</strong></td><td>序列化类ID</td><td>3，对应SERDES表</td></tr></tbody></table><h3 id="SD-PARAMS"><a href="#SD-PARAMS" class="headerlink" title="SD_PARAMS"></a>SD_PARAMS</h3><p>SD_PARAMS是在创建表时候使用STORED BY ‘storage.handler.class.name’ [WITH SERDEPROPERTIES (…)指定</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>SD_ID</strong></td><td>存储配置ID</td><td>1</td></tr><tr><td><strong>PARAM_KEY</strong></td><td>存储属性名</td><td></td></tr><tr><td><strong>PARAM_VALUE</strong></td><td>存储属性值</td><td></td></tr></tbody></table><h3 id="SERDES"><a href="#SERDES" class="headerlink" title="SERDES"></a>SERDES</h3><p>SERDES是序列化使用类的表，存储序列化使用的类信息</p><p><img src="https://yerias.github.io/hive_img/hive_metastore14.png" alt=""></p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>SERDE_ID</strong></td><td>序列化类配置ID</td><td>1</td></tr><tr><td><strong>NAME</strong></td><td>序列化类别名</td><td></td></tr><tr><td><strong>SLIB</strong></td><td>序列化类</td><td>org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</td></tr></tbody></table><h3 id="SERDE-PARAM"><a href="#SERDE-PARAM" class="headerlink" title="SERDE_PARAM"></a>SERDE_PARAM</h3><p>SERDE_PARAMS存储列分隔符，行分隔符等</p><p><img src="https://yerias.github.io/hive_img/hive_metastore15.png" alt=""></p><p>该表存储序列化的一些属性、格式信息,比如：行、列分隔符</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>SERDE_ID</strong></td><td>序列化类配置ID</td><td>1</td></tr><tr><td><strong>PARAM_KEY</strong></td><td>属性名</td><td>field.delim</td></tr><tr><td><strong>PARAM_VALUE</strong></td><td>属性值</td><td>,</td></tr></tbody></table><h2 id="Hive表字段相关的元数据表"><a href="#Hive表字段相关的元数据表" class="headerlink" title="Hive表字段相关的元数据表"></a>Hive表字段相关的元数据表</h2><h3 id="COLUMNS-V2"><a href="#COLUMNS-V2" class="headerlink" title="COLUMNS_V2"></a>COLUMNS_V2</h3><p><img src="https://yerias.github.io/hive_img/hive_metastore18.png" alt=""></p><p>该表存储表对应的字段信息。</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>CD_ID</strong></td><td>字段信息ID</td><td>1</td></tr><tr><td><strong>COMMENT</strong></td><td>字段注释</td><td></td></tr><tr><td><strong>COLUMN_NAME</strong></td><td>字段名</td><td>pt</td></tr><tr><td><strong>TYPE_NAME</strong></td><td>字段类型</td><td>string</td></tr><tr><td><strong>INTEGER_IDX</strong></td><td>字段顺序</td><td>2</td></tr></tbody></table><h2 id="Hive表分区相关的元数据表"><a href="#Hive表分区相关的元数据表" class="headerlink" title="Hive表分区相关的元数据表"></a>Hive表分区相关的元数据表</h2><p>主要涉及<strong>PARTITIONS</strong>、<strong>PARTITION_KEYS</strong>、<strong>PARTITION_KEY_VALS</strong>、<strong>PARTITION_PARAMS</strong></p><p><strong>其中PARTITION_KEYS和tbls关联，其余的PARTITIONS、PARTITION_KEY_VALS、PARTITION_PARAMS使用PART_ID关联</strong></p><p><img src="https://yerias.github.io/hive_img/hive_metastore19.png" alt=""></p><h3 id="PARTITIONS"><a href="#PARTITIONS" class="headerlink" title="PARTITIONS"></a>PARTITIONS</h3><p><img src="https://yerias.github.io/hive_img/hive_metastore20.png" alt=""></p><p>该表存储表分区的基本信息。</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>PART_ID</strong></td><td>分区ID</td><td>1</td></tr><tr><td><strong>CREATE_TIME</strong></td><td>分区创建时间</td><td></td></tr><tr><td><strong>LAST_ACCESS_TIME</strong></td><td>最后一次访问时间</td><td></td></tr><tr><td><strong>PART_NAME</strong></td><td>分区名</td><td>pt=2015-06-12</td></tr><tr><td><strong>SD_ID</strong></td><td>分区存储ID</td><td>21</td></tr><tr><td><strong>TBL_ID</strong></td><td>表ID</td><td>2</td></tr></tbody></table><h3 id="PARTITION-KEYS"><a href="#PARTITION-KEYS" class="headerlink" title="PARTITION_KEYS"></a>PARTITION_KEYS</h3><p><img src="https://yerias.github.io/hive_img/hive_metastore21.png" alt=""></p><p>该表存储分区的字段信息。</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>TBL_ID</strong></td><td>表ID</td><td>2</td></tr><tr><td><strong>PKEY_COMMENT</strong></td><td>分区字段说明</td><td></td></tr><tr><td><strong>PKEY_NAME</strong></td><td>分区字段名</td><td>pt</td></tr><tr><td><strong>PKEY_TYPE</strong></td><td>分区字段类型</td><td>string</td></tr><tr><td><strong>INTEGER_IDX</strong></td><td>分区字段顺序</td><td>1</td></tr></tbody></table><h3 id="PARTITION-KEY-VALS"><a href="#PARTITION-KEY-VALS" class="headerlink" title="PARTITION_KEY_VALS"></a>PARTITION_KEY_VALS</h3><p><img src="https://yerias.github.io/hive_img/hive_metastore22.png" alt=""></p><p>该表存储分区字段值。</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>PART_ID</strong></td><td>分区ID</td><td>2</td></tr><tr><td><strong>PART_KEY_VAL</strong></td><td>分区字段值</td><td>2015-06-12</td></tr><tr><td><strong>INTEGER_IDX</strong></td><td>分区字段值顺序</td><td>0</td></tr></tbody></table><h3 id="PARTITION-PARAMS"><a href="#PARTITION-PARAMS" class="headerlink" title="PARTITION_PARAMS"></a>PARTITION_PARAMS</h3><p><img src="https://yerias.github.io/hive_img/hive_metastore23.png" alt=""></p><p>该表存储分区的属性信息。</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>PART_ID</strong></td><td>分区ID</td><td>2</td></tr><tr><td><strong>PARAM_KEY</strong></td><td>分区属性名</td><td>numFiles、numRows</td></tr><tr><td><strong>PARAM_VALUE</strong></td><td>分区属性值</td><td>15、502195</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Idea加载Hive源码，并且在控制台查询SQL</title>
      <link href="/2018/11/14/hive/14/"/>
      <url>/2018/11/14/hive/14/</url>
      
        <content type="html"><![CDATA[<h2 id="编译源码"><a href="#编译源码" class="headerlink" title="编译源码"></a>编译源码</h2><ol><li><p>下载Hive源码:<a href="http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.16.2-src.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.16.2-src.tar.gz</a></p></li><li><p>编译Hive源码(切记不要idea里面执行命令)：mvn clean package -DskipTests=true -Phadoop-2</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mvn clean package -DskipTests=true -Phadoop-2</span><br><span class="line"></span><br><span class="line">[INFO] Reactor Summary:</span><br><span class="line">[INFO]</span><br><span class="line">[INFO] Hive 1.1.0-cdh5.16.2 ............................... SUCCESS [  3.119 s]</span><br><span class="line">[INFO] Hive Classifications ............................... SUCCESS [  2.406 s]</span><br><span class="line">[INFO] Hive Shims Common .................................. SUCCESS [  3.327 s]</span><br><span class="line">[INFO] Hive Shims 0.23 .................................... SUCCESS [  3.494 s]</span><br><span class="line">[INFO] Hive Shims Scheduler ............................... SUCCESS [  2.423 s]</span><br><span class="line">[INFO] Hive Shims ......................................... SUCCESS [  1.463 s]</span><br><span class="line">[INFO] Hive Common ........................................ SUCCESS [  8.382 s]</span><br><span class="line">[INFO] Hive Serde ......................................... SUCCESS [  8.001 s]</span><br><span class="line">[INFO] Hive Metastore ..................................... SUCCESS [ 28.285 s]</span><br><span class="line">[INFO] Hive Ant Utilities ................................. SUCCESS [  1.668 s]</span><br><span class="line">[INFO] Spark Remote Client ................................ SUCCESS [  4.915 s]</span><br><span class="line">[INFO] Hive Query Language ................................ SUCCESS [01:36 min]</span><br><span class="line">[INFO] Hive Service ....................................... SUCCESS [ 22.921 s]</span><br><span class="line">[INFO] Hive Accumulo Handler .............................. SUCCESS [  5.496 s]</span><br><span class="line">[INFO] Hive JDBC .......................................... SUCCESS [  5.797 s]</span><br><span class="line">[INFO] Hive Beeline ....................................... SUCCESS [  3.957 s]</span><br><span class="line">[INFO] Hive CLI ........................................... SUCCESS [  4.060 s]</span><br><span class="line">[INFO] Hive Contrib ....................................... SUCCESS [  4.321 s]</span><br><span class="line">[INFO] Hive HBase Handler ................................. SUCCESS [  5.518 s]</span><br><span class="line">[INFO] Hive HCatalog ...................................... SUCCESS [  1.399 s]</span><br><span class="line">[INFO] Hive HCatalog Core ................................. SUCCESS [  5.933 s]</span><br><span class="line">[INFO] Hive HCatalog Pig Adapter .......................... SUCCESS [  4.632 s]</span><br><span class="line">[INFO] Hive HCatalog Server Extensions .................... SUCCESS [  4.477 s]</span><br><span class="line">[INFO] Hive HCatalog Webhcat Java Client .................. SUCCESS [  4.903 s]</span><br><span class="line">[INFO] Hive HCatalog Webhcat .............................. SUCCESS [  7.452 s]</span><br><span class="line">[INFO] Hive HCatalog Streaming ............................ SUCCESS [  4.306 s]</span><br><span class="line">[INFO] Hive HWI ........................................... SUCCESS [  3.461 s]</span><br><span class="line">[INFO] Hive ODBC .......................................... SUCCESS [  3.061 s]</span><br><span class="line">[INFO] Hive Shims Aggregator .............................. SUCCESS [  0.840 s]</span><br><span class="line">[INFO] Hive TestUtils ..................................... SUCCESS [  1.077 s]</span><br><span class="line">[INFO] Hive Packaging 1.1.0-cdh5.16.2 ..................... SUCCESS [  4.194 s]</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 04:22 min</span><br><span class="line">[INFO] Finished at: 2020-04-12T18:50:46+08:00</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure></li><li><p>IDEA导入源码</p></li></ol><h2 id="本地调试Hive-SQL"><a href="#本地调试Hive-SQL" class="headerlink" title="本地调试Hive SQL"></a>本地调试Hive SQL</h2><ol><li><p>集群服务器启动metastore:hive –service metastore -p 9083 &amp;</p></li><li><p>分别把集群的配置文件加载到hive-cli的resources下</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">core-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://ruozedata001:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">hdfs-site.xml</span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.use.datanode.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">mapred-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">yarn-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">hive-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://ruozedata001:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>在CliDriver类中加入vm参数:<code>-Djline.WindowsTerminal.directConsole=false</code>,Hive是默认Jline输入到控制太只支持Linux和mac,所以需要关闭</p><p><img src="https://yerias.github.io/hive_img/hive_client" alt="hive client"></p></li><li><p>运行CliDriver</p></li><li><p>控制台输入</p><p><img src="https://yerias.github.io/hive_img/hive_client2.png" alt="hive client"></p></li></ol><p><em>20200415更新：</em> idea2020.1版本支持本地调试hive，无需任何设置</p><hr><p>转载自：<a href="https://blog.csdn.net/jim8973/article/details/105503221" target="_blank" rel="noopener">https://blog.csdn.net/jim8973/article/details/105503221</a></p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HIVE调优之开发调优(2)</title>
      <link href="/2018/11/13/hive/13/"/>
      <url>/2018/11/13/hive/13/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>Hadoop 框架计算特性</li><li>优化常用手段</li><li>排序选择</li><li>怎样做笛卡尔积</li><li>怎样写 in/exists 语句</li><li>设置合理的 maptask 数量</li><li>小文件合并</li><li>设置合理的 reduceTask 的数量</li><li>合理利用分桶：Bucketing 和 Sampling</li><li>合理利用分区：Partition </li><li>Join 优化</li><li>Group By 优化</li><li>合理利用文件存储格式 </li><li>本地模式执行 MapReduce</li><li>并行化处理</li><li>设置压缩存储</li></ol><h2 id="Hadoop-框架计算特性"><a href="#Hadoop-框架计算特性" class="headerlink" title="Hadoop 框架计算特性"></a>Hadoop 框架计算特性</h2><ol><li><p>数据量大不是问题，数据倾斜是个问题</p></li><li><p>jobs 数比较多的作业运行效率相对比较低，比如即使有几百行的表，如果多次关联多次 汇总，产生十几个 jobs，耗时很长。原因是 map reduce 作业初始化的时间是比较长的</p></li><li><p>sum,count,max,min 等 UDAF，不怕数据倾斜问题，hadoop 在 map 端的汇总合并优化，使 数据倾斜不成问题</p></li><li><p>count(distinct userid)，在数据量大的情况下，效率较低，如果是多 count(distinct userid,month)效率更低，因为 count(distinct)是按 group by 字段分组，按 distinct 字段排序， 一般这种分布方式是很倾斜的，比如 PV 数据，淘宝一天 30 亿的 pv，如果按性别分组，分 配 2 个 reduce，每个 reduce 期望处理 15 亿数据，但现实必定是男少女多</p></li></ol><h2 id="优化常用手段"><a href="#优化常用手段" class="headerlink" title="优化常用手段"></a>优化常用手段</h2><ol><li><p>好的模型设计事半功倍</p></li><li><p>解决数据倾斜问题</p></li><li><p>减少 job 数</p></li><li><p>设置合理的 MapReduce 的 task 数，能有效提升性能。(比如，10w+级别的计算，用 160个 reduce，那是相当的浪费，1 个足够)</p></li><li><p>了解数据分布，自己动手解决数据倾斜问题是个不错的选择。这是通用的算法优化，但 算法优化有时不能适应特定业务背景，开发人员了解业务，了解数据，可以通过业务逻辑精确有效的解决数据倾斜问题</p></li><li><p>数据量较大的情况下，慎用 count(distinct)，group by 容易产生倾斜问题</p></li><li><p>对小文件进行合并，是行之有效的提高调度效率的方法，假如所有的作业设置合理的文件数，对云梯的整体调度效率也会产生积极的正向影响</p></li><li><p>优化时把握整体，单个作业最优不如整体最优</p></li></ol><h2 id="排序选择"><a href="#排序选择" class="headerlink" title="排序选择"></a>排序选择</h2><ul><li><p><strong>cluster by</strong>：对同一字段分桶并排序，不能和 sort by 连用</p></li><li><p><strong>distribute by + sort by</strong>：分桶，保证同一字段值只存在一个结果文件当中，结合 sort by 保证 每个 reduceTask 结果有序</p></li><li><p><strong>sort by</strong>：单机排序，单个 reduce 结果有序</p></li><li><p><strong>order by</strong>：全局排序，缺陷是只能使用一个 reduce</p></li></ul><p><strong>一定要区分这四种排序的使用方式和适用场景</strong></p><h2 id="怎样做笛卡尔积"><a href="#怎样做笛卡尔积" class="headerlink" title="怎样做笛卡尔积"></a>怎样做笛卡尔积</h2><p>当 Hive 设定为严格模式（hive.mapred.mode=strict）时，不允许在 HQL 语句中出现笛卡尔积， 这实际说明了 Hive 对笛卡尔积支持较弱。因为找不到 Join key，Hive 只能使用 1 个 reducer 来完成笛卡尔积。</p><p>当然也可以使用 limit 的办法来减少某个表参与 join 的数据量，但对于需要笛卡尔积语义的 需求来说，经常是一个大表和一个小表的 Join 操作，结果仍然很大（以至于无法用单机处 理），这时 MapJoin才是最好的解决办法。MapJoin，顾名思义，会在 Map 端完成 Join 操作。 这需要将 Join 操作的一个或多个表完全读入内存。</p><p>PS：MapJoin 在子查询中可能出现未知 BUG。在大表和小表做笛卡尔积时，规避笛卡尔积的 方法是，给 Join 添加一个 Join key，<strong>原理很简单：将小表扩充一列 join key，并将小表的条目复制数倍，join</strong> <strong>key 各不相同；将大表扩充一列 join key 为随机数。</strong></p><p><strong>精髓就在于复制几倍，最后就有几个 reduce 来做，而且大表的数据是前面小表扩张 key 值 范围里面随机出来的，所以复制了几倍 n，就相当于这个随机范围就有多大 n，那么相应的， 大表的数据就被随机的分为了 n 份。并且最后处理所用的 reduce 数量也是 n，而且也不会出现数据倾斜。</strong></p><h2 id="怎样写-in-exists-语句"><a href="#怎样写-in-exists-语句" class="headerlink" title="怎样写 in/exists 语句"></a>怎样写 in/exists 语句</h2><p>虽然经过测验，hive1.2.1 也支持 in/exists 操作，但还是推荐使用 hive 的一个高效替代方案：<strong>left semi join</strong></p><p>比如说：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">where</span> a.id <span class="keyword">in</span> (<span class="keyword">select</span> b.id <span class="keyword">from</span> b);</span><br><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">where</span> <span class="keyword">exists</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> b <span class="keyword">where</span> a.id = b.id);</span><br></pre></td></tr></table></figure><p>应该转换成：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">left</span> <span class="keyword">semi</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.id = b.id;</span><br></pre></td></tr></table></figure><h2 id="设置合理的-maptask-数量"><a href="#设置合理的-maptask-数量" class="headerlink" title="设置合理的 maptask 数量"></a>设置合理的 maptask 数量</h2><ol><li><p>Map 数过大</p><p>Map 阶段输出文件太小，产生大量小文件</p><p>初始化和创建 Map 的开销很大</p></li><li><p>Map 数太小</p><p>文件处理或查询并发度小，Job 执行时间过长</p><p>大量作业时，容易堵塞集群 </p></li></ol><p>在 MapReduce 的编程案例中，我们得知，一个MR Job的 MapTask 数量是由输入分片 InputSplit 决定的。而输入分片是由 FileInputFormat.getSplit()决定的。一个输入分片对应一个 MapTask， 而输入分片是由三个参数决定的：</p><p><img src="https://yerias.github.io/hive_img/%E8%AE%BE%E7%BD%AEmap%E4%BB%BB%E5%8A%A1%E6%95%B0%E9%87%8F.png" alt="设置map任务数量"></p><p>输入分片大小的计算是这么计算出来的：</p><p><strong>long splitSize = Math.max(minSize, Math.min(maxSize, blockSize))</strong></p><p>默认情况下，输入分片大小和 HDFS 集群默认数据块大小一致，也就是默认一个数据块，启 用一个 MapTask 进行处理，这样做的好处是避免了服务器节点之间的数据传输，提高 job 处理效率</p><p>两种经典的控制 MapTask 的个数方案：减少 MapTask 数或者增加 MapTask 数</p><ol><li>减少 MapTask 数是通过合并小文件来实现，这一点主要是针对数据源</li><li>增加 MapTask 数可以通过控制上一个 job 的 reduceTask 个数</li></ol><p>因为 Hive 语句最终要转换为一系列的 MapReduce Job 的，而每一个 MapReduce Job 是由一 系列的 MapTask 和 ReduceTask 组成的，默认情况下， MapReduce 中一个 MapTask 或者一个 ReduceTask 就会启动一个 JVM 进程，一个 Task 执行完毕后， JVM 进程就退出。这样如果任 务花费时间很短，又要多次启动 JVM 的情况下，JVM 的启动时间会变成一个比较大的消耗， 这个时候，就可以通过重用 JVM 来解决：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set mapred.job.reuse.jvm.num.tasks=5</span><br></pre></td></tr></table></figure><h2 id="小文件合并"><a href="#小文件合并" class="headerlink" title="小文件合并"></a>小文件合并</h2><p>文件数目过多，会给 HDFS 带来压力，并且会影响处理效率，可以通过合并 Map 和 Reduce 的 结果文件来消除这样的影响：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.merge.mapfiles = true ##在 map only 的任务结束时合并小文件</span><br><span class="line">set hive.merge.mapredfiles = false ## true 时在 MapReduce 的任务结束时合并小文件</span><br><span class="line">set hive.merge.size.per.task = 256*1000*1000 ##合并文件的大小</span><br><span class="line">set mapred.max.split.size=256000000; ##每个 Map 最大分割大小</span><br><span class="line">set mapred.min.split.size.per.node=1; ##一个节点上 split 的最少值</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; ##执行 Map 前进行小文件合并</span><br></pre></td></tr></table></figure><h2 id="设置合理的-reduceTask-的数量"><a href="#设置合理的-reduceTask-的数量" class="headerlink" title="设置合理的 reduceTask 的数量"></a>设置合理的 reduceTask 的数量</h2><p>Hadoop MapReduce 程序中，reducer 个数的设定极大影响执行效率，这使得 Hive 怎样决定 reducer 个数成为一个关键问题。遗憾的是 Hive 的估计机制很弱，不指定 reducer 个数的情 况下，Hive 会猜测确定一个 reducer 个数，基于以下两个设定：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive.exec.reducers.bytes.per.reducer（默认为 256000000）</span><br><span class="line">hive.exec.reducers.max（默认为 1009）</span><br><span class="line">mapreduce.job.reduces=-1（设置一个常量 reducetask 数量）</span><br></pre></td></tr></table></figure><p>计算 reducer 数的公式很简单： N=min(参数 2，总输入数据量/参数 1) 通常情况下，有必要手动指定 reducer 个数。考虑到 map 阶段的输出数据量通常会比输入有大幅减少，因此即使不设定 reducer 个数，重设参数 2 还是必要的。</p><p><strong>依据 Hadoop 的经验，可以将参数 2 设定为 0.95*(集群中 datanode 个数)。</strong> </p><h2 id="合并-MapReduce-操作"><a href="#合并-MapReduce-操作" class="headerlink" title="合并 MapReduce 操作"></a>合并 MapReduce 操作</h2><p>Multi-group by 是 Hive 的一个非常好的特性，它使得 Hive 中利用中间结果变得非常方便。 例如：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">FROM (<span class="keyword">SELECT</span> a.status, b.school, b.gender <span class="keyword">FROM</span> status_updates a <span class="keyword">JOIN</span> <span class="keyword">profiles</span> b <span class="keyword">ON</span> (a.userid =</span><br><span class="line">b.userid <span class="keyword">and</span> a.ds=<span class="string">'2009-03-20'</span> ) ) subq1</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> gender_summary <span class="keyword">PARTITION</span>(ds=<span class="string">'2009-03-20'</span>)</span><br><span class="line"><span class="keyword">SELECT</span> subq1.gender, <span class="keyword">COUNT</span>(<span class="number">1</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> subq1.gender</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> school_summary <span class="keyword">PARTITION</span>(ds=<span class="string">'2009-03-20'</span>)</span><br><span class="line"><span class="keyword">SELECT</span> subq1.school, <span class="keyword">COUNT</span>(<span class="number">1</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> subq1.school</span><br></pre></td></tr></table></figure><p>上述查询语句使用了 multi-group by 特性连续 group by 了 2 次数据，使用不同的 group by key。 这一特性可以减少一次 MapReduce 操作</p><h2 id="合理利用分桶：Bucketing-和-Sampling"><a href="#合理利用分桶：Bucketing-和-Sampling" class="headerlink" title="合理利用分桶：Bucketing 和 Sampling"></a>合理利用分桶：Bucketing 和 Sampling</h2><p>Bucket 是指将数据以指定列的值为 key 进行 hash，hash 到指定数目的桶中。这样就可以支 持高效采样了。如下例就是以 userid 这一列为 bucket 的依据，共设置 32 个 buckets</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> page_view(viewTime <span class="built_in">INT</span>, userid <span class="built_in">BIGINT</span>,</span><br><span class="line"> page_url <span class="keyword">STRING</span>, referrer_url <span class="keyword">STRING</span>,</span><br><span class="line"> ip <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'IP Address of the User'</span>)</span><br><span class="line"> <span class="keyword">COMMENT</span> <span class="string">'This is the page view table'</span></span><br><span class="line"> PARTITIONED <span class="keyword">BY</span>(dt <span class="keyword">STRING</span>, country <span class="keyword">STRING</span>)</span><br><span class="line"> CLUSTERED <span class="keyword">BY</span>(userid) SORTED <span class="keyword">BY</span>(viewTime) <span class="keyword">INTO</span> <span class="number">32</span> BUCKETS</span><br><span class="line"> <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line"> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'1'</span></span><br><span class="line"> COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'2'</span></span><br><span class="line"> <span class="keyword">MAP</span> <span class="keyword">KEYS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'3'</span></span><br><span class="line"> <span class="keyword">STORED</span> <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure><p>通常情况下，Sampling 在全体数据上进行采样，这样效率自然就低，它要去访问所有数据。 而如果一个表已经对某一列制作了 bucket，就可以采样所有桶中指定序号的某个桶，这就 减少了访问量。</p><p>如下例所示就是采样了 page_view 中 32 个桶中的第三个桶的全部数据：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> page_view <span class="keyword">TABLESAMPLE</span>(<span class="keyword">BUCKET</span> <span class="number">3</span> <span class="keyword">OUT</span> <span class="keyword">OF</span> <span class="number">32</span>);</span><br></pre></td></tr></table></figure><p>如下例所示就是采样了 page_view 中 32 个桶中的第三个桶的一半数据：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> page_view <span class="keyword">TABLESAMPLE</span>(<span class="keyword">BUCKET</span> <span class="number">3</span> <span class="keyword">OUT</span> <span class="keyword">OF</span> <span class="number">64</span>);</span><br></pre></td></tr></table></figure><h2 id="合理利用分区：Partition"><a href="#合理利用分区：Partition" class="headerlink" title="合理利用分区：Partition"></a>合理利用分区：Partition</h2><p> Partition 就是分区。分区通过在创建表时启用 partitioned by 实现，用来 partition 的维度并不 是实际数据的某一列，具体分区的标志是由插入内容时给定的。当要查询某一分区的内容时 可以采用 where 语句，形似 where tablename.partition_column = a 来实现。</p><p>创建含分区的表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> page_view(viewTime <span class="built_in">INT</span>, userid <span class="built_in">BIGINT</span>,</span><br><span class="line"> page_url <span class="keyword">STRING</span>, referrer_url <span class="keyword">STRING</span>,</span><br><span class="line"> ip <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'IP Address of the User'</span>)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span>(<span class="built_in">date</span> <span class="keyword">STRING</span>, country <span class="keyword">STRING</span>)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'1'</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> TEXTFILE;</span><br></pre></td></tr></table></figure><p>载入内容，并指定分区标志</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/pv_2008-06-08_us.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> page_view</span><br><span class="line"><span class="keyword">partition</span>(<span class="built_in">date</span>=<span class="string">'2008-06-08'</span>, country=<span class="string">'US'</span>);</span><br></pre></td></tr></table></figure><p>查询指定标志的分区内容</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> page_views.* <span class="keyword">FROM</span> page_views</span><br><span class="line"> <span class="keyword">WHERE</span> page_views.date &gt;= <span class="string">'2008-03-01'</span> <span class="keyword">AND</span> page_views.date &lt;= <span class="string">'2008-03-31'</span> <span class="keyword">AND</span></span><br><span class="line">page_views.referrer_url <span class="keyword">like</span> <span class="string">'%xyz.com'</span>;</span><br></pre></td></tr></table></figure><h2 id="Join-优化"><a href="#Join-优化" class="headerlink" title="Join 优化"></a>Join 优化</h2><p>总体原则：</p><ol><li><p>优先过滤后再进行 Join 操作，最大限度的减少参与 join 的数据量</p></li><li><p>小表 join 大表，最好启动 mapjoin</p></li><li><p>Join on 的条件相同的话，最好放入同一个 job，并且 join 表的排列顺序从小到大 </p></li></ol><p><strong>在使用写有 Join 操作的查询语句时有一条原则：应该将条目少的表/子查询放在 Join 操作 符的左边。</strong>原因是在 Join 操作的 Reduce 阶段，位于 Join 操作符左边的表的内容会被加 载进内存，将条目少的表放在左边，可以有效减少发生 OOM 错误的几率。对于一条语句 中有多个 Join 的情况，如果 Join 的条件相同，比如查询</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> pv_users</span><br><span class="line"><span class="keyword">SELECT</span> pv.pageid, u.age <span class="keyword">FROM</span> page_view p</span><br><span class="line"><span class="keyword">JOIN</span> <span class="keyword">user</span> u <span class="keyword">ON</span> (pv.userid = u.userid)</span><br><span class="line"><span class="keyword">JOIN</span> newuser x <span class="keyword">ON</span> (u.userid = x.userid);</span><br></pre></td></tr></table></figure><p>如果 Join 的 key 相同，不管有多少个表，都会则会合并为一个 Map-Reduce 任务，而不 是”n”个，在做 OUTER JOIN 的时候也是一样</p><p>如果 join 的条件不相同，比如：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> pv_users</span><br><span class="line"> <span class="keyword">SELECT</span> pv.pageid, u.age <span class="keyword">FROM</span> page_view p</span><br><span class="line"> <span class="keyword">JOIN</span> <span class="keyword">user</span> u <span class="keyword">ON</span> (pv.userid = u.userid)</span><br><span class="line"> <span class="keyword">JOIN</span> newuser x <span class="keyword">on</span> (u.age = x.age);</span><br></pre></td></tr></table></figure><p>Map-Reduce 的任务数目和 Join 操作的数目是对应的，上述查询和以下查询是等价的</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--先 page_view 表和 user 表做链接</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tmptable</span><br><span class="line"> <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> page_view p <span class="keyword">JOIN</span> <span class="keyword">user</span> u <span class="keyword">ON</span> (pv.userid = u.userid);</span><br><span class="line"><span class="comment">-- 然后结果表 temptable 和 newuser 表做链接</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> pv_users</span><br><span class="line"> <span class="keyword">SELECT</span> x.pageid, x.age <span class="keyword">FROM</span> tmptable x <span class="keyword">JOIN</span> newuser y <span class="keyword">ON</span> (x.age = y.age);</span><br></pre></td></tr></table></figure><p>在编写 Join 查询语句时，如果确定是由于 join 出现的数据倾斜，那么请做如下设置：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.skewjoin.key=100000; // 这个是 join 的键对应的记录条数超过这个值则会进行 分拆，值根据具体数据量设置</span><br><span class="line">set hive.optimize.skewjoin=true; // 如果是 join 过程出现倾斜应该设置为 true</span><br></pre></td></tr></table></figure><h2 id="Group-By-优化"><a href="#Group-By-优化" class="headerlink" title="Group By 优化"></a>Group By 优化</h2><h3 id="1-Map-端部分聚合"><a href="#1-Map-端部分聚合" class="headerlink" title="1. Map 端部分聚合"></a>1. Map 端部分聚合</h3><p>并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进 行部分聚合，最后在Reduce 端得出最终结果。</p><p>MapReduce 的 combiner 组件参数包括：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.map.aggr = true 是否在 Map 端进行聚合，默认为 True</span><br><span class="line">set hive.groupby.mapaggr.checkinterval = 100000 在 Map 端进行聚合操作的条目数目</span><br></pre></td></tr></table></figure><h3 id="2-使用-Group-By-有数据倾斜的时候进行负载均衡"><a href="#2-使用-Group-By-有数据倾斜的时候进行负载均衡" class="headerlink" title="2. 使用 Group By 有数据倾斜的时候进行负载均衡"></a>2. 使用 Group By 有数据倾斜的时候进行负载均衡</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.groupby.skewindata = true</span><br></pre></td></tr></table></figure><p>当 sql 语句使用 groupby 时数据出现倾斜时，如果该变量设置为 true，那么 Hive 会自动进行 负载均衡。<strong>策略就是把 MR 任务拆分成两个：第一个先做预汇总，第二个再做最终汇总</strong></p><p>在 MR 的第一个阶段中，Map 的输出结果集合会缓存到 maptaks 中，每个 Reduce 做部分聚 合操作，并输出结果，这样处理的结果是相同 Group By Key 有可能被分发到不同的 Reduce 中， 从而达到负载均衡的目的；第二个阶段 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成 最终的聚合操作。</p><h2 id="合理利用文件存储格式"><a href="#合理利用文件存储格式" class="headerlink" title="合理利用文件存储格式"></a>合理利用文件存储格式</h2><p>创建表时，尽量使用 orc、parquet 这些列式存储格式，因为列式存储的表，每一列的数据在 物理上是存储在一起的，Hive 查询时会只遍历需要列数据，大大减少处理的数据量。</p><h2 id="本地模式执行-MapReduce"><a href="#本地模式执行-MapReduce" class="headerlink" title="本地模式执行 MapReduce"></a>本地模式执行 MapReduce</h2><p>Hive 在集群上查询时，默认是在集群上 N 台机器上运行， 需要多个机器进行协调运行，这 个方式很好地解决了大数据量的查询问题。但是当 Hive 查询处理的数据量比较小时，其实 没有必要启动分布式模式去执行，因为以分布式方式执行就涉及到跨网络传输、多节点协调 等，并且消耗资源。这个时间可以只使用本地模式来执行 mapreduce job，只在一台机器上 执行，速度会很快。启动本地模式涉及到三个参数：</p><p><img src="https://yerias.github.io/hive_img/%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F.png" alt="本地模式"></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.exec.mode.local.auto=true 是打开 hive 自动判断是否启动本地模式的开关，但是只 是打开这个参数并不能保证启动本地模式，要当 map 任务数不超过</span><br><span class="line">hive.exec.mode.local.auto.input.files.max 的个数并且 map 输入文件大小不超过</span><br><span class="line">hive.exec.mode.local.auto.inputbytes.max 所指定的大小时，才能启动本地模式。</span><br></pre></td></tr></table></figure><h2 id="并行化处理"><a href="#并行化处理" class="headerlink" title="并行化处理"></a>并行化处理</h2><p>一个 hive sql 语句可能会转为多个 mapreduce Job，每一个 job 就是一个 stage，这些 job 顺序 执行，这个在 cli 的运行日志中也可以看到。但是有时候这些任务之间并不是是相互依赖的， 如果集群资源允许的话，可以让多个并不相互依赖 stage 并发执行，这样就节约了时间，提 高了执行速度，但是如果集群资源匮乏时，启用并行化反倒是会导致各个 job 相互抢占资源 而导致整体执行性能的下降。启用并行化：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.parallel=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number=<span class="number">8</span>; //同一个 sql 允许并行任务的最大线程数</span><br></pre></td></tr></table></figure><h2 id="设置压缩存储"><a href="#设置压缩存储" class="headerlink" title="设置压缩存储"></a>设置压缩存储</h2><h3 id="1-压缩的原因"><a href="#1-压缩的原因" class="headerlink" title="1. 压缩的原因"></a>1. 压缩的原因</h3><p>Hive 最终是转为 MapReduce 程序来执行的，而 MapReduce 的性能瓶颈在于网络 IO 和 磁盘 IO，要解决性能瓶颈，最主要的是减少数据量，对数据进行压缩是个好的方式。压缩 虽然是减少了数据量，但是压缩过程要消耗 CPU 的，但是在 Hadoop 中， 往往性能瓶颈不 在于 CPU，CPU 压力并不大，所以压缩充分利用了比较空闲的 CPU</p><h3 id="2-常用压缩方法对比"><a href="#2-常用压缩方法对比" class="headerlink" title="2. 常用压缩方法对比"></a>2. 常用压缩方法对比</h3><p><img src="https://yerias.github.io/hive_img/%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%E5%AF%B9%E6%AF%94.png" alt="压缩格式对比"></p><p>各个压缩方式所对应的 Class 类：</p><p><img src="https://yerias.github.io/hive_img/%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%E5%AF%B9%E5%BA%94%E7%9A%84%E7%B1%BB.png" alt="压缩格式对应的类"></p><h3 id="3-压缩方式的选择"><a href="#3-压缩方式的选择" class="headerlink" title="3. 压缩方式的选择"></a>3. 压缩方式的选择</h3><ol><li><p>压缩比率</p><p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E6%AF%94.png" alt="压缩比"></p></li><li><p>压缩解压缩速度</p><p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E6%97%B6%E9%97%B4.png" alt="压缩时间"></p></li><li><p>是否支持 Split</p><p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9.png" alt="压缩"></p></li></ol><h3 id="4-压缩使用"><a href="#4-压缩使用" class="headerlink" title="4. 压缩使用"></a>4. 压缩使用</h3><p>Job 输出文件按照 block 以 GZip 的方式进行压缩：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set mapreduce.output.fileoutputformat.compress=true // 默认值是 false</span><br><span class="line">set mapreduce.output.fileoutputformat.compress.type=BLOCK // 默认值是 Record</span><br><span class="line">set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec // 默认值是 org.apache.hadoop.io.compress.DefaultCodec</span><br></pre></td></tr></table></figure><p>Map 输出结果也以 Gzip 进行压缩：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set mapred.map.output.compress=true</span><br><span class="line">set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.GzipCodec // 默认值是 org.apache.hadoop.io.compress.DefaultCodec</span><br></pre></td></tr></table></figure><p>对 Hive 输出结果和中间都进行压缩：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.exec.compress.output=true // 默认值是 false，不压缩</span><br><span class="line">set hive.exec.compress.intermediate=true // 默认值是 false，为 true 时 MR 设置的压缩才启用</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HIVE调优之开发调优(1)</title>
      <link href="/2018/11/12/hive/9/"/>
      <url>/2018/11/12/hive/9/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>Fetch</li><li>本地模式</li><li>JVM重用</li><li>map数量</li><li>reduce数量</li><li>推测执行</li></ol><h2 id="Fetch"><a href="#Fetch" class="headerlink" title="Fetch"></a>Fetch</h2><p>通过修改<code>hive.fetch.task.conversion</code>参数可以让一些select查询可以转换为单个获取任务，不需要执行MapReduce任务，从而最小化延迟。</p><p>目前的版本中支持none、minimal和more</p><ul><li><code>none</code>: 是禁用这一特性</li><li><code>minimal</code>: 允许使用<code>SELECT *</code>、<code>FILTER on partition columns (WHERE and HAVING clauses)</code>、<code>LIMIT only</code></li><li><code>more</code>: 最大程度的允许使用 <code>SELECT</code>, <code>FILTER</code>, <code>LIMIT only (including TABLESAMPLE, virtual columns)、where</code></li></ul><p>当前版本默认使用more</p><h2 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h2><p><code>hive.exec.mode.local.auto</code> 参数决定hive是否允许使用本地化，默认<code>hive.exec.mode.local.auto=false</code> 没有启用本地化</p><p><code>hive.exec.mode.local.auto.inputbytes.max</code> 参数规定了使用本地化处理的最大的文件字节数，默认是128M</p><p><code>hive.exec.mode.local.auto.input.files.max</code> 参数规定了使用本地化处理的最大文件数，默认是4个</p><h2 id="JVM重用"><a href="#JVM重用" class="headerlink" title="JVM重用"></a>JVM重用</h2><p>在目前使用的版本中，<code>mapreduce.job.jvm.numtasks</code> 参数可以控制Java虚拟机的回收，由于<code>mapTask</code>或者<code>reduceTask</code>都是进程，需要启用JVM，作业运行结束了关闭JVM，使用这个参数控制JVM运行完作业不关机，继续执行作业。默认是1个。</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.jvm.numtasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>How many tasks to run per jvm. If set to -1, there is no limit.  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="map数量"><a href="#map数量" class="headerlink" title="map数量"></a>map数量</h2><p><code>mapTask</code>数量由输入的文件大小、文件数和输入的文件产生多少个block决定。</p><p>那么我们如何考虑map的数量呢?</p><p>理论上讲<code>mapTask</code>越多Map作业的并行度越高，但是耗费的时间和资源也越多，map、reduce作业都是进程级别。</p><p>场景：128M的数据只有一两个列的数据记录数有几千万个，必须拆开用NTILE(n)==&gt; 改变map任务的数量</p><h2 id="reduce数量"><a href="#reduce数量" class="headerlink" title="reduce数量"></a>reduce数量</h2><p><code>mapred.reduce.tasks=-1</code> 参数决定每个作业的默认数量。通常设置为接近可用主机数量的素数。通过将此属性设置为-1,Hive将自动计算出还原器的数量。默认是-1，即自动计算</p><p><code>hive.exec.reducers.bytes.per.reducer=256M</code> 参数决定了reduce最大的字节数，在Hive 0.14.0及以后的版本中，默认为256 MB，也就是说，如果输入大小为1 GB，那么将使用4个reduce。</p><p><code>hive.exec.reducers.max=1099</code> 参数决定最大可以使用的reduce数量，如果<code>mapred.reduce.tasks</code> 参数为-1，即自动计算reduce数量，那么Hive将使用这个参数作为最大的reduce数量，自动确定reduce的数量。</p><p>计算reducer数的公式很简单N=min( <code>hive.exec.reducers.max</code> ，总输入数据量/ <code>hive.exec.reducers.bytes.per.reducer</code> )</p><p>reduce的数量决定最终文件输出的数量<br>思路：reduce数量越多，小文件越多，reduce数量越少，文件大耗费的时间多，最终在reduce文件的大小和需要消耗的时间取个折中。 如果没有reduce，那么map的数据个数决定了输出文件个数 。</p><p>Spark3.0 自动适配</p><h2 id="推测执行"><a href="#推测执行" class="headerlink" title="推测执行"></a>推测执行</h2><ol><li><p>作业完成时间取决于最慢的任务完成时间</p><p>一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。</p><p>思考：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？</p></li><li><p>推测执行机制</p><p>发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果。</p></li><li><p>执行推测任务的前提条件</p></li></ol><ul><li><p>每个Task只能有一个备份任务</p></li><li><p>当前 Job 已完成的 Task 必须不小于0.05（5%）</p></li><li><p>开启推测执行参数设置。mapred-site.xml文件中默认是打开的。</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some map tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some reduce tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul><ol start="4"><li>不能启用推测执行机制情况</li></ol><ul><li><p>任务间存在严重的数据倾斜，数据倾斜跑不过去的，开启多少个推测执行都跑不过去；</p></li><li><p>特殊任务，比如任务向数据库中写数据。</p></li></ul><ol start="5"><li><p>生产建议</p><p>一般生产生禁用此功能，除非特殊场景直接命令开启</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HIVE调优之存储格式</title>
      <link href="/2018/11/11/hive/12/"/>
      <url>/2018/11/11/hive/12/</url>
      
        <content type="html"><![CDATA[<p>目录</p><ol><li>行式数据库和列式数据库的对比</li><li>存储格式的比较</li><li>存储格式的应用</li></ol><h2 id="行式数据库和列式数据库的对比"><a href="#行式数据库和列式数据库的对比" class="headerlink" title="行式数据库和列式数据库的对比"></a>行式数据库和列式数据库的对比</h2><ol><li><p>存储比较</p><p>行式数据库存储在hdfs上式按行进行存储的，一个block存储一或多行数据。而列式数据库在hdfs上则是按照列进行存储，一个block可能有一列或多列数据。</p></li><li><p>压缩比较</p><p>对于行式数据库，必然按行压缩，当一行中有多个字段，各个字段对应的数据类型可能不一致，压缩性能压缩比就比较差。</p><p>对于列式数据库，必然按列压缩，每一列对应的是相同数据类型的数据，故列式数据库的压缩性能要强于行式数据库。</p></li><li><p>查询比较</p><p>假设执行的查询操作是：<code>select id,name from table_emp;</code></p><p>对于行式数据库，它要遍历一整张表将每一行中的id,name字段拼接再展现出来，这样需要查询的数据量就比较大，效率低。</p><p>对于列式数据库，它只需找到对应的id,name字段的列展现出来即可，需要查询的数据量小，效率高。</p><p>假设执行的查询操作是：<code>select * from table_emp;</code></p><p>对于这种查询整个表全部信息的操作，由于列式数据库需要将分散的行进行重新组合，行式数据库效率就高于列式数据库。</p><p><strong>但是，在大数据领域，进行全表查询的场景少之又少，进而我们使用较多的还是列式数据库及列式储存。</strong></p></li></ol><h2 id="存储格式的比较"><a href="#存储格式的比较" class="headerlink" title="存储格式的比较"></a>存储格式的比较</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">: SEQUENCEFILE</span><br><span class="line">| TEXTFILE  -- (Default, depending on hive.default.fileformat configuration)</span><br><span class="line">| RCFILE   -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">| ORC     -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line">| PARQUET   -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line">| AVRO    -- (Note: Available in Hive 0.14.0 and later)</span><br><span class="line">| JSONFILE  -- (Note: Available in Hive 4.0.0 and later)</span><br><span class="line">| INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname</span><br></pre></td></tr></table></figure><ul><li><p><strong>SEQUENCEFILE:</strong> Hadoop API提供的一种二进制文件支持，其具有使用方便、可分割、可压缩的特点。 </p></li><li><p><strong>TEXTFILE:</strong> textfile为默认格式，存储方式为行存储。数据不做压缩，磁盘开销大，数据解析开销大。 </p></li><li><p><strong>RCFILE（Record Columnar File）:</strong> 一种行列存储相结合的存储方式。 </p></li><li><p><strong>ORC:</strong> 数据按照行分块，每个块按照列存储，其中每个块都存储有一个索引。hive给出的新格式，属于RCFILE的升级版,性能有大幅度提升,而且数据可以压缩存储,压缩快 快速列存取。 </p><p><strong>ORC这种文件格式可以作用于表或者表的分区，可以通过以下几种方式进行指定：</strong></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">CREATE TABLE ... STORED AS ORC(常用)</span><br><span class="line">ALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORC</span><br><span class="line">SET hive.default.fileformat=Orc</span><br></pre></td></tr></table></figure></li><li><p><strong>PARQUET:</strong> Parquet也是一种行式存储，同时具有很好的压缩性能；同时可以减少大量的表扫描和反序列化的时间。</p></li></ul><p>如果要使用其他格式作为hive表数据的存储格式，则必须先存在一张相同数据的存储格式为TEXTFILE的表table_t0,然后在建表时使用“<code>insert into table table_stored_file_ORC select from table_t0;</code>”创建。或者使用”<code>create table as select from table_t0;</code>”创建。</p><p>相同数据，分别以TextFile、SequenceFile、RcFile、ORC、Parquet存储的比较。</p><ol><li><p>源文件大小</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop data]$ hdfs dfs -du -s -h /data/page_views.dat</span><br><span class="line">18.1 M  18.1 M  /data/page_views.dat</span><br></pre></td></tr></table></figure></li><li><p>TextFile</p><ul><li><p>建表&amp;加载数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_text( </span><br><span class="line">track_time <span class="keyword">string</span>, </span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>, </span><br><span class="line">session_id <span class="keyword">string</span>, </span><br><span class="line">referer <span class="keyword">string</span>, </span><br><span class="line">ip <span class="keyword">string</span>, </span><br><span class="line">end_user_id <span class="keyword">string</span>, </span><br><span class="line">city_id <span class="keyword">string</span> </span><br><span class="line">)<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> TEXTFilE;</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载导入</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">'/data/page_views.dat'</span> <span class="keyword">into</span> <span class="keyword">table</span> page_views_text;</span><br></pre></td></tr></table></figure></li><li><p>表大小</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">18.1 M  18.1 M  /user/hive/warehouse/store_format.db/page_views_text</span><br></pre></td></tr></table></figure></li><li><p>查询时读取数据量大小(单位字节，下面都用这个SQL测试)</p><p><code>select count(1) from page_views_text where session_id=&quot;B58W48U4WKZCJ5D1T3Z9ZY88RU7QA7B1&quot;;</code> </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 19024045</span><br></pre></td></tr></table></figure></li></ul></li><li><p>SequenceFile</p><ul><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_seq( </span><br><span class="line">track_time <span class="keyword">string</span>, </span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>, </span><br><span class="line">session_id <span class="keyword">string</span>, </span><br><span class="line">referer <span class="keyword">string</span>, </span><br><span class="line">ip <span class="keyword">string</span>, </span><br><span class="line">end_user_id <span class="keyword">string</span>, </span><br><span class="line">city_id <span class="keyword">string</span> </span><br><span class="line">)<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> SequenceFile;</span><br><span class="line"></span><br><span class="line"><span class="comment">#查询导入</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> page_views_Seq  <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text;</span><br></pre></td></tr></table></figure></li><li><p>表大小</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">19.6 M  19.6 M  /user/hive/warehouse/store_format.db/page_views_seq</span><br></pre></td></tr></table></figure></li><li><p>查询时读取数据量大小(字节)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 61513817</span><br></pre></td></tr></table></figure></li></ul></li><li><p>RcFile</p><ul><li><p>建表(CTAS)</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_rcfile</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> RcFile</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text</span><br></pre></td></tr></table></figure></li><li><p>表大小</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">17.9 M  17.9 M  /user/hive/warehouse/store_format.db/page_views_rcfile</span><br></pre></td></tr></table></figure></li><li><p>查询时读取数据量大小(字节)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 3726738</span><br></pre></td></tr></table></figure></li></ul></li><li><p>ORC</p><ul><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_orc</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> ORC</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text</span><br></pre></td></tr></table></figure></li><li><p>表大小</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 1258828</span><br></pre></td></tr></table></figure></li><li><p>查询时读取数据量大小(字节)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2.8 M  2.8 M  /user/hive/warehouse/store_format.db/page_views_orc</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Parquet</p><ul><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_par</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> Parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text</span><br></pre></td></tr></table></figure></li><li><p>表大小</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">13.1 M  13.1 M  /user/hive/warehouse/store_format.db/page_views_par</span><br></pre></td></tr></table></figure></li><li><p>查询时读取数据量大小(字节)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 2688348</span><br></pre></td></tr></table></figure></li></ul></li></ol><p>不同的存储格式不仅表现在存储空间上的不同，对于数据的查询，效率也不一样。因为对于不同的存储格式，执行相同的查询操作，他们访问的数据量大小是不一样的。</p><p>不同格式表存储大小的比较</p><p><img src="https://yerias.github.io/hive_img/%E8%A1%A8%E5%A4%A7%E5%B0%8F%E6%AF%94%E8%BE%83.jpg" alt="表大小比较"></p><p>不同格式表读取数据量比较</p><p><img src="https://yerias.github.io/hive_img/%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%87%8F%E6%AF%94%E8%BE%83.jpg" alt="读取数据量比较"></p><h2 id="存储格式的应用"><a href="#存储格式的应用" class="headerlink" title="存储格式的应用"></a>存储格式的应用</h2><p>原文件还是上面的那个</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop data]$ hdfs dfs -du -s -h /data/page_views.dat</span><br><span class="line">18.1 M  18.1 M  /data/page_views.dat</span><br></pre></td></tr></table></figure><ol><li><p>ORC+Zlib结合</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_orc_zlib</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> ORC </span><br><span class="line">TBLPROPERTIES(<span class="string">"orc.compress"</span>=<span class="string">"ZLIB"</span>)</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text;</span><br></pre></td></tr></table></figure><p>用ORC+Zlip之后的文件为2.8M</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2.8 M  2.8 M  /user/hive/warehouse/store_format.db/page_views_orc_zlib</span><br></pre></td></tr></table></figure></li><li><p>Parquet+gzip结合</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> parquet.compression=gzip;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_parquet_gzip</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> PARQUET </span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text;</span><br></pre></td></tr></table></figure><p>用Parquet+gzip之后的文件为3.9M</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">3.9 M  3.9 M  /user/hive/warehouse/store_format.db/page_views_parquet_gzip</span><br></pre></td></tr></table></figure></li><li><p>Parquet+Lzo结合</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.exec.compress.output=<span class="literal">true</span>;  </span><br><span class="line"><span class="keyword">SET</span> mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.lzopCodec;</span><br><span class="line"><span class="keyword">SET</span> mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec; </span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_parquet_lzo <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> PARQUET</span><br><span class="line">TBLPROPERTIES(<span class="string">"parquet.compression"</span>=<span class="string">"lzo"</span>)</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text;</span><br></pre></td></tr></table></figure><p>用Parquet+Lzo(未建立索引)之后的文件为6.2M</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">6.2 M  6.2 M  /user/hive/warehouse/store_format.db/page_views_parquet_lzo</span><br></pre></td></tr></table></figure><p>建立索引(表好像没啥用)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar $HADOOP_HOME/share/hadoop/common/hadoop-lzo.jar com.hadoop.compression.lzo.LzoIndexer /user/hive/warehouse/store_format.db/page_views_parquet_lzo</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HIVE Skewed Table&amp;List Bucketing</title>
      <link href="/2018/11/10/hive/11/"/>
      <url>/2018/11/10/hive/11/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>HIVE Skewed Table</li><li>Skewed Join Optimization(最优化)</li><li>Basic Partitioning</li><li>List Bucketing</li><li>Skewed Table vs List Bucketing Table</li><li>List Bucketing Validation</li></ol><h2 id="HIVE-Skewed-Table"><a href="#HIVE-Skewed-Table" class="headerlink" title="HIVE Skewed Table"></a>HIVE Skewed Table</h2><p>Skewed Table可用于提高一个或多个列具有偏斜值的表的性能。通过指定经常出现的值（严重偏斜），Hive会自动将它们拆分成单独的文件（或在列表存储的情况下为目录），并在查询过程中考虑到这一事实，以便它可以跳过或包括整个文件（或在列表存储的情况下为目录）。<strong>可以在表创建过程中在每个表级别上指定。</strong></p><p>若是指定了STORED AS DIRECTORIES，也就是使用列表桶（ListBucketing），hive会对倾斜的值建立子目录，查询会更加得到优化。</p><p>下面的示例显示具有三个偏斜值的一列，还可以选择使用STORED AS DIRECTORIES子句来指定列表存储。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> list_bucket_single (<span class="keyword">key</span> <span class="keyword">STRING</span>, <span class="keyword">value</span> <span class="keyword">STRING</span>)</span><br><span class="line">SKEWED <span class="keyword">BY</span> (<span class="keyword">key</span>) <span class="keyword">ON</span> (<span class="number">1</span>,<span class="number">5</span>,<span class="number">6</span>) [<span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES];</span><br></pre></td></tr></table></figure><p>这是一个带有两个倾斜列的表的示例。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> list_bucket_multiple (col1 <span class="keyword">STRING</span>, col2 <span class="built_in">int</span>, col3 <span class="keyword">STRING</span>)</span><br><span class="line">SKEWED <span class="keyword">BY</span> (col1, col2) <span class="keyword">ON</span> ((<span class="string">'s1'</span>,<span class="number">1</span>), (<span class="string">'s3'</span>,<span class="number">3</span>), (<span class="string">'s13'</span>,<span class="number">13</span>), (<span class="string">'s78'</span>,<span class="number">78</span>)) [<span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES];</span><br></pre></td></tr></table></figure><p>可以使用alter table语句来对已创建的表修改倾斜信息。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name SKEWED <span class="keyword">BY</span> (col_name1, col_name2, ...) <span class="keyword">ON</span> ([(col_name1_value, col_name2_value, ...) [, (col_name1_value, col_name2_value), ...][<span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES];</span><br></pre></td></tr></table></figure><p><code>STORED AS DIRECTORIES</code>选项确定倾斜表是否使用列表存储功能，该功能为倾斜值创建子目录。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">NOT</span> SKEWED;</span><br></pre></td></tr></table></figure><p><code>NOT SKEWED</code>选项使表不倾斜，并关闭列表存储功能（因为列表存储表始终是倾斜的）。这会影响在ALTER语句之后创建的分区，但对在ALTER语句之前创建的分区没有影响。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">NOT</span> <span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES;</span><br></pre></td></tr></table></figure><p><code>NOT STORED</code>会关闭列表存储功能，但是表仍然歪斜。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">SET</span> SKEWED LOCATION (col_name1=<span class="string">"location1"</span> [, col_name2=<span class="string">"location2"</span>, ...] );</span><br></pre></td></tr></table></figure><p>修改list bucketing倾斜值的存储位置映射。</p><h2 id="Skewed-Join-Optimization-最优化"><a href="#Skewed-Join-Optimization-最优化" class="headerlink" title="Skewed Join Optimization(最优化)"></a>Skewed Join Optimization(最优化)</h2><p>两个大数据表的连接由一组MapReduce作业完成，它们首先根据连接键对表进行排序，然后将它们连接起来，mapper将相同键的行发送到同一个reduce。<br>假设表A id字段有值1，2，3，4，并且表B也含有id列，含有值1，2，3。我们使用如下语句来进行连接。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> A.id <span class="keyword">from</span> A <span class="keyword">join</span> B <span class="keyword">on</span> A.id = B.id</span><br></pre></td></tr></table></figure><p>将会有一组mappers读这两个表并基于连接键id发送到reducers，假设id=1的行分发到Reducer R1，id=2的分发到R2等等，这些reducer对A和B进行交叉连接，R4从A得到id=4的所有行，但是不会产生任何结果。</p><p>现在我们假定A在id=1上倾斜，这样R2和R3将会很快完成但是R1会执行很长时间，因此成为job的瓶颈。若是用户知道这些倾斜信息，这种瓶颈可以使用如下方法人工避免：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> A.id <span class="keyword">from</span> A <span class="keyword">join</span> B <span class="keyword">on</span> A.id = B.id <span class="keyword">where</span> A.id &lt;&gt; <span class="number">1</span>;</span><br><span class="line"><span class="keyword">select</span> A.id <span class="keyword">from</span> A <span class="keyword">join</span> B <span class="keyword">on</span> A.id = B.id <span class="keyword">where</span> A.id = <span class="number">1</span> <span class="keyword">and</span> B.id = <span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>第一个查询没有倾斜数据将会很快的完成，如果我们假定表B中只有少量的B.id=1的行，能够直接加载到内存中，通过将B的数据存储到内存中的哈希表中，join将会高效的完成，因此可以再mappper端进行连接，而不用reduce，效率会高很多，最后合并结果。</p><p>优点：</p><ul><li>如果少量的倾斜键占了很大一部分数据，它们将不会成为瓶颈。</li></ul><p>缺点：</p><ul><li><p>表A和表B需要分别读和处理两次；</p></li><li><p>结果需要合并；</p></li><li><p>需要人工的处理倾斜数据。</p></li></ul><p>hive为了避免上面的操作，在处理数据是对倾斜值进行特殊处理，首先读表B并且存储B.id=1的数据到内存的哈希表中，运行一组mappers来读取表A并做以下操作：</p><ol><li>若id为1，使用B的id=1的哈希表来计算结果</li><li>对于其他值，发送到reducer端来join，这个reduce也会从B的mapper中得到对应需要连接的数据。</li></ol><p>使用这种方法，最终我们只读取B两次，并且A中的倾斜数据在mapper中进行连接，不会被发送到reducer，其他的键值通过map/reduce。</p><p>假设B的行很少，而键在A中倾斜。因此可以将这些行加载到内存中。<strong>若是使用ListBucketing对倾斜值单独存储，会有更好的性能。在读倾斜的数据到内存中时可以指定到倾斜目录下的数据。</strong></p><h2 id="Basic-Partitioning"><a href="#Basic-Partitioning" class="headerlink" title="Basic Partitioning"></a>Basic Partitioning</h2><p>有如下问题：存在许多表是这种格式</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> T(a, b, c, ....., x) partitioned <span class="keyword">by</span> (ds);</span><br></pre></td></tr></table></figure><p>但是以下查询需要更加高效(这不扯蛋吗，有分区的查询条件不是分区)：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> x = <span class="number">10</span>;</span><br></pre></td></tr></table></figure><p>字段x中含有倾斜数据，一般情况下x的值中大约有1000个值有重度倾斜，其他值基数很小，当然，每天倾斜的X的值可能改变，上述要求可以通过以下方式解决：</p><p>为值“ x”创建一个分区。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> T(a,b,c, .......) partitioned <span class="keyword">by</span> (ds, x)</span><br></pre></td></tr></table></figure><p>优点</p><ul><li>现有的Hive就足够了。</li></ul><p>缺点</p><ul><li>HDFS可伸缩性：HDFS中的文件数量增加。</li><li>HDFS可伸缩性：HDFS中的中间文件数量增加。例如，如果有1000个映射器和1000个分区，并且每个映射器每个键至少获得1行，我们最终将创建100万个中间文件。</li><li>Metastore的可伸缩性：Metastore会随着分区数量的增长而扩展。</li></ul><h2 id="List-Bucketing"><a href="#List-Bucketing" class="headerlink" title="List Bucketing"></a>List Bucketing</h2><p>上边方法提到将含有倾斜值得列作为分区存储，但是可能产生大量的目录，为什么不把列值不倾斜的放在一起呢，将每个倾斜的值单独存放一个目录，于是有了List Bucketing。</p><p>这个映射在表或分区级别的Metastore中维护。倾斜键的列表存储在表级别中，这个列表可以由客户端周期的提供，并且在新的分区载入时可以被更新。</p><p>如下例子，一个表含有一个x字段倾斜值的列表：6，20，30，40。当一个新的分区载入时，它会创建5个目录（4个目录对应x的4个倾斜值，另外一个目录是其余值）。这个表的数据被分成了5个部分：6，20，30，40，others。这跟上一节介绍的分桶表类似，桶的个数决定了文件的个数。倾斜键的整个列表存储在每个表或者分区中。</p><p>当使用一下查询时，hive编译器会仅仅使用x=30对应的目录去运行map-reduce。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> ds = <span class="string">'2012-04-15'</span> <span class="keyword">and</span> x = <span class="number">30</span>;</span><br></pre></td></tr></table></figure><p>若是查询是x=50，则会使用x=others对应的目录去运行map-reduce作业。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> ds = <span class="string">'2012-04-15'</span> <span class="keyword">and</span> x = <span class="number">50</span>;</span><br></pre></td></tr></table></figure><p>这种方法在一下条件下是很有效的：</p><ol><li><strong>每个分区的倾斜键占总数据的一大部分</strong>。在上边的例子中，如果倾斜的键（6，20，30，40）只占一小部分数据（比如20%）,那么在查询x=50时依然需要扫描80%的数据。</li><li><strong>每个分区的倾斜键数量非常少</strong>，因为这个倾斜值列表存在在元数据库中，在元数据库中为每个分区存储100w个倾斜键是没有意义的。</li></ol><p>这种方法也可被扩展到含有多个列产生的倾斜键，例如我们想优化一下查询</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> x = <span class="number">10</span> <span class="keyword">and</span> y = <span class="string">'b'</span>;</span><br></pre></td></tr></table></figure><p>扩展以上的方法，对于（x，y）每个倾斜值,也按照上边方式单独存储，因此元数据库会有以下映射： </p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">(10, 'a') <span class="comment">--&gt; 1, (10, 'b') --&gt; 2, (20, 'c') --&gt; 3, (others) --&gt; 4.</span></span><br></pre></td></tr></table></figure><p>因此可直接找到2对应的目录，减少要处理的数据。</p><p>同时以下查询也会有一定程度的优化：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> x = <span class="number">10</span>; </span><br><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> y = <span class="string">'b'</span>;</span><br></pre></td></tr></table></figure><p>以上两个语句在执行的过程中会裁剪掉一部分数据，例如，对x=10的查询hive编译器可以裁剪掉 ( 20 , c ) 对应的文件，对于 y = ‘b’，( 10 , ‘a’ )  和 ( 20 , ‘c’ ) 对应的文件会被裁剪掉，一定程度能够减少扫描的数据量。</p><p>这种方法不适用于以下场景：</p><ul><li>倾斜键数目非常多，元数据规模问题</li><li>许多情况下，倾斜键由多个列组成，但是在查询中，没有使用到倾斜键中的那些列。</li></ul><h2 id="Skewed-Table-vs-List-Bucketing-Table"><a href="#Skewed-Table-vs-List-Bucketing-Table" class="headerlink" title="Skewed Table vs List Bucketing Table"></a>Skewed Table vs List Bucketing Table</h2><ul><li>Skewed Table是一个表它含有倾斜的信息。</li><li>List Bucketing Table是Skewed Table，此外，它告诉hive使用列表桶的特点：为倾斜值创建子目录。</li></ul><p>以下说明两者的存储区别：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t1 (x <span class="keyword">string</span>) skewed <span class="keyword">by</span> (x) <span class="keyword">on</span> (‘a’, ‘b’) partitioned <span class="keyword">by</span> dt location ‘/<span class="keyword">user</span>/hive/warehouse/t1’;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t2 (x <span class="keyword">string</span>) skewed <span class="keyword">by</span> (x) <span class="keyword">on</span> (‘a’, ‘b’) <span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES partitioned <span class="keyword">by</span> dt location ‘/<span class="keyword">user</span>/hive/warehouse/t2’ ;</span><br></pre></td></tr></table></figure><p>两者存储的形式如下所示：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">/user/hive/warehouse/t1/dt=something/data.txt</span><br><span class="line">/user/hive/warehouse/t2/dt=something/x=a/data.txt</span><br><span class="line">/user/hive/warehouse/t2/dt=something/x=b/data.txt</span><br><span class="line">/user/hive/warehouse/t2/dt=something/default/data.txt</span><br></pre></td></tr></table></figure><h2 id="List-Bucketing-Validation"><a href="#List-Bucketing-Validation" class="headerlink" title="List Bucketing Validation"></a>List Bucketing Validation</h2><p>由于列表桶的子目录特点，它不能够与一些特征共存。</p><p>DDL</p><p>列表桶与以下共存会抛出编译错误：</p><ul><li>normal bucketing (clustered by, tablesample, etc.)</li><li>external table</li><li>“ load data …”</li><li>CTAS (Create Table As Select) queries</li></ul><p>DML</p><p>与一下DML操作共存也会跳出错误：</p><ul><li>“ insert into ”</li><li>normal bucketing (clustered by, tablesample, etc.)</li><li>external table</li><li>non-RCfile due to merge</li><li>non-partitioned table</li></ul><hr><p>参考文献：</p><p>1.<a href="https://cwiki.apache.org/confluence/display/Hive/ListBucketing" target="_blank" rel="noopener">HIVE ListBucketing</a><br>2.<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-SkewedTables" target="_blank" rel="noopener">HIVE LanguageManualDDL-SkewedTables</a><br>3.<a href="https://cwiki.apache.org/confluence/display/Hive/Skewed+Join+Optimization" target="_blank" rel="noopener">HIVE Skewed Join Optimization</a></p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大表Join大表&amp;大表Join小表&amp;group By解决数据倾斜</title>
      <link href="/2018/11/09/hive/10/"/>
      <url>/2018/11/09/hive/10/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>大表Join大表</li><li>大表Join小表</li><li>group By解决</li></ol><h2 id="大表Join大表"><a href="#大表Join大表" class="headerlink" title="大表Join大表"></a>大表Join大表</h2><h3 id="思路一：SMBJoin"><a href="#思路一：SMBJoin" class="headerlink" title="思路一：SMBJoin"></a>思路一：SMBJoin</h3><p>smb是sort  merge bucket操作，首先进行排序，继而合并，然后放到所对应的bucket中去，bucket是hive中和分区表类似的技术，就是按照key进行hash，相同的hash值都放到相同的buck中去。在进行两个表联合的时候。我们首先进行分桶，在join会大幅度的对性能进行优化。也就是说，在进行联合的时候，是bukect中的一小部分和bukect中的一小部分进行联合，table联合都是等值连接，相同的key都放到了同一个bucket中去了，那么在联合的时候就会大幅度的减小无关项的扫描。</p><ol><li><p>设置参数</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join.noconditionaltask=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li><li><p>两个表的bucket数量相等</p></li><li><p>Bucket列、Join列、Sort列、Skewed列为相同的字段</p></li><li><p>必须是应用在bucket mapjoin 的场景中</p></li><li><p>注意点</p><p>hive并不检查两个join的表是否已经做好bucket且sorted，需要用户自己去保证join的表，否则可能数据不正确。有两个办法</p><ul><li><p>hive.enforce.sorting 设置为true</p></li><li><p>手动生成符合条件的数据，通过在sql中用distributed c1 sort by c1 或者 cluster by c1，表创建时必须是CLUSTERED且SORTED，如下</p></li><li><p>创建Skewed Table提高有一个或多个列有倾斜值的表的性能，例如：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_smb_2(<span class="keyword">mid</span> <span class="keyword">string</span>,age_id <span class="keyword">string</span>)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span>(<span class="keyword">mid</span>) SORTED <span class="keyword">BY</span>(<span class="keyword">mid</span>) <span class="keyword">INTO</span> <span class="number">500</span> BUCKETS</span><br><span class="line">SKEWED <span class="keyword">BY</span>(<span class="keyword">mid</span>) <span class="keyword">on</span> (<span class="number">001</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>案例</p><ol><li><p>设置先关参数</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join.noconditionaltask=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li><li><p>创建桶表</p><p>user表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_info_bucket(userid <span class="keyword">string</span>,uname <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span>(userid) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line">SKEWED <span class="keyword">BY</span>(userid) <span class="keyword">on</span> (<span class="number">001</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line">STORED AS DIRECTORIES</span><br></pre></td></tr></table></figure><p>domain表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> domain_info_bucket(userid <span class="keyword">string</span>,domainid <span class="keyword">string</span>,<span class="keyword">domain</span> <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span> (userid) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line">SKEWED <span class="keyword">BY</span>(userid) <span class="keyword">on</span> (<span class="number">001</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line">STORED AS DIRECTORIES</span><br></pre></td></tr></table></figure></li><li><p>分别倒入数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> user_info_bucket <span class="keyword">select</span> userid ,uname <span class="keyword">from</span> <span class="keyword">user</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> domain_info_bucket <span class="keyword">select</span> userid ,domainid,<span class="keyword">domain</span> <span class="keyword">from</span> doamin</span><br></pre></td></tr></table></figure></li><li><p>查询</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> user_info_bucket u  <span class="keyword">join</span> domain_info_bucket d <span class="keyword">on</span>(u.userid==d.userid)</span><br></pre></td></tr></table></figure></li></ol></li></ol><h3 id="思路二：一分为二"><a href="#思路二：一分为二" class="headerlink" title="思路二：一分为二"></a>思路二：一分为二</h3><p>选择临时表的方式，将数据一分为二，把倾斜的key，和不倾斜的key分开处理，不倾斜的正常join，倾斜的根据情况选择mapjoin或加盐处理，最后结果union all结果</p><p>user表为用户基本表，domain为用户访问域名的宽表</p><p><strong>注意：</strong>我们其实隐含使用到了mapjoin，hive中的参数为<code>set hive.auto.convert.join=true;</code>，自动开启，默认25M，不能超过1G。</p><ol start="0"><li><p>创建中间表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tmp_table(userid <span class="keyword">string</span>,uname <span class="keyword">string</span>)</span><br><span class="line">SKEWED <span class="keyword">BY</span>(userid) <span class="keyword">on</span> (<span class="number">001</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line">STORED AS DIRECTORIES</span><br></pre></td></tr></table></figure></li><li><p><code>count(*)</code>出符合倾斜条件的数据存入中间表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> tmp_table</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">d.userid,u.uname</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span> userid</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span> </span><br><span class="line">userid,</span><br><span class="line"><span class="keyword">count</span>(userid) u_cunt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line"><span class="keyword">domain</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">userid) t</span><br><span class="line"><span class="keyword">where</span> u_cunt&gt;<span class="number">100</span>) d</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span></span><br><span class="line">(<span class="keyword">select</span> </span><br><span class="line">userid,uname</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line"><span class="keyword">user</span>) u</span><br><span class="line"><span class="keyword">on</span> d.userid = u.userid;</span><br></pre></td></tr></table></figure></li><li><p>一分为二，分别查询中出结果，再union all</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">u1.userid,u1.uname,d1.domain</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span> </span><br><span class="line">userid,uname</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line"><span class="keyword">user</span>) u1</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">(<span class="keyword">select</span> </span><br><span class="line">d.userid,d.domain</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line"><span class="keyword">domain</span> d</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span></span><br><span class="line">tmp_table t</span><br><span class="line"><span class="keyword">on</span> </span><br><span class="line">d.userid = t.userid</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">t.userid <span class="keyword">is</span> <span class="literal">null</span>) d1</span><br><span class="line"><span class="keyword">on</span> u1.userid = d1.userid</span><br><span class="line"></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">u2.userid,u2.uname,d2.domain</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span> </span><br><span class="line">userid,uname</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line"><span class="keyword">user</span>) u2</span><br><span class="line"><span class="keyword">join</span> </span><br><span class="line">(<span class="keyword">select</span> </span><br><span class="line">d.userid,d.domain</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line"><span class="keyword">domain</span> d</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span></span><br><span class="line">tmp_table t</span><br><span class="line"><span class="keyword">on</span> </span><br><span class="line">d.userid = t.userid</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">t.userid <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>) d2</span><br><span class="line"><span class="keyword">on</span> u2.userid = d2.userid</span><br></pre></td></tr></table></figure></li></ol><h2 id="大表Join小表"><a href="#大表Join小表" class="headerlink" title="大表Join小表"></a>大表Join小表</h2><h3 id="思路：MapJoin"><a href="#思路：MapJoin" class="headerlink" title="思路：MapJoin"></a>思路：MapJoin</h3><p>大表Join小表很好解决，把小表放进内存，大表再去匹配即可。</p><p>思路：</p><ol><li><p>开启MapJoin</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li><li><p>调整MapJoin小表大小，默认25M(调整为可以容忍的大小)</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize</span><br></pre></td></tr></table></figure></li><li><p>如果是MR，小表放进Map，大表进入Mapper匹配Map(使用对象存储结果)</p></li></ol><h2 id="group-By解决"><a href="#group-By解决" class="headerlink" title="group By解决"></a>group By解决</h2><h3 id="思路：加盐去盐"><a href="#思路：加盐去盐" class="headerlink" title="思路：加盐去盐"></a>思路：加盐去盐</h3><ol><li><p>开启数据倾斜时的负载均衡</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata=<span class="literal">true</span></span><br></pre></td></tr></table></figure></li><li><p>对groupby的key加盐去盐</p><p>开启上面这个参数，hive会自动拆解成两个MR，加盐去盐，最终输出结果</p><p>MR需要写两个MR，一个对key加盐，一个对key去盐</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windowing functions&amp;The OVER clause&amp;Analytics functions</title>
      <link href="/2018/11/08/hive/8/"/>
      <url>/2018/11/08/hive/8/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>The OVER clause</li><li>Analytics functions</li><li>Windowing functions</li></ol><h2 id="The-OVER-clause"><a href="#The-OVER-clause" class="headerlink" title="The OVER clause"></a>The OVER clause</h2><p>聚合函数是将多行数据按照规则聚合为一行，比如count()、sum()、min()、max()、avg()</p><p>窗口函数是在做聚合的基础上，要返回的数据不仅仅是一行</p><p>窗口函数是在窗口的基础上做统计分析，对其所作用的窗口中的每一条记录输出一条结果</p><p>窗口函数借助于over() 函数开窗</p><p>窗口函数的标准聚合函数同样包括count()、sum()、min()、max()、avg()</p><p>窗口可以在一个窗口子句中单独定义。窗口规范支持以下格式:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">(ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING</span><br></pre></td></tr></table></figure><p>窗口有以上三种定义方式，分别是<code>从某行之后到某行</code>、<code>某行之后到某行之前</code>、<code>某行到某行之前</code></p><p><code>PRECEDING</code>: 往前<br><code>FOLLOWING</code>: 往后<br><code>CURRENT ROW</code>: 当前行<br><code>UNBOUNDED</code>: 起点，<code>UNBOUNDED PRECEDING</code> 表示从前面的起点， <code>UNBOUNDED FOLLOWING</code>: 表示到后面的终点</p><ol><li><p>准备数据</p><p>window01.txt </p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ruozedata,2019-04-10,1</span><br><span class="line">ruozedata,2019-04-11,5</span><br><span class="line">ruozedata,2019-04-12,7</span><br><span class="line">ruozedata,2019-04-13,3</span><br><span class="line">ruozedata,2019-04-14,2</span><br><span class="line">ruozedata,2019-04-15,4</span><br><span class="line">ruozedata,2019-04-16,4</span><br></pre></td></tr></table></figure></li><li><p>创建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span>  <span class="keyword">database</span> window_over;</span><br><span class="line"><span class="keyword">use</span> window_over</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> window01(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">String</span>,</span><br><span class="line"><span class="built_in">date</span> <span class="keyword">String</span>,</span><br><span class="line">grade <span class="built_in">Int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure></li><li><p>导入数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/window/window01.txt'</span> <span class="keyword">into</span>  <span class="keyword">table</span>  window01;</span><br></pre></td></tr></table></figure></li><li><p>查询</p><p>问题：窗口到底怎么开? </p><p>核心：从什么地方开始到什么地方结束</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line"><span class="keyword">name</span>,</span><br><span class="line"><span class="built_in">date</span>,</span><br><span class="line">grade,</span><br><span class="line"><span class="keyword">sum</span>(grade) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> grade <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> g1,//第一行到当前行的grade求<span class="keyword">sum</span>()</span><br><span class="line"><span class="keyword">sum</span>(grade) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> grade <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">3</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> g2,//第三行到当前行的grade求<span class="keyword">sum</span>()</span><br><span class="line"><span class="keyword">sum</span>(grade) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> grade <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">3</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="number">1</span> <span class="keyword">following</span>) <span class="keyword">as</span> g3,//第三行到后一行的grade求<span class="keyword">sum</span>()</span><br><span class="line"><span class="keyword">sum</span>(grade) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> grade <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="keyword">unbounded</span> <span class="keyword">FOLLOWING</span>) <span class="keyword">as</span> g4//当前行到最后一行的grade求<span class="keyword">sum</span>()</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">window01;</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name     |date      |grade|g1|g2|g3|g4|</span><br><span class="line"><span class="comment">---------|----------|-----|--|--|--|--|</span></span><br><span class="line">ruozedata|2019-04-10|    1| 1| 1| 3|26|</span><br><span class="line">ruozedata|2019-04-14|    2| 3| 3| 6|25|</span><br><span class="line">ruozedata|2019-04-13|    3| 6| 6|10|23|</span><br><span class="line">ruozedata|2019-04-16|    4|10|10|14|20|</span><br><span class="line">ruozedata|2019-04-15|    4|14|13|18|16|</span><br><span class="line">ruozedata|2019-04-11|    5|19|16|23|12|</span><br><span class="line">ruozedata|2019-04-12|    7|26|20|20| 7|</span><br></pre></td></tr></table></figure></li></ol><h2 id="Analytics-functions"><a href="#Analytics-functions" class="headerlink" title="Analytics functions"></a>Analytics functions</h2><p>分析函数有RANK、ROW_NUMBER、DENSE_RANK、CUME_DIST、PERCENT_RANK、NTILE</p><p>这些函数可以分为三部分，第一部分是排序相关的RANK、ROW_NUMBER、DENSE_RANK，第二部分是占比相关的CUME_DIST、PERCENT_RANK，第三部分是把表切成指定分区的NTILE</p><h3 id="排序相关"><a href="#排序相关" class="headerlink" title="排序相关"></a>排序相关</h3><ol><li><p>准备数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">gifshow.com,2019-04-10,1</span><br><span class="line">gifshow.com,2019-04-11,5</span><br><span class="line">gifshow.com,2019-04-12,7</span><br><span class="line">gifshow.com,2019-04-13,3</span><br><span class="line">gifshow.com,2019-04-14,2</span><br><span class="line">gifshow.com,2019-04-15,4</span><br><span class="line">gifshow.com,2019-04-16,4</span><br><span class="line">yy.com,2019-04-10,2</span><br><span class="line">yy.com,2019-04-11,3</span><br><span class="line">yy.com,2019-04-12,5</span><br><span class="line">yy.com,2019-04-13,6</span><br><span class="line">yy.com,2019-04-14,3</span><br><span class="line">yy.com,2019-04-15,9</span><br><span class="line">yy.com,2019-04-16,7</span><br></pre></td></tr></table></figure></li><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> traffic(</span><br><span class="line"><span class="keyword">domain</span> <span class="keyword">String</span>,</span><br><span class="line"><span class="built_in">date</span> <span class="keyword">String</span>,</span><br><span class="line">grade <span class="built_in">Int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span>  <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure></li><li><p>导数</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/window/rank.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> traffic;</span><br></pre></td></tr></table></figure></li><li><p>查询</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line"><span class="keyword">domain</span>,</span><br><span class="line"><span class="built_in">date</span>,</span><br><span class="line">grade,</span><br><span class="line"><span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> r1,</span><br><span class="line">ROW_NUMBER() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> r2,</span><br><span class="line"><span class="keyword">DENSE_RANK</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> r3</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">traffic;</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">domain     |date      |grade|r1|r2|r3|</span><br><span class="line"><span class="comment">-----------|----------|-----|--|--|--|</span></span><br><span class="line">gifshow.com|2019-04-10|    1| 1| 1| 1|</span><br><span class="line">gifshow.com|2019-04-14|    2| 2| 2| 2|</span><br><span class="line">gifshow.com|2019-04-13|    3| 3| 3| 3|</span><br><span class="line">gifshow.com|2019-04-16|    4| 4| 4| 4|</span><br><span class="line">gifshow.com|2019-04-15|    4| 4| 5| 4|</span><br><span class="line">gifshow.com|2019-04-11|    5| 6| 6| 5|</span><br><span class="line">gifshow.com|2019-04-12|    7| 7| 7| 6|</span><br><span class="line">yy.com     |2019-04-10|    2| 1| 1| 1|</span><br><span class="line">yy.com     |2019-04-11|    3| 2| 2| 2|</span><br><span class="line">yy.com     |2019-04-14|    3| 2| 3| 2|</span><br><span class="line">yy.com     |2019-04-12|    5| 4| 4| 3|</span><br><span class="line">yy.com     |2019-04-13|    6| 5| 5| 4|</span><br><span class="line">yy.com     |2019-04-16|    7| 6| 6| 5|</span><br><span class="line">yy.com     |2019-04-15|    9| 7| 7| 6|</span><br></pre></td></tr></table></figure></li><li><p>总结</p><p><code>RANK()</code>：分组内生成编号，排名相同相同的名词留空位<br><code>DENSE_RANK()</code>： 分组内生成编号，排名相同相同的名词不留空位<br><code>ROW_NUMBER()</code>： 从1开始，按照排序，生成分组内记录的序号(推介使用)</p></li></ol><h3 id="占比相关"><a href="#占比相关" class="headerlink" title="占比相关"></a>占比相关</h3><p><code>CUME_DIST()</code>: 小于等于当前行值(OVER中order by指定的字段排序)的行数/分组内的总行数<br><code>PERCENT_RANK()</code>: 分组内当前行的rank -1 / 分组内总行数 -1</p><ol><li><p>准备数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">dept01,ruoze,10000</span><br><span class="line">dept01,jepson,20000</span><br><span class="line">dept01,xingxing,30000</span><br><span class="line">dept02,zhangsan,40000</span><br><span class="line">dept02,lisi,50000</span><br></pre></td></tr></table></figure></li><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> window02(</span><br><span class="line">dept <span class="keyword">String</span>,</span><br><span class="line"><span class="keyword">user</span> <span class="keyword">String</span>,</span><br><span class="line">sal <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure></li><li><p>导数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/window/window02.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> window02;</span><br></pre></td></tr></table></figure></li><li><p>查询</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">dept,</span><br><span class="line"><span class="keyword">user</span>,</span><br><span class="line">sal,</span><br><span class="line"><span class="keyword">CUME_DIST</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> sal) cume,</span><br><span class="line"><span class="keyword">PERCENT_RANK</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> sal) <span class="keyword">percent</span></span><br><span class="line"><span class="keyword">from</span> window02;</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">dept  |user    |sal  |cume1|cume2             |percent|</span><br><span class="line"><span class="comment">------|--------|-----|-----|------------------|-------|</span></span><br><span class="line">dept01|ruoze   |10000|  0.2|0.3333333333333333|      0|</span><br><span class="line">dept01|jepson  |20000|  0.4|0.6666666666666666|    0.5|</span><br><span class="line">dept01|xingxing|30000|  0.6|                 1|      1|</span><br><span class="line">dept02|zhangsan|40000|  0.8|               0.5|      0|</span><br><span class="line">dept02|lisi    |50000|    1|                 1|      1|</span><br></pre></td></tr></table></figure></li></ol><h3 id="分区相关"><a href="#分区相关" class="headerlink" title="分区相关"></a>分区相关</h3><p><code>NTILE(num)</code>：将数据按照输入的数成<code>num</code>片，并且记录分片号</p><p>场景：128M的数据只有一两个列的数据记录数有几千万个，必须拆开用<code>NTILE(n)</code>==&gt; 改变map任务的数量</p><ol><li><p>查询</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line"><span class="keyword">domain</span>,</span><br><span class="line"><span class="built_in">date</span>,</span><br><span class="line">grade,</span><br><span class="line">ntile(<span class="number">2</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> n1,</span><br><span class="line">ntile(<span class="number">3</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> n2,</span><br><span class="line">ntile(<span class="number">4</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> n3</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">traffic;</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">domain     |date      |grade|n1|n2|n3|</span><br><span class="line"><span class="comment">-----------|----------|-----|--|--|--|</span></span><br><span class="line">gifshow.com|2019-04-10|    1| 1| 1| 1|</span><br><span class="line">gifshow.com|2019-04-14|    2| 1| 1| 1|</span><br><span class="line">gifshow.com|2019-04-13|    3| 1| 1| 2|</span><br><span class="line">gifshow.com|2019-04-16|    4| 1| 2| 2|</span><br><span class="line">gifshow.com|2019-04-15|    4| 2| 2| 3|</span><br><span class="line">gifshow.com|2019-04-11|    5| 2| 3| 3|</span><br><span class="line">gifshow.com|2019-04-12|    7| 2| 3| 4|</span><br><span class="line">yy.com     |2019-04-10|    2| 1| 1| 1|</span><br><span class="line">yy.com     |2019-04-11|    3| 1| 1| 1|</span><br><span class="line">yy.com     |2019-04-14|    3| 1| 1| 2|</span><br><span class="line">yy.com     |2019-04-12|    5| 1| 2| 2|</span><br><span class="line">yy.com     |2019-04-13|    6| 2| 2| 3|</span><br><span class="line">yy.com     |2019-04-16|    7| 2| 3| 3|</span><br><span class="line">yy.com     |2019-04-15|    9| 2| 3| 4|</span><br></pre></td></tr></table></figure></li></ol><h2 id="Windowing-functions"><a href="#Windowing-functions" class="headerlink" title="Windowing functions"></a>Windowing functions</h2><p><code>LAG(col,n,default)</code>: 窗口内往上取N行的值，如果有default就取default，没有就用null</p><p><code>LEAD(col,n,default)</code>: 窗口内往下取N行的值，如果有default就取default，没有就用null</p><ol><li><p>准备数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie1,2015-04-10 10:00:02,url2</span><br><span class="line">cookie1,2015-04-10 10:00:00,url1</span><br><span class="line">cookie1,2015-04-10 10:03:04,1url3</span><br><span class="line">cookie1,2015-04-10 10:50:05,url6</span><br><span class="line">cookie1,2015-04-10 11:00:00,url7</span><br><span class="line">cookie1,2015-04-10 10:10:00,url4</span><br><span class="line">cookie1,2015-04-10 10:50:01,url5</span><br><span class="line">cookie2,2015-04-10 10:00:02,url22</span><br><span class="line">cookie2,2015-04-10 10:00:00,url11</span><br><span class="line">cookie2,2015-04-10 10:03:04,1url33</span><br><span class="line">cookie2,2015-04-10 10:50:05,url66</span><br><span class="line">cookie2,2015-04-10 11:00:00,url77</span><br><span class="line">cookie2,2015-04-10 10:10:00,url44</span><br><span class="line">cookie2,2015-04-10 10:50:01,url55</span><br></pre></td></tr></table></figure></li><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> window03(</span><br><span class="line">cookid <span class="keyword">String</span>,</span><br><span class="line"><span class="built_in">time</span> <span class="keyword">String</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">String</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure></li><li><p>导数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/window/window03.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> window03;</span><br></pre></td></tr></table></figure></li><li><p>查询</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">cookid,</span><br><span class="line"><span class="built_in">time</span>,</span><br><span class="line"><span class="keyword">url</span>,</span><br><span class="line">lag(<span class="built_in">time</span>,<span class="number">1</span>,<span class="string">'1970-00-00 00:00:00'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> cookid <span class="keyword">order</span> <span class="keyword">by</span> <span class="built_in">time</span>),</span><br><span class="line"><span class="keyword">lead</span>(<span class="built_in">time</span>,<span class="number">2</span>,<span class="string">"null"</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> cookid <span class="keyword">order</span> <span class="keyword">by</span> <span class="built_in">time</span>)</span><br><span class="line"><span class="keyword">from</span> window03;</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookid |time               |url   |lag                |lead               |</span><br><span class="line"><span class="comment">-------|-------------------|------|-------------------|-------------------|</span></span><br><span class="line">cookie1|2015-04-10 10:00:00|url1  |1970-00-00 00:00:00|2015-04-10 10:03:04|</span><br><span class="line">cookie1|2015-04-10 10:00:02|url2  |2015-04-10 10:00:00|2015-04-10 10:10:00|</span><br><span class="line">cookie1|2015-04-10 10:03:04|1url3 |2015-04-10 10:00:02|2015-04-10 10:50:01|</span><br><span class="line">cookie1|2015-04-10 10:10:00|url4  |2015-04-10 10:03:04|2015-04-10 10:50:05|</span><br><span class="line">cookie1|2015-04-10 10:50:01|url5  |2015-04-10 10:10:00|2015-04-10 11:00:00|</span><br><span class="line">cookie1|2015-04-10 10:50:05|url6  |2015-04-10 10:50:01|null               |</span><br><span class="line">cookie1|2015-04-10 11:00:00|url7  |2015-04-10 10:50:05|null               |</span><br><span class="line">cookie2|2015-04-10 10:00:00|url11 |1970-00-00 00:00:00|2015-04-10 10:03:04|</span><br><span class="line">cookie2|2015-04-10 10:00:02|url22 |2015-04-10 10:00:00|2015-04-10 10:10:00|</span><br><span class="line">cookie2|2015-04-10 10:03:04|1url33|2015-04-10 10:00:02|2015-04-10 10:50:01|</span><br><span class="line">cookie2|2015-04-10 10:10:00|url44 |2015-04-10 10:03:04|2015-04-10 10:50:05|</span><br><span class="line">cookie2|2015-04-10 10:50:01|url55 |2015-04-10 10:10:00|2015-04-10 11:00:00|</span><br><span class="line">cookie2|2015-04-10 10:50:05|url66 |2015-04-10 10:50:01|null               |</span><br><span class="line">cookie2|2015-04-10 11:00:00|url77 |2015-04-10 10:50:05|null               |</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>创建伪表&amp;自定义UDF函数&amp;MR解决数据倾斜的问题&amp;行转列案例&amp;列转行案例&amp;使用hive实现wc&amp;修改hadoop的URI带来的hive数据库路径问题&amp;多文件多目录做wc或建表带来的问题</title>
      <link href="/2018/11/07/hive/7/"/>
      <url>/2018/11/07/hive/7/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>创建伪表</li><li>自定义UDF函数</li><li>MR解决数据倾斜的问题(引入)</li><li>行转列案例</li><li>列转行案例</li><li>使用hive实现wc</li><li>修改hadoop的URI带来的hive数据库路径问题</li><li>多文件多目录做wc或建表带来的问题</li></ol><h2 id="创建伪表"><a href="#创建伪表" class="headerlink" title="创建伪表"></a>创建伪表</h2><ol><li><p>创建表dual</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dual(a <span class="keyword">string</span>);</span><br></pre></td></tr></table></figure></li><li><p>创建数据并导入到表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">touch dual.txt</span><br><span class="line">echo 'X' &gt;dual.txt</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/dual.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> dual;</span><br></pre></td></tr></table></figure></li></ol><h2 id="自定义UDF函数"><a href="#自定义UDF函数" class="headerlink" title="自定义UDF函数"></a>自定义UDF函数</h2><p>hive官网关于用户如何自定义UDF: <a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins1" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/HivePlugins1</a>. </p><p>hive查看函数: show functions;</p><p>hive查看jar包: list jars;</p><ol><li><p>首先，需要创建一个扩展UDF的新类，其中有一个或多个名为evaluate的方法</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">// 继承UDF</span><br><span class="line">public class UDFPrintf extends UDF &#123;</span><br><span class="line">    </span><br><span class="line">//方法重载</span><br><span class="line">    public void evaluate()&#123;</span><br><span class="line">        System.out.println("你不要老婆吧");</span><br><span class="line">    &#125;</span><br><span class="line">    public void evaluate(String name)&#123;</span><br><span class="line">        System.out.println(name + ",你要老婆不要?");</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>对方法添加描述，先看系统的</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc function extended upper;</span><br><span class="line">upper(str) - Returns str <span class="keyword">with</span> <span class="keyword">all</span> <span class="keyword">characters</span> <span class="keyword">changed</span> <span class="keyword">to</span> uppercase</span><br><span class="line">Synonyms: <span class="keyword">ucase</span></span><br><span class="line">Example:</span><br><span class="line">  &gt; <span class="keyword">SELECT</span> <span class="keyword">upper</span>(<span class="string">'Facebook'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;</span><br><span class="line">  'FACEBOOK'</span><br><span class="line">=================================================================</span><br><span class="line">@Description(</span><br><span class="line">    name = "upper,ucase",</span><br><span class="line">    value = "_FUNC_(str) - Returns str <span class="keyword">with</span> <span class="keyword">all</span> <span class="keyword">characters</span> <span class="keyword">changed</span> <span class="keyword">to</span> uppercase<span class="string">",</span></span><br><span class="line"><span class="string">    extended = "</span>Example:\n  &gt; <span class="keyword">SELECT</span> _FUNC_(<span class="string">'Facebook'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;\n    </span><br><span class="line">    'FACEBOOK'"</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>我们仿照上面的给自定义的方法写个描述(Description)</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">@Description(</span><br><span class="line">    name = "my_printf,ucase",</span><br><span class="line">    value = "_FUNC_(str) - 返回一个名字加上字符串",</span><br><span class="line">    extended = "Example:\n  &gt; SELECT _FUNC_('老王') FROM src LIMIT 1;\n  </span><br><span class="line">    '老王,你要老婆不要?'"</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p>打包上传到Linux，启动hive，jar包添加到class path，创建<strong>临时UDF函数</strong>，只能在当前session中有效</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">add jar /home/hadoop/lib/hive-client-1.0.0.jar;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">temporary</span> <span class="keyword">function</span> my_printf <span class="keyword">as</span> <span class="string">'org.hadoop.hive.UDF.UDFPrintf'</span>;</span><br></pre></td></tr></table></figure></li><li><p>伪表使用自定义函数</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> my_printf() <span class="keyword">from</span> dual;</span><br><span class="line">hello,小七</span><br><span class="line"><span class="keyword">select</span> my_printf(<span class="string">"老王"</span>) <span class="keyword">from</span> dual;</span><br><span class="line">hello,老王</span><br></pre></td></tr></table></figure></li><li><p>上面的方法只能创建临时函数，我们接下来将会创建永久函数，需要把jar把上传到hdfs</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hdfs dfs -mkdir /jar</span><br><span class="line">hdfs dfs -put /home/hadoop/lib/hive-client-1.0.0.jar /jar</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">FUNCTION</span> my_printf <span class="keyword">AS</span> <span class="string">'org.hadoop.hive.UDF.UDFPrintf'</span> <span class="keyword">USING</span> JAR <span class="string">'hdfs:///jar/hive-client-1.0.0.jar'</span>;</span><br></pre></td></tr></table></figure></li><li><p>然后可以在多个窗口执行创建的永久函数</p></li></ol><h2 id="MR解决数据倾斜的思想"><a href="#MR解决数据倾斜的思想" class="headerlink" title="MR解决数据倾斜的思想"></a>MR解决数据倾斜的思想</h2><p>核心思想==&gt;先加盐(随机数)，再去盐(随机数)</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">public class skew &#123;</span><br><span class="line">    private List&lt;String&gt; newList = new ArrayList&lt;&gt;();</span><br><span class="line">    private Random r = new Random();</span><br><span class="line">    public void incRandom(List list)&#123;</span><br><span class="line">        newList.clear();</span><br><span class="line">        list.forEach( word -&gt;&#123;</span><br><span class="line">            int i = r.nextInt(10);</span><br><span class="line">            newList.add(i+"_"+word);</span><br><span class="line">        &#125;);</span><br><span class="line">        newList.forEach(System.out::println);</span><br><span class="line">    &#125;</span><br><span class="line">    public void decRandom()&#123;</span><br><span class="line">        newList.forEach(word-&gt;&#123;</span><br><span class="line">            System.out.println(word.substring(word.lastIndexOf("_")+1));</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        skew skew = new skew();</span><br><span class="line">        List&lt;String&gt; arr = new ArrayList&lt;&gt;();</span><br><span class="line">        arr.add("老王");</span><br><span class="line">        arr.add("狗子");</span><br><span class="line">        arr.add("张三");</span><br><span class="line">        skew.incRandom(arr);</span><br><span class="line">        System.out.println("<span class="comment">-------------------");</span></span><br><span class="line">        skew.decRandom();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">7_老王</span><br><span class="line">2_狗子</span><br><span class="line">4_张三</span><br><span class="line"><span class="comment">-------------------</span></span><br><span class="line">老王</span><br><span class="line">狗子</span><br><span class="line">张三</span><br></pre></td></tr></table></figure><h2 id="行转列案例"><a href="#行转列案例" class="headerlink" title="行转列案例"></a>行转列案例</h2><p>实现部门号的所有学生格式为</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">(dept01,Alaowang|wangwu)</span><br></pre></td></tr></table></figure><ol><li><p>数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">laowangdept01A</span><br><span class="line">zhangsandept02A</span><br><span class="line">lisidept01B</span><br><span class="line">wangwudept01A</span><br><span class="line">zhaoliudept02A</span><br></pre></td></tr></table></figure></li><li><p>创建表结构</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> row2col(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">dept <span class="keyword">string</span>,</span><br><span class="line">grade <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure></li><li><p>导入数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/row2col.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> row2col;</span><br></pre></td></tr></table></figure></li><li><p>实现(dept01,A    laowang|wangwu)</p><ol><li><p>第一步组合dept和grade成dept_grade</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">concat_ws</span>(<span class="string">','</span>,dept,grade) dept_grade,<span class="keyword">name</span> <span class="keyword">from</span> row2col;</span><br></pre></td></tr></table></figure></li><li><p>第二步按dept_grade分组求collect_set()将返回的数组使用concat_ws()函数转成字符串</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">t.dept_grade,<span class="keyword">concat_ws</span>(<span class="string">'|'</span>,collect_set(t.name)) <span class="keyword">names</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span> <span class="keyword">concat_ws</span>(<span class="string">','</span>,dept,grade) dept_grade,<span class="keyword">name</span> <span class="keyword">from</span> row2col) t</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">t.dept_grade;</span><br></pre></td></tr></table></figure><p>collect_set()返回一个数组，concat_ws()返回一个指定字符切分数组的字符串</p></li><li><p>结果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">dept_grade|names           |</span><br><span class="line"><span class="comment">----------|----------------|</span></span><br><span class="line">dept01,A  |laowang|wangwu  |</span><br><span class="line">dept01,B  |lisi            |</span><br><span class="line">dept02,A  |zhangsan|zhaoliu|</span><br></pre></td></tr></table></figure></li></ol></li></ol><h2 id="列转行案例"><a href="#列转行案例" class="headerlink" title="列转行案例"></a>列转行案例</h2><p>实现所有课程的格式为</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">1       zhangsan        化学</span><br><span class="line">1       zhangsan        物理</span><br><span class="line">...</span><br></pre></td></tr></table></figure><ol><li><p>数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">1,zhangsan,化学:物理:数学:语文</span><br><span class="line">2,lisi,化学:数学:生物:生理:卫生</span><br><span class="line">3,wangwu,化学:语文:英语:体育:生物</span><br></pre></td></tr></table></figure></li><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> col2row(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">subjects <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">":"</span>;</span><br></pre></td></tr></table></figure></li><li><p>导数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/col2row.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> col2row;</span><br></pre></td></tr></table></figure></li><li><p>实现</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line"><span class="keyword">id</span>,<span class="keyword">name</span>,subject </span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">col2row</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(subjects) t <span class="keyword">as</span> subject;</span><br></pre></td></tr></table></figure></li><li><p>结果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">id|name    |subject|</span><br><span class="line"><span class="comment">--|--------|-------|</span></span><br><span class="line"> 1|zhangsan|化学     |</span><br><span class="line"> 1|zhangsan|物理     |</span><br><span class="line"> 1|zhangsan|数学     |</span><br><span class="line"> 1|zhangsan|语文     |</span><br><span class="line"> 2|lisi    |化学     |</span><br><span class="line"> 2|lisi    |数学     |</span><br><span class="line"> 2|lisi    |生物     |</span><br><span class="line"> 2|lisi    |生理     |</span><br><span class="line"> 2|lisi    |卫生     |</span><br><span class="line"> 3|wangwu  |化学     |</span><br><span class="line"> 3|wangwu  |语文     |</span><br><span class="line"> 3|wangwu  |英语     |</span><br><span class="line"> 3|wangwu  |体育     |</span><br><span class="line"> 3|wangwu  |生物     |</span><br></pre></td></tr></table></figure></li></ol><h2 id="使用hive实现wc"><a href="#使用hive实现wc" class="headerlink" title="使用hive实现wc"></a>使用hive实现wc</h2><ol><li><p>数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">化学:物理:数学:语文:英语</span><br><span class="line">化学:数学:生物:生理:卫生</span><br><span class="line">化学:语文:英语:体育:生物</span><br><span class="line">数学:语文:英语:体育:生物</span><br></pre></td></tr></table></figure></li><li><p>切分返回数组</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">split</span>(line,<span class="string">":"</span>) <span class="keyword">as</span> subjects <span class="keyword">from</span> wc</span><br></pre></td></tr></table></figure></li><li><p>炸开数组返回单词</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">words</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span> <span class="keyword">split</span>(line,<span class="string">":"</span>) subjects <span class="keyword">from</span> wc) <span class="keyword">as</span> t</span><br><span class="line"> <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(t.subjects) t1 <span class="keyword">as</span> words;</span><br></pre></td></tr></table></figure></li><li><p>每个单词分组求count()</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">t2.words,<span class="keyword">count</span>(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span></span><br><span class="line">words</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span> <span class="keyword">split</span>(line,<span class="string">":"</span>) subjects <span class="keyword">from</span> wc) <span class="keyword">as</span> t</span><br><span class="line"> <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(t.subjects) t1 <span class="keyword">as</span> words) t2</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> t2.words;</span><br></pre></td></tr></table></figure></li></ol><h2 id="修改hadoop的URI带来的hive数据库路径问题"><a href="#修改hadoop的URI带来的hive数据库路径问题" class="headerlink" title="修改hadoop的URI带来的hive数据库路径问题"></a>修改hadoop的URI带来的hive数据库路径问题</h2><p>hive的数据保存在hadoop中，而hive的源数据保存在mysql中</p><p>这里就有一个问题，如果修改了hadoop的fs.defaultFS这个参数</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://aliyun:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>那么hive的元数据没有修改就会找mysql中保存的路径</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">*************************** 1. row ***************************</span><br><span class="line">          DB_ID: 1</span><br><span class="line">           DESC: Default Hive database</span><br><span class="line">DB_LOCATION_URI: hdfs://aliyun:9000/user/hive/warehouse</span><br><span class="line">           NAME: default</span><br><span class="line">     OWNER_NAME: public</span><br><span class="line">     OWNER_TYPE: ROLE</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>这时候需要使用到一个命令metatool来将mysql中的元数据修改为新的仓库路径</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">-updateLocation &lt;new-loc&gt; &lt;old-loc&gt;</span><br></pre></td></tr></table></figure><h2 id="多文件多目录做wc或建表带来的问题"><a href="#多文件多目录做wc或建表带来的问题" class="headerlink" title="多文件多目录做wc或建表带来的问题"></a>多文件多目录做wc或建表带来的问题</h2><p>我们查看一下hdfs中的目录结构</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ hdfs dfs -ls /mdir<span class="comment">/*</span></span><br><span class="line"><span class="comment">-rwxr-xr-x   1 hadoop supergroup         13 2020-02-05 00:59 /mdir/1.txt</span></span><br><span class="line"><span class="comment">-rwxr-xr-x   1 hadoop supergroup         14 2020-02-05 00:59 /mdir/2.txt</span></span><br><span class="line"><span class="comment">-rwxr-xr-x   1 hadoop supergroup         11 2020-02-05 00:59 /mdir/mdir2/3.txt</span></span><br></pre></td></tr></table></figure><p>存在嵌套目录，在做wc或者建表时，将会报错</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> dir (<span class="keyword">name</span> <span class="keyword">string</span>) location <span class="string">'/mdir'</span>;</span><br><span class="line">Failed <span class="keyword">with</span> <span class="keyword">exception</span> java.io.IOException:java.io.IOException: <span class="keyword">Not</span> a <span class="keyword">file</span>: hdfs://aliyun:<span class="number">9000</span>/mdir<span class="comment">/*</span></span><br></pre></td></tr></table></figure><p>通过设置参数<code>mapreduce.input.fileinputformat.input.dir.recursive</code>为<code>true</code>来解决这个问题，该参数默认为<code>false</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dir;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>可以直接在mapped-site.xml文件中直接配配置参数</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.input.fileinputformat.input.dir.recursive&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Order By&amp;Sort By&amp;Distribute By&amp;Cluster By</title>
      <link href="/2018/11/06/hive/6/"/>
      <url>/2018/11/06/hive/6/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>全局排序（Order By）</li><li>Reduce内部排序（Sort By）</li><li>分区排序（Distribute By）</li><li>Cluster By</li></ol><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><ol><li><p>准备测试数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">7369SMITHCLERK79021980-12-17800.0020</span><br><span class="line">7499ALLENSALESMAN76981981-2-201600.00300.0030</span><br><span class="line">7521WARDSALESMAN76981981-2-221250.00500.0030</span><br><span class="line">7566JONESMANAGER78391981-4-22975.0020</span><br><span class="line">7654MARTINSALESMAN76981981-9-281250.001400.0030</span><br><span class="line">7698BLAKEMANAGER78391981-5-12850.0030</span><br><span class="line">7782CLARKMANAGER78391981-6-92450.0010</span><br><span class="line">7788SCOTTANALYST75661987-4-193000.0020</span><br><span class="line">7839KINGPRESIDENT1981-11-175000.0010</span><br><span class="line">7844TURNERSALESMAN76981981-9-81500.000.0030</span><br><span class="line">7876ADAMSCLERK77881987-5-231100.0020</span><br><span class="line">7900JAMESCLERK76981981-12-3950.0030</span><br><span class="line">7902FORDANALYST75661981-12-33000.0020</span><br><span class="line">7934MILLERCLERK77821982-1-231300.0010</span><br></pre></td></tr></table></figure></li><li><p>创建emp</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> emp(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">job <span class="keyword">string</span>,</span><br><span class="line">mgr <span class="built_in">int</span>,</span><br><span class="line">hiredate <span class="keyword">string</span>, </span><br><span class="line">sal <span class="keyword">double</span>, </span><br><span class="line">comm <span class="keyword">double</span>,</span><br><span class="line">deptno <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure></li><li><p>加载数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/emp.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> emp;</span><br></pre></td></tr></table></figure></li><li><p>验证数据</p><p><code>select * from emp;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br></pre></td></tr></table></figure></li></ol><h2 id="全局排序（Order-By）-慎用"><a href="#全局排序（Order-By）-慎用" class="headerlink" title="全局排序（Order By）[慎用]"></a>全局排序（Order By）[慎用]</h2><p>语句格式:     <code>order by col1,col2 asc/desc</code></p><p>使用order by语句排序的是全局排序，只能有一个reduce作用来完成，与此对应的是sort by由多个reduce来完成</p><p>案例实操</p><ol><li><p>查询员工信息按工资升序排列</p><p><code>select * from emp order by sal;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br></pre></td></tr></table></figure></li><li><p>查询员工信息按工资降序排列(增加一个替换null值的功能)</p><p><code>select *,nvl(comm,&#39;-1&#39;) from emp order by sal desc;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|_c1   |</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|------|</span></span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|-1    |</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|-1    |</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|-1    |</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|-1    |</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|-1    |</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|-1    |</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|300.0 |</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|0.0   |</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|-1    |</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|1400.0|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|500.0 |</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|-1    |</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|-1    |</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|-1    |</span><br></pre></td></tr></table></figure></li><li><p>按照员工薪水的2倍排序</p><p><code>select *,sal+nvl(comm,0) as salandcomm from emp order by salandcomm;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|salandcomm|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|----------|</span></span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|       800|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|       950|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|      1100|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|      1300|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|      1500|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|      1750|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|      1900|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|      2450|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|      2650|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|      2850|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|      2975|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|      3000|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|      3000|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|      5000|</span><br></pre></td></tr></table></figure></li><li><p>按照部门和工资升序排序</p><p><code>select * from emp order by deptno,sal+nvl(comm,0);</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br></pre></td></tr></table></figure></li></ol><h2 id="Reduce内部排序（Sort-By）"><a href="#Reduce内部排序（Sort-By）" class="headerlink" title="Reduce内部排序（Sort By）"></a>Reduce内部排序（Sort By）</h2><p>对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用<strong>sort by</strong>。</p><p>Sort by为每个reducer产生一个排序文件。每个Reducer内部进行排序，对全局结果集来说不是排序。</p><ol><li><p>设置reduce个数，不然一个一个reduce没有效果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br></pre></td></tr></table></figure></li><li><p>根据部门编号降序查看员工信息</p><p><code>select * from emp order by deptno desc;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br></pre></td></tr></table></figure></li><li><p>将查询结果导入到文件中（按照部门编号降序排序）[指定了格式]</p><p><code>insert overwrite local directory &#39;/home/hadoop/emp&#39; row format delimited fields terminated by &quot;\t&quot; select * from emp order by deptno desc;</code></p><p>打开/home/hadoop/emp下的文件</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun emp.txt]$ cat 000000_0 </span><br><span class="line">7521WARDSALESMAN76981981-2-221250.0500.030</span><br><span class="line">7499ALLENSALESMAN76981981-2-201600.0300.030</span><br><span class="line">7900JAMESCLERK76981981-12-3950.0\N30</span><br><span class="line">7844TURNERSALESMAN76981981-9-81500.00.030</span><br><span class="line">7698BLAKEMANAGER78391981-5-12850.0\N30</span><br><span class="line">7654MARTINSALESMAN76981981-9-281250.01400.030</span><br><span class="line">7876ADAMSCLERK77881987-5-231100.0\N20</span><br><span class="line">7902FORDANALYST75661981-12-33000.0\N20</span><br><span class="line">7788SCOTTANALYST75661987-4-193000.0\N20</span><br><span class="line">7369SMITHCLERK79021980-12-17800.0\N20</span><br><span class="line">7566JONESMANAGER78391981-4-22975.0\N20</span><br><span class="line">7934MILLERCLERK77821982-1-231300.0\N10</span><br><span class="line">7839KINGPRESIDENT\N1981-11-175000.0\N10</span><br><span class="line">7782CLARKMANAGER78391981-6-92450.0\N10</span><br></pre></td></tr></table></figure></li></ol><h2 id="发送分区排序（Distribute-By）"><a href="#发送分区排序（Distribute-By）" class="headerlink" title="发送分区排序（Distribute By）"></a>发送分区排序（Distribute By）</h2><p>在有些情况下，我们需要控制某个特定行应该到哪个reducer，通常是为了进行后续的聚集操作。<strong>distribute by</strong> 子句可以做这件事。<strong>distribute by</strong>类似MR中<strong>partition</strong>（自定义分区），进行分区，结合<strong>sort by</strong>使用。 </p><p>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><ol><li><p>先按照部门编号分区，再按照员工编号降序排序。</p><p><code>select * from emp distribute by deptno sort by empno desc;</code> </p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br></pre></td></tr></table></figure><p><code>注意:</code> </p><ol><li>distribute by的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。</li><li>Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。</li></ol></li></ol><h2 id="Cluster-By"><a href="#Cluster-By" class="headerlink" title="Cluster By"></a>Cluster By</h2><p>当<code>distribute by</code>和<code>sorts by</code>字段相同时，可以使用<code>cluster by</code>方式。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</p><p>当分区字段和排序字段都是部门编号的时候我们可以这么做</p><p><code>select * from emp cluster by deptno;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br></pre></td></tr></table></figure><p><code>注意:</code> 注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HS2&amp;Hive的复杂数据结构&amp;行列互转&amp;常用函数&amp;静动态分区表&amp;桶表</title>
      <link href="/2018/11/05/hive/5/"/>
      <url>/2018/11/05/hive/5/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>HS2</li><li>复杂数据结构</li><li>行列互转</li><li>常用函数</li><li>静动态分区表</li><li>桶表</li></ol><h2 id="SH2"><a href="#SH2" class="headerlink" title="SH2"></a>SH2</h2><p>HS2是HiveServer2的简称</p><ul><li><p>HS2: Server端，默认端口10000</p><p>修改端口的方式是通过设置hive.server2.thrift.port的值</p></li><li><p>beeline: Client端</p><p>连接方式: <code>./beeline -u jdbc:hive2://hadoop001:10000/matestore -n hadoop</code></p></li></ul><h2 id="复杂数据结构"><a href="#复杂数据结构" class="headerlink" title="复杂数据结构"></a>复杂数据结构</h2><p>复杂数据类型有三种，分别是: array、map、structs</p><h3 id="array"><a href="#array" class="headerlink" title="array"></a>array</h3><p><code>array</code>的建表格式:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hive_array(<span class="keyword">name</span> <span class="keyword">string</span>, work_locations <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line">COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span>;</span><br></pre></td></tr></table></figure><p>注意: <code>COLLECTION ITEMS TERMINATED BY &#39;,&#39;</code>是指定array数组中的元素分隔符</p><h4 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h4><ol><li><p>第一步准备数据</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pk      beijing,shanghai,tianjin,hangzhou</span><br><span class="line">jepson  changchu,chengdu,wuhan,beijing</span><br></pre></td></tr></table></figure></li><li><p>第二步建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> array_(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">address <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span> </span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure></li><li><p>第三步装载数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/hive_array.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> array_;</span><br></pre></td></tr></table></figure></li><li><p>第四步查询数据是否加载成功</p><p><code>select * from array_;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name  |address                                    |</span><br><span class="line"><span class="comment">------|-------------------------------------------|</span></span><br><span class="line">pk    |["beijing","shanghai","tianjin","hangzhou"]|</span><br><span class="line">jepson|["changchu","chengdu","wuhan","beijing"]   |</span><br></pre></td></tr></table></figure></li></ol><h4 id="对array数组的”取”"><a href="#对array数组的”取”" class="headerlink" title="对array数组的”取”"></a>对array数组的”取”</h4><ol><li><p>根据数组的下标取出指定元素，下标从0开始</p><p><code>select name,address[1] as address from array_;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name  |address |</span><br><span class="line"><span class="comment">------|--------|</span></span><br><span class="line">pk    |shanghai|</span><br><span class="line">jepson|chengdu |</span><br></pre></td></tr></table></figure></li><li><p>获取数组中元素的个数</p><p><code>select name,size(address) as size from array_;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name  |size|</span><br><span class="line"><span class="comment">------|----|</span></span><br><span class="line">pk    |   4|</span><br><span class="line">jepson|   4|</span><br></pre></td></tr></table></figure></li><li><p>获取数组中包含某元素的记录</p><p><code>select name,address from array_ where  array_contains(address,&quot;shanghai&quot;);</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name|address                                    |</span><br><span class="line"><span class="comment">----|-------------------------------------------|</span></span><br><span class="line">pk  |["beijing","shanghai","tianjin","hangzhou"]|</span><br></pre></td></tr></table></figure></li></ol><h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><p><code>map</code>的建表格式:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hive_map(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>, members <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;, age <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line">COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'#'</span></span><br><span class="line"><span class="keyword">MAP</span> <span class="keyword">KEYS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">':'</span>;</span><br></pre></td></tr></table></figure><p>注意: <code>COLLECTION ITEMS TERMINATED BY &#39;#&#39;</code>是指定map键值对中的元素分隔符，<code>MAP KEYS TERMINATED BY &#39;:&#39;</code>是指定key和value的分隔符</p><h4 id="准备数据-1"><a href="#准备数据-1" class="headerlink" title="准备数据"></a>准备数据</h4><ol><li><p>第一步准备数据</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1,zhangsan,father:xiaoming#mother:xiaohuang#brother:xiaoxu,28</span><br><span class="line">2,lisi,father:mayun#mother:huangyi#brother:guanyu,22</span><br><span class="line">3,wangwu,father:wangjianlin#mother:ruhua#sister:jingtian,29</span><br><span class="line">4,mayun,father:mayongzhen#mother:angelababy,26</span><br></pre></td></tr></table></figure></li><li><p>第二步建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> map_(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">family <span class="keyword">map</span>&lt;<span class="keyword">String</span>,<span class="keyword">String</span>&gt;,</span><br><span class="line">age <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"#"</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">":"</span>;</span><br></pre></td></tr></table></figure></li><li><p>第三步加载数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/hive_map.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> map_;</span><br></pre></td></tr></table></figure></li><li><p>第四步查询数据是否加载成功</p><p><code>select * from map_;</code></p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">id|name    |family                                                       |age|</span><br><span class="line"><span class="comment">--|--------|-------------------------------------------------------------|---|</span></span><br><span class="line"> 1|zhangsan|&#123;"father":"xiaoming","mother":"xiaohuang","brother":"xiaoxu"&#125;| 28|</span><br><span class="line"> 2|lisi    |&#123;"father":"mayun","mother":"huangyi","brother":"guanyu"&#125;     | 22|</span><br><span class="line"> 3|wangwu  |&#123;"father":"wangjianlin","mother":"ruhua","sister":"jingtian"&#125;| 29|</span><br><span class="line"> 4|mayun   |&#123;"father":"mayongzhen","mother":"angelababy"&#125;                | 26|</span><br></pre></td></tr></table></figure></li></ol><h4 id="对map键值对的”取”"><a href="#对map键值对的”取”" class="headerlink" title="对map键值对的”取”"></a>对map键值对的”取”</h4><ol><li><p>根据key取value</p><p><code>select name,family[&#39;father&#39;] as father,age from map_;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name    |father     |age|</span><br><span class="line"><span class="comment">--------|-----------|---|</span></span><br><span class="line">zhangsan|xiaoming   | 28|</span><br><span class="line">lisi    |mayun      | 22|</span><br><span class="line">wangwu  |wangjianlin| 29|</span><br><span class="line">mayun   |mayongzhen | 26|</span><br></pre></td></tr></table></figure></li><li><p>取出所有key集合</p><p><code>select name,map_keys(family) as map_keys,age from map_;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name    |map_keys                     |age|</span><br><span class="line"><span class="comment">--------|-----------------------------|---|</span></span><br><span class="line">zhangsan|["father","mother","brother"]| 28|</span><br><span class="line">lisi    |["father","mother","brother"]| 22|</span><br><span class="line">wangwu  |["father","mother","sister"] | 29|</span><br><span class="line">mayun   |["father","mother"]          | 26|</span><br></pre></td></tr></table></figure></li><li><p>取出所有的value集合</p><p><code>select name,map_values(family) as map_values,age from map_;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name    |map_values                        |age|</span><br><span class="line"><span class="comment">--------|----------------------------------|---|</span></span><br><span class="line">zhangsan|["xiaoming","xiaohuang","xiaoxu"] | 28|</span><br><span class="line">lisi    |["mayun","huangyi","guanyu"]      | 22|</span><br><span class="line">wangwu  |["wangjianlin","ruhua","jingtian"]| 29|</span><br><span class="line">mayun   |["mayongzhen","angelababy"]       | 26|</span><br></pre></td></tr></table></figure></li><li><p>求key的数量(对key集合求size)</p><p><code>select name,size(map_keys(family)) as key_size,age from map_;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name    |key_size|age|</span><br><span class="line"><span class="comment">--------|--------|---|</span></span><br><span class="line">zhangsan|       3| 28|</span><br><span class="line">lisi    |       3| 22|</span><br><span class="line">wangwu  |       3| 29|</span><br><span class="line">mayun   |       2| 26|</span><br></pre></td></tr></table></figure></li><li><p>求key是否包含某个元素(对key的集合求contains)</p><p><code>select name,family,age from map_ where array_contains((map_keys(family)),&quot;brother&quot;);</code></p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">name    |family                                                       |age|</span><br><span class="line"><span class="comment">--------|-------------------------------------------------------------|---|</span></span><br><span class="line">zhangsan|&#123;"father":"xiaoming","mother":"xiaohuang","brother":"xiaoxu"&#125;| 28|</span><br><span class="line">lisi    |&#123;"father":"mayun","mother":"huangyi","brother":"guanyu"&#125;     | 22|</span><br></pre></td></tr></table></figure></li></ol><h3 id="structs"><a href="#structs" class="headerlink" title="structs"></a>structs</h3><p><code>structs</code>的建表格式:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hive_struct(</span><br><span class="line">ip <span class="keyword">string</span>, info <span class="keyword">struct</span>&lt;<span class="keyword">name</span>:<span class="keyword">string</span>,age:<span class="built_in">int</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'#'</span></span><br><span class="line">COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">':'</span>;</span><br></pre></td></tr></table></figure><p>注意: <code>COLLECTION ITEMS TERMINATED BY &#39;:&#39;</code>是指定每个元素之间的分隔符</p><h4 id="准备数据-2"><a href="#准备数据-2" class="headerlink" title="准备数据"></a>准备数据</h4><ol><li><p>第一步准备数据</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">192.168.1.1#zhangsan:40</span><br><span class="line">192.168.1.2#lisi:50</span><br><span class="line">192.168.1.3#wangwu:60</span><br><span class="line">192.168.1.4#zhaoliu:70</span><br></pre></td></tr></table></figure></li><li><p>第二步建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> structs_(</span><br><span class="line">address <span class="keyword">string</span>,</span><br><span class="line"><span class="string">`user`</span> <span class="keyword">struct</span>&lt;<span class="keyword">name</span>:<span class="keyword">string</span>,age:<span class="built_in">int</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"#"</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">":"</span>;</span><br></pre></td></tr></table></figure></li><li><p>第三步加载数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/hive_struct.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> structs_;</span><br></pre></td></tr></table></figure></li><li><p>第四步查询数据是否装载成功</p><p><code>select * from structs_;</code></p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">address    |user                          |</span><br><span class="line"><span class="comment">-----------|------------------------------|</span></span><br><span class="line">192.168.1.1|[&#123;"name":"zhangsan","age":40&#125;]|</span><br><span class="line">192.168.1.2|[&#123;"name":"lisi","age":50&#125;]    |</span><br><span class="line">192.168.1.3|[&#123;"name":"wangwu","age":60&#125;]  |</span><br><span class="line">192.168.1.4|[&#123;"name":"zhaoliu","age":70&#125;] |</span><br></pre></td></tr></table></figure></li></ol><h4 id="对structs结构体的”取”"><a href="#对structs结构体的”取”" class="headerlink" title="对structs结构体的”取”"></a>对structs结构体的”取”</h4><ol><li><p>取出结构体中的元素</p><p><code>select address,user.name as name from structs_;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">address    |name    |</span><br><span class="line"><span class="comment">-----------|--------|</span></span><br><span class="line">192.168.1.1|zhangsan|</span><br><span class="line">192.168.1.2|lisi    |</span><br><span class="line">192.168.1.3|wangwu  |</span><br><span class="line">192.168.1.4|zhaoliu |</span><br></pre></td></tr></table></figure></li></ol><h2 id="行列互转"><a href="#行列互转" class="headerlink" title="行列互转"></a>行列互转</h2><p>这是一个复杂数据类型的综合练习</p><h3 id="准备数据-3"><a href="#准备数据-3" class="headerlink" title="准备数据"></a>准备数据</h3><ol><li><p>第一步准备数据</p><p>数据1: session点击广告记录</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">11      ad_101  2014-05-01 06:01:12.334+01</span><br><span class="line">22      ad_102  2014-05-01 07:28:12.342+01</span><br><span class="line">33      ad_103  2014-05-01 07:50:12.33+01</span><br><span class="line">11      ad_104  2014-05-01 09:27:12.33+01</span><br><span class="line">22      ad_103  2014-05-01 09:03:12.324+01</span><br><span class="line">33      ad_102  2014-05-02 19:10:12.343+01</span><br><span class="line">11      ad_101  2014-05-02 09:07:12.344+01</span><br><span class="line">35      ad_105  2014-05-03 11:07:12.339+01</span><br><span class="line">22      ad_104  2014-05-03 12:59:12.743+01</span><br><span class="line">77      ad_103  2014-05-03 18:04:12.355+01</span><br><span class="line">99      ad_102  2014-05-04 00:36:39.713+01</span><br><span class="line">33      ad_101  2014-05-04 19:10:12.343+01</span><br><span class="line">11      ad_101  2014-05-05 09:07:12.344+01</span><br><span class="line">35      ad_102  2014-05-05 11:07:12.339+01</span><br><span class="line">22      ad_103  2014-05-05 12:59:12.743+01</span><br><span class="line">77      ad_104  2014-05-05 18:04:12.355+01</span><br><span class="line">99      ad_105  2014-05-05 20:36:39.713+01</span><br></pre></td></tr></table></figure><p>数据2: 广告的详情</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ad_101  http://www.google.com   catalog8|catalog1</span><br><span class="line">ad_102  http://www.sohu.com     catalog6|catalog3</span><br><span class="line">ad_103  http://www.baidu.com    catalog7</span><br><span class="line">ad_104  http://www.qq.com       catalog5|catalog1|catalog4|catalog9</span><br><span class="line">ad_105  http://sina.com</span><br></pre></td></tr></table></figure></li><li><p>第二步创建表</p><p>表1: 动作表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> click_tbl(</span><br><span class="line">cookie_id <span class="keyword">string</span>,  </span><br><span class="line">ad_id <span class="built_in">int</span>,</span><br><span class="line"><span class="built_in">time</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure><p>表2: 广告表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ad_tbl(</span><br><span class="line">ad_id <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">catalogs <span class="built_in">array</span>&lt;<span class="keyword">String</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"|"</span></span><br></pre></td></tr></table></figure></li><li><p>第三步加载数据</p><p>给动作表加载数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/click_log.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> click_tbl;</span><br></pre></td></tr></table></figure><p>给广告表加载数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/ad_list.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> ad_tbl;</span><br></pre></td></tr></table></figure></li><li><p>第四步查询数据是否装载成功</p><p>动作表查询</p><p><code>select * from click_tbl;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id |time                      |</span><br><span class="line"><span class="comment">---------|------|--------------------------|</span></span><br><span class="line">       11|ad_101|2014-05-01 06:01:12.334+01|</span><br><span class="line">       22|ad_102|2014-05-01 07:28:12.342+01|</span><br><span class="line">       33|ad_103|2014-05-01 07:50:12.33+01 |</span><br><span class="line">       11|ad_104|2014-05-01 09:27:12.33+01 |</span><br><span class="line">       22|ad_103|2014-05-01 09:03:12.324+01|</span><br><span class="line">       33|ad_102|2014-05-02 19:10:12.343+01|</span><br><span class="line">       11|ad_101|2014-05-02 09:07:12.344+01|</span><br><span class="line">       35|ad_105|2014-05-03 11:07:12.339+01|</span><br><span class="line">       22|ad_104|2014-05-03 12:59:12.743+01|</span><br><span class="line">       77|ad_103|2014-05-03 18:04:12.355+01|</span><br><span class="line">       99|ad_102|2014-05-04 00:36:39.713+01|</span><br><span class="line">       33|ad_101|2014-05-04 19:10:12.343+01|</span><br><span class="line">       11|ad_101|2014-05-05 09:07:12.344+01|</span><br><span class="line">       35|ad_102|2014-05-05 11:07:12.339+01|</span><br><span class="line">       22|ad_103|2014-05-05 12:59:12.743+01|</span><br><span class="line">       77|ad_104|2014-05-05 18:04:12.355+01|</span><br><span class="line">       99|ad_105|2014-05-05 20:36:39.713+01|</span><br></pre></td></tr></table></figure><p>广告把查询</p><p><code>select * from ad_tbl;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ad_id |url                  |catalogs                               |</span><br><span class="line"><span class="comment">------|---------------------|---------------------------------------|</span></span><br><span class="line">ad_101|http://www.google.com|["catalog8|catalog1"]                  |</span><br><span class="line">ad_102|http://www.sohu.com  |["catalog6|catalog3"]                  |</span><br><span class="line">ad_103|http://www.baidu.com |["catalog7"]                           |</span><br><span class="line">ad_104|http://www.qq.com    |["catalog5|catalog1|catalog4|catalog9"]|</span><br><span class="line">ad_105|http://sina.com      |NULL                                   |</span><br></pre></td></tr></table></figure></li></ol><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><h4 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h4><p>查询每个人访问的广告</p><ol><li><p>去重(collect_set): </p><p><code>select cookie_id,collect_set(ad_id) ad_id from click_tbl group by cookie_id;</code></p></li></ol><ul><li><p>结果:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id                       |</span><br><span class="line"><span class="comment">---------|----------------------------|</span></span><br><span class="line">       11|["ad_101","ad_104"]         |</span><br><span class="line">       22|["ad_102","ad_103","ad_104"]|</span><br><span class="line">       33|["ad_103","ad_102","ad_101"]|</span><br><span class="line">       35|["ad_105","ad_102"]         |</span><br><span class="line">       77|["ad_103","ad_104"]         |</span><br><span class="line">       99|["ad_102","ad_105"]         |</span><br></pre></td></tr></table></figure></li></ul><ol start="2"><li><p>不去重(collect_list):</p><p><code>select cookie_id,collect_list(ad_id) ad_id from click_tbl group by cookie_id;</code></p></li></ol><ul><li><p>结果:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id                                |</span><br><span class="line"><span class="comment">---------|-------------------------------------|</span></span><br><span class="line">       11|["ad_101","ad_104","ad_101","ad_101"]|</span><br><span class="line">       22|["ad_102","ad_103","ad_104","ad_103"]|</span><br><span class="line">       33|["ad_103","ad_102","ad_101"]         |</span><br><span class="line">       35|["ad_105","ad_102"]                  |</span><br><span class="line">       77|["ad_103","ad_104"]                  |</span><br><span class="line">       99|["ad_102","ad_105"]                  |</span><br></pre></td></tr></table></figure></li></ul><h4 id="聚合查询"><a href="#聚合查询" class="headerlink" title="聚合查询"></a>聚合查询</h4><ol><li><p>查询每个人访问相同广告的次数</p><p><code>select cookie_id,ad_id,count(1) amount from click_tbl group by  cookie_id,ad_id;</code></p><p>结果:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id |amount|</span><br><span class="line"><span class="comment">---------|------|------|</span></span><br><span class="line">       11|ad_101|     3|</span><br><span class="line">       11|ad_104|     1|</span><br><span class="line">       22|ad_102|     1|</span><br><span class="line">       22|ad_103|     2|</span><br><span class="line">       22|ad_104|     1|</span><br><span class="line">       33|ad_101|     1|</span><br><span class="line">       33|ad_102|     1|</span><br><span class="line">       33|ad_103|     1|</span><br><span class="line">       35|ad_102|     1|</span><br><span class="line">       35|ad_105|     1|</span><br><span class="line">       77|ad_103|     1|</span><br><span class="line">       77|ad_104|     1|</span><br><span class="line">       99|ad_102|     1|</span><br><span class="line">       99|ad_105|     1|</span><br></pre></td></tr></table></figure></li><li><p>查询每个人访问的广告详情</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">click_tmp.cookie_id,click_tmp.ad_id,ad_tbl.url,ad_tbl.catalogs </span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">ad_tbl</span><br><span class="line"><span class="keyword">JOIN</span></span><br><span class="line">(<span class="keyword">select</span> cookie_id,ad_id,<span class="keyword">count</span>(<span class="number">1</span>) amount <span class="keyword">from</span> click_tbl <span class="keyword">group</span> <span class="keyword">by</span>  cookie_id,ad_id) click_tmp</span><br><span class="line"><span class="keyword">on</span> click_tmp.ad_id=ad_tbl.ad_id;</span><br></pre></td></tr></table></figure><p>结果:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id |url                  |catalogs                                     |</span><br><span class="line"><span class="comment">---------|------|---------------------|---------------------------------------------|</span></span><br><span class="line">       11|ad_101|http://www.google.com|["catalog8","catalog1"]                      |</span><br><span class="line">       11|ad_104|http://www.qq.com    |["catalog5","catalog1","catalog4","catalog9"]|</span><br><span class="line">       22|ad_102|http://www.sohu.com  |["catalog6","catalog3"]                      |</span><br><span class="line">       22|ad_103|http://www.baidu.com |["catalog7"]                                 |</span><br><span class="line">       22|ad_104|http://www.qq.com    |["catalog5","catalog1","catalog4","catalog9"]|</span><br><span class="line">       33|ad_101|http://www.google.com|["catalog8","catalog1"]                      |</span><br><span class="line">       33|ad_102|http://www.sohu.com  |["catalog6","catalog3"]                      |</span><br><span class="line">       33|ad_103|http://www.baidu.com |["catalog7"]                                 |</span><br><span class="line">       35|ad_102|http://www.sohu.com  |["catalog6","catalog3"]                      |</span><br><span class="line">       35|ad_105|http://sina.com      |NULL                                         |</span><br><span class="line">       77|ad_103|http://www.baidu.com |["catalog7"]                                 |</span><br><span class="line">       77|ad_104|http://www.qq.com    |["catalog5","catalog1","catalog4","catalog9"]|</span><br><span class="line">       99|ad_102|http://www.sohu.com  |["catalog6","catalog3"]                      |</span><br><span class="line">       99|ad_105|http://sina.com      |NULL                                         |</span><br></pre></td></tr></table></figure></li><li><p>把catalogs中的数据元素排序</p><p><code>select ad_id,url,sort_array(catalogs) as catalogs from ad_tbl;</code></p><p>结果:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ad_id |url                  |catalogs                                     |</span><br><span class="line"><span class="comment">------|---------------------|---------------------------------------------|</span></span><br><span class="line">ad_101|http://www.google.com|["catalog1","catalog8"]                      |</span><br><span class="line">ad_102|http://www.sohu.com  |["catalog3","catalog6"]                      |</span><br><span class="line">ad_103|http://www.baidu.com |["catalog7"]                                 |</span><br><span class="line">ad_104|http://www.qq.com    |["catalog1","catalog4","catalog5","catalog9"]|</span><br><span class="line">ad_105|http://sina.com      |NULL                                         |</span><br></pre></td></tr></table></figure></li></ol><h4 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h4><p>查询每个广告详情</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">ad_id,<span class="keyword">catalog</span></span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">ad_tbl</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(catalogs) t <span class="keyword">as</span> <span class="keyword">catalog</span>;</span><br></pre></td></tr></table></figure><p>结果:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ad_id |catalog |</span><br><span class="line"><span class="comment">------|--------|</span></span><br><span class="line">ad_101|catalog8|</span><br><span class="line">ad_101|catalog1|</span><br><span class="line">ad_102|catalog6|</span><br><span class="line">ad_102|catalog3|</span><br><span class="line">ad_103|catalog7|</span><br><span class="line">ad_104|catalog5|</span><br><span class="line">ad_104|catalog1|</span><br><span class="line">ad_104|catalog4|</span><br><span class="line">ad_104|catalog9|</span><br></pre></td></tr></table></figure><p><code>注意:</code> 如果ad_tbl的catalogs字段是String类型的，那么在explode炸开的时候要转换成数组，也就是用split把字段元素按’|’切分开返回一个数组</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">lateral view outer explode(split(catalogs,'\\|')) t as catalog</span><br></pre></td></tr></table></figure><h2 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h2><p>查用函数的用法查询: <code>desc function extended 函数名;</code></p><h3 id="时间函数"><a href="#时间函数" class="headerlink" title="时间函数"></a>时间函数</h3><p><code>current_date:</code> 返回当前日期</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">current_date</span><span class="comment">--2019-12-21</span></span><br></pre></td></tr></table></figure><p><code>current_timestamp:</code> 返回当前时间</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">current_timestamp</span><span class="comment">--2019-12-21 02:23:44</span></span><br></pre></td></tr></table></figure><p><code>unix_timestamp:</code> 时间转秒</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">unix_timestamp</span>(<span class="string">'2019-08-15 16:40:00'</span>,<span class="string">'yyyy-MM-dd HH:mm:ss'</span>)   <span class="comment">--1565858400</span></span><br></pre></td></tr></table></figure><p><code>from_unixtime:</code> 秒转时间</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">1565858389</span>,<span class="string">'yyyy-MM-dd HH:mm:ss'</span>)  <span class="comment">--2019-08-15 16:39:49</span></span><br><span class="line"><span class="keyword">select</span> from_unixtime(<span class="keyword">cast</span>(<span class="keyword">substr</span>(<span class="number">1553184000488</span>,<span class="number">1</span>,<span class="number">10</span>) <span class="keyword">as</span> <span class="built_in">int</span>),<span class="string">'yyyy-MM-dd HH:mm:ss'</span>)  <span class="comment">--2019-03-22 00:00:00</span></span><br><span class="line"><span class="keyword">select</span> from_unixtime(<span class="keyword">unix_timestamp</span>(),<span class="string">'yyyy-MM-dd HH:mm:ss'</span>)   <span class="comment">-- 2019-08-15 17:18:55</span></span><br></pre></td></tr></table></figure><p><code>to_date:</code> 返回日期</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">to_date</span>(<span class="string">'2009-07-30 04:17:52'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;<span class="comment">--2009-07-30</span></span><br></pre></td></tr></table></figure><p><code>year:</code> 返回年份</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">year</span>(<span class="string">'2009-07-30'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;<span class="comment">--2009</span></span><br></pre></td></tr></table></figure><p><code>month:</code> 返回月份</p><p><code>day:</code> 返回日期</p><p><code>hour:</code> 返回小时</p><p><code>minute:</code> 返回分钟</p><p><code>second:</code> 返回毫秒</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">second</span>(<span class="string">'2009-07-30 12:58:59'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>; <span class="comment">--59</span></span><br></pre></td></tr></table></figure><p><code>date_add:</code> 增加指定时间</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">date_add</span>(<span class="string">'2009-07-30'</span>, <span class="number">1</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;<span class="comment">--2009-07-31</span></span><br></pre></td></tr></table></figure><p><code>date_sub:</code> 减少指定时间</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">date_sub</span>(<span class="string">'2009-07-30'</span>, <span class="number">1</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;<span class="comment">--2009-07-29</span></span><br></pre></td></tr></table></figure><h3 id="数值函数"><a href="#数值函数" class="headerlink" title="数值函数"></a>数值函数</h3><p><code>round:</code> 返回指定范围的数值</p><p><code>ceil:</code> 返回天花板，取最大的整数值</p><p><code>floor:</code> 返回地板，去最小的整数值</p><p><code>abs:</code> 返回绝对值</p><p><code>least:</code> 数列中最小值 ==&gt; min</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">least</span>(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;<span class="comment">--1</span></span><br></pre></td></tr></table></figure><h3 id="字符串函数"><a href="#字符串函数" class="headerlink" title="字符串函数"></a>字符串函数</h3><p><code>substr:</code> 返回截取字符串</p><p><code>concat:</code> 返回连接字符串</p><p><code>concat_ws:</code> 根据特定格式组合字符串</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">concat_ws</span>(<span class="string">'.'</span>, <span class="string">'www'</span>, <span class="built_in">array</span>(<span class="string">'facebook'</span>, <span class="string">'com'</span>)) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;<span class="comment">--www.facebook.com</span></span><br></pre></td></tr></table></figure><p><code>length:</code> 返回字符串的长度，整数值</p><p><code>split:</code> 返回切分后的字符串数组</p><p><code>upper:</code> 字符转大写</p><p><code>lower:</code> 字符转小写</p><h3 id="Json处理函数"><a href="#Json处理函数" class="headerlink" title="Json处理函数"></a>Json处理函数</h3><ol><li><p>第一步准备数据</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"1"</span>,<span class="attr">"rate"</span>:<span class="string">"5"</span>,<span class="attr">"time"</span>:<span class="string">"978300760"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"2"</span>,<span class="attr">"rate"</span>:<span class="string">"4"</span>,<span class="attr">"time"</span>:<span class="string">"978702109"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"3"</span>,<span class="attr">"rate"</span>:<span class="string">"3"</span>,<span class="attr">"time"</span>:<span class="string">"978401968"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"4"</span>,<span class="attr">"rate"</span>:<span class="string">"4"</span>,<span class="attr">"time"</span>:<span class="string">"978300275"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"5"</span>,<span class="attr">"rate"</span>:<span class="string">"3"</span>,<span class="attr">"time"</span>:<span class="string">"978801091"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br></pre></td></tr></table></figure></li><li><p>第二步建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> rating_json(</span><br><span class="line"><span class="keyword">json</span> <span class="keyword">string</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p>第三步加载数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/rating.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> rating_json;</span><br></pre></td></tr></table></figure></li><li><p>第四步查看数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">json                                                    |</span><br><span class="line"><span class="comment">--------------------------------------------------------|</span></span><br><span class="line">&#123;"movie":"1","rate":"5","time":"978300760","userid":"1"&#125;|</span><br><span class="line">&#123;"movie":"2","rate":"4","time":"978702109","userid":"1"&#125;|</span><br><span class="line">&#123;"movie":"3","rate":"3","time":"978401968","userid":"1"&#125;|</span><br><span class="line">&#123;"movie":"4","rate":"4","time":"978300275","userid":"1"&#125;|</span><br><span class="line">&#123;"movie":"5","rate":"3","time":"978801091","userid":"1"&#125;|</span><br></pre></td></tr></table></figure></li><li><p>对数据进行处理</p><p><code>json_tuple:</code> 返回指定json文件的指定字段，返回的是字符串</p><p><code>select json_tuple(json,&quot;movie&quot;,&quot;rate&quot;,&quot;time&quot;,&quot;userid&quot;) as (movie,rate,time,userid) from rating_json;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">movie|rate|time     |userid|</span><br><span class="line"><span class="comment">-----|----|---------|------|</span></span><br><span class="line">1    |5   |978300760|1     |</span><br><span class="line">2    |4   |978702109|1     |</span><br><span class="line">3    |3   |978401968|1     |</span><br><span class="line">4    |4   |978300275|1     |</span><br><span class="line">5    |3   |978801091|1     |</span><br></pre></td></tr></table></figure><p><code>parse_url_tuple:</code> 返回指定url的指定字段，返回的是字符串</p><p><code>select parse_url_tuple(&quot;https://www.baidu.com/bigdate/spark?cookie_id=10&quot;,&#39;HOST&#39;,&#39;PATH&#39;,&#39;QUERY&#39;,&#39;QUERY:cookie_id&#39;);</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">c0           |c1            |c2          |c3|</span><br><span class="line"><span class="comment">-------------|--------------|------------|--|</span></span><br><span class="line">www.baidu.com|/bigdate/spark|cookie_id=10|10|</span><br></pre></td></tr></table></figure></li></ol><h3 id="Null值处理函数"><a href="#Null值处理函数" class="headerlink" title="Null值处理函数"></a>Null值处理函数</h3><p><code>isnull:</code> 指定字段的元素如果为null则返回true</p><p><code>isnotnull:</code> 指定字段的元素如果不为null则返回true</p><p><code>elt:</code> 指定返回的元素</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">elt</span>(<span class="number">1</span>, <span class="string">'face'</span>, <span class="string">'book'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;<span class="comment">--face</span></span><br></pre></td></tr></table></figure><p><code>nvl:</code> 替换null值</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> nvl(<span class="literal">null</span>,<span class="string">'bla'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;<span class="comment">--bla</span></span><br></pre></td></tr></table></figure><h3 id="其他函数"><a href="#其他函数" class="headerlink" title="其他函数"></a>其他函数</h3><p><code>cast:</code> 转换数据类型</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cast(time as bigint)<span class="comment">--时间转数值</span></span><br><span class="line">cast(string as date)<span class="comment">--字符串转日期</span></span><br></pre></td></tr></table></figure><p><code>注意:</code> binary只能转string</p><h2 id="静动态分区表"><a href="#静动态分区表" class="headerlink" title="静动态分区表"></a>静动态分区表</h2><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><ol><li><p>数据源</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">1,jack,shanghai,20190129</span><br><span class="line">2,kevin,beijing,20190130</span><br><span class="line">3,lucas,hangzhou,20190129</span><br><span class="line">4,lily,hangzhou,20190130</span><br></pre></td></tr></table></figure></li><li><p>创建源数据表（外表）</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> prit_tbl(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">address <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">day</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure></li><li><p>加载数据：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/partition.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> prit_tbl;</span><br></pre></td></tr></table></figure></li><li><p>创建分区表（外表）</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> prit_tbl(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">address <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">day</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure></li></ol><h3 id="静态分区加载数据"><a href="#静态分区加载数据" class="headerlink" title="静态分区加载数据"></a>静态分区加载数据</h3><ul><li><p>静态分区缺点：每次写入都要明确指定分区日期。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> prit_tbl <span class="keyword">partition</span>(<span class="keyword">day</span>=<span class="string">"20190129"</span>) <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span>,address <span class="keyword">from</span> test_tbl <span class="keyword">where</span> <span class="keyword">day</span> = <span class="string">"20190129"</span> ;</span><br></pre></td></tr></table></figure><p><code>注意:</code> 并且在查询处不能包含分区字段day，否则会报错</p></li></ul><h3 id="动态分区加载数据"><a href="#动态分区加载数据" class="headerlink" title="动态分区加载数据"></a>动态分区加载数据</h3><ul><li><p>查看表分区</p><p><code>show partitions prit_tbl;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">partition   |</span><br><span class="line"><span class="comment">------------|</span></span><br><span class="line">day=20190129|</span><br><span class="line">day=20190130|</span><br></pre></td></tr></table></figure></li><li><p>自动识别分区，不需要明确指定</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> prit_tbl <span class="keyword">partition</span>(<span class="keyword">day</span>) <span class="keyword">select</span> * <span class="keyword">from</span> test_tbl;</span><br></pre></td></tr></table></figure></li><li><p>查询验证：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> prit_tbl;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> prit_tbl <span class="keyword">where</span> <span class="keyword">day</span> = <span class="number">20190129</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> prit_tbl <span class="keyword">where</span> <span class="keyword">day</span> = <span class="number">20190130</span>;</span><br></pre></td></tr></table></figure><p>HDFS web界面验证</p></li></ul><h3 id="分区注意"><a href="#分区注意" class="headerlink" title="分区注意"></a>分区注意</h3><ol><li><p>尽量不要是用动态分区，因为动态分区的时候，将会为每一个分区分配reducer数量，当分区数量多的时候，reducer数量将会增加，对服务器是一种灾难。</p></li><li><p>动态分区和静态分区的区别: 静态分区不管有没有数据都将会创建该分区，动态分区是有结果集将创建，否则不创建。</p></li><li><p>hive动态分区的严格模式和hive提供的<code>hive.mapred.mode的</code>严格模式,为了阻止用户不小心提交恶意<code>hql</code>。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive.mapred.mode=nostrict : strict</span><br></pre></td></tr></table></figure><p>如果该模式值为strict，将会阻止以下三种查询：</p><ol><li>对分区表查询，where中过滤字段不是分区字段。</li><li>笛卡尔积join查询，join查询语句，不带on条件 或者 where条件。</li><li>对order by查询，有order by的查询不带limit语句。</li></ol></li></ol><h2 id="桶表"><a href="#桶表" class="headerlink" title="桶表"></a>桶表</h2><h3 id="分桶的概念"><a href="#分桶的概念" class="headerlink" title="分桶的概念"></a>分桶的概念</h3><p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。</p><p>分桶是将数据集分解成更容易管理的若干部分的另一个技术。</p><p><code>分区针对的是数据的存储路径；分桶针对的是数据文件。</code></p><h3 id="分桶的好处"><a href="#分桶的好处" class="headerlink" title="分桶的好处"></a>分桶的好处</h3><ul><li>分桶规则：对分桶字段值进行哈希，哈希值除以桶的个数求余，余数决定了该条记录在哪个桶中，也就是余数相同的在一个桶中。</li><li>优点：<ol><li>提高join查询效率 </li><li>提高抽样效率</li></ol></li></ul><h3 id="分桶实践"><a href="#分桶实践" class="headerlink" title="分桶实践"></a>分桶实践</h3><ol start="2"><li><p>准备数据</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1,name1</span><br><span class="line">2,name2</span><br><span class="line">3,name3</span><br><span class="line">4,name4</span><br><span class="line">5,name5</span><br><span class="line">6,name6</span><br><span class="line">7,name7</span><br><span class="line">8,name8</span><br><span class="line">9,name9</span><br></pre></td></tr></table></figure></li><li><p>创建桶表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> bucket_tbl(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>) </span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure></li><li><p>创建中间表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> mid_tbl(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure></li><li><p>加载数据到中间表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/bucket.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> mid_tbl;</span><br></pre></td></tr></table></figure></li><li><p>设置强制分桶</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set hive.enforce.bucketing = true; --Hive 2.x 不需要这一步</span><br><span class="line">set mapreduce.job.reduces=-1;</span><br></pre></td></tr></table></figure><p>在 Hive 0.x and 1.x 版本，必须使用设置 <code>hive.enforce.bucketing = true</code>，表示强制分桶，允许程序根据表结构自动选择正确数量的 Reducer 和 cluster by column 来进行分桶。</p></li><li><p>插入数据到分桶表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> bucket_tbl <span class="keyword">select</span> * <span class="keyword">from</span> mid_tbl;</span><br></pre></td></tr></table></figure></li><li><p>查看结果</p><p>HDFS：<code>桶是以文件的形式存在的，而不是像分区那样以文件夹的形式存在。</code></p><p><img src="https://upload-images.jianshu.io/upload_images/12267859-d7e00bb44b332b5b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/format/webp" alt="分桶文件"></p></li></ol><h3 id="有序的分桶表"><a href="#有序的分桶表" class="headerlink" title="有序的分桶表"></a>有序的分桶表</h3><ul><li><p>如果要按id升序排序可以这样建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_bucket_sorted (</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'ID'</span>, </span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'名字'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">comment</span> <span class="string">'测试分桶'</span></span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>) </span><br><span class="line">sorted <span class="keyword">by</span> (<span class="keyword">id</span>) </span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span> ;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> bucket_tbl(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>) sorted <span class="keyword">by</span> (<span class="keyword">id</span>) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure><p><code>注意:</code> 同样需要中间表insert插入数据</p></li><li><p>好处:</p><p>因为每个桶内的数据是排序的，这样每个桶进行连接时就变成了高效的归并排序</p></li></ul><h3 id="分桶抽样查询"><a href="#分桶抽样查询" class="headerlink" title="分桶抽样查询"></a>分桶抽样查询</h3><p>对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。</p><h4 id="SQL示例"><a href="#SQL示例" class="headerlink" title="SQL示例"></a>SQL示例</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from test_bucket tablesample (bucket 1 out of 2);</span><br><span class="line">OK</span><br><span class="line">8   name8</span><br><span class="line">4   name4</span><br><span class="line">2   name2</span><br><span class="line">6   name6</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from test tablesample (bucket 1 out of 2 on id);</span><br><span class="line">OK</span><br><span class="line">2   name2</span><br><span class="line">8   name8</span><br><span class="line">4   name4</span><br><span class="line">6   name6</span><br></pre></td></tr></table></figure><h4 id="区别"><a href="#区别" class="headerlink" title="区别:"></a>区别:</h4><ul><li>分桶表后面可以不带on 字段名，不带时默认的是按分桶字段,也可以带，而没有分桶的表则必须带</li><li>按分桶字段取样时，因为分桶表是直接去对应的桶中拿数据，在表比较大时会提高取样效率</li></ul><h4 id="语法"><a href="#语法" class="headerlink" title="语法:"></a>语法:</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">tablesample (bucket x out of y on id);</span><br></pre></td></tr></table></figure><p>x表示从哪个桶开始，y代表分几个桶，也可以理解分x为分子，y为分母，及将表分为y份（桶），取第x份（桶）</p><p>所以这时对于分桶表是有要求的，y为桶数的倍数或因子，</p><ul><li><p>x=1,y=2，取2(4/y)个bucket的数据，分别桶1和桶3(1+y)</p></li><li><p>x=1,y=4, 取1(4/y)个bucket的数据，即桶1</p></li><li><p>x=2,y=8, 取1/2(4/y)个bucket的数据，即桶1的一半</p><p> x的值必须小于等于y的值</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive数据类型&amp;DDL数据定义(增删查改)&amp;DML数据操作(导入导出)</title>
      <link href="/2018/11/04/hive/4/"/>
      <url>/2018/11/04/hive/4/</url>
      
        <content type="html"><![CDATA[<h2 id="Hive数据类型"><a href="#Hive数据类型" class="headerlink" title="Hive数据类型"></a>Hive数据类型</h2><h3 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h3><table><thead><tr><th>Hive数据类型</th><th>Java数据类型</th><th>长度</th><th>例子</th></tr></thead><tbody><tr><td>TINYINT</td><td>byte</td><td>1byte有符号整数</td><td>20</td></tr><tr><td>SMALINT</td><td>short</td><td>2byte有符号整数</td><td>20</td></tr><tr><td>INT</td><td>int</td><td>4byte有符号整数</td><td>20</td></tr><tr><td>BIGINT</td><td>long</td><td>8byte有符号整数</td><td>20</td></tr><tr><td>BOOLEAN</td><td>boolean</td><td>布尔类型，true或者false</td><td>TRUE FALSE</td></tr><tr><td>FLOAT</td><td>float</td><td>单精度浮点数</td><td>3.14159</td></tr><tr><td>DOUBLE</td><td>double</td><td>双精度浮点数</td><td>3.14159</td></tr><tr><td>STRING</td><td>string</td><td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td><td>‘now is the time’ “for all good men”</td></tr><tr><td>TIMESTAMP</td><td></td><td>时间类型</td><td></td></tr><tr><td>BINARY</td><td></td><td>字节数组</td><td></td></tr></tbody></table><p>对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</p><h3 id="集合数据类型"><a href="#集合数据类型" class="headerlink" title="集合数据类型"></a>集合数据类型</h3><table><thead><tr><th>数据类型</th><th>描述</th><th>语法示例</th></tr></thead><tbody><tr><td>STRUCT</td><td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td><td>struct()  例如struct&lt;street:string,  city:string&gt;</td></tr><tr><td>MAP</td><td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td><td>map()  例如map&lt;string, int&gt;</td></tr><tr><td>ARRAY</td><td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’,  ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td><td>Array()  例如array<string></td></tr></tbody></table><p>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p><ul><li><p>案例实操</p><ol><li><p>假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问的格式为</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"name"</span>: <span class="string">"songsong"</span>,</span><br><span class="line">    <span class="attr">"friends"</span>: [<span class="string">"bingbing"</span> , <span class="string">"lili"</span>] ,       <span class="comment">//列表Array, </span></span><br><span class="line">    <span class="attr">"children"</span>: &#123;                      <span class="comment">//键值Map,</span></span><br><span class="line">        <span class="attr">"xiao song"</span>: <span class="number">18</span> ,</span><br><span class="line">        <span class="attr">"xiaoxiao song"</span>: <span class="number">19</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">"address"</span>: &#123;                      <span class="comment">//结构Struct,</span></span><br><span class="line">        <span class="attr">"street"</span>: <span class="string">"hui long guan"</span> ,</span><br><span class="line">        <span class="attr">"city"</span>: <span class="string">"beijing"</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>基于上述数据结构，我们在Hive里创建对应的表，并导入数据。 创建本地测试文件test.txt</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing</span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</span><br></pre></td></tr></table></figure><p>注意：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。</p></li><li><p>Hive上创建测试表test</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">test</span>(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">friends <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">children <span class="keyword">map</span>&lt;<span class="keyword">string</span>, <span class="built_in">int</span>&gt;,</span><br><span class="line">address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>, city:<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'_'</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">':'</span></span><br><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span>;<span class="comment">#默认"\n"</span></span><br></pre></td></tr></table></figure><p>字段解释：</p><p><code>row format delimited fields terminated by &#39;,&#39;</code>      – 列分隔符</p><p><code>collection items terminated by &#39;_&#39;</code>                             – MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</p><p><code>map keys terminated by &#39;:&#39;</code>                                              – MAP中的key与value的分隔符</p><p><code>lines terminated by &#39;\n&#39;;</code>                                                – 行分隔符</p></li><li><p>导入文本数据到测试表</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath ‘/opt/module/datas/test.txt’into table test</span><br></pre></td></tr></table></figure></li><li><p>访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select friends[1],children['xiao song'],address.city from test</span><br><span class="line">where name="songsong";</span><br><span class="line">OK</span><br><span class="line">_c0     _c1     city</span><br><span class="line">lili    18      beijing</span><br><span class="line">Time taken: 0.076 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure></li></ol></li></ul><h2 id="DDL数据定义"><a href="#DDL数据定义" class="headerlink" title="DDL数据定义"></a>DDL数据定义</h2><h3 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] database_name</span><br><span class="line">[<span class="keyword">COMMENT</span> database_comment]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[<span class="keyword">WITH</span> DBPROPERTIES (property_name=property_value, ...)];</span><br></pre></td></tr></table></figure><ul><li>创建一个数据库，数据库在HDFS上的默认存储路径是/user/hive/warehouse/*.db。</li></ul><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br></pre></td></tr></table></figure><ul><li>避免要创建的数据库已经存在错误，增加if not exists判断。（标准写法）</li></ul><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already exists</span><br><span class="line">hive (default)&gt; create database if not exists db_hive;</span><br></pre></td></tr></table></figure><ul><li>创建一个数据库，指定数据库在HDFS上存放的位置</li></ul><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive2 location '/db_hive2.db';</span><br></pre></td></tr></table></figure><h3 id="查询数据库"><a href="#查询数据库" class="headerlink" title="查询数据库"></a>查询数据库</h3><h4 id="显示数据库"><a href="#显示数据库" class="headerlink" title="显示数据库"></a>显示数据库</h4><ol><li><p>显示数据库</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure></li><li><p>过滤显示查询的数据库</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; show databases like 'db_hive*';</span><br><span class="line">OK</span><br><span class="line">db_hive</span><br><span class="line">db_hive_1</span><br></pre></td></tr></table></figure></li></ol><h4 id="查看数据库详情"><a href="#查看数据库详情" class="headerlink" title="查看数据库详情"></a>查看数据库详情</h4><ol><li><p>显示数据库信息</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; desc database db_hive;</span><br><span class="line">OK</span><br><span class="line">db_hivehdfs://aliyun:9000/user/hive/warehouse/db_hive.dbhadoopUSER</span><br></pre></td></tr></table></figure></li><li><p>显示数据库详细信息，extended</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br><span class="line">OK</span><br><span class="line">db_hivehdfs://aliyun:9000/user/hive/warehouse/db_hive.dbhadoopUSER</span><br></pre></td></tr></table></figure></li></ol><h4 id="切换当前数据库"><a href="#切换当前数据库" class="headerlink" title="切换当前数据库"></a>切换当前数据库</h4><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; use db_hive;</span><br></pre></td></tr></table></figure><h3 id="修改数据库"><a href="#修改数据库" class="headerlink" title="修改数据库"></a>修改数据库</h3><p>用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter database db_hive set dbproperties('createtime'='20170830');</span><br></pre></td></tr></table></figure><p>在hive中查看修改结果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br><span class="line">db_name <span class="keyword">comment</span> location        owner_name      owner_type      <span class="keyword">parameters</span></span><br><span class="line">db_hive         hdfs://aliyun:<span class="number">8020</span>/<span class="keyword">user</span>/hive/warehouse/db_hive.db    hadoop <span class="keyword">USER</span>    &#123;createtime=<span class="number">20170830</span>&#125;</span><br></pre></td></tr></table></figure><h3 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h3><ul><li><p>删除空数据库</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt;drop database db_hive2;</span><br></pre></td></tr></table></figure></li><li><p>如果删除的数据库不存在，最好采用 if exists判断数据库是否存在</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: SemanticException [Error 10072]: Database does not exist: db_hive</span><br><span class="line">hive&gt; drop database if exists db_hive2;</span><br></pre></td></tr></table></figure></li><li><p>如果数据库不为空，可以采用cascade命令，强制删除</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database db_hive is not empty. One or more tables exist.)</span><br><span class="line">hive&gt; drop database db_hive cascade;</span><br></pre></td></tr></table></figure></li></ul><h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><ul><li><p>建表语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name </span><br><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment] </span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) </span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS] </span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] </span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name=property_value, ...)]</span><br><span class="line">[<span class="keyword">AS</span> select_statement]</span><br></pre></td></tr></table></figure></li><li><p>字段解释说明 </p><ol><li><p><code>CREATE TABLE</code> 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。</p></li><li><p><code>EXTERNAL</code>关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实际数据的路径（LOCATION），在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</p></li><li><p><code>COMMENT</code>为表和列添加注释。</p></li><li><p><code>PARTITIONED BY</code>创建分区表</p></li><li><p><code>CLUSTERED BY</code>创建分桶表</p></li><li><p><code>SORTED BY</code>不常用，对桶中的一个或多个列另外排序</p></li><li><p><code>ROW FORMAT</code> </p><p><code>DELIMITED [FIELDS TERMINATED BY char]</code> </p><p><code>[COLLECTION ITEMS TERMINATED BY char]</code></p><p><code>[MAP KEYS TERMINATED BY char]</code> </p><p><code>[LINES TERMINATED BY char]</code> </p><p><code>SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</code></p><p>用户在建表的时候可以自定义<code>SerDe</code>或者使用自带的<code>SerDe</code>。如果没有指定<code>ROW FORMAT</code> 或者<code>ROW FORMAT DELIMITED</code>，将会使用自带的<code>SerDe</code>。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的<code>SerDe</code>，Hive通过<code>SerDe</code>确定表的具体的列的数据。</p><p><code>SerDe</code>是<code>Serialize</code>/<code>Deserilize</code>的简称， hive使用<code>Serde</code>进行行对象的序列与反序列化。</p></li><li><p><code>STORED AS</code>指定存储文件类型</p><p>常用的存储文件类型：<code>SEQUENCEFILE</code>（二进制序列文件）、<code>TEXTFILE</code>（文本）、<code>RCFILE</code>（列式存储格式文件）</p><p>如果文件数据是纯文本，可以使用<code>STORED AS TEXTFILE</code>。如果数据需要压缩，使用 <code>STORED AS</code> <code>SEQUENCEFILE</code>。</p></li><li><p><code>LOCATION</code> ：指定表在HDFS上的存储位置。</p></li><li><p><code>AS</code>：后跟查询语句，根据查询结果创建表。</p></li><li><p><code>LIKE</code>允许用户复制现有的表结构，但是不复制数据。</p></li></ol></li></ul><h4 id="管理表"><a href="#管理表" class="headerlink" title="管理表"></a>管理表</h4><ul><li><p>理论</p><p>默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。 当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据。</p></li><li><p>案例实操</p><ol><li><p>普通创建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student2(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile</span><br><span class="line">location <span class="string">'/user/hive/warehouse/student2'</span>;</span><br></pre></td></tr></table></figure></li><li><p>根据查询结果创建表（查询的结果会添加到新创建的表中）</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student3 <span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></li><li><p>根据已经存在的表结构创建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student4 <span class="keyword">like</span> student;</span><br></pre></td></tr></table></figure></li><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure></li></ol></li></ul><h4 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h4><ul><li><p>理论</p><p>因为表是外部表，所以Hive并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。</p></li><li><p>管理表和外部表的使用场景</p><p>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p></li><li><p>案例实操</p><p>分别创建部门和员工外部表，并向表中导入数据。</p><ol><li><p>上传数据到HDFS</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir /student;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /student;</span><br></pre></td></tr></table></figure></li><li><p>建表语句</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create external table stu (</span><br><span class="line">id int, </span><br><span class="line">name string) </span><br><span class="line">row format delimited fields terminated by '\t' </span><br><span class="line">location '/student';</span><br></pre></td></tr></table></figure></li><li><p>查看创建的表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from stu_external;</span><br><span class="line">OK</span><br><span class="line">stu_external.id stu_external.name</span><br><span class="line">1001    lisi</span><br><span class="line">1002    wangwu</span><br></pre></td></tr></table></figure></li><li><p>查看表格式化数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted dept;</span><br><span class="line">Table Type:             EXTERNAL_TABLE</span><br></pre></td></tr></table></figure></li><li><p>删除外部表</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; drop table stu_external;</span><br></pre></td></tr></table></figure><p>外部表删除后，hdfs中的数据还在，但是metadata中stu_external的元数据已被删除</p></li></ol></li></ul><h4 id="管理表与外部表的互相转换"><a href="#管理表与外部表的互相转换" class="headerlink" title="管理表与外部表的互相转换"></a>管理表与外部表的互相转换</h4><ol><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure></li><li><p>修改内部表student2为外部表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'TRUE'</span>);</span><br></pre></td></tr></table></figure></li><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             EXTERNAL_TABLE</span><br></pre></td></tr></table></figure></li><li><p>修改外部表student2为内部表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'FALSE'</span>);</span><br></pre></td></tr></table></figure></li><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure><p>注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写！</p></li></ol><h3 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h3><p>分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。</p><h4 id="分区表基本操作"><a href="#分区表基本操作" class="headerlink" title="分区表基本操作"></a>分区表基本操作</h4><ol><li><p>引入分区表（需要根据日期对日志进行管理）</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">/user/hive/warehouse/log_partition/20170702/20170702.log</span><br><span class="line">/user/hive/warehouse/log_partition/20170703/20170703.log</span><br><span class="line">/user/hive/warehouse/log_partition/20170704/20170704.log</span><br></pre></td></tr></table></figure></li><li><p>创建分区表语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table dept (</span><br><span class="line">deptno int, </span><br><span class="line">dname string, </span><br><span class="line">loc string</span><br><span class="line">)</span><br><span class="line">partitioned by (month string)</span><br><span class="line">row format delimited fields terminated by '\t';</span><br></pre></td></tr></table></figure><p><code>注意：</code> 分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。</p></li><li><p>加载数据到分区表中</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept partition(month='201709');</span><br><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept partition(month='201708');</span><br><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept partition(month='201707’);</span><br></pre></td></tr></table></figure><p><code>注意：</code> 分区表加载数据时，必须指定分区</p></li><li><p>查询分区表中数据</p><ol><li><p>单分区查询</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month='201709';</span><br></pre></td></tr></table></figure></li><li><p>多分区联合查询</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month='201709'</span><br><span class="line">              union all</span><br><span class="line">              <span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201708'</span></span><br><span class="line">              <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">              <span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201707'</span>;</span><br><span class="line"></span><br><span class="line">_u3.deptno      _u3.dname       _u3.loc _u3.month</span><br><span class="line">10      ACCOUNTING      NEW YORK        201707</span><br><span class="line">10      ACCOUNTING      NEW YORK        201708</span><br><span class="line">10      ACCOUNTING      NEW YORK        201709</span><br><span class="line">20      RESEARCH        DALLAS  201707</span><br><span class="line">20      RESEARCH        DALLAS  201708</span><br><span class="line">20      RESEARCH        DALLAS  201709</span><br><span class="line">30      SALES   CHICAGO 201707</span><br><span class="line">30      SALES   CHICAGO 201708</span><br><span class="line">30      SALES   CHICAGO 201709</span><br><span class="line">40      OPERATIONS      BOSTON  201707</span><br><span class="line">40      OPERATIONS      BOSTON  201708</span><br><span class="line">40      OPERATIONS      BOSTON  201709</span><br></pre></td></tr></table></figure></li></ol></li><li><p>增加分区</p><ol><li><p>创建单个分区</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(month='201706') ;</span><br></pre></td></tr></table></figure></li><li><p>同时创建多个分区</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(month='201705') partition(month='201704');</span><br></pre></td></tr></table></figure></li></ol></li><li><p>删除分区</p><ol><li><p>删除单个分区</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (month='201704');</span><br></pre></td></tr></table></figure></li><li><p>同时删除多个分区</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (month='201705'), partition (month='201706');</span><br></pre></td></tr></table></figure></li></ol></li><li><p>查看分区表有多少分区</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show partitions dept_partition;</span><br></pre></td></tr></table></figure></li><li><p>查看分区表结构</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; desc formatted dept_partition;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Partition Information          </span></span><br><span class="line"><span class="comment"># col_name              data_type               comment             </span></span><br><span class="line">month                   string</span><br></pre></td></tr></table></figure></li></ol><h4 id="分区表注意事项"><a href="#分区表注意事项" class="headerlink" title="分区表注意事项"></a>分区表注意事项</h4><ul><li><p>创建二级分区表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition2(</span><br><span class="line">               deptno int, </span><br><span class="line">               dname string, </span><br><span class="line">               loc string</span><br><span class="line">               )</span><br><span class="line">               partitioned by (month string, day string)</span><br><span class="line">               row format delimited fields terminated by '\t';</span><br></pre></td></tr></table></figure></li><li><p>正常的加载数据</p><ol><li><p>加载数据到二级分区表中</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table</span><br><span class="line"> default.dept_partition2 partition(month='201709', day='13');</span><br></pre></td></tr></table></figure></li><li><p>查询分区数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='13';</span><br></pre></td></tr></table></figure></li></ol></li><li><p>把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式</p><ol><li><p>方式一：上传数据后修复(<code>禁用</code>)</p><p>上传数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br></pre></td></tr></table></figure><p>查询数据（查询不到刚上传的数据）</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='12';</span><br></pre></td></tr></table></figure><p>执行修复命令</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; msck repair table dept_partition2;</span><br></pre></td></tr></table></figure><p>再次查询数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='12';</span><br></pre></td></tr></table></figure></li><li><p>方式二：上传数据后添加分区(<code>推介</code>)</p><p>上传数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br></pre></td></tr></table></figure><p>执行添加分区</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 add partition(month='201709',</span><br><span class="line"> day='11');</span><br></pre></td></tr></table></figure><p>查询数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='11';</span><br></pre></td></tr></table></figure></li><li><p>方式三：创建文件夹后load数据到分区</p><p>创建目录</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=10;</span><br></pre></td></tr></table></figure><p>上传数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table</span><br><span class="line"> dept_partition2 partition(month='201709',day='10');</span><br></pre></td></tr></table></figure><p>查询数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='10';</span><br></pre></td></tr></table></figure></li></ol></li></ul><h3 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h3><h4 id="重命名表"><a href="#重命名表" class="headerlink" title="重命名表"></a>重命名表</h4><ol><li><p>语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">RENAME</span> <span class="keyword">TO</span> new_table_name</span><br></pre></td></tr></table></figure></li><li><p>实操案例</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 rename to dept_partition3;</span><br></pre></td></tr></table></figure></li></ol><h4 id="增加、修改和删除表分区"><a href="#增加、修改和删除表分区" class="headerlink" title="增加、修改和删除表分区"></a>增加、修改和删除表分区</h4><p>详见分区表基本操作。</p><h4 id="增加-修改-替换列信息"><a href="#增加-修改-替换列信息" class="headerlink" title="增加/修改/替换列信息"></a>增加/修改/替换列信息</h4><ul><li><p>语法</p><ol><li><p>更新列</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">CHANGE</span> [<span class="keyword">COLUMN</span>] col_old_name col_new_name column_type [<span class="keyword">COMMENT</span> col_comment] [<span class="keyword">FIRST</span>|<span class="keyword">AFTER</span> column_name]</span><br></pre></td></tr></table></figure></li><li><p>增加和替换列</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">ADD</span>|<span class="keyword">REPLACE</span> <span class="keyword">COLUMNS</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)</span><br></pre></td></tr></table></figure></li></ol><p><code>注：</code> ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。</p></li><li><p>实操案例</p><ol><li><p>查询表结构</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure></li><li><p>添加列</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add columns(deptdesc string);</span><br></pre></td></tr></table></figure></li><li><p>查询表结构</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure></li><li><p>更新列</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition change column deptdesc desc int;</span><br></pre></td></tr></table></figure></li><li><p>查询表结构</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure></li><li><p>替换列</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition replace columns(deptno string, dname</span><br><span class="line"> string, loc string);</span><br></pre></td></tr></table></figure></li><li><p>查询表结构</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure></li></ol></li></ul><h3 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; drop table dept_partition;</span><br></pre></td></tr></table></figure><h2 id="DML数据操作"><a href="#DML数据操作" class="headerlink" title="DML数据操作"></a>DML数据操作</h2><h3 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h3><h4 id="向表中装载数据（Load）"><a href="#向表中装载数据（Load）" class="headerlink" title="向表中装载数据（Load）"></a>向表中装载数据（Load）</h4><ul><li><p>语法</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; load data [local] inpath '/opt/module/datas/student.txt' [overwrite] into table student [partition (partcol1=val1,…)];</span><br></pre></td></tr></table></figure><p><code>load data:</code> 表示加载数据</p><p><code>local:</code> 表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</p><p><code>inpath:</code> 表示加载数据的路径</p><p><code>overwrite:</code> 表示覆盖表中已有数据，否则表示追加</p><p><code>into table:</code> 表示加载到哪张表</p><p><code>student:</code> 表示具体的表</p><p><code>partition:</code> 表示上传到指定分区</p></li><li><p>实操案例</p><ol><li><p>创建一张表</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table student(id string, name string) row format delimited fields terminated by '\t';</span><br></pre></td></tr></table></figure></li><li><p>加载本地文件到hive</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/student.txt' into table default.student;</span><br></pre></td></tr></table></figure></li><li><p>加载HDFS文件到hive中</p><p>上传文件到HDFS</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/hadoop/hive;</span><br></pre></td></tr></table></figure><p>加载HDFS上数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data inpath '/user/hadoop/hive/student.txt' into table default.student;</span><br></pre></td></tr></table></figure></li><li><p>加载数据覆盖表中已有的数据</p><p>上传文件到HDFS</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/hadoop/hive;</span><br></pre></td></tr></table></figure><p>加载数据覆盖表中已有的数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data inpath '/user/hadoop/hive/student.txt' overwrite into table default.student;</span><br></pre></td></tr></table></figure></li></ol></li></ul><h4 id="通过查询语句向表中插入数据（Insert）"><a href="#通过查询语句向表中插入数据（Insert）" class="headerlink" title="通过查询语句向表中插入数据（Insert）"></a>通过查询语句向表中插入数据（Insert）</h4><ol><li><p>创建一张分区表</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table student(id int, name string) partitioned by (month string) row format delimited fields terminated by '\t';</span><br></pre></td></tr></table></figure></li><li><p>基本插入数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert into table  student partition(month='201709') values(1,'wangwu'),(2,’zhaoliu’);</span><br></pre></td></tr></table></figure></li><li><p>基本模式插入（根据单张表查询结果）</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite table student partition(month='201708')</span><br><span class="line">             <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br></pre></td></tr></table></figure><p><code>insert into：</code> 以追加数据的方式插入到表或分区，原有数据不会删除</p><p><code>insert overwrite：</code> 会覆盖表或分区中已存在的数据</p><p><code>注意：</code> insert不支持插入部分字段</p></li><li><p>多表（多分区）插入模式（根据多张表查询结果）</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; from student</span><br><span class="line">              <span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201707'</span>)</span><br><span class="line">              <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span></span><br><span class="line">              <span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201706'</span>)</span><br><span class="line">              <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br></pre></td></tr></table></figure></li></ol><h4 id="查询语句中创建表并加载数据（CTAS）"><a href="#查询语句中创建表并加载数据（CTAS）" class="headerlink" title="查询语句中创建表并加载数据（CTAS）"></a>查询语句中创建表并加载数据（CTAS）</h4><p>详见创建表。</p><p>根据查询结果创建表（查询的结果会添加到新创建的表中）</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student3 <span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><h4 id="创建表时通过Location指定加载数据路径"><a href="#创建表时通过Location指定加载数据路径" class="headerlink" title="创建表时通过Location指定加载数据路径"></a>创建表时通过Location指定加载数据路径</h4><ol><li><p>上传数据到hdfs上</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir /student;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /student;</span><br></pre></td></tr></table></figure></li><li><p>创建表，并指定在hdfs上的位置</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create external table if not exists student5(</span><br><span class="line">              id int, name string</span><br><span class="line">              )</span><br><span class="line">              row format delimited fields terminated by '\t'</span><br><span class="line">              location '/student;</span><br></pre></td></tr></table></figure></li><li><p>查询数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from student5;</span><br></pre></td></tr></table></figure></li></ol><h4 id="Import数据到指定Hive表中"><a href="#Import数据到指定Hive表中" class="headerlink" title="Import数据到指定Hive表中"></a>Import数据到指定Hive表中</h4><p>注意：先用export导出后，再将数据导入。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; import table student2 partition(month='201709') from</span><br><span class="line"> '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure><h3 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h3><h4 id="Insert导出"><a href="#Insert导出" class="headerlink" title="Insert导出"></a>Insert导出</h4><ol><li><p>将查询的结果导出到本地</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory '/opt/module/datas/export/student' select * from student;</span><br></pre></td></tr></table></figure></li><li><p>将查询的结果格式化导出到本地</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive(default)&gt;insert overwrite local directory '/opt/module/datas/export/student1' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' select * from student;</span><br></pre></td></tr></table></figure></li><li><p>将查询的结果导出到HDFS上(没有local)</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite directory '/user/hadoop/student2' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' select * from student;</span><br></pre></td></tr></table></figure></li></ol><h4 id="Hadoop命令导出到本地"><a href="#Hadoop命令导出到本地" class="headerlink" title="Hadoop命令导出到本地"></a>Hadoop命令导出到本地</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -get /user/hive/warehouse/student/month=201709/000000_0</span><br><span class="line">/opt/module/datas/export/student3.txt;</span><br></pre></td></tr></table></figure><h4 id="Hive-Shell-命令导出"><a href="#Hive-Shell-命令导出" class="headerlink" title="Hive Shell 命令导出"></a>Hive Shell 命令导出</h4><p>基本语法：（hive -f/-e 执行语句或者脚本 &gt; file）</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop102 hive]$ bin/hive -e 'select * from default.student;' &gt;</span><br><span class="line"> /opt/module/datas/export/student4.txt;</span><br></pre></td></tr></table></figure><h4 id="Export导出到HDFS上"><a href="#Export导出到HDFS上" class="headerlink" title="Export导出到HDFS上"></a>Export导出到HDFS上</h4><figure class="highlight"><table><tr><td class="code"><pre><span class="line">(defahiveult)&gt; export table default.student to '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure><p>export和import主要用于两个Hadoop平台集群之间Hive表迁移。</p><h4 id="Sqoop导出"><a href="#Sqoop导出" class="headerlink" title="Sqoop导出"></a>Sqoop导出</h4><p>后续的博客讲解</p><h3 id="清除表中数据（Truncate）"><a href="#清除表中数据（Truncate）" class="headerlink" title="清除表中数据（Truncate）"></a>清除表中数据（Truncate）</h3><p>注意：Truncate只能删除管理表，不能删除外部表中数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; truncate table student;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的部署和初始化工作&amp;验证Hive部署成功</title>
      <link href="/2018/11/03/hive/3/"/>
      <url>/2018/11/03/hive/3/</url>
      
        <content type="html"><![CDATA[<h3 id="Hive安装部署"><a href="#Hive安装部署" class="headerlink" title="Hive安装部署"></a>Hive安装部署</h3><h4 id="Hive安装及配置"><a href="#Hive安装及配置" class="headerlink" title="Hive安装及配置"></a>Hive安装及配置</h4><ol><li><p>把apache-hive-1.2.1-bin.tar.gz上传到linux的/opt/software目录下</p></li><li><p>解压apache-hive-1.2.1-bin.tar.gz到/opt/module/目录下面</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun software]$ tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li><li><p>修改apache-hive-1.2.1-bin.tar.gz的名称为hive</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun module]$ mv apache-hive-1.2.1-bin/ hive</span><br></pre></td></tr></table></figure></li><li><p>修改/opt/module/hive/conf目录下的hive-env.sh.template名称为hive-env.sh</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun conf]$ mv hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure></li><li><p>配置hive-env.sh文件</p><ol><li><p>配置HADOOP_HOME路径</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure></li><li><p>配置HIVE_CONF_DIR路径</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HIVE_CONF_DIR=/opt/module/hive/conf</span><br></pre></td></tr></table></figure></li></ol></li></ol><h4 id="Hadoop集群配置"><a href="#Hadoop集群配置" class="headerlink" title="Hadoop集群配置"></a>Hadoop集群配置</h4><ol><li><p>必须启动hdfs和yarn</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[hadoop@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></li><li><p>在HDFS上创建/tmp和/user/hive/warehouse两个目录并修改他们的同组权限可写</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop-2.7.2]$ bin/hadoop fs -mkdir /tmp</span><br><span class="line">[hadoop@aliyun hadoop-2.7.2]$ bin/hadoop fs -mkdir -p /user/hive/warehouse</span><br><span class="line">[hadoop@aliyun hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /tmp</span><br></pre></td></tr></table></figure></li></ol><h4 id="Hive基本操作"><a href="#Hive基本操作" class="headerlink" title="Hive基本操作"></a>Hive基本操作</h4><ol><li><p>启动hive</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive</span><br></pre></td></tr></table></figure></li><li><p>查看数据库</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure></li><li><p>打开默认数据库</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; use default;</span><br></pre></td></tr></table></figure></li><li><p>显示default数据库中的表</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure></li><li><p>创建一张表</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; create table student(id int, name string);</span><br></pre></td></tr></table></figure></li><li><p>显示数据库中有几张表</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure></li><li><p>查看表的结构</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc student;</span><br></pre></td></tr></table></figure></li><li><p>向表中插入数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; insert into student values(1000,"ss");</span><br></pre></td></tr></table></figure></li><li><p>查询表中数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from student;</span><br></pre></td></tr></table></figure></li><li><p>退出hive</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; quit;</span><br></pre></td></tr></table></figure></li></ol><ul><li>说明：<br><code>数据库：</code>在hdfs中表现为${hive.metastore.warehouse.dir}目录下一个文件夹<br><code>表：</code>在hdfs中表现所属db目录下一个文件夹，文件夹中存放该表中的具体数据</li></ul><h3 id="MySql安装"><a href="#MySql安装" class="headerlink" title="MySql安装"></a>MySql安装</h3><ul><li>安装过程请看我的另外一个博客: <a href="https://yerias.github.io/2018/10/01/DataWarehouse/mysql/1/">https://yerias.github.io/2018/10/01/DataWarehouse/mysql/1/</a></li></ul><h3 id="Hive元数据配置到MySql"><a href="#Hive元数据配置到MySql" class="headerlink" title="Hive元数据配置到MySql"></a>Hive元数据配置到MySql</h3><h4 id="驱动拷贝"><a href="#驱动拷贝" class="headerlink" title="驱动拷贝"></a>驱动拷贝</h4><ol><li><p>在/opt/software/mysql-libs目录下解压mysql-connector-java-5.1.27.tar.gz驱动包</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun mysql-libs]# tar -zxvf mysql-connector-java-5.1.27.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>拷贝/opt/software/mysql-libs/mysql-connector-java-5.1.27目录下的mysql-connector-java-5.1.27-bin.jar到/opt/module/hive/lib/</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun mysql-connector-java-5.1.27]# cp mysql-connector-java-5.1.27-bin.jar</span><br><span class="line">/opt/module/hive/lib/</span><br></pre></td></tr></table></figure></li></ol><h4 id="配置Metastore到MySql"><a href="#配置Metastore到MySql" class="headerlink" title="配置Metastore到MySql"></a>配置Metastore到MySql</h4><ol><li><p>在/opt/module/hive/conf目录下创建一个hive-site.xml</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun conf]$ touch hive-site.xml</span><br><span class="line">[hadoop@aliyun conf]$ vi hive-site.xml</span><br></pre></td></tr></table></figure></li><li><p>根据官方文档配置参数，拷贝数据到hive-site.xml文件中<br><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;?xml version="1.0"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">&lt;value&gt;jdbc:mysql://aliyun:3306/metastore?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">&lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;root&lt;/value&gt;</span><br><span class="line">&lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">&lt;value&gt;000000&lt;/value&gt;</span><br><span class="line">&lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li></ol><h4 id="多窗口启动Hive测试"><a href="#多窗口启动Hive测试" class="headerlink" title="多窗口启动Hive测试"></a>多窗口启动Hive测试</h4><ol><li><p>先启动MySQL</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun mysql-libs]$ mysql -uroot -p000000</span><br></pre></td></tr></table></figure><p>查看有几个数据库</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; show databases;</span><br><span class="line">   +<span class="comment">--------------------+</span></span><br><span class="line">   | Database |</span><br><span class="line">   +<span class="comment">--------------------+</span></span><br><span class="line">   | information_schema |</span><br><span class="line">   | mysql |</span><br><span class="line">   | performance_schema |</span><br><span class="line">   | test |</span><br><span class="line">   +<span class="comment">--------------------+</span></span><br></pre></td></tr></table></figure></li><li><p>再次打开多个窗口，分别启动hive</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive</span><br></pre></td></tr></table></figure></li><li><p>启动hive后，回到MySQL窗口查看数据库，显示增加了metastore数据库</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; show databases;</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line">| Database |</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line">| information_schema |</span><br><span class="line">| metastore |</span><br><span class="line">| mysql |</span><br><span class="line">| performance_schema |</span><br><span class="line">| test |</span><br><span class="line">+<span class="comment">--------------------+</span></span><br></pre></td></tr></table></figure></li></ol><h4 id="HiveJDBC访问"><a href="#HiveJDBC访问" class="headerlink" title="HiveJDBC访问"></a>HiveJDBC访问</h4><ol><li><p>启动hiveserver2服务</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hiveserver2</span><br></pre></td></tr></table></figure></li><li><p>启动beeline</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/beeline</span><br><span class="line">Beeline version 1.2.1 by Apache Hive</span><br><span class="line"><span class="meta">beeline&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>连接hiveserver2</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">beeline&gt;</span><span class="bash"> !connect jdbc:hive2://aliyun:10000（回车）</span></span><br><span class="line">Connecting to jdbc:hive2://aliyun:10000</span><br><span class="line">Enter username for jdbc:hive2://aliyun:10000: hadoop（回车）</span><br><span class="line">Enter password for jdbc:hive2://aliyun:10000: （直接回车）</span><br><span class="line">Connected to: Apache Hive (version 1.2.1)</span><br><span class="line">Driver: Hive JDBC (version 1.2.1)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">0: jdbc:hive2://aliyun:10000&gt; show databases;</span><br><span class="line">+----------------+--+</span><br><span class="line">| database_name |</span><br><span class="line">+----------------+--+</span><br><span class="line">| default |</span><br><span class="line">| hive_db2 |</span><br><span class="line">+----------------+--+</span><br></pre></td></tr></table></figure></li></ol><h4 id="Hive常用交互命令"><a href="#Hive常用交互命令" class="headerlink" title="Hive常用交互命令"></a>Hive常用交互命令</h4><ol><li><p>Hive命令帮助</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive -help</span><br><span class="line">usage: hive</span><br><span class="line">-d,--define &lt;key=value&gt; Variable subsitution to apply to hive</span><br><span class="line">commands. e.g. -d A=B or --define A=B</span><br><span class="line">--database &lt;databasename&gt; Specify the database to use</span><br><span class="line">-e &lt;quoted-query-string&gt; SQL from command line</span><br><span class="line">-f &lt;filename&gt; SQL from files</span><br><span class="line">-H,--help Print help information</span><br><span class="line">--hiveconf &lt;property=value&gt; Use value for given property</span><br><span class="line">--hivevar &lt;key=value&gt; Variable subsitution to apply to hive</span><br><span class="line">commands. e.g. --hivevar A=B</span><br><span class="line">-i &lt;filename&gt; Initialization SQL file</span><br><span class="line">-S,--silent Silent mode in interactive shell</span><br><span class="line">-v,--verbose Verbose mode (echo executed SQL to the console)</span><br></pre></td></tr></table></figure></li><li><p><code>&quot;-e&quot;</code>不进入hive的交互窗口执行sql语句</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive -e "select id from student;"</span><br></pre></td></tr></table></figure></li><li><p><code>&quot;-f&quot;</code>执行脚本中sql语句</p><ol><li><p>在/opt/module/datas目录下创建hivef.sql文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun datas]$ touch hivef.sql</span><br></pre></td></tr></table></figure><p>文件中写入正确的sql语句</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">select * from student;</span><br></pre></td></tr></table></figure></li><li><p>执行文件中的sql语句</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive -f /opt/module/datas/hivef.sql</span><br></pre></td></tr></table></figure></li><li><p>执行文件中的sql语句并将结果写入文件中</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive -f /opt/module/datas/hivef.sql &gt; /opt/module/datas/hive_result.txt</span><br></pre></td></tr></table></figure></li></ol></li></ol><h4 id="Hive其他命令操作"><a href="#Hive其他命令操作" class="headerlink" title="Hive其他命令操作"></a>Hive其他命令操作</h4><ol><li><p>退出hive窗口：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash"><span class="built_in">exit</span>;</span></span><br><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">quit;</span></span><br></pre></td></tr></table></figure><p>在新版的hive中没区别了，在以前的版本是有的：<br>exit:先隐性提交数据，再退出；<br>quit:不提交数据，退出；</p></li><li><p>在hive cli命令窗口中如何查看hdfs文件系统</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">dfs -ls /;</span></span><br></pre></td></tr></table></figure></li><li><p>在hive cli命令窗口中如何查看本地文件系统</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">! ls /opt/module/datas;</span></span><br></pre></td></tr></table></figure></li><li><p>查看在hive中输入的所有历史命令</p><ol><li><p>进入到当前用户的根目录/root或/home/hadoop</p></li><li><p>查看. hivehistory文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ cat .hivehistory</span><br></pre></td></tr></table></figure></li></ol></li></ol><h3 id="Hive常见属性配置"><a href="#Hive常见属性配置" class="headerlink" title="Hive常见属性配置"></a>Hive常见属性配置</h3><h4 id="Hive数据仓库位置配置"><a href="#Hive数据仓库位置配置" class="headerlink" title="Hive数据仓库位置配置"></a>Hive数据仓库位置配置</h4><ol><li><p>Default数据仓库的最原始位置是在hdfs上的：/user/hive/warehouse路径下。</p></li><li><p>在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹。</p></li><li><p>修改default数据仓库原始位置（将hive-default.xml.template如下配置信息拷贝到hive-site.xml文件中）。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">&lt;description&gt;location of default database for the warehouse&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>配置同组用户有执行权限</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/hdfs dfs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure></li></ol><h4 id="查询后信息显示配置"><a href="#查询后信息显示配置" class="headerlink" title="查询后信息显示配置"></a>查询后信息显示配置</h4><ol><li><p>在hive-site.xml文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.cli.print.header&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.cli.print.current.db&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li><li><p>重新启动hive，对比配置前后差异。</p></li></ol><h4 id="Hive运行日志信息配置"><a href="#Hive运行日志信息配置" class="headerlink" title="Hive运行日志信息配置"></a>Hive运行日志信息配置</h4><ol><li><p>Hive的log默认存放在/tmp/hadoop/hive.log目录下（当前用户名下）</p></li><li><p>修改hive的log存放日志到/opt/module/hive/logs</p><ol><li><p>修改/opt/module/hive/conf/hive-log4j.properties.template文件名称为hive-log4j.properties</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun conf]$ pwd</span><br><span class="line">/opt/module/hive/conf</span><br><span class="line">[hadoop@aliyun conf]$ mv hive-log4j.properties.template hive-log4j.properties</span><br></pre></td></tr></table></figure></li></ol></li><li><p>在hive-log4j.properties文件中修改log存放位置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive.log.dir=/opt/module/hive/logs</span><br></pre></td></tr></table></figure></li></ol><h4 id="参数配置方式"><a href="#参数配置方式" class="headerlink" title="参数配置方式"></a>参数配置方式</h4><ol><li><p>查看当前所有的配置信息</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"><span class="built_in">set</span>;</span></span><br></pre></td></tr></table></figure></li><li><p>参数的配置三种方式</p><ol><li><p>配置文件方式<br>默认配置文件：hive-default.xml<br>用户自定义配置文件：hive-site.xml</p><p><code>注意：</code>用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。</p></li><li><p>命令行参数方式<br>启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。<br>例如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10;</span><br></pre></td></tr></table></figure><p>注意：仅对本次hive启动有效<br>查看参数设置：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure></li><li><p>参数声明方式<br>可以在HQL中使用SET关键字设定参数<br>例如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks=100;</span><br></pre></td></tr></table></figure><p>注意：仅对本次hive启动有效。<br>查看参数设置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure></li></ol><p>上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷粒影音8道SQL题(各种Top N)</title>
      <link href="/2018/11/02/hive/2/"/>
      <url>/2018/11/02/hive/2/</url>
      
        <content type="html"><![CDATA[<h4 id="data表字段"><a href="#data表字段" class="headerlink" title="data表字段"></a>data表字段</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">videoId string <span class="keyword">comment</span> <span class="string">"视频唯一id"</span>, </span><br><span class="line">uploader <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">"视频上传者"</span>,</span><br><span class="line">age <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">"视频年龄"</span>,</span><br><span class="line"><span class="keyword">category</span> <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;<span class="keyword">comment</span> <span class="string">"视频类别"</span>,</span><br><span class="line"><span class="keyword">length</span> <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">"视频长度"</span>,</span><br><span class="line">views <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">"观看次数"</span>,</span><br><span class="line">rate <span class="built_in">float</span> <span class="keyword">comment</span> <span class="string">"视频评分"</span>,</span><br><span class="line">ratings <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">"流量"</span>,</span><br><span class="line">comments <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">"评论数"</span>,</span><br><span class="line">relatedId <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;<span class="keyword">comment</span> <span class="string">"相关视频id"</span></span><br></pre></td></tr></table></figure><h4 id="user表字段"><a href="#user表字段" class="headerlink" title="user表字段"></a>user表字段</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">uploader String<span class="keyword">comment</span> <span class="string">"上传者用户名"</span>,</span><br><span class="line">videos <span class="built_in">int</span><span class="keyword">comment</span> <span class="string">"上传视频数"</span>,</span><br><span class="line">friends <span class="built_in">int</span><span class="keyword">comment</span> <span class="string">"朋友数量"</span>,</span><br></pre></td></tr></table></figure><h4 id="8道题目-思路"><a href="#8道题目-思路" class="headerlink" title="8道题目(思路)"></a>8道题目(思路)</h4><p>统计硅谷影音视频网站的常规指标，各种TopN指标：</p><ol><li>统计视频观看数Top10</li><li>统计视频类别热度Top10</li><li>统计视频观看数Top20所属类别</li><li>统计视频观看数Top50所关联视频的所属类别Rank</li><li>统计每个类别中的视频热度Top10</li><li>统计每个类别中视频流量Top10</li><li>统计上传视频最多的用户Top10以及他们上传的视频</li><li>统计每个类别视频观看数Top10</li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive中的字符集编码若干问题</title>
      <link href="/2018/11/01/hive/1/"/>
      <url>/2018/11/01/hive/1/</url>
      
        <content type="html"><![CDATA[<h3 id="个人初始开发环境的基本情况以及Hive元数据库说明"><a href="#个人初始开发环境的基本情况以及Hive元数据库说明" class="headerlink" title="个人初始开发环境的基本情况以及Hive元数据库说明"></a>个人初始开发环境的基本情况以及Hive元数据库说明</h3><ol><li><p>hive的元数据库改成了mysql(安装完mysql之后也没有进行其它别的设置)</p></li><li><p>hive-site.xml中设置元数据库对应的配置为  <code>jdbc:mysql://ip:3306/metastore?createDatabaseIfNotExist=true</code></p></li><li><p>普通情况下咱们的mysql默认编码是latin1,但是我们在日常开发中大多数情况下需要用到utf-8编码,如果是默认latin1的话,咱们的中文存储进去容易乱码,所以说大家在遇到一些数据乱码的情况话,最好把mysql的编码改成utf-8.</p></li></ol><p><code>注意:</code> 但是在这里要非常严重强调的一点:hive的元数据metastore在mysql的数据库,不管是数据库本身,还是里面的表编码都必须是latin1(CHARACTER SET latin1 COLLATE latin1_bin)!!!!!</p><h4 id="验证方式"><a href="#验证方式" class="headerlink" title="验证方式:"></a>验证方式:</h4><p>可以通过客户端软件在数据库上右键属性查看,也可以通过命令查看</p><p>mysql&gt; show create database hive_cz3q;</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">-----------+-----------------------------------------------------------------------------------------+</span></span><br><span class="line">| Database  | <span class="keyword">Create</span> <span class="keyword">Database</span>                                                                         |</span><br><span class="line">+<span class="comment">-----------+-----------------------------------------------------------------------------------------+</span></span><br><span class="line">| hive_cz3q | <span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> <span class="string">`hive_cz3q`</span> <span class="comment">/*!40100 DEFAULT CHARACTER SET latin1 COLLATE latin1_bin */</span> |</span><br><span class="line">+<span class="comment">-----------+-----------------------------------------------------------------------------------------+</span></span><br></pre></td></tr></table></figure><p>不然会有类似如下的错误:</p><p> <img src="https://yerias.github.io/hive_img/610238-20170903131046858-396716990.png" alt="图片1"></p><p>那么怎么修改mysql的编码为utf8呢?这里提供了在线安装修改和离线方式安装下的修改方式供大家选择!</p><h3 id="乱码的情况"><a href="#乱码的情况" class="headerlink" title="乱码的情况:"></a>乱码的情况:</h3><p> 向hive的表中 创建表,表语句部分如下:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ods.ods_order</span><br><span class="line">(</span><br><span class="line">   ORDER_ID             <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'订单ID'</span>,</span><br><span class="line">   ORDER_NO             <span class="built_in">varchar</span>(<span class="number">30</span>)  <span class="keyword">comment</span> <span class="string">'订单编号（唯一字段），</span></span><br><span class="line"><span class="string">   DEALER_ID            int comment '</span>门店<span class="keyword">ID</span><span class="string">',</span></span><br><span class="line"><span class="string">   CUST_ID              int comment '</span>客户<span class="keyword">ID</span><span class="string">',</span></span><br></pre></td></tr></table></figure><p>在创建表的时候，字段可以有 comment，但是 comment 建议不要用中文说明，因为我们说过，hive 的 metastore 支持的字符集是 latin1，所以中文写入的时候会有编码问题，如下图！ </p><p>然后通过desc ods_order 查看 对应的comment中是中文的地方,通过Xshell显示全部都是 “?” 问号.  同时确认了Xshell支持显示中文(排除Xshell的问题).</p><p><code>以上就是说Hive在字段定义时的Comment中文乱码问题.</code></p><p><img src="https://yerias.github.io/hive_img/610238-20170903134309687-1604377616.png" alt="图片2"></p><p>有了上述的问题，那么我们该如何去解决注释中文乱码问题呢？ </p><h3 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h3><h4 id="首先进行Mysql的编码设置"><a href="#首先进行Mysql的编码设置" class="headerlink" title="首先进行Mysql的编码设置"></a>首先进行Mysql的编码设置</h4><h5 id="离线安装mysql的修改方式"><a href="#离线安装mysql的修改方式" class="headerlink" title="离线安装mysql的修改方式"></a>离线安装mysql的修改方式</h5><ol><li><p>修改编码,设置为utf8</p><p>拷贝 mysql 的配置文件/usr/share/mysql/my-small.cnf 到/etc/my.cnf </p><p>在mysql 配置文件/etc/my.cnf 中增加以下内容</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[client]下面增加</span><br><span class="line">default-character-set=utf8</span><br><span class="line">在[mysqld]下面增加</span><br><span class="line">default-character-set=utf8</span><br><span class="line">init_connect='SET NAMES utf8'</span><br></pre></td></tr></table></figure><p><em>2020/03/09更新</em>：<code>default-character-set=utf8</code>，如果这样改会导致5.7版本mysql无法打开所以要改为 <code>character-set-server=utf8</code>，(可选)改完后，要重新创建表才能使用。</p></li><li><p>重启mysql 服务(这样确保缺省编码是utf8)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">service mysqld restart</span><br></pre></td></tr></table></figure></li><li><p>验证编码是否改成了utf8:</p><p>输入命令 “\s”</p><p><img src="https://yerias.github.io/hive_img/610238-20170903132119671-1333763157.png" alt="图片3"></p><p>输入命令:show variables like ‘char%’</p><p><img src="https://yerias.github.io/hive_img/610238-20170903132210124-717565896.png" alt="图片4"></p><p>输入命令:show variables like “colla%”;</p><p> <img src="https://yerias.github.io/hive_img/610238-20170903132350968-1341436179.png" alt="图片5"></p><p>OK修改成功!</p></li><li><p>这样在启动hive,向hive中插入的表中comment等有汉字的情况,就可以正常的显示(如下为本人测试的部分显示结果):</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://localhost:10000/ods&gt; desc ods_order;</span><br><span class="line">+<span class="comment">--------------------------+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--+</span></span><br><span class="line">|         col_name         |       data_type       |                                                                   <span class="keyword">comment</span>                                                                   |</span><br><span class="line">+<span class="comment">--------------------------+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--+</span></span><br><span class="line">| order_id                 | <span class="built_in">int</span>                   | 订单<span class="keyword">ID</span>                                                                                                                                        |</span><br><span class="line">| order_no                 | <span class="built_in">varchar</span>(<span class="number">30</span>)           | 订单编号（唯一字段），前缀字符表示订单来源：a，Andriod；b，微博；c，WEB；e，饿了么；i，Iphone；m，Mobile；x，微信； z，中粮我买网；l，其它。 接着<span class="number">3</span>位数字代表订单城市编号；接着字符z与后面的真正订单编号分隔。这套机制从<span class="number">2014</span>年<span class="number">12</span>月开始实施。</span><br></pre></td></tr></table></figure></li></ol><h5 id="在线安装mysql的修改方式"><a href="#在线安装mysql的修改方式" class="headerlink" title="在线安装mysql的修改方式"></a>在线安装mysql的修改方式</h5><ol><li><p>修改编码,设置为utf-8</p><p> 在 mysql 配置文件/etc/my.cnf（不需要拷贝）中[mysqld]的下面增加以下内容</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">init_connect='SET collation_connection = utf8_unicode_ci'</span><br><span class="line">init_connect='SET NAMES utf8'</span><br><span class="line">character-set-server=utf8</span><br><span class="line">collation-server=utf8_unicode_ci</span><br><span class="line">skip-character-set-client-handshake</span><br></pre></td></tr></table></figure><p> <img src="https://yerias.github.io/hive_img/610238-20170903133604796-492434925.png" alt="图片6"></p></li><li><p>重启mysqld服务</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">service mysql restart</span><br></pre></td></tr></table></figure></li><li><p>和离线方式一样验证编码是否确实修改;</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">show variables like 'char%';</span><br></pre></td></tr></table></figure><p> <img src="https://yerias.github.io/hive_img/610238-20170903133820296-974086333.png" alt="图片7"></p></li></ol><h4 id="针对元数据库metastore中的表-分区-视图的编码设置"><a href="#针对元数据库metastore中的表-分区-视图的编码设置" class="headerlink" title="针对元数据库metastore中的表,分区,视图的编码设置"></a>针对元数据库metastore中的表,分区,视图的编码设置</h4><p>因为我们知道 metastore 支持数据库级别，表级别的字符集是 latin1，那么我们<code>只需要把相应注释的地方的字符集由 latin1 改成 utf-8</code>，就可以了。用到注释的就三个地方，表、分区、视图。如下修改分为两个步骤：</p><ol><li><p>进入数据库 Metastore 中执行以下 5 条 SQL 语句 </p><ol><li>修改表字段注解和表注解<br>alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8<br>alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8</li><li>修改分区字段注解：<br>alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8<br>alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8</li><li>修改索引注解：<br>alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</li></ol></li><li><p>修改 metastore 的连接 URL</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://IP:3306/db_name?createDatabaseIfNotExist=true&amp;amp;useUnicode=true&amp;amp;characterEncoding=UTF-8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><p><strong>测试结果：</strong></p><p><img src="https://yerias.github.io/hive_img/610238-20170903134604093-93033588.png" alt="图片8"></p><p>以上就能完美的解决这个问题.</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>有时候在使用xml作为配置文件的时候，应该要使用xml的编码规则来进行适当的设置。如<code>&amp;</code>在xml文件中应该写为<code>&amp;amp;</code></p><p>下面给出xml中一些特殊符号的编码转换：</p><table><thead><tr><th>代码</th><th>符号</th><th>描述</th></tr></thead><tbody><tr><td><code>&amp;lt;</code></td><td>&lt;</td><td>小于号</td></tr><tr><td><code>&amp;gt;</code></td><td>&gt;</td><td>大于号</td></tr><tr><td><code>&amp;amp;</code></td><td>&amp;</td><td>and字符</td></tr><tr><td><code>&amp;apos;</code></td><td>‘</td><td>单引号</td></tr><tr><td><code>&amp;quot;</code></td><td>“</td><td>双引号</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MR调优之压缩</title>
      <link href="/2018/10/16/hadoop/12/"/>
      <url>/2018/10/16/hadoop/12/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>什么是压缩</li><li>压缩的好处与坏处</li><li>常见的压缩格式</li><li>优缺点比较</li><li>如何选择压缩格式</li><li>MR配置文件压缩格式</li><li>Hive配置文件压缩格式</li></ol><h2 id="什么是压缩"><a href="#什么是压缩" class="headerlink" title="什么是压缩"></a>什么是压缩</h2><p>压缩就是通过某种技术（算法）把原始文件变小，相应的解压就是把压缩后的文件变成原始文件。嘿嘿是不是又可以变大又可以变小。</p><h2 id="压缩的好处与坏处"><a href="#压缩的好处与坏处" class="headerlink" title="压缩的好处与坏处"></a>压缩的好处与坏处</h2><p><strong>好处</strong></p><ul><li>减少存储磁盘空间</li><li>降低IO(网络的IO和磁盘的IO)</li><li>加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度</li></ul><p><strong>坏处</strong></p><ul><li>由于使用数据时，需要先将数据解压，加重<strong>CPU</strong>负荷</li></ul><h2 id="常见的压缩格式"><a href="#常见的压缩格式" class="headerlink" title="常见的压缩格式"></a>常见的压缩格式</h2><table><thead><tr><th align="left">格式</th><th align="left">可分割</th><th align="left">平均压缩速度</th><th align="left">文本文件压缩效率</th><th align="left">Hadoop压缩编解码器</th><th align="center">纯Java实现</th><th align="left">原生</th><th align="left">备注</th></tr></thead><tbody><tr><td align="left">gzip</td><td align="left">否</td><td align="left">快</td><td align="left">高</td><td align="left">org.apache.hadoop.io.compress.GzipCodec</td><td align="center">是</td><td align="left">是</td><td align="left"></td></tr><tr><td align="left">lzo</td><td align="left">是（取决于所使用的库）</td><td align="left">非常快</td><td align="left">中等</td><td align="left">com.hadoop.compression.lzo.LzoCodec</td><td align="center">是</td><td align="left">是</td><td align="left">需要在每个节点上安装LZO</td></tr><tr><td align="left">bzip2</td><td align="left">是</td><td align="left">慢</td><td align="left">非常高</td><td align="left">org.apache.hadoop.io.compress.Bzip2Codec</td><td align="center">是</td><td align="left">是</td><td align="left">为可分割版本使用纯Java</td></tr><tr><td align="left">zlib</td><td align="left">否</td><td align="left">慢</td><td align="left">中等</td><td align="left">org.apache.hadoop.io.compress.DefaultCodec</td><td align="center">是</td><td align="left">是</td><td align="left">Hadoop 的默认压缩编解码器</td></tr><tr><td align="left">Snappy</td><td align="left">否</td><td align="left">非常快</td><td align="left">低</td><td align="left">org.apache.hadoop.io.compress.SnappyCodec</td><td align="center">否</td><td align="left">是</td><td align="left">Snappy 有纯Java的移植版，但是在Spark/Hadoop中不能用</td></tr></tbody></table><p>一个简单的案例对于集中压缩方式之间的压缩大小和压缩时间进行一个感观性的认识</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">测试环境:</span><br><span class="line">8 core i7 cpu </span><br><span class="line">8GB memory</span><br><span class="line">64 bit CentOS</span><br><span class="line">1.4GB Wikipedia Corpus 2-gram text input</span><br></pre></td></tr></table></figure><p>压缩比</p><p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E6%AF%94.png" alt="压缩比"></p><p>压缩时间比</p><p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E6%97%B6%E9%97%B4.png" alt="压缩时间"></p><p>可以看出，压缩比越高，压缩时间越长</p><h2 id="优缺点比较"><a href="#优缺点比较" class="headerlink" title="优缺点比较"></a>优缺点比较</h2><table><thead><tr><th align="left">压缩格式</th><th align="left">优点</th><th align="left">缺点</th><th></th></tr></thead><tbody><tr><td align="left"><strong>gzip</strong></td><td align="left">压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便</td><td align="left"><strong>不支持split</strong></td><td></td></tr><tr><td align="left"><strong>lzo</strong></td><td align="left">压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便</td><td align="left">压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）</td><td></td></tr><tr><td align="left"><strong>snappy</strong></td><td align="left">压缩速度快；支持hadoop native库</td><td align="left"><strong>不支持split</strong>；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令</td><td></td></tr><tr><td align="left"><strong>bzip2</strong></td><td align="left">支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便</td><td align="left">压缩/解压速度慢；不支持native</td><td></td></tr></tbody></table><h2 id="如何选择压缩格式"><a href="#如何选择压缩格式" class="headerlink" title="如何选择压缩格式"></a>如何选择压缩格式</h2><p>从两方面考虑：Storage + Compute；</p><ol><li>Storage ：基于HDFS考虑，减少了存储文件所占空间，提升了数据传输速率；如gzip、bzip2。</li><li>Compute：基于YARN上的计算(MapReduce/Hive/Spark/….)速度的提升；如lzo、lz4、snappy。</li></ol><p>不同的场景选择不同的压缩方式，肯定没有一个一劳永逸的方法，如果选择高压缩比，那么对于cpu的性能要求要高，同时压缩、解压时间耗费也多；选择压缩比低的，对于磁盘io、网络io的时间要多，空间占据要多；<strong>对于支持分割的，可以实现并行处理</strong>。</p><ol><li>IO密集型：使用压缩</li><li>运算密集型：慎用压缩</li></ol><h2 id="压缩在MapReduce中的应用场景"><a href="#压缩在MapReduce中的应用场景" class="headerlink" title="压缩在MapReduce中的应用场景"></a>压缩在MapReduce中的应用场景</h2><p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E5%9C%A8MR%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF.png" alt="压缩在MR的应用场景"></p><p>压缩在hadoop中的应用场景总结在三方面：<strong>输入</strong>，<strong>中间</strong>，<strong>输出</strong>。</p><p>整体思路：hdfs ==&gt; map ==&gt; shuffle ==&gt; reduce</p><ol><li>Use Compressd Map Input: 从HDFS中读取文件进行Mapreuce作业，如果数据很大，可以使用压缩并且<strong>选择支持分片的压缩方式（Bzip2,LZO）</strong>，可以实现并行处理，提高效率，减少磁盘读取时间，同时选择合适的存储格式例如Sequence Files，RC,ORC等；</li><li>Compress Intermediate Data: Map输出作为Reducer的输入，需要经过shuffle这一过程，需要把数据读取到一个环形缓冲区，然后读取到本地磁盘，所以选择压缩可以减少了存储文件所占空间，提升了数据传输速率，建议<strong>使用压缩速度快的压缩方式，例如Snappy和LZO.</strong></li><li>Compress Reducer Output: 进行归档处理或者链式Mapreduce的工作（该作业的输出作为下个作业的输入），压缩可以减少了存储文件所占空间，提升了数据传输速率，如果作为归档处理，可以<strong>采用高的压缩比（Gzip,Bzip2）</strong>，如果作为下个作业的输入，考虑<strong>是否要分片</strong>进行选择。</li></ol><h2 id="MR配置文件压缩格式"><a href="#MR配置文件压缩格式" class="headerlink" title="MR配置文件压缩格式"></a>MR配置文件压缩格式</h2><p>hadoop自带不支持split的gzip和支持split的bzip2，我们还手动安装了lzo的压缩方式</p><ol><li><p>修改<code>core-site.xml</code>文件</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">    org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">    org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">    org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">    com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">    com.hadoop.compression.lzo.LzopCodec,</span><br><span class="line">    org.apache.hadoop.io.compress.Lz4Codec,</span><br><span class="line">    org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">    <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>修改<code>mapred-site.xml</code>文件</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">#开启支持压缩</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.output.fileoutputformat.compress<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">#压缩方式/最终输出的压缩方式</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.output.fileoutputformat.compress.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.compress.BZip2Codec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line">#中间压缩(可选，Snappy需要手动安装)</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.map.output.compression.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.compress.SnappyCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>中间压缩</strong>：中间压缩就是处理作业map任务和reduce任务之间的数据，对于中间压缩，最好选择一个节省CPU耗时的压缩方式（快）</p><p><strong>最终压缩</strong>：可以选择高压缩比，减少了存储文件所占空间，提升了数据传输速率<br><code>mapred-site.xml</code> 中设置</p></li><li><p>验证，跑个wc看最终输出文件的后缀</p></li></ol><p><strong>更换压缩方式只需要修改中间输出或者最终输出的压缩类即可</strong></p><h2 id="Hive配置文件压缩格式"><a href="#Hive配置文件压缩格式" class="headerlink" title="Hive配置文件压缩格式"></a>Hive配置文件压缩格式</h2><ol><li><p>配置压缩功能</p><p>hive配置文件压缩格式只需要配置两个参数</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">//开启压缩功能</span><br><span class="line">SET hive.exec.compress.output=true; </span><br><span class="line">//设置最终以bz2格式存储</span><br><span class="line">SET mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Code;</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：不建议再配置文件中设置</p></li><li><p>使用压缩</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">#创建表：</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载数据：</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/page_views.dat"</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> page_views;  </span><br><span class="line"></span><br><span class="line"><span class="comment">#配置压缩格式</span></span><br><span class="line">hive：</span><br><span class="line"><span class="keyword">SET</span> hive.exec.compress.output=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">SET</span> mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建压缩表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_bzip2 <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span> <span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> MapReduce </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HADOOP安装LZO压缩</title>
      <link href="/2018/10/15/hadoop/13/"/>
      <url>/2018/10/15/hadoop/13/</url>
      
        <content type="html"><![CDATA[<h3 id="编译安装lzo与lzop"><a href="#编译安装lzo与lzop" class="headerlink" title="编译安装lzo与lzop"></a>编译安装lzo与lzop</h3><p> <strong>在集群的每一台主机上都需要编译安装！！！</strong></p><ol><li><p>下载编译安装lzo文件，<a href="http://www.oberhumer.com/opensource/lzo/download" target="_blank" rel="noopener"><strong>版本可以下载最新的</strong></a> </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget http://www.oberhumer.com/opensource/lzo/download/lzo-2.10.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>编译安装(保证主机上有gcc与g++)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xvzf   lzo-2.10.tar.gz</span><br><span class="line">cd lzo-2.10</span><br><span class="line">./configure --enable-shared</span><br><span class="line">make -j 10</span><br><span class="line">make install</span><br><span class="line">cp /usr/local/lib/*lzo* /usr/lib</span><br></pre></td></tr></table></figure><p><strong>安装完成后需要将 cp部分文件到/usr/lib中，这个步骤不做会抛</strong>   lzop: error while loading shared libraries: liblzo2.so.2: cannot open shared object file: No such file or directory</p></li><li><p>下载编译lzop，<a href="http://www.lzop.org/download/" target="_blank" rel="noopener"><strong>最新版选择</strong></a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget http://www.lzop.org/download/lzop-1.04.tar.gz</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xvzf lzop-1.04.tar.gz </span><br><span class="line">cd lzop-1.04 </span><br><span class="line">./configure </span><br><span class="line">make -j 10 </span><br><span class="line">make install</span><br></pre></td></tr></table></figure></li></ol><h3 id="安装、编译hadoop-lzo-master"><a href="#安装、编译hadoop-lzo-master" class="headerlink" title="安装、编译hadoop-lzo-master"></a>安装、编译hadoop-lzo-master</h3><p>需在linux环境中安装,在windows上编译不过</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget https://github.com/twitter/hadoop-lzo/archive/master.zip</span><br></pre></td></tr></table></figure><ol><li><p>解压</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">unzip master.zip </span><br><span class="line">cd hadoop-lzo-master/</span><br></pre></td></tr></table></figure></li><li><p>编辑<em>pom.xm</em>l修改hadoop的版本号与你集群中hadoop版本一致   </p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">hadoop.current.version</span>&gt;</span>2.6.0-cdh5.16.2<span class="tag">&lt;/<span class="name">hadoop.current.version</span>&gt;</span></span><br></pre></td></tr></table></figure><p>pom文件增加cloudera的仓库地址</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--允许发布版本，禁止快照版--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">releases</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">releases</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>false<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>检查所在主机是否有maven,如果没有需要安装,如下(安装了maven即可跳过):</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget http://mirrors.hust.edu.cn/apache/maven/maven-3/3.5.4/binaries/apache-maven-3.5.4-bin.tar.gz</span><br><span class="line"></span><br><span class="line">tar -zxvf apache-maven-3.5.4-bin.tar.gz</span><br><span class="line"></span><br><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line">添加环境变量:</span><br><span class="line">MAVEN_HOME=/usr/local/apache-maven-3.5.4</span><br><span class="line">export MAVEN_HOME</span><br><span class="line">export PATH=$&#123;PATH&#125;:$&#123;MAVEN_HOME&#125;/bin</span><br><span class="line"></span><br><span class="line">保存退出profile</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>在maven中配置阿里云和cloudera的仓库</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>nexus-aliyun<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>*,!cloudera<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Nexus aliyun<span class="tag">&lt;/<span class="name">name</span>&gt;</span>                     </span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span></span><br><span class="line">        http://maven.aliyun.com/nexus/content/groups/public</span><br><span class="line">    <span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>导入hadoop-lzo编译时需要路径信息</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export CFLAGS=-m64</span><br><span class="line">export CXXFLAGS=-m64</span><br><span class="line">export C_INCLUDE_PATH=/home/hadoop/app/hadoop/lzo/include</span><br><span class="line">export LIBRARY_PATH=/home/hadoop/app/hadoop/lzo/lib</span><br></pre></td></tr></table></figure></li><li><p>maven编译安装</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mvn clean package -Dmaven.test.skip=true</span><br></pre></td></tr></table></figure><p><strong>编译安装没有异常结束后往下继续   PS:如果在mvn这里出现异常，请解决后再继续，注意权限问题</strong></p></li><li><p>编译成功后会有target文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd target/native/Linux-amd64-64/</span><br><span class="line">mkdir ~/hadoop-lzo-files</span><br><span class="line">tar -cBf - -C lib . | tar -xBvf - -C ~/hadoop-lzo-files</span><br></pre></td></tr></table></figure></li><li><p>在 ~/hadoop-lzo-files 目录下产生几个文件,执行cp</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp ~/hadoop-lzo-files/libgplcompression*  $HADOOP_HOME/lib/native/</span><br></pre></td></tr></table></figure><p><strong>注意！！！上面这一步的cp文件也要同步到集群其他主机的hadoop的对应目录下</strong></p></li><li><p>cp  hadoop-lzo的jar包到hadoop目录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp target/hadoop-lzo-0.4.21-SNAPSHOT.jar $HADOOP_HOME/share/hadoop/common/</span><br></pre></td></tr></table></figure><p><strong>注意！！！上面这一步的cp文件也要同步到集群其他主机的hadoop的对应目录下</strong></p></li></ol><h3 id="配置hadoop配置文件"><a href="#配置hadoop配置文件" class="headerlink" title="配置hadoop配置文件"></a>配置hadoop配置文件</h3><ol><li><p>修改<strong>core-site.xml</strong>(<em>如果配置过了不需要配置</em>)</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">        org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">        com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzopCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.Lz4Codec,</span><br><span class="line">        org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">    <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>修改<strong>mapred-site.xml</strong>中的压缩方式</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">#开启压缩</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.compress.map.output<span class="tag">&lt;/<span class="name">name</span>&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span>    </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">#配置压缩方式</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.map.output.compression.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span>    </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>跑个wc验证输出文件是否压缩</p></li><li><p>创建索引</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop jar $HADOOP_HOME/share/hadoop/common/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer /data/lzo-index/large_wc.txt.lzo</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce使用压缩以及在MR中的通用做法</title>
      <link href="/2018/10/15/hadoop/14/"/>
      <url>/2018/10/15/hadoop/14/</url>
      
        <content type="html"><![CDATA[<p>上一步中我们在Hadoop中安装了lzo的压缩方式，现在将测试如何在MapReduce程序中使用压缩</p><ol><li><p>在MapReduce中使用压缩，要注意三个位置，分别是map输入文件的压缩格式，map输出的压缩格式，和reduce最终输出的压缩格式</p><ul><li><p>首先配置使用压缩</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">mapreduce.output.fileoutputformat.compress=<span class="keyword">true</span></span><br></pre></td></tr></table></figure></li><li><p>第二步配置输入文件的压缩格式(如lzo):</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">mapreduce.job.inputformat<span class="class">.<span class="keyword">class</span></span>=com.hadoop.mapreduce.LzoTextInputFormat</span><br></pre></td></tr></table></figure></li><li><p>第三步配置map输出的文件压缩格式(如snappy):</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">mapred.map.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec</span><br></pre></td></tr></table></figure></li><li><p>第四步配置reduce输出的文件压缩格式(可不配置，这里配置lzo):</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Codec</span><br></pre></td></tr></table></figure></li></ul></li><li><p>我们随意找个表，查看表结构(这里只拿出来了我们想看的)</p><p>desc formatted emp;</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Storage Information  </span><br><span class="line">SerDe Library:      org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe </span><br><span class="line">InputFormat:        org.apache.hadoop.mapred.TextInputFormat </span><br><span class="line">OutputFormat:       org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</span><br></pre></td></tr></table></figure><p>由于hive底层也是跑的MapReduce，现在我们就能知道为什么要设置InputFormat和OutputFormat了。</p></li><li><p>一般不固定写在配置文件中，而是提交作业的时候手动指定，通过-D 指定参数</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop jar hadoop-mapreduce-examples-<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.16</span><span class="number">.2</span>.jar wordcount \</span><br><span class="line"> -D mapreduce.output.fileoutputformat.compress=<span class="keyword">true</span> \</span><br><span class="line"> -D io.compression.codec.lzo<span class="class">.<span class="keyword">class</span></span>=com.hadoop.compression.lzo.LzoCodec \</span><br><span class="line"> -D mapreduce.job.inputformat<span class="class">.<span class="keyword">class</span></span>=com.hadoop.mapreduce.LzoTextInputFormat \</span><br><span class="line"> -D mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzopCodec \</span><br><span class="line"> /data/lzo-index/  /out</span><br></pre></td></tr></table></figure></li><li><p>给.lzo文件创建索引</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop jar $HADOOP_HOME/share/hadoop/common/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer /data/lzo-index/large_wc.txt.lzo</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据倾斜&amp;MRchain解决数据倾斜&amp;大小表Reduce Join&amp;大小表Map Join&amp;SQL的执行计划</title>
      <link href="/2018/10/14/hadoop/11/"/>
      <url>/2018/10/14/hadoop/11/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>数据倾斜</li><li>MRchain解决数据倾斜</li><li>大小表Reduce Join</li><li>大小表Map Join</li><li>SQL的执行计划</li></ol><h2 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h2><ol><li><p>数据倾斜怎么造成的</p><p>mapreduce计算是将map相同的key丢到reduce，在reduce中进行聚合操作,在map和reduce中间有个shuffle操作，shuffle会将map阶段相同的key划分到reduce阶段中的一个reduce中去，数据倾斜就是数据的key 的分化严重不均，造成一部分数据很多，一部分数据很少的局面。</p></li><li><p>数据倾斜产生的问题</p><ul><li><p>有一个或多个reduce卡住</p></li><li><p>各种container报错OOM</p></li><li><p>读写的数据量极大，至少远远超过其它正常的reduce</p></li><li><p>伴随着数据倾斜，会出现任务被kill等各种诡异的表现。</p></li></ul></li><li><p>原因和解决方法</p><p>原因:</p><ul><li>单个值有大量记录(1.内存的限制存在，2.可能会对集群其他任务的运行产生不稳定的影响)</li><li>唯一值较多(单个唯一值的记录数占用内存不会超过分配给reduce的内存)</li></ul><p>解决办法:</p><ul><li><p>增加reduce个数</p></li><li><p>使用自定义partitioner</p></li><li><p>增加reduce 的jvm内存（效果不好）</p></li><li><p>map 阶段将造成倾斜的key 先分成多组加随机数并且在reduce阶段去除随机数</p></li><li><p>从业务和数据上解决数据倾斜</p><p>我们通过设计的角度尝试解决它</p><ul><li>数据预处理，过滤掉异常值</li><li>将数据打散让它的并行度变大，再汇集</li></ul></li><li><p>平台的优化方法</p><ul><li>join 操作中，使用 map join 在 map 端就先进行 join ，免得到reduce 时卡住</li><li>能先进行 group 操作的时候先进行 group 操作，把 key 先进行一次 reduce,之后再进行 count 或者 distinct count 操作</li><li>设置map端输出、中间结果压缩</li></ul></li></ul></li></ol><h2 id="MRchain解决数据倾斜"><a href="#MRchain解决数据倾斜" class="headerlink" title="MRchain解决数据倾斜"></a>MRchain解决数据倾斜</h2><p>核心思想: 第一个mapredue把具有数据倾斜特性的数据加盐(随机数)，进行聚合；第二个mapreduce把第一个mapreduce的加盐结果进行去盐，再聚合，问题是两个MR IO高。</p><p>参考代码:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.item;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.utils.FileUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.chain.ChainMapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.chain.ChainReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.Random;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-02-04 14:50</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ChainMRDriver</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Random r;</span><br><span class="line">    String in = <span class="string">"data/skew/access.txt"</span>;</span><br><span class="line">    String out1 = <span class="string">"out/mr1"</span>;</span><br><span class="line">    String out2 = <span class="string">"out/mr2"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(conf, <span class="keyword">new</span> ChainMRDriver(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">super</span>.getConf();</span><br><span class="line">        FileUtil.checkFileIsExists(conf, out1);</span><br><span class="line">        FileUtil.checkFileIsExists(conf, out2);</span><br><span class="line"></span><br><span class="line">        Job job1 = Job.getInstance(conf);</span><br><span class="line">        Job job2 = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        job1.setJarByClass(ChainMRDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job2.setJarByClass(ChainMRDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        ChainMapper.addMapper(job1, ChainMRDriver.incRanDomMapper<span class="class">.<span class="keyword">class</span>, <span class="title">LongWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line">        ChainReducer.setReducer(job1, ChainMRDriver.incRanDomReduver<span class="class">.<span class="keyword">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line"></span><br><span class="line">        ChainMapper.addMapper(job2, ChainMRDriver.decRanDomMapper<span class="class">.<span class="keyword">class</span>, <span class="title">LongWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line">        ChainReducer.setReducer(job2, ChainMRDriver.decRanDomReduver<span class="class">.<span class="keyword">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job1, <span class="keyword">new</span> Path(in));</span><br><span class="line">        FileOutputFormat.setOutputPath(job1, <span class="keyword">new</span> Path(out1));</span><br><span class="line">        FileInputFormat.setInputPaths(job2, <span class="keyword">new</span> Path(out1));</span><br><span class="line">        FileOutputFormat.setOutputPath(job2, <span class="keyword">new</span> Path(out2));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//提交job1和job2 job1--&gt;job2 必须按照顺序提交</span></span><br><span class="line">        System.out.println(<span class="string">"=============第一阶段=============="</span>);</span><br><span class="line">        <span class="keyword">boolean</span> b = job1.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">if</span> (b) &#123;</span><br><span class="line">            System.out.println(<span class="string">"=============第二阶段=============="</span>);</span><br><span class="line">            <span class="keyword">boolean</span> b1 = job2.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">            <span class="keyword">return</span> b1 ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">incRanDomMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//创建对象</span></span><br><span class="line">            r = <span class="keyword">new</span> Random();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//把数据读出来，加盐  www.baidu.com   2</span></span><br><span class="line">            String[] line = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">            String incR = r.nextInt(<span class="number">10</span>) +<span class="string">"_"</span>+line[<span class="number">0</span>];</span><br><span class="line">            <span class="keyword">int</span> number = Integer.parseInt(line[<span class="number">1</span>]);</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(incR), <span class="keyword">new</span> IntWritable(number));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (r != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">//回收对象</span></span><br><span class="line">                r = <span class="keyword">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">incRanDomReduver</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">int</span> sums = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (IntWritable sum : values) &#123;</span><br><span class="line">                sums += sum.get();</span><br><span class="line">            &#125;</span><br><span class="line">            context.write(key, <span class="keyword">new</span> IntWritable(sums));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">decRanDomMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="comment">//去盐 聚合</span></span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            String[] line = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">            String decWord = line[<span class="number">0</span>].split(<span class="string">"_"</span>)[<span class="number">1</span>];</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(decWord), <span class="keyword">new</span> IntWritable(Integer.parseInt(line[<span class="number">1</span>])));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">decRanDomReduver</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">int</span> sums = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (IntWritable sum : values) &#123;</span><br><span class="line">                sums += sum.get();</span><br><span class="line">            &#125;</span><br><span class="line">            context.write(key, <span class="keyword">new</span> IntWritable(sums));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="SQL的执行计划"><a href="#SQL的执行计划" class="headerlink" title="SQL的执行计划"></a>SQL的执行计划</h2><p>如何运行SQL的执行计划</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">EXPLAIN</span> [<span class="keyword">EXTENDED</span>] Syntax</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">EXPLAIN</span> <span class="keyword">select</span> e.empno,e.ename,d.deptno,d.dname <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure><p>解析这句SQL的执行计划</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">+-----------------------------------------------------------------+--+</span><br><span class="line">                      Explain                       </span><br><span class="line">+-----------------------------------------------------------------+--+</span><br><span class="line"> STAGE DEPENDENCIES:     <span class="comment">//阶段性依赖                           </span></span><br><span class="line">   Stage-<span class="number">4</span> is a root stage   <span class="comment">//这是一个根依赖                       </span></span><br><span class="line">   Stage-<span class="number">3</span> depends on stages: Stage-<span class="number">4</span>    <span class="comment">//Stage-3依赖Stage-4           </span></span><br><span class="line">   Stage-<span class="number">0</span> depends on stages: Stage-<span class="number">3</span>    <span class="comment">//Stage-0依赖Stage-3           </span></span><br><span class="line">                                                    </span><br><span class="line"> STAGE PLANS:   <span class="comment">// 阶段性计划                                    </span></span><br><span class="line">   Stage: Stage-<span class="number">4</span>     <span class="comment">//阶段4                              </span></span><br><span class="line">     Map Reduce Local Work   <span class="comment">//这是一个本地作业                    </span></span><br><span class="line">       Alias -&gt; Map Local Tables:    <span class="comment">// Map本地表的别名为d 即表dept              </span></span><br><span class="line">         d                                          </span><br><span class="line">           Fetch Operator   <span class="comment">//抓取                        </span></span><br><span class="line">             limit: -<span class="number">1</span>    <span class="comment">//limit为-1，即把数据全部读出来了                   </span></span><br><span class="line">       Alias -&gt; Map Local Operator Tree:  <span class="comment">//Map本地操作树          </span></span><br><span class="line">         d                                          </span><br><span class="line">           TableScan     <span class="comment">//表扫描                           </span></span><br><span class="line">             alias: d    <span class="comment">//别名d                           </span></span><br><span class="line">             Statistics: Num rows: <span class="number">2</span> Data size: <span class="number">284</span> Basic stats: PARTIAL Column stats: NONE <span class="comment">//统计 </span></span><br><span class="line">             Filter Operator  <span class="comment">//过滤                      </span></span><br><span class="line">               predicate: <span class="function">deptno is not <span class="title">null</span> <span class="params">(type: <span class="keyword">boolean</span>)</span>  <span class="comment">//断定：deptno字段 不为空</span></span></span><br><span class="line"><span class="function">               Statistics: Num rows: 1 Data size: 142 Basic stats: COMPLETE Column stats: NONE    <span class="comment">//统计</span></span></span><br><span class="line"><span class="function">               HashTable Sink Operator  <span class="comment">//输出类型为HashTable           </span></span></span><br><span class="line"><span class="function">                 keys:                              </span></span><br><span class="line"><span class="function">                   0 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                   1 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                                                    </span></span><br><span class="line"><span class="function">   Stage: Stage-3   <span class="comment">//阶段3                                </span></span></span><br><span class="line"><span class="function">     Map Reduce                                     </span></span><br><span class="line"><span class="function">       Map Operator Tree:    <span class="comment">//Map操作树                       </span></span></span><br><span class="line"><span class="function">           TableScan   <span class="comment">//表扫描                             </span></span></span><br><span class="line"><span class="function">             alias: e  <span class="comment">//e表 即emp表                             </span></span></span><br><span class="line"><span class="function">             Statistics: Num rows: 6 Data size: 657 Basic stats: COMPLETE Column stats: NONE  <span class="comment">//统计</span></span></span><br><span class="line"><span class="function">             Filter Operator    <span class="comment">//过滤                    </span></span></span><br><span class="line"><span class="function">               predicate: deptno is not <span class="title">null</span> <span class="params">(type: <span class="keyword">boolean</span>)</span> <span class="comment">//断定：deptno字段 不为空</span></span></span><br><span class="line"><span class="function">               Statistics: Num rows: 3 Data size: 328 Basic stats: COMPLETE Column stats: NONE   <span class="comment">//统计</span></span></span><br><span class="line"><span class="function">               Map Join Operator    <span class="comment">// Map Join  操作               </span></span></span><br><span class="line"><span class="function">                 condition map:   <span class="comment">//Map条件                 </span></span></span><br><span class="line"><span class="function">                      Inner Join 0 to 1             </span></span><br><span class="line"><span class="function">                 keys:                              </span></span><br><span class="line"><span class="function">                   0 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                   1 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                 outputColumnNames: _col0, _col1, _col11, _col12  <span class="comment">//数值字段在表中的位置</span></span></span><br><span class="line"><span class="function">                 Statistics: Num rows: 3 Data size: 360 Basic stats: COMPLETE Column stats: NONE<span class="comment">//统计 </span></span></span><br><span class="line"><span class="function">                 Select Operator   <span class="comment">//Select操作                 </span></span></span><br><span class="line"><span class="function">                   expressions: <span class="title">_col0</span> <span class="params">(type: <span class="keyword">int</span>)</span>, <span class="title">_col1</span> <span class="params">(type: string)</span>, <span class="title">_col11</span> <span class="params">(type: <span class="keyword">int</span>)</span>, <span class="title">_col12</span> <span class="params">(type: string)</span> <span class="comment">//表达式</span></span></span><br><span class="line"><span class="function">                   outputColumnNames: _col0, _col1, _col2, _col3 <span class="comment">//数值字段在表中的位置</span></span></span><br><span class="line"><span class="function">                   Statistics: Num rows: 3 Data size: 360 Basic stats: COMPLETE Column stats: NONE   <span class="comment">//统计</span></span></span><br><span class="line"><span class="function">                   File Output Operator   <span class="comment">//文件输出操作          </span></span></span><br><span class="line"><span class="function">                     compressed: <span class="keyword">false</span>    <span class="comment">//是否压缩：否          </span></span></span><br><span class="line"><span class="function">                     Statistics: Num rows: 3 Data size: 360 Basic stats: COMPLETE Column stats: NONE<span class="comment">//统计 </span></span></span><br><span class="line"><span class="function">                     table:   <span class="comment">//表文件的输入、输出、序列化类型                      </span></span></span><br><span class="line"><span class="function">                         input format: org.apache.hadoop.mapred.TextInputFormat <span class="comment">//文件输入格式</span></span></span><br><span class="line"><span class="function">                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat <span class="comment">//文件输出格式</span></span></span><br><span class="line"><span class="function">                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe <span class="comment">//反序列化</span></span></span><br><span class="line"><span class="function">       Local Work:                                  </span></span><br><span class="line"><span class="function">         Map Reduce Local Work                      </span></span><br><span class="line"><span class="function">                                                    </span></span><br><span class="line"><span class="function">   Stage: Stage-0   <span class="comment">//阶段0                                </span></span></span><br><span class="line"><span class="function">     Fetch Operator                                 </span></span><br><span class="line"><span class="function">       limit: -1     <span class="comment">// limit设置的值                               </span></span></span><br><span class="line"><span class="function">       Processor Tree:                              </span></span><br><span class="line"><span class="function">         ListSink                                   </span></span><br><span class="line"><span class="function">                                                    </span></span><br><span class="line"><span class="function">+-----------------------------------------------------------------+--+</span></span><br></pre></td></tr></table></figure><p>从执行计划得知，hive中执行SQL语句底层执行的是MapReduce。</p><p>我们在SQL中关联了两张表分别是emp dept，并从两张表中取出某些字段，在SQL执行计划中共分为三个阶段，分别是stage4、stage3、stage0。</p><p>stage4是根stage，stage3依赖stage4，同时stage0依赖stage3。</p><p>stage4是一个本地作业，读取的是dept表，输出一个Map类型的hashTable，关联的key是两张表的deptno，在执行计划中表现为0 deptno和 1 deptno，即执行的MapReduce中的Key是deptno字段。</p><p>stage3是MapReduce中的Map阶段，扫描emp表，执行一个Map Join操作，条件是两张表的dept字段相等(内连接)，现在我们得到的是一张包含所有字段的大表，得到需要的字段的对应位置，并且匹配字段的类型，在输出的时候检查是否需要压缩，以及输入、输出、和序列化类型</p><p>Stage-0阶段取出limit中指定的记录数</p><p>总结: 我们发现执行该SQL没有Reduce阶段，在现有的版本中默认设置<code>hive.auto.convert.join</code>(是否自动转换为mapjoin)为true，该参数配合<code>hive.mapjoin.smalltable.filesize</code>参数(小表的最大文件大小)默认为25M。即小于25M的表为小表，自动转为mapjoin，小表上传到hadoop缓存，提供给各个大表join使用。大表和小表根据关联的key形成一张大表，取出select需要的字段，最后根据limit设置的值取出对应的记录数。</p><p>参考参数：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--是否自动转换为mapjoin</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">true</span>;</span><br><span class="line"><span class="comment">--小表的最大文件大小，默认为25000000，即25M</span></span><br><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize = <span class="number">25000000</span>;</span><br><span class="line"><span class="comment">--是否将多个mapjoin合并为一个</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask = <span class="literal">true</span>;</span><br><span class="line"><span class="comment">--多个mapjoin转换为1个时，所有小表的文件大小总和的最大值。</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask.size = <span class="number">10000000</span>;</span><br></pre></td></tr></table></figure><h2 id="大小表Reduce-Join-emp、dept"><a href="#大小表Reduce-Join-emp、dept" class="headerlink" title="大小表Reduce Join(emp、dept)"></a>大小表Reduce Join(emp、dept)</h2><p>Reduce Join的核心思路是定义输出字段作为一个实体类，用来作为输出，实体类中定义一个标志用来区分表的来源</p><ol><li><p>将大小两个表在SQL中join的字段作为MapReduce中的key，原因是MapReduce中的key具有排序和分区的作用</p></li><li><p>Map中获取context中切片所在的文件名，按行获取文件中的数据并且根据获取的文件名分别将数据set到对象中，并写出Map。</p></li><li><p>Reduce中每次获取key相同的一组value值数据，这组value值既有dept中的数据，也</p></li></ol><p>   有emp中的数据，只要他们有相同的key，就会在shuffle中丢到一个reduce，这时候获取这组数据的值，根据flag来判断来自哪个表，如果是dept表则将数据设置到新new出来的对象中，添加到List列表中，同时创建一个保存emp表中数据的变量，由于emp表是小表，emp表中需要的数据对应dept/emp中的key的字段是唯一的，所以只需要把value中所有的对象都遍历循环出来，dept表数据添加到了List列表，emp表的数据添加到了变量中，最后循环List列表把变量set到每一个对象中，即完成了全部对象的全部成员属性。最后输出即可。</p><p>   参考代码:</p>   <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.hadoop.join;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.pojo.JoinMain;</span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.utils.FileUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-01-29 16:39</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReduceJoinDriver</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    String in = <span class="string">"data/join/"</span>;</span><br><span class="line">    String out = <span class="string">"out/"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(conf, <span class="keyword">new</span> ReduceJoinDriver(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//获得configuration</span></span><br><span class="line">        Configuration conf = <span class="keyword">super</span>.getConf();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//检查文件夹</span></span><br><span class="line">        FileUtil.checkFileIsExists(conf, out);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用新方法这里怎么操作?</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置驱动类</span></span><br><span class="line">        job.setJarByClass(ReduceJoinDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置Map/Reducer类</span></span><br><span class="line">        job.setMapperClass(JoinMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(JoinReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置Map参数类型</span></span><br><span class="line">        job.setMapOutputKeyClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(JoinMain<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//job.setNumReduceTasks(3);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置Reducer参数类型</span></span><br><span class="line">        job.setOutputKeyClass(JoinMain<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置文件的输入输出</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(in));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(out));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//提交任务</span></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> b ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">JoinMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">JoinMain</span>&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            FileSplit fileSplit = (FileSplit) context.getInputSplit();</span><br><span class="line">            name = fileSplit.getPath().getName();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//做一个传入的表的判断</span></span><br><span class="line">            <span class="keyword">if</span> (name.contains(<span class="string">"emp"</span>))&#123;  <span class="comment">//emp</span></span><br><span class="line">                <span class="comment">//读取数据</span></span><br><span class="line">                String[] lines = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">                <span class="keyword">if</span> (lines.length ==<span class="number">8</span>)&#123;</span><br><span class="line">                    <span class="comment">//细粒度划分</span></span><br><span class="line">                    Integer empno = Integer.parseInt(lines[<span class="number">0</span>].trim());</span><br><span class="line">                    String ename = lines[<span class="number">1</span>];</span><br><span class="line">                    Integer deptno = Integer.parseInt(lines[lines.length-<span class="number">1</span>].trim());</span><br><span class="line">                    <span class="comment">//写入数据</span></span><br><span class="line">                    context.write(<span class="keyword">new</span> IntWritable(deptno),<span class="keyword">new</span> JoinMain(empno,ename,deptno,<span class="string">""</span>,<span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;      <span class="comment">//dept</span></span><br><span class="line">                <span class="comment">//读取数据</span></span><br><span class="line">                String[] lines = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">                <span class="keyword">if</span> (lines.length ==<span class="number">3</span>)&#123;</span><br><span class="line">                    <span class="keyword">int</span> deptno = Integer.parseInt(lines[<span class="number">0</span>].trim());</span><br><span class="line">                    String dname = lines[<span class="number">1</span>];</span><br><span class="line">                    context.write(<span class="keyword">new</span> IntWritable(deptno),<span class="keyword">new</span> JoinMain(<span class="number">0</span>, <span class="string">""</span>, deptno, dname, <span class="number">2</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">super</span>.cleanup(context);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">JoinReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">IntWritable</span>, <span class="title">JoinMain</span>,<span class="title">JoinMain</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="comment">//核心思路在 每个deptno组 进一次reduce ，前提是map中的key是deptno</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(IntWritable key, Iterable&lt;JoinMain&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            List&lt;JoinMain&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            String dname=<span class="string">""</span>;</span><br><span class="line">            <span class="comment">// 1.取出map中每行数据，判断flag值</span></span><br><span class="line">            <span class="comment">// 2.取出所有的emp中数据放入list中</span></span><br><span class="line">            <span class="comment">// 3.取出dept中的dname赋值给变量</span></span><br><span class="line">            <span class="comment">// 4.取出属于这个deptno中的所有数据，并给dname赋值</span></span><br><span class="line">            <span class="comment">// 5.每条赋值dname的数据写入reduce</span></span><br><span class="line">            <span class="keyword">for</span> (JoinMain main : values)&#123;</span><br><span class="line">                <span class="comment">// emp表</span></span><br><span class="line">                <span class="keyword">if</span> (main.getFlag() == <span class="number">1</span>)&#123;</span><br><span class="line">                    <span class="comment">//给emp表全部行重新赋值</span></span><br><span class="line">                    JoinMain m = <span class="keyword">new</span> JoinMain();</span><br><span class="line">                    m.setDeptno(main.getDeptno());</span><br><span class="line">                    m.setEmpno(main.getEmpno());</span><br><span class="line">                    m.setEname(main.getEname());</span><br><span class="line">                    <span class="comment">//写出到list</span></span><br><span class="line">                    list.add(m);</span><br><span class="line">                &#125;<span class="keyword">else</span> <span class="keyword">if</span> (main.getFlag() ==<span class="number">2</span> )&#123; <span class="comment">//dept</span></span><br><span class="line">                    <span class="comment">//拿到dept表中的dname</span></span><br><span class="line">                dname = main.getDname();</span><br><span class="line">            &#125;&#125;</span><br><span class="line">            <span class="comment">//循环赋值</span></span><br><span class="line">            <span class="keyword">for</span> (JoinMain bean : list) &#123;</span><br><span class="line">                bean.setDname(dname);</span><br><span class="line">                context.write(bean,NullWritable.get());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="大小表Map-Join-emp、dept"><a href="#大小表Map-Join-emp、dept" class="headerlink" title="大小表Map Join(emp、dept)"></a>大小表Map Join(emp、dept)</h2><p>Map Join的核心思想是把小表添加到缓存中(Map中)，在map中读取大表每行数据时set到对象值时取出小表(Map)对应key的值即可</p><ol><li><p>setup中，通过context获取小表文件切片的路径，然后通过读取流的方式读取为字符，按行获取到字符后切分，使用HashMap结构设置key和value分别为map方法中大表join时需要的键和值。</p></li><li><p>在map方法中读取文件数据，并且根据key取出HashMap(小表)中的value，一起set到对象中即可，最后写出，写出时，可以把value设置为NullWritable。</p><p>参考代码:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.hadoop.join;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.pojo.JoinMain;</span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.utils.FileUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"><span class="keyword">import</span> org.codehaus.groovy.runtime.wrappers.LongWrapper;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-02-01 23:10</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapJoinDriver</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String in = <span class="string">"data/join/emp.txt"</span>;</span><br><span class="line">    <span class="keyword">private</span> String out = <span class="string">"out"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(conf, <span class="keyword">new</span> MapJoinDriver(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> strings</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> : int</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@describe</span> : 设置配置文件，不用设置reduce</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@author</span> : tunan</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@date</span> : 2020/2/1 23:14</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">super</span>.getConf();</span><br><span class="line">        FileUtil.checkFileIsExists(conf,out);</span><br><span class="line"></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(MapJoinDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"data/join/dept.txt"</span>));</span><br><span class="line"></span><br><span class="line">        job.setMapperClass(MapperJoin<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(JoinMain<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(in));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(out));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> b ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MapperJoin</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">JoinMain</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> HashMap&lt;Integer, String&gt; hashMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//得到缓存文件路径</span></span><br><span class="line">            <span class="comment">//String path = context.getCacheFiles()[0].getPath().toString();</span></span><br><span class="line">            String path = context.getCacheFiles()[<span class="number">0</span>].getPath();</span><br><span class="line">            <span class="comment">/*URI[] files = context.getCacheFiles();   //URI 通过getPath()解码 没有toString()方法</span></span><br><span class="line"><span class="comment">            String s = files[0].getPath();*/</span></span><br><span class="line">            <span class="comment">//得到文件</span></span><br><span class="line">            <span class="comment">//File file = new File(cacheFiles[0]);</span></span><br><span class="line">            <span class="comment">//String path = file.getPath();</span></span><br><span class="line">            <span class="comment">//得到文件的流        InputStreamReader将字节转换为字符</span></span><br><span class="line">            BufferedReader br = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(<span class="keyword">new</span> FileInputStream(path)));</span><br><span class="line">            <span class="comment">//读取文件为字符串</span></span><br><span class="line">            String line ;</span><br><span class="line">            <span class="keyword">while</span>(StringUtils.isNotEmpty(line=br.readLine()))&#123;</span><br><span class="line">                <span class="comment">//切分字符串得到字符串数组</span></span><br><span class="line">                String[] split = line.split(<span class="string">"\t"</span>);</span><br><span class="line">                hashMap.put(Integer.parseInt(split[<span class="number">0</span>]),split[<span class="number">1</span>]);</span><br><span class="line">            &#125;</span><br><span class="line">            IOUtils.closeStream(br);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@describe</span> :</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@author</span> : tunan</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> key</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@return</span> : void</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@date</span> : 2020/2/1 23:38</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            String[] line = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">            <span class="keyword">if</span> (line.length &gt;= <span class="number">8</span>)&#123;</span><br><span class="line">                Integer empno = Integer.parseInt(line[<span class="number">0</span>].trim());</span><br><span class="line">                String ename = line[<span class="number">1</span>];</span><br><span class="line">                Integer deptno = Integer.parseInt(line[line.length-<span class="number">1</span>].trim());</span><br><span class="line">                String dname = hashMap.get(deptno);</span><br><span class="line">                context.write(<span class="keyword">new</span> JoinMain(empno,ename,deptno,dname),NullWritable.get());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">super</span>.cleanup(context);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> MapReduce </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>InputFormat&amp;Partitioner&amp;Conbiner&amp;Sort&amp;OutputFormat</title>
      <link href="/2018/10/13/hadoop/10/"/>
      <url>/2018/10/13/hadoop/10/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>InputFormat</li><li>Partitioner</li><li>Conbiner</li><li>Sort</li><li>OutputFormat</li></ol><h2 id="InputFormat"><a href="#InputFormat" class="headerlink" title="InputFormat"></a>InputFormat</h2><p>在数据进入map之前，会进过一系列的格式化操作</p><ol><li>在客户端submitJob()方法提交作业前，会获取配置信息，形成一个任务分配的规划</li><li>提交文件分片(文件夹)和应用程序jar包</li><li>MR运行MapTask根据InputFormat读取文件，这里将详细介绍InputFormat</li></ol><p>InputFormat是一个抽象类，MR默认是用TextInputFormat方法读取文件</p><p>TextInputFormat是按行读取文件中的数据，实际上TextInputFormat中只实现了createRecordReader()和isSplitable()两个方法，它的具体实现在FileInputFormat中就已经实现的，FileInputFormat也是一个抽象类。</p><p>NLineInputFormat也继承于FileInputFormat，它的特点是按特定的行数读取数据</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 设置指定的InputFormat(重点)</span></span><br><span class="line">NLineInputFormat.setNumLinesPerSplit(job, <span class="number">3</span>);</span><br><span class="line">job.setInputFormatClass(NLineInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure><p>DBInputFormat继承于DBInputFormat的同时实现了InputFormat，这个方法可以从数据库中读取数据，写入HDFS，类似于Sqoop，需要注意的是它的实体类要同时继承DBWritable和Writable，提交到HDFS上执行的时候需要指定jdbc的jar包(不推介使用)。</p><h2 id="Partitioner"><a href="#Partitioner" class="headerlink" title="Partitioner"></a>Partitioner</h2><p>MR的默认分区规则是按照key分区，相同的key到一个reduce方法中去，<strong>Partitoner可以自定义分区规则</strong>，自定义类继承Partitioner&lt;Text, Flow&gt;，泛型是map输出的key和value类型</p><p>参考代码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Partitoner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">Flow</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, Flow flow, <span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">        String phone = text.toString();</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"137"</span>.equals(phone.substring(<span class="number">0</span>,<span class="number">3</span>)))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"136"</span>.equals(phone.substring(<span class="number">0</span>,<span class="number">3</span>)))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>要在Driver中指定Partitioner的类，并且指定reduce的个数，这里的<strong>reduce设置的个数一定要和Partitoner分区中返回的分区个数相同</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 设置Partitoner</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">3</span>);</span><br><span class="line">job.setPartitionerClass(Partitoner<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure><p>如果reduce设置的数量大于分区个数，则会产生空的输出文件，即空的reduce。</p><p>如果reduce设置的个数小于分区个数，则会报错，表示多余的数据没有分区可去。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">java.io.IOException: Illegal partition <span class="keyword">for</span> <span class="number">13826544101</span> (<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h2 id="Conbiner"><a href="#Conbiner" class="headerlink" title="Conbiner"></a>Conbiner</h2><p>Conbiner是合并，即是map阶段的reduce，可以自定义，也可以直接使用reduce方法，需要在Driver中指定，需要注意的是不能改变业务逻辑(不适用于乘积)</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//设置conbiner</span></span><br><span class="line">job.setCombinerClass(FlowReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure><h2 id="Sort"><a href="#Sort" class="headerlink" title="Sort"></a>Sort</h2><p>Sort是分区排序，需要知道的是MR的key默认是有序的，如果要自定义排序规则需要将实体类实现<code>WritableComparable&lt;FlowSort&gt;</code>接口，泛型就传入实体类的类名，WritableComparable实际上是继承了Writable和Comparable<T>，Writable是Hadoop自己实现的，Comparable是Java中的类</p><p>实体类参考代码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowSort o)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Long.compare(o.sum, <span class="keyword">this</span>.sum);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>需要注意的是使用自定义排序的实体类要放到mapreduce方法key的位置，使之有序。</p><p>主要注意的是Sort是每个reduce中有序，如果设置了多个reduce，则只能保证每个reduce内部有序</p><h2 id="OutputFormat"><a href="#OutputFormat" class="headerlink" title="OutputFormat"></a>OutputFormat</h2><p>有一类很常见的需求：按照一定的规则把数据给我写到某个文件中去</p><p>OutputFormat是一个接口，实现它的类有FileOutputFormat和DBOutputFormat，使用和InputFormat差不多，用的不多，不写了</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> MapReduce </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka集群和客户端不在同一个网段报错 -Batch containing 11 record(s) expired due to timeout while requesting metadata</title>
      <link href="/2018/10/12/PE/4/"/>
      <url>/2018/10/12/PE/4/</url>
      
        <content type="html"><![CDATA[<p><strong>背景：</strong><br>Kafka集群在一个192.168.0.x网段的，而我们的生产者在192.168.17.x网段的一台机器上，故当生产者发送消息给Kafka时，<br>无法将消息发送过去。</p><p><strong>错误：</strong><br>11:21:13,936 ERROR KafkaProducer - Batch containing 11 record(s) expired due to timeout while requesting metadata from brokers for onlinelogs-1</p><p><strong>分析：</strong><br>1.做Kafka集群的demo测试是OK的，详情参考：<a href="http://blog.itpub.net/30089851/viewspace-2132049/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-2132049/</a></p><p>2.在生产者机器上 telnet kafka节点ip 9092 ,也是通的，甚是奇怪。</p><p>3.谷歌了几下：<br>需要配置 <code>advertised.host.name</code> 参数，将Kafka各个节点，该参数配置为当前机器的IP即可，重启生效配置，测试OK。</p><p>新版本的参数：<a href="http://kafka.apache.org/documentation/#advertised.listeners" target="_blank" rel="noopener">advertised.listeners</a></p>]]></content>
      
      
      <categories>
          
          <category> 生产故障案例 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生产故障案例 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark、IDEA和Maven的环境准备&amp;Hadoop的依赖以及常用API&amp;WordCount Debug流程&amp;map、reduce方法的参数类型和作用&amp;瘦包在服务器上的jar包依赖</title>
      <link href="/2018/10/12/hadoop/9/"/>
      <url>/2018/10/12/hadoop/9/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>Spark、IDEA和Maven的环境准备</li><li>hadoop的依赖以及常用API</li><li>WordCount Debug流程</li><li>map、reduce方法的参数类型和作用</li><li>Writable和WritableComparable的作用</li><li>瘦包在服务器上的jar包依赖</li></ol><h2 id="Spark、IDEA和Maven的环境准备"><a href="#Spark、IDEA和Maven的环境准备" class="headerlink" title="Spark、IDEA和Maven的环境准备"></a>Spark、IDEA和Maven的环境准备</h2><p>环境:</p><ol><li>Spark3.0</li><li>IDEA19.3</li><li>Maven3.6.3(安装配置阿里云的镜像)</li></ol><h2 id="Hadoop的依赖以及常用API"><a href="#Hadoop的依赖以及常用API" class="headerlink" title="Hadoop的依赖以及常用API"></a>Hadoop的依赖以及常用API</h2><p>依赖:</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.6.0-cdh5.16.2&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>常用API:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">FileSystem fileSystem; <span class="comment">//核心</span></span><br><span class="line">open()<span class="comment">//打开文件返回流</span></span><br><span class="line">mkdirs()<span class="comment">//创建目录</span></span><br><span class="line">create()<span class="comment">//创建文件</span></span><br><span class="line">copyFromLocalFile()<span class="comment">//从本地复制文件到hdfs，类似于get</span></span><br><span class="line">copyToLocalFile()<span class="comment">//从hdfs复制文件到本地，类似于put</span></span><br><span class="line">listFiles()<span class="comment">//列出目录下的所有文件，可以迭代</span></span><br><span class="line">delete()<span class="comment">//删除文件，不存在报错</span></span><br><span class="line">deleteOnExit()<span class="comment">//删除存在的文件,不存在不报错</span></span><br></pre></td></tr></table></figure><h2 id="WordCount-Debug流程"><a href="#WordCount-Debug流程" class="headerlink" title="WordCount Debug流程"></a>WordCount Debug流程</h2><ol><li><p>编译</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">job.waitForCompletion(<span class="keyword">true</span>);</span><br></pre></td></tr></table></figure></li><li><p>提交</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (state == JobState.DEFINE) &#123;</span><br><span class="line">   submit();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>兼容新老API</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">setUseNewAPI();</span><br></pre></td></tr></table></figure></li><li><p>本地连接/服务器连接</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">connect();</span><br></pre></td></tr></table></figure></li><li><p>检查配置、输出路径等</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> JobSubmitter submitter = getJobSubmitter(cluster.getFileSystem(),cluster.getClient());</span><br><span class="line"><span class="function"><span class="keyword">public</span> JobStatus <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException,ClassNotFoundException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster);</span><br><span class="line">&#125;</span><br><span class="line">checkSpecs(job);<span class="comment">//validate the jobs output specs</span></span><br></pre></td></tr></table></figure></li><li><p>把该作业的配置信息加到分布式缓存中</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Configuration conf = job.getConfiguration();</span><br><span class="line">addMRFrameworkToDistributedCache(conf);</span><br></pre></td></tr></table></figure></li><li><p>创建该Job对应的存放目录</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br></pre></td></tr></table></figure></li><li><p>拿到该Job对应的ID(local/application)</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">JobID jobId = submitClient.getNewJobID();</span><br></pre></td></tr></table></figure></li><li><p>jobStagingArea/jobid</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Path submitJobDir = <span class="keyword">new</span> Path(jobStagingArea, jobId.toString());</span><br></pre></td></tr></table></figure></li><li><p>拷贝job对应的信息到jobStagingArea/jobid</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">copyAndConfigureFiles(job, submitJobDir);</span><br></pre></td></tr></table></figure></li><li><p>完成我们输入数据的切片(默认128MB，预留10%浮动空间)</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> maps = writeSplits(job, submitJobDir);</span><br></pre></td></tr></table></figure></li><li><p>作业文件提交到指定目录</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">writeConf(conf, submitJobFile);</span><br></pre></td></tr></table></figure></li><li><p>提交作业</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure></li></ol><h2 id="map、reduce方法的参数类型和作用"><a href="#map、reduce方法的参数类型和作用" class="headerlink" title="map、reduce方法的参数类型和作用"></a>map、reduce方法的参数类型和作用</h2><ul><li><p>继承Mapper后实现map方法</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br></pre></td></tr></table></figure><p>该方法中的参数分别是<code>LongWritable key, Text value, Context context</code></p><p>前两个参数是map方法中输入的键和值，输入的键和值必须是LongWritable类型和Text类型，因为是按offset读取每行本文数据，最后一个参数context，它是MapReduce的上下文。什么都可以拿出来。</p></li><li><p>继承Reducer后实现reduce方法</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span></span></span><br></pre></td></tr></table></figure><p>该方法中的参数分别是<code>Text key, Iterable&lt;IntWritable&gt; values, Context context</code></p><p>前两个参数是reduce方法中输入的键和值，输入的键和值对应map中输出的键值类型，并且值是一个Iterable类型，因为在shuffle阶段相同key的value分到了一起，是一个可迭代的参数。因为是按offset读取每行本文数据，最后一个参数context，它是MapReduce的上下文。什么都可以拿出来。</p></li></ul><h2 id="Writable和WritableComparable的作用"><a href="#Writable和WritableComparable的作用" class="headerlink" title="Writable和WritableComparable的作用"></a>Writable和WritableComparable的作用</h2><p>Writable是hadoop中的序列化接口，是一个接口，只定义了两个方法，分别是<code>write()</code>和<code>readFields()</code>方法，用于hadoop序列化时的读和写；<code>WritableComparable</code>也是一个序列化接口，只是在序列化的同时同时实现了java中的<code>Comparable&lt;T&gt;</code>接口，具有排序的特性。</p><p>hadoop是java写的，那么为什么hadoop要实现自己的序列化接口</p><ul><li>java序列化数据结果比较大、传输效率比较低、不能跨语言对接</li></ul><p>hadoop使用的是RPC协议传送数据，且hadoop是应用在大集群上，所以hadoop的序列化必须做到</p><ul><li>占用空间更小</li><li>传输速度更快</li><li>扩展性更强，支持多种格式的序列化</li><li>兼容性更好，需要支持多种语言，如java、scala、python等</li></ul><p>所以hadoop实现了自己的序列化接口Writable：<code>压缩</code>、<code>速度</code>、<code>扩展性</code>、<code>兼容性</code>都比java更优秀</p><p>另外:</p><ol><li><em>序列化的对象，他们超越了JVM的生死，不顾生他们的母亲，化作永恒。</em>static和transient修饰的属性除外，因为static修饰的属性是在编译时静态生成的，而对象是动态生成的，又因为transient修饰的属性禁止了属性的序列化。</li><li><em>把“活的”对象序列化，就是把“活的”对象转化成一串字节，而“反序列化”，就是从一串字节里解析出“活的”对象。</em></li></ol><h2 id="瘦包在服务器上的jar包依赖"><a href="#瘦包在服务器上的jar包依赖" class="headerlink" title="瘦包在服务器上的jar包依赖"></a>瘦包在服务器上的jar包依赖</h2><p>打包好的mapreduce程序上传到云主机，由于是瘦包，缺少某些依赖，比如连接mysql的的jar包，现在我们就解决缺少依赖的问题</p><ol><li><p>将下载好的jar包上传到云主机上</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">cp mysql-connector-java-<span class="number">5.1</span><span class="number">.27</span>-bin.jar ~/lib/</span><br></pre></td></tr></table></figure></li><li><p>将jar包加载到hadoop的classpath中</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:~/lib/mysql-connector-java-<span class="number">5.1</span><span class="number">.27</span>-bin.jar</span><br></pre></td></tr></table></figure></li><li><p>用hadoop jar 执行jar文件时，加上-libjars参数</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop jar hadoop-mapreduce-examples-<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.16</span><span class="number">.2</span>.jar wordcount -libjars /home/hadoop/lib/mysql-connector-java-<span class="number">5.1</span><span class="number">.27</span>-bin.jar  /<span class="number">1</span>.txt /out</span><br></pre></td></tr></table></figure></li></ol><p>如果上诉方法有问题可以使用hadoop的分布式缓存</p><ol><li><p>把jar包传到集群上，命令如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop fs -put mysql-connector-java-<span class="number">5.1</span><span class="number">.27</span>.jar /lib</span><br></pre></td></tr></table></figure></li><li><p>在mr程序提交job前，添加一下语句：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">job.addArchiveToClassPath(<span class="keyword">new</span> Path(<span class="string">"hdfs://aliyun:9000/lib/mysql-connector-java-5.1.27.jar"</span>));</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> MapReduce </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YARN的调优&amp;YARN的三种调度器</title>
      <link href="/2018/10/11/hadoop/8/"/>
      <url>/2018/10/11/hadoop/8/</url>
      
        <content type="html"><![CDATA[<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><ol><li>梳理YARN资源调优参数</li><li>调度器整理三种，区别是什么，CDH默认是什么</li></ol><h3 id="YARN的资源调优"><a href="#YARN的资源调优" class="headerlink" title="YARN的资源调优"></a>YARN的资源调优</h3><p><code>背景:</code> 假设每台服务器拥有内存128G 16物理core，怎么分配？</p><ol><li><p>装完CentOS，消耗内存1G</p></li><li><p>系统预览15%-20%内存(包含1.1)，以防全部使用导致系统夯住 和 oom机制事件，或者给未来部署组件预览点空间(<code>128*20%=25.6G==26G</code>)</p></li><li><p>假设只有DN NM节点，余下内存: <code>128-26=102G</code></p><ol><li><p>给DN进程(自身)2G，给NM进程(自身)4G，剩余<code>102-2-4=96G</code></p></li><li><p>container内存的分配共96G</p><ul><li><p><code>yarn.nodemanager.resource.memory-mb</code>     共 96G</p></li><li><p><code>yarn.scheduler.minimum-allocation-mb</code>    最少1G    极限情况下，只有96个container 内存1G</p></li><li><p><code>yarn.scheduler.maximum-allocation-mb</code>    最多96G  极限情况下，只有1个container 内存96G</p><p>container的内存会自动增加，默认1G递增，那么container的个数的范围: 1-96个</p></li></ul></li><li><p>container物理核分配 (物理核:虚拟核 =1:2 ==&gt;16:32)</p><ul><li><p><code>yarn.nodemanager.resource.cpu-vcores</code>            共 32个</p></li><li><p><code>yarn.scheduler.minimum-allocation-vcores</code>     最少1个   极限情况下，只有32个container    </p></li><li><p><code>yarn.scheduler.maximum-allocation-vcores</code>     最多32个 极限情况下，只有1个container</p><p>container的物理核会自动增加，默认1个递增，那么container的个数的范围: :1-32个</p></li></ul></li><li><p><code>关键:</code> cloudera公司推荐，一个container的vcore最好不要超过5，那么我们设置4</p><ul><li><p><code>yarn.scheduler.maximum-allocation-vcores</code>    4   </p><p>目前为止，极限情况下，共有8个container (32/4)</p></li></ul></li><li><p>综合memory+vcore的分配</p><ol><li><p>一共有32个vcore，一个container的vcore是4个，那么分配container一共有8个</p></li><li><p>重新分配核</p><ul><li><p><code>yarn.nodemanager.resource.cpu-vcores</code>            共32个</p></li><li><p><code>yarn.scheduler.minimum-allocation-vcores</code>    最少4个    </p></li><li><p><code>yarn.scheduler.maximum-allocation-vcores</code>    最多4个   极限container 8个</p></li></ul></li><li><p>根据物理核重新分配内存</p><ul><li><p><code>yarn.nodemanager.resource.memory-mb</code>      共96G</p></li><li><p><code>yarn.scheduler.minimum-allocation-mb</code>    最少12G  </p></li><li><p><code>yarn.scheduler.maximum-allocation-mb</code>    最多12G   (极限container 8个)</p></li></ul></li><li><p>分配后的每个container的物理核数是4个，内存大小是12G，当然spark计算时内存不够大，这个参数肯定要调大，那么这种理想化的设置个数必然要打破，以memory为主</p></li></ol></li><li><p>假如 256G内存 56core，请问参数如何设置</p><ol><li><p>首先减去系统内存开销和其他进程开销，</p><ul><li><p>系统开销: 256*0.2=52G</p></li><li><p>DN开销: 2G</p></li><li><p>NM开销: 4G</p></li><li><p>Hbase开销: 暂无</p></li><li><p>升序内存容量: 256-52-2-4=198G</p></li></ul></li><li><p>确定每个container的物理核数量是4个，56/4=14个container容器</p></li><li><p>确定了最多分配14个container容器，每个容器的内存应该分配的容量是: 198/14==&gt;14G</p><p><strong>那么每个container的最大核数设置4，最大内存数设置14G</strong></p></li></ol></li><li><p>假如该节点还有组件，比如hbase regionserver进程，那么该如何设置？</p><p>总容量减就完事了。    </p></li></ol><p>所有的配置信息在<code>yarn-default.xm</code>l文件中</p><h4 id="内存参数默认值"><a href="#内存参数默认值" class="headerlink" title="内存参数默认值:"></a>内存参数默认值:</h4><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>yarn.nodemanager.resource.memory-mb</td><td>-1</td><td>可以分配给容器的物理内存总量(以MB为单位)。</td></tr><tr><td>yarn.scheduler.minimum-allocation-mb</td><td>1024</td><td>RM上每个容器请求的最小分配</td></tr><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>8192</td><td>RM上每个容器请求的最大分配</td></tr></tbody></table><h4 id="核数参数默认值"><a href="#核数参数默认值" class="headerlink" title="核数参数默认值:"></a>核数参数默认值:</h4><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>yarn.nodemanager.resource.cpu-vcores</td><td>-1</td><td>可以为容器分配的vcore总数量。</td></tr><tr><td>yarn.scheduler.minimum-allocation-vcores</td><td>1</td><td>RM上每个容器请求的最小虚拟CPU核心分配。</td></tr><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>4</td><td>RM上每个容器请求的最大虚拟CPU核心分配。</td></tr></tbody></table></li></ol><h3 id="Yarn的三种调度器"><a href="#Yarn的三种调度器" class="headerlink" title="Yarn的三种调度器"></a>Yarn的三种调度器</h3><ul><li><p>Apache hadoop2.x的默认调度器是Capacity Scheduler(计算调度器)</p></li><li><p>CDH的默认调度器是Fair Scheduler(公平调度器)</p></li></ul><h4 id="Yarn三种调度策略对比"><a href="#Yarn三种调度策略对比" class="headerlink" title="Yarn三种调度策略对比"></a>Yarn三种调度策略对比</h4><p>在Yarn中有三种调度器可以选择：FIFO Scheduler ，Capacity Scheduler，FairScheduler。</p><ol><li><p>FIFO Scheduler</p><p><img src="https://img-blog.csdnimg.cn/20181101090612286.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MzgyNDUz,size_16,color_FFFFFF,t_70" alt="队列调度器"></p><p>FIFO Scheduler把应用按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的应用进行分配资源，待最头上的应用需求满足后再给下一个分配，以此类推。</p><p>FIFO Scheduler是最简单也是最容易理解的调度器，也不需要任何配置，但它并不适用于共享集群。大的应用可能会占用所有集群资源，这就导致其它应用被阻塞。在共享集群中，更适合采用Capacity Scheduler或Fair Scheduler，这两个调度器都允许大任务和小任务在提交的同时获得一定的系统资源。从图中可以看出，在FIFO 调度器中，小任务会被大任务阻塞。</p></li><li><p>Capacity Scheduler</p><p><img src="https://img-blog.csdnimg.cn/20181101091012607.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MzgyNDUz,size_16,color_FFFFFF,t_70" alt="计算调度器"></p><p>而对于Capacity调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间。</p></li><li><p>Fair Scheduler</p><p><img src="https://img-blog.csdnimg.cn/20181101091843173.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MzgyNDUz,size_16,color_FFFFFF,t_70" alt="公平调度器"></p><p>在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。如上图所示，当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。</p><p>需要注意的是，在上图Fair调度器中，从第二个任务提交到获得资源会有一定的延迟，因为它需要等待第一个任务释放占用的Container。小任务执行完成之后也会释放自己占用的资源，大任务又获得了全部的系统资源。最终的效果就是Fair调度器即得到了高的资源利用率又能保证小任务及时完成。</p></li></ol><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn application -kill &lt;Application ID&gt;#杀死进程</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> Yarn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> Yarn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>迁移单节点MySQL到主备MySQL节点遇到的数据重刷故障案例</title>
      <link href="/2018/10/11/PE/3/"/>
      <url>/2018/10/11/PE/3/</url>
      
        <content type="html"><![CDATA[<p><strong>起因:</strong></p><p>是单节点 mysql 迁移到 主备mysql ，测试环境测试了业务数据，但是没有测试功能，如发送短信。</p><p><strong>过程:</strong></p><p>MySQL的上游接着有Kafka和Redis，这两个组件中存着有最近的历史数据，在生产环境下数据重刷，导致短信数据重发，临时解决办法是马上kill进程，想到了将历史数据重刷，但是将所有短信发送到测试通道。</p><p><strong>启发:</strong></p><p>测试环境一定要100%模拟生产环境，如果有模拟不到的地方，一定要千万小心。</p>]]></content>
      
      
      <categories>
          
          <category> 生产故障案例 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生产故障案例 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MR的执行流程&amp;初探文件压缩&amp;初探文件格式&amp;分片数与任务数&amp;shuffle的执行流程&amp;WordCount的执行流程</title>
      <link href="/2018/10/10/hadoop/7/"/>
      <url>/2018/10/10/hadoop/7/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 mr on yarn流程 </li><li>整理 文件格式有哪些 优缺点 </li><li>整理 压缩格式有哪些 优缺点 </li><li>spilt–&gt;map task关系 </li><li>wordcount的剖解图 </li><li>shuffle的理解 </li></ol><h3 id="mr-on-yarn流程"><a href="#mr-on-yarn流程" class="headerlink" title="mr on yarn流程"></a>mr on yarn流程</h3><p><img src="https://yerias.github.io/hadoop_img/rm_on_yarn.JPG" alt="rm_on_yarn"></p><h4 id="mr-on-yarn的工作流程简略分为两步"><a href="#mr-on-yarn的工作流程简略分为两步" class="headerlink" title="mr on yarn的工作流程简略分为两步:"></a>mr on yarn的工作流程简略分为两步:</h4><ol><li>启动应用程序管理器，申请资源。</li><li>运行任务，直到任务运行完成。</li></ol><h4 id="mr-on-yarn的工作流程详细分为八步"><a href="#mr-on-yarn的工作流程详细分为八步" class="headerlink" title="mr on yarn的工作流程详细分为八步:"></a>mr on yarn的工作流程详细分为八步:</h4><ol><li>用户向资源管理器(ResourceManager)提交作业，作业包括MapReduce应用程序管理器，启动MapReduce应用程序管理器的程序和用户自己编写的MapReduce程序。用于提交的所有作业都由ApplicationManager(全局应用程序管理器)管理。</li><li>资源管理器为该应用程序分配一个容器(Container)，并与对应的节点管理器(NodeManager)通信，要求它在这个容器中启动MapReduce应用程序管理器。</li><li>MapReduce应用程序管理器首先向资源管理器注册，这样用户可以直接通过资源管理器查看应用程序的运行状态，然后它将为各个任务申请资源，并监控他们的运行状态，直到运行结束，即重复步骤4-7。</li><li>MapReduce应用程序管理器采用轮询的方式通过RPC协议向资源管理器申请和领取资源。</li><li>MapReduce应用程序管理器申请到资源后，便与对应的节点管理器通信，要求启动任务。</li><li>节点管理器为任务设置好运行环境，包括环境变量、Jar包、二进制程序等，然后将任务启动命令写到另外一个脚本中，并通过该脚本启动任务。</li><li>各个任务通过RPC协议向MapReduce应用程序管理器汇报自己的状态和进度，MapReduce应用程序随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。在应用程序运行过程中，用户可以随时通过RPC协议向MapReduce应用程序管理器查询应用程序当前的运行状态。</li><li>应用程序运行完成后，MapReduce应用程序管理器向资源管理器注销并关闭自己。</li></ol><h3 id="文件格式有哪些-优缺点"><a href="#文件格式有哪些-优缺点" class="headerlink" title="文件格式有哪些 优缺点"></a>文件格式有哪些 优缺点</h3><p><strong>Hadoop中的文件格式大致上分为面向行和面向列两类：</strong></p><ol><li><p>面向行：同一行的数据存储在一起，即连续存储。SequenceFile,MapFile,Avro Datafile。采用这种方式，如果只需要访问行的一小部分数据，亦需要将整行读入内存，推迟序列化一定程度上可以缓解这个问题，但是从磁盘读取整行数据的开销却无法避免。面向行的存储适合于整行数据需要同时处理的情况。</p></li><li><p>面向列：整个文件被切割为若干列数据，每一列数据一起存储。Parquet , RCFile,ORCFile。面向列的格式使得读取数据时，可以跳过不需要的列，适合于只处于行的一小部分字段的情况。但是这种格式的读写需要更多的内存空间，因为需要缓存行在内存中（为了获取多行中的某一列）。同时不适合流式写入，因为一旦写入失败，当前文件无法恢复，而面向行的数据在写入失败时可以重新同步到最后一个同步点，所以Flume采用的是面向行的存储格式。</p></li></ol><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-0c5b3f7a2eceaaef.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="普通二维表"></p><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-dbe2595ee1e293b1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="行存储和列存储"></p><p>下面介绍几种相关的文件格式，它们在Hadoop体系上被广泛使用：</p><h4 id="SequenceFile"><a href="#SequenceFile" class="headerlink" title="SequenceFile"></a>SequenceFile</h4><p>SequenceFile是Hadoop API 提供的一种二进制文件,它将数据以的形式序列化到文件中。这种二进制文件内部使用Hadoop 的标准的Writable 接口实现序列化和反序列化。它与Hadoop API中的MapFile 是互相兼容的。Hive 中的SequenceFile 继承自Hadoop API 的SequenceFile,不过它的key为空,使用value 存放实际的值, 这样是为了避免MR 在运行map 阶段的排序过程。如果你用Java API 编写SequenceFile,并让Hive 读取的话,请确保使用value字段存放数据,否则你需要自定义读取这种SequenceFile 的InputFormat class 和OutputFormat class。</p><p>SequenceFile的文件结构如下：</p><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-3f5cd8d90742ec24.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="SequenceFile详解图1"></p><p>根据是否压缩，以及采用记录压缩还是块压缩，存储格式有所不同：</p><ul><li><p>不压缩:</p><p>按照记录长度、Key长度、Value程度、Key值、Value值依次存储。长度是指字节数。采用指定的Serialization进行序列化。</p></li><li><p>Record压缩:</p><p>只有value被压缩，压缩的codec保存在Header中。</p></li><li><p>Block压缩:</p><p>多条记录被压缩在一起，可以利用记录之间的相似性，更节省空间。Block前后都加入了同步标识。Block的最小值由io.seqfile.compress.blocksize属性设置。 </p></li></ul><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-d21745547eb4c021.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="SequenceFile详解图1"></p><h4 id="Avro"><a href="#Avro" class="headerlink" title="Avro"></a>Avro</h4><p>Avro是一种用于支持数据密集型的二进制文件格式。它的文件格式更为紧凑,若要读取大量数据时,Avro能够提供更好的序列化和反序列化性能。并 且Avro数据文件天生是带Schema定义的,所以它不需要开发者在API 级别实现自己的Writable对象。最近多个Hadoop 子项目都支持Avro 数据格式,如Pig 、Hive、Flume、Sqoop和Hcatalog。</p><h4 id="RCFile"><a href="#RCFile" class="headerlink" title="RCFile"></a>RCFile</h4><p>RCFile是Hive推出的一种专门面向列的数据格式。 它遵循“先按列划分,再垂直划分”的设计理念。当查询过程中,针对它并不关心的列时,它会在IO上跳过这些列。需要说明的是,RCFile在map阶段从 远端拷贝仍然是拷贝整个数据块,并且拷贝到本地目录后RCFile并不是真正直接跳过不需要的列,并跳到需要读取的列, 而是通过扫描每一个row group的头部定义来实现的,但是在整个HDFS Block 级别的头部并没有定义每个列从哪个row group起始到哪个row group结束。所以在读取所有列的情况下,RCFile的性能反而没有SequenceFile高。</p><p>Hive的Record Columnar File,这种类型的文件先将数据按行划分成Row Group，在Row Group内部，再将数据按列划分存储。其结构如下：</p><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-0a6f19b8bb6ee4e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/688/format/webp" alt="RCFile详解图1"></p><p>相比较于单纯地面向行和面向列：</p><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-0df474935c56807d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/649/format/webp" alt="RCFile详解图2"></p><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-6d56c39e3445288e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/615/format/webp" alt="RCFile详解图3"></p><h4 id="ORC"><a href="#ORC" class="headerlink" title="ORC"></a>ORC</h4><p>ORC（Optimized Record Columnar File)提供了一种比RCFile更加高效的文件格式。其内部将数据划分为默认大小为250M的Stripe。每个Stripe包括索引、数据和Footer。索引存储每一列的最大最小值，以及列中每一行的位置。</p><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-1bb66728d866b469.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/580/format/webp" alt="ORCFile详解图"></p><p>在Hive中，如下命令用于使用ORCFile：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE ... STORED AS ORC</span><br><span class="line"></span><br><span class="line">ALTER TABLE ... SET FILEFORMAT ORC</span><br><span class="line"></span><br><span class="line">SET hive.default.fileformat=ORC</span><br></pre></td></tr></table></figure><h4 id="Parquet"><a href="#Parquet" class="headerlink" title="Parquet"></a>Parquet</h4><p>一种通用的面向列的存储格式，基于Google的Dremel。特别擅长处理深度嵌套的数据。</p><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-b45e32049ab54cbb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1124/format/webp" alt="img"></p><p>对于嵌套结构，Parquet将其转换为平面的列存储，嵌套结构通过Repeat Level和Definition Level来表示（R和D），在读取数据重构整条记录的时候，使用元数据重构记录的结构。下面是R和D的一个例子：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">AddressBook &#123;</span><br><span class="line"> contacts: &#123;</span><br><span class="line">  phoneNumber: &quot;555 987 6543&quot;</span><br><span class="line"> &#125;</span><br><span class="line"> contacts: &#123;</span><br><span class="line">&#125;&#125;</span><br><span class="line">AddressBook &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-36b68fc1d8e2b99b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/577/format/webp" alt="Parquet详解图"></p><h4 id="文件存储大小比较与分析"><a href="#文件存储大小比较与分析" class="headerlink" title="文件存储大小比较与分析"></a>文件存储大小比较与分析</h4><p>我们选取一个TPC-H标准测试来说明不同的文件格式在存储上的开销。因为此数据是公开的,所以读者如果对此结果感兴趣,也可以对照后面的实验自行 做一遍。Orders 表文本格式的原始大小为1.62G。 我们将其装载进Hadoop 并使用Hive 将其转化成以上几种格式,在同一种LZO 压缩模式下测试形成的文件的大小</p><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-34e05b3cb0e72740.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/849/format/webp" alt="文件存储大小比较与分析"></p><h4 id="不同格式文件大小对比"><a href="#不同格式文件大小对比" class="headerlink" title="不同格式文件大小对比"></a>不同格式文件大小对比</h4><ul><li><p>从上述实验结果可以看到,SequenceFile无论在压缩和非压缩的情况下都比原始纯文本TextFile大,其中非压缩模式下大11%, 压缩模式下大6.4%。这跟SequenceFile的文件格式的定义有关: SequenceFile在文件头中定义了其元数据,元数据的大小会根据压缩模式的不同略有不同。一般情况下,压缩都是选取block 级别进行的,每一个block都包含key的长度和value的长度,另外每4K字节会有一个sync-marker的标记。对于TextFile文件格 式来说不同列之间只需要用一个行间隔符来切分,所以TextFile文件格式比SequenceFile文件格式要小。但是TextFile 文件格式不定义列的长度,所以它必须逐个字符判断每个字符是不是分隔符和行结束符。因此TextFile 的反序列化开销会比其他二进制的文件格式高几十倍以上。</p></li><li><p>RCFile文件格式同样也会保存每个列的每个字段的长度。但是它是连续储存在头部元数据块中,它储存实际数据值也是连续的。另外RCFile 会每隔一定块大小重写一次头部的元数据块(称为row group,由hive.io.rcfile.record.buffer.size控制,其默认大小为4M),这种做法对于新出现的列是必须的,但是如 果是重复的列则不需要。RCFile 本来应该会比SequenceFile 文件大,但是RCFile 在定义头部时对于字段长度使用了Run Length Encoding进行压缩,所以RCFile 比SequenceFile又小一些。Run length Encoding针对固定长度的数据格式有非常高的压缩效率,比如Integer、Double和Long等占固定长度的数据类型。在此提一个特例—— Hive 0.8引入的TimeStamp 时间类型,如果其格式不包括毫秒,可表示为”YYYY-MM-DD HH:MM:SS”,那么就是固定长度占8个字节。如果带毫秒,则表示为”YYYY-MM-DD HH:MM:SS.fffffffff”,后面毫秒的部分则是可变的。</p></li><li><p>Avro文件格式也按group进行划分。但是它会在头部定义整个数据的模式(Schema), 而不像RCFile那样每隔一个row group就定义列的类型,并且重复多次。另外,Avro在使用部分类型的时候会使用更小的数据类型,比如Short或者Byte类型,所以Avro的数 据块比RCFile 的文件格式块更小。</p></li></ul><h3 id="压缩格式有哪些-优缺点"><a href="#压缩格式有哪些-优缺点" class="headerlink" title="压缩格式有哪些 优缺点"></a>压缩格式有哪些 优缺点</h3><h4 id="压缩的好处和坏处"><a href="#压缩的好处和坏处" class="headerlink" title="压缩的好处和坏处"></a>压缩的好处和坏处</h4><p><strong>好处</strong></p><ul><li>减少存储磁盘空间</li><li>降低IO(网络的IO和磁盘的IO)</li><li>加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度</li></ul><p><strong>坏处</strong></p><ul><li>由于使用数据时，需要先将数据解压，加重CPU负荷</li></ul><h4 id="压缩格式"><a href="#压缩格式" class="headerlink" title="压缩格式"></a>压缩格式</h4><p><img src="https://ruozedata.github.io/assets/blogImg/%E5%8E%8B%E7%BC%A91.png" alt="压缩格式"></p><p><img src="https://ruozedata.github.io/assets/blogImg/%E5%8E%8B%E7%BC%A92.png" alt="压缩空间比较"></p><p><img src="https://ruozedata.github.io/assets/blogImg/yasuo3.png" alt="压缩时间比较"></p><p>可以看出，压缩空间比值越高，压缩时间越长，压缩比：<code>Snappy&gt;LZ4&gt;LZO&gt;GZIP&gt;BZIP2</code></p><table><thead><tr><th align="left">压缩格式</th><th align="left">优点</th><th align="left">缺点</th><th></th></tr></thead><tbody><tr><td align="left"><strong>gzip</strong></td><td align="left">压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便</td><td align="left">不支持split</td><td></td></tr><tr><td align="left"><strong>lzo</strong></td><td align="left">压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便</td><td align="left">压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）</td><td></td></tr><tr><td align="left"><strong>snappy</strong></td><td align="left">压缩速度快；支持hadoop native库</td><td align="left">不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令d. bzip2</td><td></td></tr><tr><td align="left"><strong>bzip2</strong></td><td align="left">支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便</td><td align="left">压缩/解压速度慢；不支持native</td><td></td></tr></tbody></table><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>不同的场景选择不同的压缩方式，肯定没有一个一劳永逸的方法，如果选择高压缩比，那么对于cpu的性能要求要高，同时压缩、解压时间耗费也多；选择压缩比低的，对于磁盘io、网络io的时间要多，空间占据要多；对于支持分割的，可以实现并行处理。</p><h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><p>一般在HDFS 、Hive、HBase中会使用；<br>当然一般较多的是结合Spark 来一起使用。</p><h3 id="Spilt–-gt-Map-Task关系"><a href="#Spilt–-gt-Map-Task关系" class="headerlink" title="Spilt–&gt;Map Task关系"></a>Spilt–&gt;Map Task关系</h3><p>Reduce Task默认是1个，Map Task默认是2个，但是实际运行场景下，Map Task的个数和切片的个数保持一致，而切片的个数又与文件数和文件大小相关联。切片默认大小决定文件被分成多少个切片，执行多少个Map Task。</p><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>mapreduce.job.maps</td><td>2</td><td>The default number of map tasks per job.</td></tr><tr><td>mapreduce.job.reduces</td><td>1</td><td>The default number of reduce tasks per job.</td></tr></tbody></table><h3 id="shuffle的理解"><a href="#shuffle的理解" class="headerlink" title="shuffle的理解"></a>shuffle的理解</h3><p>俩字: 洗牌</p><p>shuffle阶段又可以分为Map端的shuff和reduce端的shuffle</p><p><img src="https:/yerias.github.io/hadoop_img/shuffer.jpg" alt="shuffe过程"></p><h4 id="map端的shuffle"><a href="#map端的shuffle" class="headerlink" title="map端的shuffle"></a>map端的shuffle</h4><ul><li>map端会处理出入数据并产生中间结果，这个中间结果会写到本地磁盘，而不是HDFS。每个map的输出会先写到内存缓冲区中，当写入的数据达到设定的阈值时，系统将会启动一个线程将缓冲区的数据写到磁盘，这个过程叫做spill(溢写)。</li><li>在spill之前，会先进行两次排序，首先根据数据所属的partition进行排序，然后每个partition中的数据再按key来排序，partition的目的是将记录划分到不同的reduce上，以期望能达到负载均衡，以后的reduce就会根据partition来读取自己对应的数据。接着运行combiner(如果设置了的话)，combiner的本质也是一个reduce，其目的是对将要写入到磁盘的文件先进行一次处理，这样，写入到磁盘的数据量就会减少。最后将数据写到本地磁盘产生spill文件(spill文件保存在{mapred.local.dir}指定的目录中，map任务结束后就会被删除)。</li><li>最后，每个map任务可能产生多个spill文件，在每个map任务完成前，会通过多路归并算法将这些spill文件合并成一个文件。至此，map的shuffle过程就结束了。</li></ul><h4 id="reduce端的shuffle"><a href="#reduce端的shuffle" class="headerlink" title="reduce端的shuffle"></a>reduce端的shuffle</h4><ul><li>reduce端的shuffle主要包括三个阶段，copy、sort(merge)和reduce</li><li>首先将map端产生的输出文件拷贝到reduce端，但每个reduce如何知道自己应该处理哪些数据呢？因为map端进行partition的时候，实际上就相当于指定了每个reduce要处理的数据(partition就对应了reduce)，所以reduce在拷贝的数据的时候只需拷贝与自己对应的partition中的数据即可。每个reduce会处理一个或多个partiton，但需要先将自己对应的partition中的数据从每个map的输出结果中拷贝出来。</li><li>接下来就是sort阶段，也称为merge阶段，因为这个阶段的主要工作是执行了归并排序。从map端拷贝到reduce端的数据都是有序的，所以很适合归并排序。最终在reduce端生产一个较大的文件作为reduce的输入。</li><li>最后就是reduce阶段了，在这个过程中产生最终的输出结果，并将其写到HDFS上。</li></ul><h3 id="WordCount的剖解图"><a href="#WordCount的剖解图" class="headerlink" title="WordCount的剖解图"></a>WordCount的剖解图</h3><p><img src="https:/yerias.github.io/hadoop_img/wordcount.jpg" alt="wordcount执行流程"></p><h4 id="Map任务处理"><a href="#Map任务处理" class="headerlink" title="Map任务处理"></a>Map任务处理</h4><ol><li>读取HDFS中的文件，每一行解析成一个&lt;K,V&gt;值。每个键值对调用一次map函数。</li><li>重写map()方法，接收1产生的&lt;K,V&gt;值进行处理，转为新的&lt;K,V&gt;输出。</li><li>对2输出的&lt;K,V&gt;值进行分区，默认一盒分区。</li><li>对不同分区中的数据进行排序(按照K)、分组。分组指的是相同Key的Value放到一个集合中。</li><li>(可选)对分组后的数据进行合并。</li></ol><h4 id="Reduce任务处理"><a href="#Reduce任务处理" class="headerlink" title="Reduce任务处理"></a>Reduce任务处理</h4><ol><li>多个Map任务的输出，按照不同的分区，通过网络copy到不同的Reduce节点上。</li><li>对多个map的输出进行合并、排序。重写reduce()方法，接收的是分组后的数据，实现自己的业务逻辑，处理后产生新的&lt;K,V&gt;值输出</li><li>对reduce输出的&lt;K,V&gt;写到HDFS中。</li></ol><hr><p>整理来自：<a href="https://ruozedata.github.io/2018/04/18/、https://www.jianshu.com/p/43630456a18a" target="_blank" rel="noopener">https://ruozedata.github.io/2018/04/18/、https://www.jianshu.com/p/43630456a18a</a></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> MapReduce </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MapReduce </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS Block损坏恢复</title>
      <link href="/2018/10/10/PE/2/"/>
      <url>/2018/10/10/PE/2/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">上传:</span><br><span class="line">-bash-4.2$ hdfs dfs -mkdir /blockrecover</span><br><span class="line">-bash-4.2$ echo &quot;www.ruozedata.com&quot; &gt; ruozedata.md</span><br><span class="line"></span><br><span class="line">-bash-4.2$ hdfs dfs -put ruozedata.md /blockrecover</span><br><span class="line">-bash-4.2$ hdfs dfs -ls /blockrecover</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 hdfs supergroup         18 2019-03-03 14:42 /blockrecover/ruozedata.md</span><br><span class="line">-bash-4.2$ </span><br><span class="line"></span><br><span class="line">校验: 健康状态</span><br><span class="line">-bash-4.2$ hdfs fsck /</span><br><span class="line">Connecting to namenode via http://yws76:50070/fsck?ugi=hdfs&amp;path=%2F</span><br><span class="line">FSCK started by hdfs (auth:SIMPLE) from /192.168.0.76 for path / at Sun Mar 03 14:44:44 CST 2019</span><br><span class="line">...............................................................................Status: HEALTHY</span><br><span class="line"> Total size:    50194618424 B</span><br><span class="line"> Total dirs:    354</span><br><span class="line"> Total files:   1079</span><br><span class="line"> Total symlinks:                0</span><br><span class="line"> Total blocks (validated):      992 (avg. block size 50599413 B)</span><br><span class="line"> Minimally replicated blocks:   992 (100.0 %)</span><br><span class="line"> Over-replicated blocks:        0 (0.0 %)</span><br><span class="line"> Under-replicated blocks:       0 (0.0 %)</span><br><span class="line"> Mis-replicated blocks:         0 (0.0 %)</span><br><span class="line"> Default replication factor:    3</span><br><span class="line"> Average block replication:     3.0</span><br><span class="line"> Corrupt blocks:                0</span><br><span class="line"> Missing replicas:              0 (0.0 %)</span><br><span class="line"> Number of data-nodes:          3</span><br><span class="line"> Number of racks:               1</span><br><span class="line">FSCK ended at Sun Mar 03 14:44:45 CST 2019 in 76 milliseconds</span><br><span class="line"></span><br><span class="line">The filesystem under path &apos;/&apos; is HEALTHY</span><br><span class="line">-bash-4.2$</span><br></pre></td></tr></table></figure><h3 id="直接DN节点上删除文件一个block的一个副本-3副本"><a href="#直接DN节点上删除文件一个block的一个副本-3副本" class="headerlink" title="直接DN节点上删除文件一个block的一个副本(3副本)"></a>直接DN节点上删除文件一个block的一个副本(3副本)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">删除块和meta文件:</span><br><span class="line">[root@yws87 subdir135]# rm -rf blk_1075808214 blk_1075808214_2068515.meta</span><br><span class="line"></span><br><span class="line">直接重启HDFS，直接模拟损坏效果，然后fsck检查:</span><br><span class="line">-bash-4.2$ hdfs fsck /</span><br><span class="line">Connecting to namenode via http://yws77:50070/fsck?ugi=hdfs&amp;path=%2F</span><br><span class="line">FSCK started by hdfs (auth:SIMPLE) from /192.168.0.76 for path / at Sun Mar 03 16:02:04 CST 2019</span><br><span class="line">.</span><br><span class="line">/blockrecover/ruozedata.md:  Under replicated BP-1513979236-192.168.0.76-1514982530341:blk_1075808214_2068515. Target Replicas is 3 but found 2 live replica(s), 0 decommissioned replica(s), 0 decommissioning replica(s).</span><br><span class="line">...............................................................................Status: HEALTHY</span><br><span class="line"> Total size:    50194618424 B</span><br><span class="line"> Total dirs:    354</span><br><span class="line"> Total files:   1079</span><br><span class="line"> Total symlinks:                0</span><br><span class="line"> Total blocks (validated):      992 (avg. block size 50599413 B)</span><br><span class="line"> Minimally replicated blocks:   992 (100.0 %)</span><br><span class="line"> Over-replicated blocks:        0 (0.0 %)</span><br><span class="line"> Under-replicated blocks:       1 (0.10080645 %)</span><br><span class="line"> Mis-replicated blocks:         0 (0.0 %)</span><br><span class="line"> Default replication factor:    3</span><br><span class="line"> Average block replication:     2.998992</span><br><span class="line"> Corrupt blocks:                0</span><br><span class="line"> Missing replicas:              1 (0.033602152 %)</span><br><span class="line"> Number of data-nodes:          3</span><br><span class="line"> Number of racks:               1</span><br><span class="line">FSCK ended at Sun Mar 03 16:02:04 CST 2019 in 148 milliseconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The filesystem under path &apos;/&apos; is HEALTHY</span><br><span class="line">-bash-4.2$</span><br></pre></td></tr></table></figure><h3 id="手动修复hdfs-debug"><a href="#手动修复hdfs-debug" class="headerlink" title="手动修复hdfs debug"></a>手动修复hdfs debug</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-bash-4.2$ hdfs |grep debug</span><br><span class="line">没有输出debug参数的任何信息结果！</span><br><span class="line">故hdfs命令帮助是没有debug的，但是确实有hdfs debug这个组合命令，切记。</span><br><span class="line"></span><br><span class="line">修复命令:</span><br><span class="line">-bash-4.2$ hdfs debug  recoverLease  -path /blockrecover/ruozedata.md -retries 10</span><br><span class="line">recoverLease SUCCEEDED on /blockrecover/ruozedata.md</span><br><span class="line">-bash-4.2$ </span><br><span class="line"></span><br><span class="line">直接DN节点查看，block文件和meta文件恢复:</span><br><span class="line">[root@yws87 subdir135]# ll</span><br><span class="line">total 8</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 56 Mar  3 14:28 blk_1075808202</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 14:28 blk_1075808202_2068503.meta</span><br><span class="line">[root@yws87 subdir135]# ll</span><br><span class="line">total 24</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 56 Mar  3 14:28 blk_1075808202</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 14:28 blk_1075808202_2068503.meta</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 18 Mar  3 15:23 blk_1075808214</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 15:23 blk_1075808214_2068515.meta</span><br></pre></td></tr></table></figure><h3 id="自动修复"><a href="#自动修复" class="headerlink" title="自动修复"></a>自动修复</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">当数据块损坏后，DN节点执行directoryscan操作之前，都不会发现损坏；</span><br><span class="line">也就是directoryscan操作是间隔6h</span><br><span class="line">dfs.datanode.directoryscan.interval : 21600</span><br><span class="line"></span><br><span class="line">在DN向NN进行blockreport前，都不会恢复数据块;</span><br><span class="line">也就是blockreport操作是间隔6h</span><br><span class="line">dfs.blockreport.intervalMsec : 21600000</span><br><span class="line"></span><br><span class="line">当NN收到blockreport才会进行恢复操作。</span><br></pre></td></tr></table></figure><p>具体参考生产上HDFS（CDH5.12.0）对应的版本的文档参数:<a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.12.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.12.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>生产上本人一般倾向于使用 手动修复方式，但是前提要手动删除损坏的block块。</p><p>切记，是删除损坏block文件和meta文件，而不是删除hdfs文件。</p><p>当然还可以先把文件get下载，然后hdfs删除，再对应上传。</p><p>切记删除不要执行: hdfs fsck / -delete 这是删除损坏的文件， 那么数据不就丢了嘛；除非无所谓丢数据，或者有信心从其他地方可以补数据到hdfs！</p><h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><ul><li>那么如何确定一个文件的损失的块位置，哪几种方法呢？</li><li>CDH的配置里搜索没有这两个参数，怎么调整生效呢？</li></ul><p>转载来源: [<a href="https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/]" target="_blank" rel="noopener">https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/]</a>(<a href="https://ruozedata.github.io/2019/06/06/生产HDFS" target="_blank" rel="noopener">https://ruozedata.github.io/2019/06/06/生产HDFS</a> Block损坏恢复最佳实践(含思考题)/)</p>]]></content>
      
      
      <categories>
          
          <category> 生产故障案例 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生产故障案例 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop的Pid文件</title>
      <link href="/2018/10/09/hadoop/6/"/>
      <url>/2018/10/09/hadoop/6/</url>
      
        <content type="html"><![CDATA[<h3 id="存储位置"><a href="#存储位置" class="headerlink" title="存储位置"></a>存储位置</h3><p>hadoop启动之后，pid文件是存储哪里？<br>我们可以通过查看 hadoop-env.sh文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat etc/hadoop/hadoop-env.sh从下图可以看出hadoop默认的pid文件是存储到/tmp目录的</span><br></pre></td></tr></table></figure><p>从下图可以看出hadoop默认的pid文件是存储到/tmp目录的</p><p>从下图可以看出hadoop默认的pid文件是存储到/tmp目录的</p><p><img src="https://img-blog.csdnimg.cn/20190729214437420.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="hadoop-env.sh文件"></p><p>从下图可以看出，后缀名是.pid的就是hadoop的pid文件</p><p><img src="https://img-blog.csdnimg.cn/20190729214915879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="/tmp目录"></p><h3 id="启动和停止"><a href="#启动和停止" class="headerlink" title="启动和停止"></a>启动和停止</h3><p>我们启动的时候，是执行sbin/start-df.sh文件，我们看一看这个文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190729215631410.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="start-df.sh文件"></p><p>从上面这个图可以看出，启动namenode节点的时候，调用了hadoop-daemons.sh文件了，我们再看看这个文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat sbin/hadoop-daemons.sh</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190729215911670.png" alt="hadoop-daemons.sh文件"></p><p>从上图可以看出，在最后一行又调用了hadoop-daemon.sh文件，我们在看看这个文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat sbin/hadoop-daemon.sh</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190729220412390.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="hadoop-daemon.sh文件"></p><p><strong>从上面两张图可以得出结论:</strong></p><ol><li>hadoop启动的时候，会生成pid文件，并把进程号写入到pid文件</li><li>hadoop停止的时候，会到pid文件中获取进程号，然后停止进程，最后删除pid文件</li></ol><p><strong>下面我们做一下验证：</strong></p><ol><li><p>看下namenode的进程号是不是和pid文件里的进程号一样</p><p><img src="https://img-blog.csdnimg.cn/20190729221743612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="start-jps命令"></p><p>从上图可以看出，进程号是一样的，说明我们前面的推理是正确的</p></li><li><p>我们把生成号的namenode的pid文件名字改一下，停止的时候脚本会找不到pid文件，也就不会停止namenode进程了</p><p><img src="https://img-blog.csdnimg.cn/20190729222300551.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="stop-jps命令"></p><p>从上图可以看出，我们的两个推理是正确的</p></li><li><p>tmp目录的弊端<br>linux的/tmp目录会自动清理一段时间没有访问的文件，一般都是30天，假如hadoop启动了30天以上，那么pid文件会被删除，再调用停止的时候会停止不了，生产上一般不会放在/tmp目录下，下面我们自己创建个目录存放pid文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">创建文件夹</span></span><br><span class="line">mkdir -p /data/tmp</span><br><span class="line"><span class="meta">#</span><span class="bash">赋予权限</span></span><br><span class="line">chmod 777 -R /data/tmp</span><br><span class="line">然后修改etc/hadoop/hadoop-env.sh文件</span><br></pre></td></tr></table></figure><p>然后修改etc/hadoop/hadoop-env.sh文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vi /etc/hadoop/hadoop-env.sh</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190730105216506.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="修改hadoop-env.sh文件的pid目录"></p><p>然后启动</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p>启动之后，我们查看pid文件</p><p><img src="https://img-blog.csdnimg.cn/20190730105343192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="新的pid目录"></p></li></ol><hr><p>原文链接：<a href="https://blog.csdn.net/u010452388/article/details/97686380" target="_blank" rel="noopener">https://blog.csdn.net/u010452388/article/details/97686380</a></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DataNode OOM溢出</title>
      <link href="/2018/10/09/PE/1/"/>
      <url>/2018/10/09/PE/1/</url>
      
        <content type="html"><![CDATA[<h3 id="DataNode的内存溢出报错"><a href="#DataNode的内存溢出报错" class="headerlink" title="DataNode的内存溢出报错"></a>DataNode的内存溢出报错</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2017-12-17 23:58:14,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1437036909-![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.36-1509097205664:blk_1074725940_987917, type=HAS_DOWNSTREAM_IN_PIPELINE terminating</span><br><span class="line">2017-12-17 23:58:31,425 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode is out of memory. Will retry in 30 seconds.</span><br><span class="line">java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">at java.lang.Thread.start0(Native Method)</span><br><span class="line">at java.lang.Thread.start(Thread.java:714)</span><br><span class="line">at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:154)</span><br><span class="line">at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">2017-12-17 23:59:01,426 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode is out of memory. Will retry in 30 seconds.</span><br><span class="line">java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">at java.lang.Thread.start0(Native Method)</span><br><span class="line">at java.lang.Thread.start(Thread.java:714)</span><br><span class="line">at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:154)</span><br><span class="line">at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">2017-12-17 23:59:05,520 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode is out of memory. Will retry in 30 seconds.</span><br><span class="line">java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">at java.lang.Thread.start0(Native Method)</span><br><span class="line">at java.lang.Thread.start(Thread.java:714)</span><br><span class="line">at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:154)</span><br><span class="line">at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">2017-12-17 23:59:31,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1437036909-![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.36-1509097205664:blk_1074725951_987928 src: /![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.54:40478 dest: /![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.48:50010</span><br></pre></td></tr></table></figure><h3 id="CDH查看DataNode的内存情况"><a href="#CDH查看DataNode的内存情况" class="headerlink" title="CDH查看DataNode的内存情况"></a>CDH查看DataNode的内存情况</h3><p><img src="https://yerias.github.io/hadoop_img/20191205092342.jpg" alt="datanode内存使用图"></p><h3 id="明明1G的内存都没有使用，为什么会报OOM？"><a href="#明明1G的内存都没有使用，为什么会报OOM？" class="headerlink" title="明明1G的内存都没有使用，为什么会报OOM？"></a>明明1G的内存都没有使用，为什么会报OOM？</h3><p>可以确定是操作系统哪里设置错了，我想应该是把产品环境的某个参数配置错了，系统本身的影响肯定不会有了，因为产品环境上我们只create了800左右个线程，就OOM了，那应该就是配置的问题了</p><p>解决方法：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1.</span><br><span class="line"></span><br><span class="line">echo "kernel.threads-max=196605" &gt;&gt; /etc/sysctl.conf</span><br><span class="line">echo "kernel.pid_max=196605" &gt;&gt; /etc/sysctl.conf</span><br><span class="line">echo "vm.max_map_count=393210" &gt;&gt; /etc/sysctl.conf</span><br><span class="line">sysctl -p </span><br><span class="line"></span><br><span class="line">2.</span><br><span class="line"></span><br><span class="line">/etc/security/limits.conf</span><br><span class="line">* soft nofile 196605</span><br><span class="line">* hard nofile 196605</span><br><span class="line">* soft nproc 196605</span><br><span class="line">* hard nproc 196605</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 生产故障案例 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生产故障案例 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop2.7.6之前和Hadoop2.8.4之后的副本存放策略</title>
      <link href="/2018/10/08/hadoop/5/"/>
      <url>/2018/10/08/hadoop/5/</url>
      
        <content type="html"><![CDATA[<h3 id="新旧版本的副本存放策略比较"><a href="#新旧版本的副本存放策略比较" class="headerlink" title="新旧版本的副本存放策略比较"></a>新旧版本的副本存放策略比较</h3><p>Hadoop2.7.6及以下版本是按照旧的策略进行副本存放的，官网文档描述如下：</p><p><img src="https://img-blog.csdnimg.cn/20191017161508850.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FjY3B0YW5nZ2FuZw==,size_16,color_FFFFFF,t_70" alt="旧版本的副本存放策略"></p><p>在常见情况下，当复制因子为3时，HDFS的放置策略是将一个副本放置在本地机架中的一个节点上，将另一个副本放置在本地机架中的另一个节点上，最后一个副本放置在不同机架中的另一个节点上。</p><p>Hadoop2.8.4及以上版本是按照新的策略进行副本存放的，官网文档描述如下：  </p><p><img src="https://img-blog.csdnimg.cn/20191017161742230.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FjY3B0YW5nZ2FuZw==,size_16,color_FFFFFF,t_70" alt="新版本的副本存放策略"></p><p>在常见情况下，当复制因子为3时，HDFS的放置策略是：如果写入器在数据节点上，则将一个副本放置在本地计算机上；否则，在随机数据节点上，HDFS将另一个副本放置在不同（远程）机架中的节点上的，最后一个位于同一远程机架中的其他节点上。</p><h3 id="新版本的副本存放策略思想"><a href="#新版本的副本存放策略思想" class="headerlink" title="新版本的副本存放策略思想"></a>新版本的副本存放策略思想</h3><p>最后，再把新版本的副本存放策略的基本思想描述如下：</p><p>第一个副本存放Client所在的节点上（假设Client不在集群的范围内，则第一个副本存储节点是随机选取的。当然系统会不选择那些太满或者太忙的节点）</p><p>第二个副本存放在与第一个节点不同机架中的一个节点上。</p><p>第三个副本和第二个在同一个机架，随机放在不同的节点上。</p><p>如果还有很多其他的副本就随机放在集群中的各个节点上。</p><hr><p>原文链接：<a href="https://blog.csdn.net/accptanggang/article/details/102609318" target="_blank" rel="noopener">https://blog.csdn.net/accptanggang/article/details/102609318</a></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux下MySQL进程死掉的可能解决方案</title>
      <link href="/2018/10/08/mysql/5/"/>
      <url>/2018/10/08/mysql/5/</url>
      
        <content type="html"><![CDATA[<p>linux下mysql进程死掉，且无法启动mysql服务，查看myql日志，发现如下日志：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">2019-10-10 18:11:03 9772 [Note] InnoDB: Initializing buffer pool, size = 128.0M</span><br><span class="line">InnoDB: mmap(136019968 bytes) failed; errno 12</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] InnoDB: Cannot allocate memory for the buffer pool</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Plugin 'InnoDB' init function returned error.</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Unknown/unsupported storage engine: InnoDB</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Aborting</span><br></pre></td></tr></table></figure><p>其中InnoDB: mmap(136019968 bytes) failed; errno 12是关键的错误信息。<br>从网上查资料，有人说修改innodb_buffer_pool_size，经过测试无效。<br>有人说是swap分区为0导致的此错误，使用free -m命令查看系统内存，发现swap确实为0。使用如下命令建立一个临时的swap分区：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">d if=/dev/zero of=/swap bs=1M count=512  //创建一个swap文件，大小为512M</span><br><span class="line">mkswap /swap                              //将swap文件变为swap分区文件</span><br><span class="line">swapon /swap                              //将其映射为swap分区</span><br></pre></td></tr></table></figure><p>此时使用<code>free -m</code>命令即可看到swap分区已存在了，然后启动mysql服务即可。<br>为了保证下次系统启动后，此swap分区被自动加载，需要修改系统的fstab文件，操作如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">vi /etc/fstab</span><br><span class="line">//在其中添加如下一行</span><br><span class="line">/swap swap swap defaults 0 0</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL中的Top N</title>
      <link href="/2018/10/08/mysql/4/"/>
      <url>/2018/10/08/mysql/4/</url>
      
        <content type="html"><![CDATA[<h3 id="切入点"><a href="#切入点" class="headerlink" title="切入点"></a>切入点</h3><p>MySQL没有获取Top N的这种函数，但是在MySQL中求Top N又是必须掌握的点</p><p>比如查询分组后的最大值、最小值所在的整行记录或者分组后的Top N行记录</p><p>下面我们就如何在MySQL中求Top N做出深度的思考和验证</p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>测试表结构如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">&gt;&gt; CREATE TABLE `student` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `name` varchar(20) DEFAULT NULL,</span><br><span class="line">  `course` varchar(20) DEFAULT NULL,</span><br><span class="line">  `score` int(11) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8</span><br></pre></td></tr></table></figure><p> 插入数据：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">&gt;&gt; insert into student(name,course,score)</span><br><span class="line"><span class="keyword">values</span></span><br><span class="line">(<span class="string">'张三'</span>,<span class="string">'语文'</span>,<span class="number">80</span>),</span><br><span class="line">(<span class="string">'李四'</span>,<span class="string">'语文'</span>,<span class="number">90</span>),</span><br><span class="line">(<span class="string">'王五'</span>,<span class="string">'语文'</span>,<span class="number">93</span>),</span><br><span class="line">(<span class="string">'张三'</span>,<span class="string">'数学'</span>,<span class="number">77</span>),</span><br><span class="line">(<span class="string">'李四'</span>,<span class="string">'数学'</span>,<span class="number">68</span>),</span><br><span class="line">(<span class="string">'王五'</span>,<span class="string">'数学'</span>,<span class="number">99</span>),</span><br><span class="line">(<span class="string">'张三'</span>,<span class="string">'英语'</span>,<span class="number">90</span>),</span><br><span class="line">(<span class="string">'李四'</span>,<span class="string">'英语'</span>,<span class="number">50</span>),</span><br><span class="line">(<span class="string">'王五'</span>,<span class="string">'英语'</span>,<span class="number">89</span>);</span><br></pre></td></tr></table></figure><p>查看结果：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">&gt;&gt; select * from student;</span><br><span class="line">+<span class="comment">----+--------+--------+-------+</span></span><br><span class="line">| id | name   | course | score |</span><br><span class="line">+<span class="comment">----+--------+--------+-------+</span></span><br><span class="line">|  1 | 张三   | 语文   |    80 |</span><br><span class="line">|  2 | 李四   | 语文   |    90 |</span><br><span class="line">|  3 | 王五   | 语文   |    93 |</span><br><span class="line">|  4 | 张三   | 数学   |    77 |</span><br><span class="line">|  5 | 李四   | 数学   |    68 |</span><br><span class="line">|  6 | 王五   | 数学   |    99 |</span><br><span class="line">|  7 | 张三   | 英语   |    90 |</span><br><span class="line">|  8 | 李四   | 英语   |    50 |</span><br><span class="line">|  9 | 王五   | 英语   |    89 |</span><br><span class="line">+<span class="comment">----+--------+--------+-------+</span></span><br></pre></td></tr></table></figure><h3 id="TOP-1"><a href="#TOP-1" class="headerlink" title="TOP 1"></a>TOP 1</h3><p>查询每门课程分数最高的学生以及成绩</p><ol><li><p>我们先拆分题目，这是一题查询分组求最大值的题目，拆分后的题目是：查询 每门课程 分数最高 的学生以及成绩</p><p>我们首先按照常规思路来写SQL: </p><p>select 学生姓名，学生分数</p><p>group by 课程</p><p>max(分数) </p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">s.name,s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">s.course;</span><br></pre></td></tr></table></figure><p>得出的查询结果是:</p><table><thead><tr><th>name</th><th>course</th><th>score</th></tr></thead><tbody><tr><td>张三</td><td>数学</td><td>99</td></tr><tr><td>张三</td><td>英语</td><td>90</td></tr><tr><td>张三</td><td>语文</td><td>93</td></tr></tbody></table><h4 id="问题-为什么姓名都是张三？课程和对应的成绩又全是对的？"><a href="#问题-为什么姓名都是张三？课程和对应的成绩又全是对的？" class="headerlink" title="问题: 为什么姓名都是张三？课程和对应的成绩又全是对的？"></a>问题: 为什么姓名都是张三？课程和对应的成绩又全是对的？</h4><p>我预测是因为没有把姓名加入group的分组字段，那么我们把姓名加入group的分组字段后试试看</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">s.name,s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">s.name,s.course;</span><br></pre></td></tr></table></figure><p>得出的查询结果是:</p><table><thead><tr><th>name</th><th>course</th><th>score</th></tr></thead><tbody><tr><td>张三</td><td>数学</td><td>77</td></tr><tr><td>张三</td><td>英语</td><td>90</td></tr><tr><td>张三</td><td>语文</td><td>80</td></tr><tr><td>李四</td><td>数学</td><td>68</td></tr><tr><td>李四</td><td>英语</td><td>50</td></tr><tr><td>李四</td><td>语文</td><td>90</td></tr><tr><td>王五</td><td>数学</td><td>99</td></tr><tr><td>王五</td><td>英语</td><td>89</td></tr><tr><td>王五</td><td>语文</td><td>93</td></tr></tbody></table><p>结果还是不对，这次把所有的字段都查询出来了，字段的排序规则是先按姓名分组，再按课程分组，因为课程是唯一的，所以跟直接查询的结果一样。</p></li><li><p>我们回到上一步，上一步的课程和成绩对应上了，姓名没有对应上，我们干脆就不要姓名和，拿课程和成绩作为一张表再和自己联结一次，以课程和成绩作为过滤字段，说不定就能得到想要的姓名字段。</p><p>思路：</p><ol><li><p>先课程分组求出最高的分数</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">s.course;</span><br></pre></td></tr></table></figure></li><li><p>把前面得出的结果作为表t再自联结一次</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">s.name,s.score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student s</span><br><span class="line"><span class="keyword">join</span> t </span><br><span class="line"><span class="keyword">on</span> s.course=t.course <span class="keyword">and</span> s.score=t.score;</span><br></pre></td></tr></table></figure></li><li><p>把t替换成查询出来的结果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">s.name,s.course,s.score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student s</span><br><span class="line"><span class="keyword">join</span> (<span class="keyword">select</span> </span><br><span class="line">s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">s.course) t </span><br><span class="line"><span class="keyword">on</span> s.course=t.course <span class="keyword">and</span> s.score=t.score;</span><br></pre></td></tr></table></figure><p>得出的查询结果是:</p><table><thead><tr><th>name</th><th>course</th><th>score</th></tr></thead><tbody><tr><td>王五</td><td>语文</td><td>93</td></tr><tr><td>王五</td><td>数学</td><td>99</td></tr><tr><td>张三</td><td>英语</td><td>90</td></tr></tbody></table><p>和原数据比较，这就是我们要得到的每门课程的top1。</p></li></ol></li></ol><h3 id="TOP-N"><a href="#TOP-N" class="headerlink" title="TOP N"></a>TOP N</h3><p>查询每门课程前两名的学生以及成绩</p><p>首先求Top 1的方法不适用与Top N，然后毫无头绪。。。</p><p>翻看其他人的博客后，发现求Top N的核心是: <code>自联结表的需求字段比较，也就是自己跟自己比较，然后把比较的结果求count()，最后控制过滤的记录数即可</code></p><h4 id="注意-自己和自己比较，可以通过联结和子查询，我们这里使用子查询"><a href="#注意-自己和自己比较，可以通过联结和子查询，我们这里使用子查询" class="headerlink" title="注意:  自己和自己比较，可以通过联结和子查询，我们这里使用子查询"></a>注意:  自己和自己比较，可以通过联结和子查询，我们这里使用子查询</h4><p>思路:</p><ol><li><p>首先是子查询，然后是自己和自己比较，得出一个count()值，最后使用where过滤这个count值</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">a.name,a.course,a.score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student a</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line"><span class="number">2</span>&gt;(<span class="keyword">select</span> <span class="keyword">count</span>(b.score) <span class="keyword">from</span> student b <span class="keyword">where</span> a.course=b.course <span class="keyword">and</span> a.score&lt;b.score)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> a.course <span class="keyword">desc</span>,a.score <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><p>梳理这段SQL，select字段不难，from字段不难，order by字段不难，难就难在where字段，我们先不看为什么使用2大于这个子查询，先把注意力放在子查询的where字段中的两个表成绩比较，实际上理解了为什么这么比较这题就解出来了。</p></li><li><p>我们画图来解释。。。只是做为示范，所以只取一个课程的成绩比较，其他的一样</p><p><img src="https://yerias.github.io/hadoop_img/20191205012529.jpg" alt="a.score&lt;b.score比较图"></p><p>可以看出，表a中的成绩越大，满足<code>a.score&lt;b.score</code>的次数越少，<code>where条件过滤count()的值越少越满足Top N的条件，可以根据where条件灵活控制过滤的记录数,我们这里是2，即取Top 2的记录。</code></p><h4 id="再提出一个问题，为什么要用a-score-lt-b-score而不是a-score-gt-b-score？"><a href="#再提出一个问题，为什么要用a-score-lt-b-score而不是a-score-gt-b-score？" class="headerlink" title="再提出一个问题，为什么要用a.score&lt;b.score而不是a.score&gt;b.score？"></a>再提出一个问题，为什么要用<code>a.score&lt;b.score</code>而不是<code>a.score&gt;b.score</code>？</h4><p>通过结果可以倒推出来，我们<code>select</code>语句中要的是表a，根据题意表a必定是比较中较大的值。如果使用<code>a.score&gt;b.score</code>，where条件限制的是满足最少的条件，把表a中最大的值给过滤了，那么得出的的count()结果是反的。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataWarehouse </tag>
            
            <tag> MySQL </tag>
            
            <tag> Rank </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS的副本存放策略&amp;HDFS的读写流程&amp;Pid文件详解&amp;HDFS常用命令&amp;HDFS的回收站机制&amp;安全模式详解&amp;单、多节点的磁盘均衡策略</title>
      <link href="/2018/10/07/hadoop/4/"/>
      <url>/2018/10/07/hadoop/4/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理副本放置策略</li><li>整理读写流程</li><li>整理pid文件</li><li>整理hdfs dfs 常用命令</li><li>整理多节点，单节点的磁盘均衡</li><li>整理安全模式</li></ol><h3 id="副本放置策略"><a href="#副本放置策略" class="headerlink" title="副本放置策略"></a>副本放置策略</h3><p>副本存放策略存在新旧两个版本</p><p>具体可参考我的另一个博客:<a href="https://yerias.github.io/2018/10/11/DataWarehouse/hadoop/5/">https://yerias.github.io/2018/10/11/DataWarehouse/hadoop/5/</a></p><h4 id="hadoop2-7-6之前的副本存放策略"><a href="#hadoop2-7-6之前的副本存放策略" class="headerlink" title="hadoop2.7.6之前的副本存放策略"></a>hadoop2.7.6之前的副本存放策略</h4><ul><li>副本一：同机架的不同节点 </li><li>副本二：同机架的另一节点 </li><li>副本三：不同机架的另一节点 </li><li>其他副本：随机挑选</li></ul><h4 id="hadoop2-8-4之后的副本存放策略"><a href="#hadoop2-8-4之后的副本存放策略" class="headerlink" title="hadoop2.8.4之后的副本存放策略"></a>hadoop2.8.4之后的副本存放策略</h4><ul><li>副本一：同Client的节点上 </li><li>副本二：不同机架中的节点上 </li><li>副本三：同第二个副本的机架中的另一个节点上</li><li>其他副本：随机挑选</li></ul><h4 id="副本存放策略优点"><a href="#副本存放策略优点" class="headerlink" title="副本存放策略优点"></a>副本存放策略优点</h4><ul><li>提高系统的可靠性</li><li>提供负载均衡</li><li>提高访问效率</li></ul><h3 id="读写流程"><a href="#读写流程" class="headerlink" title="读写流程"></a>读写流程</h3><h4 id="读取流程-FSData-InputStream"><a href="#读取流程-FSData-InputStream" class="headerlink" title="读取流程(FSData InputStream)"></a>读取流程(FSData InputStream)</h4><p><img src="http://yerias.github.io/hadoop_img/read.jpg" alt="hdfs读取数据流程"></p><ol><li>首先调用FileSystem对象的open()方法，获得一个分布式文件系统(DistributedFileSystem)的实例。</li><li>分布式文件系统(DistributedFileSystem)通过RPC获得文件的第一批块(Block)的位置信息，<a href="">同一个块按照副本数会返回多个位置信息</a>，这些位置信息按照Hadoop拓扑结构排序，距离客户端近的排在前面。</li><li>前两步会返回一个文件系统数据输入流(FSDataInputStream)对象，该对象会被封装为分布式文件系统输入流(DFSInputStream)对象，DFSInputStream可以方便地管理DataNode和NameNode的数据流。客户端调用read方法，DFSInputStream会找出离客户端最近的DataNode并连接。</li><li>数据从DataNode源源不断地流向客户端</li><li>如果第一个块的数据读完了，就会管理指向第一个块的DataNode的连接，接着读取下一个块。这些操作对客户端来说是透明的，从客户端的角度看来只是在读一个持续不断的数据流。</li><li>如果第一批块都读取完了，DFSInputStream就会去NameNode拿下一批块的位置信息，然后继续读，如果所有的块都读完了，这时就会关闭掉所有的流。</li></ol><p><code>注意:</code> 如果在读数据的时候，DFSInputStream和DataNode的通信发生异常，就会尝试连接正在读的块的排序第二近的DataNode，并且会记录哪个DataNode发生错误，剩余的块读的时候就会直接跳过该DataNode。DFSInputStream也会检查块的校验和，如果发现一个坏的块，就会先报告到NameNode，然后DFSIputStream在其它的DataNode上读取该块的数据。</p><h4 id="写入流程-FSData-OutputStream"><a href="#写入流程-FSData-OutputStream" class="headerlink" title="写入流程(FSData OutputStream)"></a>写入流程(FSData OutputStream)</h4><p><img src="http://yerias.github.io/hadoop_img/write.jpg" alt="hdfs写入数据流程"></p><ol><li>客户端在同过调用分布式文件系统(DistributedFileSystem)的create()方法创建新文件</li><li>DistributedFileSystem通过RPC调用NameNode去创建一个没有块关联的新文件，创建前NameNode会做各种校验，比如文件是否存在，客户端有没有权限等。如果通过校验，NameNode就会记录下新文件，否则就会抛出I/O异常。</li><li>前两步结合，会返回文件系统数据输出流(FSDataOutputStream)的对象，与读文件的时候相似，DistributedFileSystem被封装成分布式文件系统的输出流(DFSOutputStream)。DFSOutputStream可以协调NameNode和DataNode的通信。客户端开始写数据到DFSOutputStream，DFSOutputStream会把数据切分成一个个的数据包(packet)，然后排成数据队列(data quenc)</li><li>接下来，数据队列中的数据包首先输出到数据管道(多个datanode节点组成数据管道)中的第一个DataNode(写数据包)，第一个DataNode又把数据包输出到第二个DataNode中，依次类推。</li><li>DFSOutputStream还维护着一个队列叫做确认队列(ack quenc)，这个队列也是由数据包组成，用于等待DataNode收到数据返回确认数据包，当数据管道中的所有DataNode都表示已经收到了确认信息的时候，这时ack quenc才会把对应的数据包移除掉。</li><li>客户端完成写数据后，调用close()方法关闭写入数据流。</li><li>客户端通知NameNode把文件标记为已完成。然后NameNode把文件写成功的结果反馈给客户端。此时就表示客户端已完成整个HDFS的写数据流程。</li></ol><h5 id="如果写数据的过程中某个DataNode发生错误，会采取以下的步骤处理。"><a href="#如果写数据的过程中某个DataNode发生错误，会采取以下的步骤处理。" class="headerlink" title="如果写数据的过程中某个DataNode发生错误，会采取以下的步骤处理。"></a>如果写数据的过程中某个DataNode发生错误，会采取以下的步骤处理。</h5><ol><li>管道关闭</li><li>正常的DataNode上正在写的块会有一个新ID(需要和NameNode通信)，而失败的DataNode上的那个不完整的块会在上报心跳的时候被删除。</li><li>失败的DataNode会被移除出数据管道，块中剩余的数据包继续写入管道中的其他两个DataNode。</li><li>NameNode会标记这个块的副本个数少于指定值，块的副本会稍后在另一个DataNode创建。</li><li>有些时候多个DataNode会失败，只要<code>dfs.replication.min</code>(缺省是1个)属性定义的指定个数的DataNode写入数据成功了，整个写入过程就算成功，缺少的副本会进行异步的恢复。</li></ol><p><code>注意:</code> 只有调用sync()方法，客户端才确保该文件的写操作已经全部完成， 当客户端调用close()方法时，会默认调用sync()方法。</p><h3 id="pid文件"><a href="#pid文件" class="headerlink" title="pid文件"></a>pid文件</h3><p>pid文件具体作用请参考我的另外一个博客:<a href="https://yerias.github.io/2018/10/12/DataWarehouse/hadoop/6/">https://yerias.github.io/2018/10/12/DataWarehouse/hadoop/6/</a></p><p>在这里我们只简单说一下修改pid文件的生成目录的步骤，在修改hadoop文件的时候，hadoop最好是stop状态，否则需要kill进程。</p><ol><li><p>创建/home/hadoop/tmp目录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir /home/hadoop/tmp</span><br></pre></td></tr></table></figure></li><li><p>修改/home/hadoop/tmp的权限</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chmod -R 777 /hadoop/hadoop/tmp</span><br></pre></td></tr></table></figure></li><li><p>修改hadoop-env.sh文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HADOOP_PID_DIR=/home/hadoop/tmp</span><br></pre></td></tr></table></figure></li><li><p>修改yarn-env.sh</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export YARN_PID_DIR=/home/hadoop/tmp</span><br></pre></td></tr></table></figure></li></ol><h3 id="hdfs-dfs-常用命令"><a href="#hdfs-dfs-常用命令" class="headerlink" title="hdfs dfs 常用命令"></a>hdfs dfs 常用命令</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">[-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [&lt;path&gt; ...]]</span><br><span class="line">[-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">[-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span><br></pre></td></tr></table></figure><p>这些命令很点单，和linux的作用一样，这里不做演示。。。</p><h3 id="回收站"><a href="#回收站" class="headerlink" title="回收站"></a>回收站</h3><p>回收站的作用是把hdfs上删除的文件保存一定的时间然后自动删除，Apache是默认关闭的，CDH默认是开启的。</p><p>Apache的参数是由<code>core-default.xml</code>文件控制的<code>fs.trash.interval</code>属性</p><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>fs.trash.interval</td><td>0</td><td>检查点被删除的分钟数。如果为零，则禁用垃圾特性。单位秒</td></tr></tbody></table><p>一般在生产环境下设置保存7天</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun hadoop]# vim core-site.xml </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;10080&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p><code>注意:</code> 切记检查生产环境是否开启回收站,开了回收站，慎用 <code>-skipTrash</code></p><h3 id="安全模式"><a href="#安全模式" class="headerlink" title="安全模式"></a>安全模式</h3><p>安全模式是hadoop的一种保护机制，安全模式下不能进行修改文件的操作，但是可以浏览目录结构、查看文件内容的。</p><p>如果NN的log显示<code>Name node is in safe mode</code> ，正常手动让其离开安全模式，这种操作很少做。</p><p><strong>一般进入safemode情况有:</strong></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1. 启动或者重新启动hdfs时</span><br></pre></td></tr></table></figure><pre><code>2. HDFS维护升级时3. 块文件损坏等。。。</code></pre><p>可以使用<code>fsck</code>检查一下HDFS的健康度，然后进行下一步操作</p><p><code>hdfs fsck / :</code> 用这个命令可以检查整个文件系统的健康状况,但是要注意它不会主动恢复备份缺失的block,这个是由NameNode单独的线程异步处理的</p><p><strong>fsck相关介绍:</strong></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs fsck</span><br><span class="line">　　　　Usage:DFSck &lt;path&gt; [-move | -delete | -openforwrite] [-files [-blocks [-locations | -racks]]]</span><br><span class="line">　　　　&lt;path&gt; 检查这个目录中的文件是否完整</span><br><span class="line">　　　　-move 破损的文件移至/lost+found目录</span><br><span class="line">　　　　-delete 删除破损的文件</span><br><span class="line">　　　　-openforwrite 打印正在打开写操作的文件</span><br><span class="line">　　　　-files 打印正在check的文件名</span><br><span class="line">　　　　-blocks 打印block报告(需要和-files参数一起使用)</span><br><span class="line">　　　　-locations 打印每个block的位置信息(需要和-files参数一起使用)</span><br><span class="line">　　　　-racks 打印位置信息的网络拓扑图(需要和-files参数一起使用)</span><br></pre></td></tr></table></figure><p>一般我们会查看 / 目录下的损坏文件，然后根据损坏文件的路径手动进行<code>hdfs debug</code>修复</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs fsck / -list-corruptfileblocks</span><br></pre></td></tr></table></figure><h3 id="多节点的磁盘均衡"><a href="#多节点的磁盘均衡" class="headerlink" title="多节点的磁盘均衡"></a>多节点的磁盘均衡</h3><p>由于集群中的一些服务器如CPU、磁盘、网络的差异，副本存放并不会一直保持均衡，这就造成某一些服务器的磁盘占用率达到90%，而另外一些服务器的磁盘占用率只有60%或者80%。所以就有必要手动进行均衡操作，事实上hadoop的sbin目录下也有这个命令</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-rwxr-xr-x 1 hadoop hadoop 1128 Jun  3  2019 start-balancer.sh  #开始</span><br><span class="line">-rwxr-xr-x 1 hadoop hadoop 1179 Jun  3  2019 stop-balancer.sh#停止</span><br></pre></td></tr></table></figure><p>那么集群中的磁盘占用率怎么才算正常？这个由参数<code>threshold</code>控制，默认threshold=10，即各个服务器保持所有服务器的磁盘占用空间的平均值上下浮动10%，可能不好理解，我们用上面的占用率90%、60%和80%算一下。</p><p>这三台的平均占用率是:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(90+60+80)/3=76%</span><br></pre></td></tr></table></figure><p>那么<code>threshold</code>参数就控制这三台其中的任意一台的磁盘占用率不得超过86%，不得低于66%。</p><h4 id="那么怎么做呢？"><a href="#那么怎么做呢？" class="headerlink" title="那么怎么做呢？"></a>那么怎么做呢？</h4><p>在进行磁盘均衡之前，我们需要重新设置一下balancer的带宽限制，在<code>hdfs-default.xml</code>文件中的<code>dfs.datanode.balance.bandwidthPerSec</code>属性，默认是10M，生产环境下一般设置为30M</p><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>dfs.datanode.balance.bandwidthPerSec</td><td>10m</td><td>指定每个datanode可用于平衡目的的最大带宽(以每秒字节数为单位)。</td></tr></tbody></table><p>在<code>hadoop</code>的<code>hdfs-site.xml</code>文件中覆盖一下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun hadoop]# vim hdfs-site.xml </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.balance.bandwidthPerSec&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;30m&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>怎么做？</p><p>写个shell脚本，每天凌晨执行<code>./start-balancer.sh</code>调度一次，达到数据平衡，毛刺修正，调度执行完成自动关闭，不需要执行<code>./stop-balancer.sh</code>手段关闭，除非特殊情况。</p><h3 id="单节点的磁盘均衡"><a href="#单节点的磁盘均衡" class="headerlink" title="单节点的磁盘均衡"></a>单节点的磁盘均衡</h3><p>在官网中的描述: </p><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html" target="_blank" rel="noopener">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html</a></p><p>在官网中的描述中有这么一句: <code>dfs.disk.balancer.enabled must be set to true in hdfs-site.xml.</code></p><p>翻译过来就是: 必须在<code>hdfs-site.xml</code>中将<code>dfs.disk.balancer.enabled</code>设置为<code>true</code>。</p><p>这是因为默认情况下，群集上未启用磁盘平衡器</p><p>那么我们先去设置一下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun hadoop]# vim hdfs-site.xml </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.disk.balancer.enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h4 id="场景"><a href="#场景" class="headerlink" title="场景:"></a>场景:</h4><p>假如我们现在有三个数据盘</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/data01   90%</span><br><span class="line">/data02   60%</span><br><span class="line">/data03   80%</span><br></pre></td></tr></table></figure><p>现在磁盘用的差不多了，准备加入一个盘<code>/data04   0%</code></p><p>我们这时候是不是要进行单节点服务器的磁盘均衡？</p><h4 id="怎么做？"><a href="#怎么做？" class="headerlink" title="怎么做？"></a>怎么做？</h4><ol><li><p>生成hadoop001.plan.json</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -plan hadoop001#hadoop001是datanode的主机名</span><br></pre></td></tr></table></figure></li><li><p>执行</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -execute hadoop001.plan.json #hadoop001是datanode的主机名</span><br></pre></td></tr></table></figure></li><li><p>查询状态</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -query ruozedata001 #hadoop001是datanode的主机名</span><br></pre></td></tr></table></figure></li></ol><h4 id="什么时候手动或调度执行？"><a href="#什么时候手动或调度执行？" class="headerlink" title="什么时候手动或调度执行？"></a>什么时候手动或调度执行？</h4><ol><li>新盘加入</li><li>监控服务器的磁盘剩余空间小于阈值10%，发邮件预警 ，手动执行</li></ol><h4 id="怎么在DataNode中挂载磁盘？"><a href="#怎么在DataNode中挂载磁盘？" class="headerlink" title="怎么在DataNode中挂载磁盘？"></a>怎么在DataNode中挂载磁盘？</h4><p>由<code>hdfs-default.xml</code>文件的<code>dfs.datanode.data.dir</code>属性控制</p><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>dfs.datanode.data.dir</td><td>file://${hadoop.tmp.dir}/dfs/data</td><td>确定DFS数据节点应该将其块存储在本地文件系统的何处。多个目录使用逗号分隔。</td></tr></tbody></table><p>假如我们现在有/data01,/data02,/data03,/data04四个目录需要挂载在该DataNode节点中</p><p>修改<code>hdfs-site.xml</code>文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun hadoop]# vim hdfs-site.xml </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/data01,/data02,/data03,/data04&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h5 id="为什么DN的生产上要挂载多个物理的磁盘目录，而不是做一个raid-磁盘阵列"><a href="#为什么DN的生产上要挂载多个物理的磁盘目录，而不是做一个raid-磁盘阵列" class="headerlink" title="为什么DN的生产上要挂载多个物理的磁盘目录，而不是做一个raid(磁盘阵列)"></a>为什么DN的生产上要挂载多个物理的磁盘目录，而不是做一个raid(磁盘阵列)</h5><p>为了高效率写  高效率读</p><p><code>注意:</code> 提前规划好2-3年存储量 ，避免后期加磁盘维护的工作量</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据重刷机制(抛砖引玉)</title>
      <link href="/2018/10/07/mysql/%E6%95%B0%E6%8D%AE%E9%87%8D%E5%88%B7%E6%9C%BA%E5%88%B6/"/>
      <url>/2018/10/07/mysql/%E6%95%B0%E6%8D%AE%E9%87%8D%E5%88%B7%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<h4 id="先抛出几个问题"><a href="#先抛出几个问题" class="headerlink" title="先抛出几个问题"></a>先抛出几个问题</h4><ol><li><p>存储是不是基石？</p></li><li><p>假如存储不挂，数据真的准确吗？</p></li><li><p>存储挂了，数据还准确吗？</p></li><li><p>如何校验是否正确？如何让其正确？机制是不是必须有？</p></li></ol><p>注：<code>sqoop</code>抽数据，无<code>error</code>丢数据的概率很小</p><p>数据质量校验：数据量校验 <code>count</code>相同吗？<code>count</code>相同内容相同吗？</p><p>数据量相同–&gt;数据量不同 重刷机制 补or删 <code>spark</code> 95%–&gt;数据内容不同？ 抽样 5%</p><h4 id="现在重点理解一下重刷机制"><a href="#现在重点理解一下重刷机制" class="headerlink" title="现在重点理解一下重刷机制"></a>现在重点理解一下重刷机制</h4><p>背景：用<code>count</code>校验上下游的数据不准确</p><p>引入重刷机制：通过对上下游的两个表求<code>full outer join</code>来对比字段的<code>null</code>值</p><p>上游表a</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| id   | data  | age  |</span><br><span class="line">| <span class="comment">---- | ----- | ---- |</span></span><br><span class="line">| 1    | data1 | 18   |</span><br><span class="line">| 2    | data2 | 19   |</span><br><span class="line">| 3    | data3 | 20   |</span><br><span class="line">| 7    | data7 | 22   |</span><br></pre></td></tr></table></figure><p>下游表b</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| id   | data  | age  |</span><br><span class="line">| <span class="comment">---- | ----- | ---- |</span></span><br><span class="line">| 1    | data1 | 18   |</span><br><span class="line">| 2    | data2 | 19   |</span><br><span class="line">| 3    | data3 | 20   |</span><br><span class="line">| 7    | data7 | 22   |</span><br></pre></td></tr></table></figure><p>我们发现表 a 和表 b 对比 表 a 少了 5 、6 多了 7 ，表 b 少了 2 、 7 多了 6，我们现在对两个表做 <code>full outer join</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| aid  | data   | age  | bid  |</span><br><span class="line">| <span class="comment">---- | ------ | ---- | ---- |</span></span><br><span class="line">| 1    | ruoze1 | 18   | 1    |</span><br><span class="line">| 2    | ruoze2 | 19   | null |</span><br><span class="line">| 3    | ruoze3 | 20   | 3    |</span><br><span class="line">| 7    | ruoze7 | 22   | null |</span><br><span class="line">| null | null   | null | 5    |</span><br><span class="line">| null | null   | null | 6    |</span><br></pre></td></tr></table></figure><p>以表 a 为标准，对生成后的大表做筛选，分别查找 <code>aid</code> 和 <code>bid</code> 为 <code>null</code> 的记录</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">from</span> t <span class="keyword">where</span> aid=<span class="literal">null</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">from</span> t <span class="keyword">where</span> bid=<span class="literal">null</span></span><br></pre></td></tr></table></figure><p>发现 <code>bid</code>为 5 、 6 的行 <code>aid</code> 为 <code>null</code>，说明 <code>bid</code> 下游数据多了，根据 <code>bid</code> 重新构建</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> b <span class="keyword">where</span> bid=<span class="number">5</span>     </span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> b <span class="keyword">where</span> bid=<span class="number">6</span></span><br></pre></td></tr></table></figure><p>发现 <code>aid</code> 为 2 、 7 的 <code>bid</code> 为<code>null</code>，说明 <code>bid</code> 下游数据少了，根据 <code>aid</code> 重新构建</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="number">2</span> ruoze2 <span class="number">19</span> </span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="number">7</span> ruoze7 <span class="number">22</span></span><br></pre></td></tr></table></figure><p>经过重新构建也就是重刷后的数据是</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| aid  | data   | age  | bid  |</span><br><span class="line">| <span class="comment">---- | ------ | ---- | ---- |</span></span><br><span class="line">| 1    | ruoze1 | 18   | 1    |</span><br><span class="line">| 2    | ruoze2 | 19   | 2    |</span><br><span class="line">| 3    | ruoze3 | 20   | 3    |</span><br><span class="line">| 7    | ruoze7 | 22   | 7    |</span><br></pre></td></tr></table></figure><h4 id="深度思考："><a href="#深度思考：" class="headerlink" title="深度思考："></a>深度思考：</h4><p><code>full outer join</code> 其实就是先 <code>left join</code> 和后 <code>right join</code> 的两个结果，为 <code>null</code> 的刚好是缺少的或者多的，而交集是上下游都有的数据，需要做的是 <code>left join</code> 为 <code>null</code> 做 <code>insert</code> 或者 <code>delete</code>，还是 <code>right join</code> 为 null 做 <code>insert</code> 或者 <code>delete</code>。</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataWarehouse </tag>
            
            <tag> Data </tag>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>理解数据块、副本数、小文件的概念&amp;掌握HDFS架构&amp;掌握NN和SNN交互流程</title>
      <link href="/2018/10/06/hadoop/3/"/>
      <url>/2018/10/06/hadoop/3/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理对 块大小 副本数的理解</li><li>整理对小文件的理解</li><li>整理HDFS架构</li><li>整理SNN流程</li></ol><h3 id="修改hdfs的数据保存文件"><a href="#修改hdfs的数据保存文件" class="headerlink" title="修改hdfs的数据保存文件"></a>修改hdfs的数据保存文件</h3><p>在开始完成今天的目标之前，我们还要做一个事情，那就是修改hdfs的nn、nd、snn文件保存的目录，这个目录默认保存在/tmp目录下，那么为什么会保存在/tmp目录下呢，实际上是由<code>core-default.xml</code>默认参数决定的</p><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>hadoop.tmp.dir</td><td>/tmp/hadoop-${user.name}</td><td>A base for other temporary directories.</td></tr></tbody></table><p>因为/tmp目录具有固定周期清除文件的特性，所以我们这里需要改变hadoop的存储文件路径，防止丢失文件</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ vim core-site.xml </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在这之后我们还要对/home/hadoop/tmp目录做一下调整</p><ul><li><p><code>chmod -R 777 /home/hadoop/tmp</code></p></li><li><p><code>mv /tmp/hadoop-hadoop/dfs /home/hadoop/tmp/</code></p></li><li><p><code>test</code> </p></li></ul><h3 id="对块大小和副本数的理解"><a href="#对块大小和副本数的理解" class="headerlink" title="对块大小和副本数的理解"></a>对块大小和副本数的理解</h3><h4 id="块的理解"><a href="#块的理解" class="headerlink" title="块的理解"></a>块的理解</h4><p>hadoop1.x的块大小是64M，hadoop2.x的块大小是128M，块的大小是<code>hdfs-default.xml</code>文件中的<code>dfs.blocksize</code>属性控制，它是hdfs存储处理数据的最小单元，可以根据实际需求改变块大小，但是一般不建议这么做。</p><table><thead><tr><th>KEY</th><th>VALUE</th></tr></thead><tbody><tr><td>dfs.blocksize</td><td>134217728(128M)</td></tr></tbody></table><p>hadoop1.x的块大小是64M，hadoop2.x的块大小是128M，块的大小是<code>hdfs-default.xml</code>文件中的<code>dfs.blocksize</code>属性控制，它是hdfs存储处理数据的最小单元，可以根据实际需求改变块大小，但是一般不建议这么做。</p><p><code>块大小为什么要设计成128M？</code></p><p>是为了最小化寻址时间，目前磁盘的传输速率普遍是在100M/S左右，所以设计成128M每块。</p><h4 id="副本数的理解"><a href="#副本数的理解" class="headerlink" title="副本数的理解"></a>副本数的理解</h4><p>副本的设置让hadoop具有高可靠性的特点，数据不会轻易丢失。副本是存储在dn中的，由<code>hdfs-default.xml</code>文件的<code>dfs.replication</code>参数控制，伪分布式部署是1份，集群部署是3份，不建议修改。</p><table><thead><tr><th>KEY</th><th>VALUE</th></tr></thead><tbody><tr><td>dfs.replication</td><td>3</td></tr></tbody></table><h3 id="对小文件的理解"><a href="#对小文件的理解" class="headerlink" title="对小文件的理解"></a>对小文件的理解</h3><p>一般来说，小文件是文件大小小于10M的数据，由于hadoop的架构特性，它只能有一台主nn，如果小文件特别多的话，小文件的块也特别多，nn需要维护的块的元数据信息的条数也多，所以我们一般把小文件合并成大文件再放到hdfs上，也有上传hdfs后合并，这样来减少nn维护的块的元数据数量。具体合并的方式，以后再讲。</p><h3 id="整理HDFS架构"><a href="#整理HDFS架构" class="headerlink" title="整理HDFS架构"></a>整理HDFS架构</h3><p>HDFS由NameNode、SecondaryNameNode、DataNode三个组件组成</p><h4 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h4><p>NameNode也被称为名称节点或元数据节点，是HDFS主从架构中的主节点，相当于HDFS的大脑，它管理文件系统的命名空间，维护着整个文件系统的目录树以及目录树中所有子目录和文件。</p><p>这些信息以两个文件的形式持久化保存在本地磁盘上，一个是命令空间镜像FSImage(File System Image)，主要是用来存储HDFS的元数据信息。还有一个是命令空间镜像的编辑日志(Editlog)，该文件保存用户对命令空间镜像的修改信息。</p><h4 id="SecondeayNameNode"><a href="#SecondeayNameNode" class="headerlink" title="SecondeayNameNode"></a>SecondeayNameNode</h4><p>SecondaryNameNode也被称为元数据节点，是HDFS主从架构中的备用节点，主要用于定期合并命名空间镜像(FSImage)和命令空间镜像的操作日志(Editlog)，是一个辅助NameNode的守护进程。</p><p>定期合并FSImage和Editlog的周期时间是由<code>hdfs-default.xml</code>文件的<code>dfs.namenode.checkpoint.period</code>属性决定的，默认一小时合并一次，同时如果Editlog操作日志记录满 1000000条也会触发合并机制，由<code>dfs.namenode.checkpoint.txns</code>属性控制，两者满足一个即可。</p><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>dfs.namenode.checkpoint.period</td><td>3600</td><td>两个周期性检查点之间的秒数。</td></tr><tr><td>dfs.namenode.checkpoint.txns</td><td>1000000</td><td>两个周期性检查点之间的名称空间记录数。</td></tr></tbody></table><p>虽然SecondaryNameNode能够减轻单点故障，但是还会有风险，因为总有一段时间的数据是没有同步的。</p><h5 id="问题-为什么SecondaryNameNode要辅助NameNode定期合并FSImage文件和Editlog文件？"><a href="#问题-为什么SecondaryNameNode要辅助NameNode定期合并FSImage文件和Editlog文件？" class="headerlink" title="问题: 为什么SecondaryNameNode要辅助NameNode定期合并FSImage文件和Editlog文件？"></a>问题: 为什么SecondaryNameNode要辅助NameNode定期合并FSImage文件和Editlog文件？</h5><p>FSImage文件实际上是HDFS文件系统中元数据的一个永久性检查点(checkpoint)，但也并不是每一个写操作都会更新到这个文件中，因为FSImage是一个大型文件，如果频繁地执行写操作，会导致系统运行极其缓慢，那么如何解决呢？</p><p>解决方案就是NameNode将命令空间的改动信息写入命令空间的Editlog，但随着时间的推移，Editlog文件会越来越大，一旦发生故障，那么将需要花费很长的时间进行回滚操作，所以可以像传统的关系型数据库一样，定期地合并FSImage和Editlog，但是如果由NameNode来做合并操作，由于NameNode在为集群提供服务的同时可能无法提供足够的资源，所以为了解决这一问题，SecondaryNameNode就应运而生了。</p><h4 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h4><p>DataNode也被称为数据节点，它是HDFS主从架构在的从节点，它存储数据块和数据块校验和它在NameNode的指导下完成数据的IO操作。</p><p><img src="https://yerias.github.io/hadoop_img/image-20191202110000146.png" alt="数据块和数据库校验和"></p><p>DataNode会不断地向NameNode发送心跳和块报告信息，并执行来自NameNode的指令。</p><p>发送心跳是为了告诉nn我还活着，通过<code>hdfs-default.xml</code>文件的<code>dfs.heartbeat.interval</code>参数可以得知，每3秒发送一次</p><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>dfs.heartbeat.interval</td><td>3</td><td>Determines datanode heartbeat interval in seconds.</td></tr></tbody></table><p>发送块报告信息是为了扫描数据目录并协调内存块和磁盘块之间的差异的，从<code>hdfs-default.xml</code>文件的<code>dfs.datanode.directoryscan.interval</code>属性和<code>dfs.blockreport.intervalMsec</code>可以得知每6小时发送一次块报告，生产环境下建议缩短周期（3小时）</p><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>dfs.datanode.directoryscan.interval</td><td>21600</td><td>DataNode扫描数据目录并协调内存块和磁盘块之间的差异的时间间隔(以秒为单位)，发现损坏块</td></tr><tr><td>dfs.blockreport.intervalMsec</td><td>21600000</td><td>确定以毫秒为单位的块报告间隔，恢复数据块</td></tr></tbody></table><p>在这里我们需要知道一个hadoop命令，该命令仅适用于高级用户，不正确的使用可能会导致数据丢失。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun subdir0]$ hdfs debug</span><br><span class="line">Usage: hdfs debug &lt;command&gt; [arguments]</span><br><span class="line"></span><br><span class="line">These commands are for advanced users only.</span><br><span class="line"></span><br><span class="line">Incorrect usages may result in data loss. Use at your own risk.</span><br><span class="line"></span><br><span class="line">verifyMeta -meta &lt;metadata-file&gt; [-block &lt;block-file&gt;]</span><br><span class="line">computeMeta -block &lt;block-file&gt; -out &lt;output-metadata-file&gt;</span><br><span class="line">recoverLease -path &lt;path&gt; [-retries &lt;num-retries&gt;]</span><br><span class="line">[hadoop@aliyun subdir0]$</span><br></pre></td></tr></table></figure><h5 id="手动修复"><a href="#手动修复" class="headerlink" title="手动修复"></a>手动修复</h5><p><code>hdfs debug</code>的作用是在多副本的环境下手动修复元数据、块或者副本，我们在这里只说修改副本，这里的xxx是指副本路径，该路径必须驻留在HDFS文件系统上，由<code>hdfs fsck</code>命令查找。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">[hadoop@aluyuncurrent]$</span><span class="bash"> hdfs debug recoverLease -path xxx -retries 10</span></span><br></pre></td></tr></table></figure><h5 id="自动修复"><a href="#自动修复" class="headerlink" title="自动修复"></a><code>自动修复</code></h5><p><a href="https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/" target="_blank" rel="noopener">https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/</a></p><p>但是有可能: 手动修复 + 自动修复都是失败的 </p><p>这就需要保证数据仓库的数据质量和数据重刷机制恢复 </p><h5 id="问题-DataNode是如何存储和管理数据块的？"><a href="#问题-DataNode是如何存储和管理数据块的？" class="headerlink" title="问题: DataNode是如何存储和管理数据块的？"></a>问题: DataNode是如何存储和管理数据块的？</h5><ol><li>DataNode节点是以数据块的形式在本地Linux文件系统上保存HDFS文件的内容，并对外提供文件数据访问功能。</li><li>DataNode节点的一个基本功能就是管理这些保存在Linux文件系统上的数据</li><li>DataNode节点是将数据块以Linux文件的形式保存在本地的存储系统上</li></ol><h3 id="SecondaryNameNode和NameNode的交互流程"><a href="#SecondaryNameNode和NameNode的交互流程" class="headerlink" title="SecondaryNameNode和NameNode的交互流程"></a>SecondaryNameNode和NameNode的交互流程</h3><p><img src="https://yerias.github.io/hadoop_img/IMG_0614(20191202-125538).JPG" alt="SecondaryNameNode和NameNode的交互流程"></p><ol><li><code>SecondaryNameNode</code>引导<code>NameNode</code>滚动更新操作日志，并开始将新的操作日志写进<code>edits.new</code>。</li><li><code>SecondaryNameNode</code>将<code>NameNode</code>的<code>FSImage</code>文件和<code>Edits</code>文件复制到本地的检查点目录。</li><li><code>SecondaryNameNode</code>将<code>FSImage</code>文件导入内存，回放编辑日志<code>Edits</code>文件，将其合并到<code>FSImage.ckpt</code>文件，并将新的<code>FSImage.ckpt</code>文件压缩后写入磁盘。</li><li><code>SecondaryNameNode</code>将新的<code>FSImage.ckpt</code>文件传回<code>NameNode</code>。</li><li><code>NameNode</code>在接收新的<code>FSImage.ckpt</code>文件后，将<code>FSImage.ckpt</code>替换为<code>FSImage</code>，然后直接加载和启用该文件</li><li><code>NameNode</code>将<code>Edits.new</code>更名为<code>Edits</code>。默认情况下，该过程1小时内发生1次，或者当编辑日志达到默认值1000000条也会触发。</li></ol><h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><ol><li><p>NN的fsimage的个数默认是保留2个</p><p><img src="https://yerias.github.io/hadoop_img/57Y5RSNU_6%5DY886.jpg" alt="fsimage文件"></p><p>控制的参数是<code>hdfs-default.xml</code>文件的<code>dfs.namenode.num.checkpoints.retained</code>参数</p><table><thead><tr><th>KEY</th><th>VALUE</th></tr></thead><tbody><tr><td>dfs.namenode.num.checkpoints.retained</td><td>2</td></tr></tbody></table></li><li><p>NN的editlog文件不会保留所有的，至于保留的个数还是周期，解决中。。。</p><p><img src="https://yerias.github.io/hadoop_img/C(J%60DYZC@_%7BEA%5B3(GMDRK%7DI.jpg" alt="editlog文件"></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> HDFS </tag>
            
            <tag> DataWarehouse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>yarn的伪分布式部署&amp;jps的原理&amp;oom-killer&amp;/tmp目录的clean机制</title>
      <link href="/2018/10/05/hadoop/2/"/>
      <url>/2018/10/05/hadoop/2/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>搭建 yarn伪分布式</li><li>跑mr count案例</li><li>整理 jps命令</li><li>Linux两个机制 oom  clean</li></ol><h3 id="Yarn伪分布式部署-amp-单点-amp-主从架构"><a href="#Yarn伪分布式部署-amp-单点-amp-主从架构" class="headerlink" title="Yarn伪分布式部署&amp;单点&amp;主从架构"></a>Yarn伪分布式部署&amp;单点&amp;主从架构</h3><p>由于hadoop集成了MapReduce和Yarn，所以这里我们只要修改相关的配置文件即可使用</p><p>我们需要配置<code>mapred-site.xml</code>和<code>yarn-site.xml</code>这两个文件，由于<code>mapred-site.xml</code>不存在所以需要复制一个出来</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ cp mapred-site.xml.template mapred-site.xml</span><br></pre></td></tr></table></figure><p>然后用<code>vim</code>打开<code>mapred-site.xml</code>并修改</p><p>vim mapred-site.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>然后用<code>vim</code>打开<code>yarn-site.xml</code>并修改文件</p><p>vim yarn-site.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>aliyun:38088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="comment">&lt;!-- 日志聚集功能使能 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 日志保留时间设置7天 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>注意:</code> 这是使用38088替代默认的8088是为了避免云主机被挖矿</p><p>最后启动<code>Yarn</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ start-yarn.sh</span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/yarn-hadoop-resourcemanager-aliyun.out</span><br><span class="line">aliyun: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/yarn-hadoop-nodemanager-aliyun.out</span><br><span class="line"></span><br><span class="line">[hadoop@aliyun hadoop]$ jps</span><br><span class="line">32501 NodeManager</span><br><span class="line">349 Jps</span><br><span class="line">32415 ResourceManager</span><br></pre></td></tr></table></figure><p>最后再用<code>ps -ef</code>命令验证<code>ResourceManager</code>程序是否真的启动完成</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ netstat -nlp|grep 32415</span><br><span class="line">(Not all processes could be identified, non-owned process info</span><br><span class="line"> will not be shown, you would have to be root to see it all.)</span><br><span class="line">tcp        0      0 172.16.39.48:38088      0.0.0.0:*               LISTEN      32415/java          </span><br><span class="line">tcp        0      0 0.0.0.0:8030            0.0.0.0:*               LISTEN      32415/java          </span><br><span class="line">tcp        0      0 0.0.0.0:8031            0.0.0.0:*               LISTEN      32415/java          </span><br><span class="line">tcp        0      0 0.0.0.0:8032            0.0.0.0:*               LISTEN      32415/java          </span><br><span class="line">tcp        0      0 0.0.0.0:8033            0.0.0.0:*               LISTEN      32415/java</span><br></pre></td></tr></table></figure><p>启动完成，可以去玩<code>wordcount</code>案例了</p><h3 id="使用MapReduce运行WordCount案例"><a href="#使用MapReduce运行WordCount案例" class="headerlink" title="使用MapReduce运行WordCount案例"></a>使用MapReduce运行WordCount案例</h3><p>在data目录下创建mapreduce的输入文件</p><p>[hadoop@aliyun data]$ vim name.log</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ruozedata</span><br><span class="line">ruoze</span><br><span class="line">jepson</span><br><span class="line">huhu</span><br><span class="line">ye</span><br><span class="line">tunan</span><br><span class="line">afei</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">3</span><br><span class="line">2</span><br><span class="line">1</span><br><span class="line">a b c a a aaaa bcd</span><br><span class="line">ruoze jepson</span><br></pre></td></tr></table></figure><p>把name.log文件put到hdfs上</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun data]$ hdfs dfs -mkdir -p /wordcount/input</span><br><span class="line">[hadoop@aliyun data]$ hdfs dfs -put name.log /wordcount/input</span><br><span class="line">[hadoop@aliyun data]$ hdfs dfs -ls /wordcount/input/</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         86 2019-12-01 17:46 /wordcount/input/name.log</span><br></pre></td></tr></table></figure><p>使用hadoop jar 运行案例</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar wordcount /wordcount/input /wordcount/output</span><br></pre></td></tr></table></figure><p>查看结果</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ hdfs dfs -cat /wordcount/output/*</span><br><span class="line">12</span><br><span class="line">22</span><br><span class="line">32</span><br><span class="line">a3</span><br><span class="line">aaaa1</span><br><span class="line">afei1</span><br><span class="line">b1</span><br><span class="line">bcd1</span><br><span class="line">c1</span><br><span class="line">huhu1</span><br><span class="line">jepson2</span><br><span class="line">ruoze2</span><br><span class="line">ruozedata1</span><br><span class="line">tunan1</span><br><span class="line">ye1</span><br></pre></td></tr></table></figure><h3 id="jps命令不为人知的地方"><a href="#jps命令不为人知的地方" class="headerlink" title="jps命令不为人知的地方"></a>jps命令不为人知的地方</h3><p>我们想既然jps可以直接运行，肯定在APTH路径下，我们何不which一下看看</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ which jps</span><br><span class="line">/usr/java/jdk/bin/jps</span><br></pre></td></tr></table></figure><p>原来jps是一个java命令，我们使用一下jps看看它有什么作用</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ jps</span><br><span class="line">883 DataNode</span><br><span class="line">2117 Jps</span><br><span class="line">32501 NodeManager</span><br><span class="line">1082 SecondaryNameNode</span><br><span class="line">781 NameNode</span><br><span class="line">32415 ResourceManager</span><br></pre></td></tr></table></figure><p>jps命令打印出来了hadoop组件里面的程序pid和程序名，引申出这些程序都属于java程序，结合jps属于java命令，得出jps打印出来所有运行的java程序</p><p>我们可以从启动程序的脚本中得到pid和程序名存放的文件，这里就不去debug，他们默认存放在/tmp目录下一个叫做hsperfdata_用户名的文件下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun tmp]$ ll hsperfdata_hadoop/</span><br><span class="line">total 160</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:08 1082</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:08 32415</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:08 32501</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:08 781</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:08 883</span><br></pre></td></tr></table></figure><p>每个pid对应是一些二进制文件，没啥好看的，我们发现这个文件下还有一些以pid结尾的文件，文件保存的其实也是各自对应的pid号码，但是这里的.pid文件是进程自己创建的。用来管理和结束进程的。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun tmp]$ ll grep *.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 4 Dec  1 17:39 hadoop-hadoop-datanode.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 4 Dec  1 17:39 hadoop-hadoop-namenode.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 5 Dec  1 17:39 hadoop-hadoop-secondarynamenode.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 6 Dec  1 17:37 yarn-hadoop-nodemanager.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 6 Dec  1 17:37 yarn-hadoop-resourcemanager.pid</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun tmp]$ cat hadoop-hadoop-datanode.pid </span><br><span class="line">883</span><br></pre></td></tr></table></figure><p>所以jps命令的作用就明白了，在使用jsp命令时，会去/tmp目录下/hsperfdata_username目录下找到程序的pid和程序名并打印出来，/tmp/hsperfdata_username目录会存放该用户所有已经启动的java进程信息。</p><p>这里需要get到一个很关键的知识点，它可能让你在shell脚本中犯错，那就是进程所属的用户去执行 jps命令，只显示自己的相关的进程信息，也就是说，其他用户使用jps命令查看不到本用户启动的程序，root用户可以看所有的，但是显示不可用，我们这里就用root用户尝试一下</p><p>我在这里翻车了，使用root显示出来了所有的进程，原因不明，但是其他普通用户无法显示</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">aliyun:mysqladmin:/usr/local/mysql:&gt;jps</span><br><span class="line">2346 Jps</span><br></pre></td></tr></table></figure><p>这同样能得出我们想要的结果，那就是其他用户使用jps不能查看到本用户启动的java程序。其原理我们看看这个文件夹的权限就知道了</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun tmp]$ ll hsperfdata_hadoop/</span><br><span class="line">total 128</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:26 1082</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:26 32415</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:26 32501</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:26 781</span><br></pre></td></tr></table></figure><p>最后总结一下我们在判断一个程序是否在运行时，切不可使用jps命令，因为jps命令只能查看本用户的java进程，那我们在shell脚本中应该如何判断一个程序是否存在？使用万能的ps -ef。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun tmp]$ ps -ef|grep 781 | grep -v grep | wc -l</span><br><span class="line">1</span><br></pre></td></tr></table></figure><h3 id="Linux的oom-kill和clean机制"><a href="#Linux的oom-kill和clean机制" class="headerlink" title="Linux的oom-kill和clean机制"></a>Linux的oom-kill和clean机制</h3><p>在最后我们说一下在使用hadoop时会遇到的两个坑，而且还都是Linux自带的特性，以及解决的方法。</p><p><code>oom-killer机制:</code> 大数据程序是非常吃内存的，而在Linux内核检测到系统内存不足后，会触发oom-killer，挑选占用最大内存的进程杀掉。</p><p>如果我们的进程突然断了，首先查看日志找<code>EROOR</code>，有<code>ERROR</code>具体分析，没有<code>ERROR</code>但是<code>INFO</code>信息断了，很可能就是触发了<code>oom-killer机制</code>，使用<code>free -h</code>命令查看内存使用情况，再使用<code>cat /var/log/messages | grep oom</code>命令查看有没有类似于<code>Killed process</code> 的命令，如果有，就是触发了<code>oom-killer机制</code></p><p><code>clean机制:</code> Linux的/tmp目录是一个临时目录，它有一个机制，默认清理超过30天的内容，而前面使用<code>jps</code>命令的时候就发现，<code>hadoop</code>的进程<code>pid</code>都存放在<code>/tmp</code>目录中，启动进程的时候去<code>/tmp</code>目录下创建对应的<code>pid</code>文件，结束进程的时候去<code>/tmp</code>目录下找到程序对应的<code>pid</code>用来结束进程并删除<code>pid</code>文件，那么引申出来一个问题，如果我们的<code>hadoop</code>组件进程启动时间超过了30天了呢，<code>pid</code>文件被清理，结束命令找不到<code>pid</code>号，会再重新创建一个<code>pid</code>，结果就是<code>pid</code>号紊乱，进程无法正常结束。</p><p><code>解决的办法</code>就是在家目录下面创建一个tmp目录，然后把hdfs和yarn的pid号管理文件夹设置成家目录下的tmp目录即可。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ vi hadoop-env.sh</span><br><span class="line">export HADOOP_PID_DIR=/home/hadoop/tmp</span><br><span class="line"></span><br><span class="line">[hadoop@aliyun hadoop]$ cat yarn-env.sh</span><br><span class="line">export YARN_PID_DIR=/home/hadoop/tmp</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> Yarn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> MapReduce </tag>
            
            <tag> Yarn </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS的伪分布式部署&amp;HADOOP的常用命令</title>
      <link href="/2018/10/04/hadoop/1/"/>
      <url>/2018/10/04/hadoop/1/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>安装hadoop的hdfs伪分布式部署</li><li>hadoop fs常规命令</li><li>配置文件在官方哪里找</li><li>整理 jdk、ssh、hosts文件</li></ol><h3 id="1-安装hadoop的hdfs伪分布式部署"><a href="#1-安装hadoop的hdfs伪分布式部署" class="headerlink" title="1.安装hadoop的hdfs伪分布式部署"></a>1.安装hadoop的hdfs伪分布式部署</h3><ol><li><p>创建用户和目录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# useradd hadoop</span><br><span class="line">[root@aliyun ~]# su - hadoop</span><br><span class="line">[hadoop@aliyun ~]$ mkdir app software sourcecode log tmp data lib</span><br><span class="line">[hadoop@aliyun ~]$ ll</span><br><span class="line">total 28</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 app　　　　#解压的文件夹  软连接</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 data　　　#数据</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 lib　　　　#第三方的jar</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 log　　　　#日志文件夹</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 software　#压缩包</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 sourcecode　　#源代码编译</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 tmp　　　　#临时文件夹</span><br></pre></td></tr></table></figure></li><li><p>下载/上传压缩包</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ cd software/</span><br><span class="line">[hadoop@aliyun software]$ wget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>解压</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun software]$ tar -xzvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ../app/</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">[hadoop@aliyun software]$ cd ../app/</span><br><span class="line">[hadoop@aliyun app]$ ln -s hadoop-2.6.0-cdh5.16.2/ hadoop</span><br><span class="line">[hadoop@aliyun app]$ ll</span><br><span class="line">total 4</span><br><span class="line">lrwxrwxrwx  1 hadoop hadoop   23 Nov 28 11:36 hadoop -&gt; hadoop-2.6.0-cdh5.16.2/</span><br><span class="line">drwxr-xr-x 14 hadoop hadoop 4096 Jun  3 19:11 hadoop-2.6.0-cdh5.16.2</span><br></pre></td></tr></table></figure></li><li><p>环境要求</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun java]# mkdir /usr/java</span><br><span class="line">[root@aliyun java]# cd /usr/java</span><br><span class="line">[root@aliyun java]# rz -E</span><br><span class="line">[root@aliyun java]# tar -xzvf jdk-8u144-linux-x64.tar.gz</span><br><span class="line">[root@aliyun java]# chown -R  root:root jdk1.8.0_144/</span><br><span class="line">[root@aliyun java]# ln -s jdk1.8.0_144/ jdk</span><br><span class="line">[root@aliyun java]# ll</span><br><span class="line">total 4</span><br><span class="line">lrwxrwxrwx 1 root root   13 Nov 28 12:01 jdk -&gt; jdk1.8.0_144/</span><br><span class="line">drwxr-xr-x 8 root root 4096 Jul 22  2017 jdk1.8.0_144</span><br><span class="line">[root@aliyun java]# vim /etc/profile</span><br><span class="line">    #env</span><br><span class="line">    export JAVA_HOME=/usr/java/jdk</span><br><span class="line">    export PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">[root@aliyun java]# source /etc/profile</span><br><span class="line">[root@aliyun java]# which java</span><br><span class="line">/usr/java/jdk/bin/java</span><br></pre></td></tr></table></figure></li><li><p>JAVA_HOME 显性配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ vi hadoop-env.sh</span><br><span class="line">export JAVA_HOME=/usr/java/jdk</span><br><span class="line">[root@aliyun java]# cat /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">172.16.39.48 aliyun</span><br></pre></td></tr></table></figure></li><li><p>配置文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">etc/hadoop/core-site.xml:</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://aliyun:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line">etc/hadoop/hdfs-site.xml:</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>ssh无密码信任关系</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">家目录下输入</span><br><span class="line"><span class="meta">  $</span><span class="bash"> ssh-keygen -t rsa -P <span class="string">''</span> -f ~/.ssh/id_rsa</span></span><br><span class="line"><span class="meta">  $</span><span class="bash"> cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span></span><br><span class="line"><span class="meta">  $</span><span class="bash"> chmod 0600 ~/.ssh/authorized_keys</span></span><br><span class="line">[hadoop@aliyun ~]$ ssh aliyun date</span><br><span class="line">Thu Nov 28 12:15:08 CST 2019</span><br></pre></td></tr></table></figure></li><li><p>环境变量 hadoop</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ vi .bashrc</span><br><span class="line">export HADOOP_HOME=/home/hadoop/app/hadoop</span><br><span class="line">export PATH=$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin:$PATH</span><br><span class="line">[hadoop@aliyun ~]$ source .bashrc </span><br><span class="line">[hadoop@aliyun ~]$ which hadoop</span><br><span class="line">~/app/hadoop/bin/hadoop</span><br></pre></td></tr></table></figure></li><li><p>格式化</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ hdfs namenode -format</span><br><span class="line">has been successfully formatted.</span><br></pre></td></tr></table></figure></li><li><p>第一次启动</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ start-dfs.sh </span><br><span class="line">[hadoop@aliyun ~]$ jps</span><br><span class="line">10804 SecondaryNameNode</span><br><span class="line">10536 NameNode</span><br><span class="line">10907 Jps</span><br><span class="line">10654 DataNode</span><br><span class="line">[hadoop@aliyun ~]$</span><br></pre></td></tr></table></figure><p>坑：第一次启动会输入yes确定信任关系，我们打开./ssh下的known_hosts文件，这个文件中存放信任关系</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun .ssh]$ cat known_hosts</span><br><span class="line">aliyun,172.16.39.48 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBCjHBKn/7LF5sfbae1OLkK5QoWm11Xn8RZs1JTc7K8v4RFum1OKIjArocvRjLOYPsq5ezYo8TlBHTrAgeUcvkBM=</span><br><span class="line">localhost ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBCjHBKn/7LF5sfbae1OLkK5QoWm11Xn8RZs1JTc7K8v4RFum1OKIjArocvRjLOYPsq5ezYo8TlBHTrAgeUcvkBM=</span><br><span class="line">0.0.0.0 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBCjHBKn/7LF5sfbae1OLkK5QoWm11Xn8RZs1JTc7K8v4RFum1OKIjArocvRjLOYPsq5ezYo8TlBHTrAgeUcvkBM=</span><br></pre></td></tr></table></figure><p>将来也许在启动hadoop的时候一直要输入密码，就是这里面已经存在了主机的信任关系，但是密匙对是新的，删除这个文件或者内容即可</p></li><li><p>DN SNN都以 aliyun启动</p></li></ol><ul><li><p>NN：core-site.xml fs.defaultFS控制</p></li><li><p>DN: slaves文件</p></li><li><p>2NN:hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;aliyun:50090&lt;/value&gt;       #注意端口号，新旧版本有区别</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;aliyun:50091&lt;/value&gt;       #注意端口号，新旧版本有区别</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li></ul><h3 id="2-hadoop-fs常规命令"><a href="#2-hadoop-fs常规命令" class="headerlink" title="2.hadoop fs常规命令"></a>2.hadoop fs常规命令</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -mkdir /</span><br><span class="line">hadoop fs -put</span><br><span class="line">hadoop fs -get</span><br><span class="line">hadoop fs -cat</span><br><span class="line">hadoop fs -rm</span><br><span class="line">hadoop fs -ls</span><br></pre></td></tr></table></figure><h3 id="3-配置文件在官方哪里找"><a href="#3-配置文件在官方哪里找" class="headerlink" title="3.配置文件在官方哪里找"></a><strong>3.配置文件在官方哪里找</strong></h3><p><strong><a href="https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation" target="_blank" rel="noopener">https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation</a></strong></p><h3 id="4-整理-jdk、ssh、hosts文件"><a href="#4-整理-jdk、ssh、hosts文件" class="headerlink" title="4.整理 jdk、ssh、hosts文件"></a>4.整理 jdk、ssh、hosts文件</h3><p>jdk和ssh是hadoop运行的先决条件</p><p>hosts文件存放主机名和ip地址的映射</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>掌握where、group、join语句和写SQL</title>
      <link href="/2018/10/03/mysql/3/"/>
      <url>/2018/10/03/mysql/3/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 sql的where各种条件</li><li>整理 sql的group</li><li>整理 sql的join</li><li>04txt文件的案例 9句sql</li><li>整理刚才分享的小知识点</li><li>补充资料文件夹 去看看执行</li><li>彩蛋 视频 sql</li></ol><h3 id="1-整理-sql-的-where-各种条件"><a href="#1-整理-sql-的-where-各种条件" class="headerlink" title="1.整理 sql 的 where 各种条件"></a>1.整理 sql 的 where 各种条件</h3><p><code>where 子句:</code> 如需有条件地从表中选取数据，可将 where 子句添加到 SELECT 语句。</p><p>语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> 列名称 <span class="keyword">FROM</span> 表名称 <span class="keyword">WHERE</span> 列 运算符 值</span><br></pre></td></tr></table></figure><p>下面的运算符可在 where 子句中使用：</p><table><thead><tr><th>操作符</th><th>描述</th></tr></thead><tbody><tr><td>=</td><td>等于</td></tr><tr><td>&lt;&gt;</td><td>不等于</td></tr><tr><td>&gt;</td><td>大于</td></tr><tr><td>&lt;</td><td>小于</td></tr><tr><td>&gt;=</td><td>大于等于</td></tr><tr><td>&lt;=</td><td>小于等于</td></tr><tr><td>BETWEEN</td><td>在某个范围内</td></tr><tr><td>LIKE</td><td>搜索某种模式</td></tr></tbody></table><h3 id="2-整理-sql-的-group"><a href="#2-整理-sql-的-group" class="headerlink" title="2.整理 sql 的 group"></a>2.整理 sql 的 group</h3><p>聚合函数 (比如 SUM) 常常需要添加 group by语句。</p><p><code>group by语句:</code> group by语句用于结合聚合函数，根据一个或多个列对结果集进行分组。</p><p>语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name, aggregate_function(column_name)</span><br><span class="line"><span class="keyword">FROM</span> table_name</span><br><span class="line"><span class="keyword">where</span> column_name <span class="keyword">operator</span> <span class="keyword">value</span></span><br><span class="line"><span class="keyword">group</span> bycolumn_name</span><br></pre></td></tr></table></figure><h3 id="3-整理-sql-的-join"><a href="#3-整理-sql-的-join" class="headerlink" title="3.整理 sql 的 join"></a>3.整理 sql 的 join</h3><p>join分为inner join、left join、right join，分别表示内联结，左联结，右联结</p><p><code>inner join:</code> 在表中存在至少一个匹配时，INNER JOIN 关键字返回行。</p><p>语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name(s)</span><br><span class="line"><span class="keyword">FROM</span> table_name1</span><br><span class="line"><span class="keyword">INNER</span> <span class="keyword">JOIN</span> table_name2 </span><br><span class="line"><span class="keyword">ON</span> table_name1.column_name=table_name2.column_name</span><br></pre></td></tr></table></figure><p><code>注意:</code> inner 与 join是相同的。</p><p><code>left join:</code> left join 关键字会从左表 (table_name1) 那里返回所有的行，即使在右表 (table_name2) 中没有匹配的行。</p><p>语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name(s)</span><br><span class="line"><span class="keyword">FROM</span> table_name1</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> table_name2 </span><br><span class="line"><span class="keyword">ON</span> table_name1.column_name=table_name2.column_name</span><br></pre></td></tr></table></figure><p><code>注意:</code> 在某些数据库中， left join 称为 left outer join。</p><p><code>right join</code> right join 关键字会右表 (table_name2) 那里返回所有的行，即使在左表 (table_name1) 中没有匹配的行。</p><p>语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name(s)</span><br><span class="line"><span class="keyword">FROM</span> table_name1</span><br><span class="line"><span class="keyword">right</span> <span class="keyword">join</span> table_name2 </span><br><span class="line"><span class="keyword">ON</span> table_name1.column_name=table_name2.column_name</span><br></pre></td></tr></table></figure><p><code>注意:</code> 在某些数据库中， right join 称为 right outer join。</p><h3 id="4-04txt文件的案例9句sql"><a href="#4-04txt文件的案例9句sql" class="headerlink" title="4. 04txt文件的案例9句sql"></a>4. 04txt文件的案例9句sql</h3><ul><li>查询出部门编号为30的所有员工的编号和姓名</li><li>找出部门编号为10中所有经理，和部门编号为20中所有销售员的详细资料。</li><li>查询所有员工详细信息，用工资降序排序，如果工资相同使用入职日期升序排序</li><li>列出薪金大于1500的各种工作及从事此工作的员工人数。</li><li>列出在销售部工作的员工的姓名，假定不知道销售部的部门编号。</li><li>查询姓名以S开头的\以S结尾\包含S字符\第二个字母为L __</li><li>查询每种工作的最高工资、最低工资、人数</li><li>列出薪金高于公司平均薪金的所有员工号，员工姓名，所在部门名称，上级领导，工资，工资等级</li><li>列出薪金高于在部门30工作的 所有/任何一个员工的薪金的员工姓名和薪金、部门名称。</li></ul><h3 id="5-整理刚才分享的小知识点"><a href="#5-整理刚才分享的小知识点" class="headerlink" title="5.整理刚才分享的小知识点"></a>5.整理刚才分享的小知识点</h3><ol><li><p>关于count()的使用细节</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">使用count(id)替换count(*)的使用，可以提升性能</span><br></pre></td></tr></table></figure></li><li><p>sum()与count()的区别与联想</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sum()计算统计字段的和，count()统计字段的数量，容易混淆，建议使用sum()联想count()，使用count()联想sum()，并区分</span><br></pre></td></tr></table></figure></li><li><p>sql语句的执行顺序</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> yyyyy <span class="keyword">from</span> rzdata</span><br><span class="line"><span class="keyword">where</span> xxx</span><br><span class="line"><span class="keyword">group</span> byxxx <span class="keyword">having</span> xxx </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> xxx</span><br><span class="line"><span class="keyword">limit</span> xxx ;</span><br></pre></td></tr></table></figure></li><li><p>all和any的区别</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">all()的用法是表示要满足字段中所有值即最大值，any()的用法表示的是满足字段值任意一个值即最小值</span><br></pre></td></tr></table></figure></li><li><p>聚合函数中的null值</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">如果在进行数值计算的时候，字段中存在null值，则计算的结果是null值，在进行数值运算的时候使用IFNULL(expression, alt_value)替换null值为0，则能计算出结果</span><br><span class="line">如果第一个参数的表达式 expression 为 NULL，则返回第二个参数的备用值。</span><br></pre></td></tr></table></figure></li><li><p>unoin和union all的区别</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">union的用法是把联合查询中的语句如果整体重复则去重，unoin all不会去重</span><br></pre></td></tr></table></figure></li></ol><h3 id="6-补充资料文件夹-去看看执行"><a href="#6-补充资料文件夹-去看看执行" class="headerlink" title="6.补充资料文件夹 去看看执行"></a>6.补充资料文件夹 去看看执行</h3><p>　　资料文件中</p><h3 id="7-彩蛋-视频-sql"><a href="#7-彩蛋-视频-sql" class="headerlink" title="7.彩蛋 视频 sql"></a>7.彩蛋 视频 sql</h3><p>　　百度云中</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataWarehouse </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>掌握MySQL的建表规范、DDL语句和权限操作</title>
      <link href="/2018/10/02/mysql/2/"/>
      <url>/2018/10/02/mysql/2/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 建表规范</li><li>整理 DDL语句的</li><li>整理 三句话</li></ol><h3 id="首先看一个建表例子，再去研究应该遵循哪些规范"><a href="#首先看一个建表例子，再去研究应该遵循哪些规范" class="headerlink" title="首先看一个建表例子，再去研究应该遵循哪些规范"></a>首先看一个建表例子，再去研究应该遵循哪些规范</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> rzdata(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">not</span> <span class="literal">null</span> auto_increment,</span><br><span class="line"></span><br><span class="line"><span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">200</span>),</span><br><span class="line">age  <span class="built_in">int</span>(<span class="number">3</span>),</span><br><span class="line"></span><br><span class="line">createuser <span class="built_in">varchar</span>(<span class="number">200</span>) ,</span><br><span class="line">createtime <span class="built_in">timestamp</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">default</span> <span class="keyword">current_timestamp</span>,</span><br><span class="line">updateuser <span class="built_in">varchar</span>(<span class="number">200</span>) ,</span><br><span class="line">updatetime <span class="built_in">timestamp</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">default</span> <span class="keyword">current_timestamp</span> <span class="keyword">on</span> <span class="keyword">update</span> <span class="keyword">current_timestamp</span>,</span><br><span class="line"></span><br><span class="line">primary <span class="keyword">key</span> (<span class="keyword">id</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure><ol><li><p>表名</p><p>不能是中文，不能是汉语拼音 ，不然很low</p></li><li><p>风格统一</p><p>统一所有表的风格，可以从已有的表中查看，或者找leader检查，方便后期维护</p></li><li><p>第一个字段</p><p>第一个字段必须是id，并且自增长，是主键，没有意义 –&gt;拓展: 为什么？</p></li><li><p>主键</p><p>一张表只有一个主键，primary key == unique+not null</p></li><li><p>后四个字段</p><p>后四个字段包括：用户、创建时间、修改用户、修改时间</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">createuser varchar(200) ,</span><br><span class="line">createtime timestamp not null default current_timestamp,</span><br><span class="line">updateuser varchar(200) ,</span><br><span class="line">updatetime timestamp not null default current_timestamp on <span class="keyword">update</span> <span class="keyword">current_timestamp</span>,</span><br></pre></td></tr></table></figure></li><li><p>业务字段</p><p>业务字段需要唯一存在，使用unique约束，如订单号</p><p>业务字段都必须加上注释</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">COMMENT</span> <span class="string">'用户名称'</span></span><br></pre></td></tr></table></figure></li><li><p>字符集CHARSET</p><p>查看字符集</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">&gt;&gt; show variables like '%char%';</span><br><span class="line">+<span class="comment">--------------------------+---------------------------------------------------------------+</span></span><br><span class="line">| Variable_name            | Value                                                         |</span><br><span class="line">+<span class="comment">--------------------------+---------------------------------------------------------------+</span></span><br><span class="line">| character_set_client     | utf8                                                          |</span><br><span class="line">| character_set_connection | utf8                                                          |</span><br><span class="line">| character_set_database   | latin1                                                        |</span><br><span class="line">| character_set_filesystem | binary                                                        |</span><br><span class="line">| character_set_results    | utf8                                                          |</span><br><span class="line">| character_set_server     | latin1                                                        |</span><br><span class="line">| character_set_system     | utf8                                                          |</span><br><span class="line">| character_sets_dir       | /usr/local/mysql-5.6.23-linux-glibc2.5-x86_64/share/charsets/ |</span><br><span class="line">+<span class="comment">--------------------------+---------------------------------------------------------------+</span></span><br><span class="line">8 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li></ol><h3 id="DDL语句以及需要注意的点"><a href="#DDL语句以及需要注意的点" class="headerlink" title="DDL语句以及需要注意的点"></a>DDL语句以及需要注意的点</h3><p><code>查询语句：</code>select 查询字段 from 表 ;</p><p>注意：</p><ol><li><p>生产环境下不要用 * 代替所有字段</p><p>错误示范：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from tb_user;</span><br><span class="line">+<span class="comment">----+----------+----------------------------------+-------------+---------------------+----------------------------------+</span></span><br><span class="line">| id | username | password                         | phone       | created             | salt                             |</span><br><span class="line">+<span class="comment">----+----------+----------------------------------+-------------+---------------------+----------------------------------+</span></span><br><span class="line">| 28 | zhangsan | e21d44f200365b57fab2641cd31226d4 | 13600527634 | 2018-05-25 17:52:03 | 05b0f203987e49d2b72b20b95e0e57d9 |</span><br><span class="line">| 30 | leyou    | 4de9a93b3f95d468874a3c1bf3b25a48 | 15855410440 | 2018-09-30 11:37:30 | 4565613d4b0e434cb496d4eb87feb45f |</span><br><span class="line">+<span class="comment">----+----------+----------------------------------+-------------+---------------------+----------------------------------+</span></span><br><span class="line">2 rows in <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>正确示范：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; select username,password from tb_user;</span><br><span class="line">+<span class="comment">----------+----------------------------------+</span></span><br><span class="line">| username | password                         |</span><br><span class="line">+<span class="comment">----------+----------------------------------+</span></span><br><span class="line">| zhangsan | e21d44f200365b57fab2641cd31226d4 |</span><br><span class="line">| leyou    | 4de9a93b3f95d468874a3c1bf3b25a48 |</span><br><span class="line">+<span class="comment">----------+----------------------------------+</span></span><br><span class="line">2 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>查询语句如果数据量特别大必须使用where 或者 limit，否则需要使用大量的资源</p><p>错误示范：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,image,letter <span class="keyword">from</span> tb_brand;</span><br></pre></td></tr></table></figure><p>正确示范：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,image,letter <span class="keyword">from</span> tb_brand <span class="keyword">limit</span> <span class="number">100</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,image,letter <span class="keyword">from</span> tb_brand <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">100</span>;</span><br></pre></td></tr></table></figure></li></ol><p><code>新增语句：</code>insert into 表名（字段1，字段2…） values（数据1，数据2…）;</p><p>注意：在表名后加上对应要添加的字段名</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tb_brand (<span class="keyword">name</span>，letter) <span class="keyword">values</span>(tunan，T);</span><br></pre></td></tr></table></figure><p><code>修改语句：</code>update 表名 set 修改后的字段 where 条件；</p><p>注意：一定要加上条件，否则是全局修改</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> tb_brand <span class="keyword">set</span> <span class="keyword">name</span>=<span class="string">"xiaoqi"</span> <span class="keyword">where</span> <span class="keyword">id</span> = <span class="number">1</span>;</span><br></pre></td></tr></table></figure><p><code>删除语句：</code>delete from 表名 where 条件；</p><p>注意：一定要加上条件，否则是全局删除；</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> tb_brand tb <span class="keyword">where</span> <span class="keyword">id</span> = <span class="number">1</span>;</span><br></pre></td></tr></table></figure><h3 id="当某条SQL验证拖累进程时怎么办？"><a href="#当某条SQL验证拖累进程时怎么办？" class="headerlink" title="当某条SQL验证拖累进程时怎么办？"></a>当某条SQL验证拖累进程时怎么办？</h3><p>使用 show processlist；查看mysql中的 sql 进程</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; show processlist;</span><br><span class="line">+<span class="comment">-----+------+---------------------+--------+---------+------+----------+------------------+</span></span><br><span class="line">| Id  | User | Host                | db     | Command | Time | State    | Info             |</span><br><span class="line">+<span class="comment">-----+------+---------------------+--------+---------+------+----------+------------------+</span></span><br><span class="line">| 272 | root | localhost           | leyou1 | Query   |    0 | starting | <span class="keyword">show</span> <span class="keyword">processlist</span> |</span><br><span class="line">| <span class="number">273</span> | root | <span class="number">121.62</span><span class="number">.184</span><span class="number">.34</span>:<span class="number">56629</span> | leyou1 | <span class="keyword">Sleep</span>   |  <span class="number">448</span> |          | <span class="literal">NULL</span>             |</span><br><span class="line">| <span class="number">274</span> | root | <span class="number">121.62</span><span class="number">.184</span><span class="number">.34</span>:<span class="number">56631</span> | leyou1 | <span class="keyword">Sleep</span>   |  <span class="number">592</span> |          | <span class="literal">NULL</span>             |</span><br><span class="line">+<span class="comment">-----+------+---------------------+--------+---------+------+----------+------------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>然后根据 id 删除即可</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">kill</span> <span class="keyword">id</span>;</span><br></pre></td></tr></table></figure><h3 id="必须要记住的三条命令"><a href="#必须要记住的三条命令" class="headerlink" title="必须要记住的三条命令"></a>必须要记住的三条命令</h3><p><code>修改密码：</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> <span class="keyword">user</span> <span class="keyword">set</span> <span class="keyword">password</span>=<span class="keyword">password</span>(<span class="string">'密码'</span>) <span class="keyword">where</span> <span class="keyword">user</span>=<span class="string">'用户名'</span>;</span><br></pre></td></tr></table></figure><p><code>修改权限:</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> 用户名@<span class="string">'%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'密码'</span>;</span><br></pre></td></tr></table></figure><p><code>刷新权限：</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataWarehouse </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL二进制部署和DBeaver连接MySQL</title>
      <link href="/2018/10/01/mysql/1/"/>
      <url>/2018/10/01/mysql/1/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>二进制部署mysql</li><li>重新部署</li><li>部署dbeaver 打通 mysql</li></ol><h3 id="二进制部署mysql"><a href="#二进制部署mysql" class="headerlink" title="二进制部署mysql"></a>二进制部署mysql</h3><p>[<a href="https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt]" target="_blank" rel="noopener">https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt]</a>(<a href="https://github.com/Hackeruncle/MySQL/blob/master/MySQL" target="_blank" rel="noopener">https://github.com/Hackeruncle/MySQL/blob/master/MySQL</a> 5.6.23 Install.txt)</p><p>京东云下部署代码：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">scripts/mysql_install_db  \</span><br><span class="line"><span class="comment">--user=mysqladmin \</span></span><br><span class="line"><span class="comment">--basedir=/usr/local/mysql \</span></span><br><span class="line"><span class="comment">--datadir=/usr/local/mysql/data</span></span><br></pre></td></tr></table></figure><p>必须要装三个环境：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">yum <span class="keyword">install</span> -y perl</span><br><span class="line">yum <span class="keyword">install</span> -y autoconf</span><br><span class="line">yum <span class="keyword">install</span> -y libaio</span><br></pre></td></tr></table></figure><h3 id="重新部署"><a href="#重新部署" class="headerlink" title="重新部署"></a>重新部署</h3><ol><li><p>删除压缩文件和数据文件</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">rm -rf arch<span class="comment">/* data/*</span></span><br></pre></td></tr></table></figure></li><li><p>重置执行脚本文件</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">scripts/mysql_install_db  \</span><br><span class="line"><span class="comment">--user=mysqladmin \</span></span><br><span class="line"><span class="comment">--basedir=/usr/local/mysql \</span></span><br><span class="line"><span class="comment">--datadir=/usr/local/mysql/data</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="部署dbeaver-打通-mysql"><a href="#部署dbeaver-打通-mysql" class="headerlink" title="部署dbeaver 打通 mysql"></a>部署dbeaver 打通 mysql</h3><p>修改用户密码：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> <span class="keyword">user</span> <span class="keyword">set</span> <span class="keyword">password</span>=<span class="keyword">password</span>(<span class="string">'ruozedata'</span>) <span class="keyword">where</span> <span class="keyword">user</span>=<span class="string">'root'</span>;</span><br></pre></td></tr></table></figure><p>查看用户权限信息：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">user</span>,<span class="keyword">password</span>,host <span class="keyword">from</span> <span class="keyword">user</span>;</span><br></pre></td></tr></table></figure><p>给用户添加权限：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> root@<span class="string">'%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'ruozedata'</span>;</span><br><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;　　<span class="comment">#刷新权限</span></span><br></pre></td></tr></table></figure><p>DBeaver连接Mysql：</p><p><img src="https://img2018.cnblogs.com/i-beta/1657732/201911/1657732-20191121181225170-233024101.png" alt="DBeaver连接设置"> </p><p> <img src="https://img2018.cnblogs.com/i-beta/1657732/201911/1657732-20191121181334218-1241892255.png" alt="DBeaver连接添加jar包"></p><h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><p>为了保证 ip 地址的安全性和可用性</p><p>　　1. 在 linux 的 /etc/hosts 文件中配置 内网ip 地址和主机名的映射环境，这样在shell脚本或者代码中使用主机名替代 ip 使用</p><p>　　2. 如果是云主机，在windows中的hosts文件中配置外网ip地址和主机名的映射</p><p><code>注意：</code>hosts文件中的前两行切记不能删，否则可能带来bug</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataWarehouse </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的test命令</title>
      <link href="/2018/09/30/linux/shell/9/"/>
      <url>/2018/09/30/linux/shell/9/</url>
      
        <content type="html"><![CDATA[<p>Shell中的 test 命令用于检查某个条件是否成立，它可以进行<code>数值</code>、<code>字符</code>和<code>文件</code>三个方面的测试。</p><h3 id="数值测试"><a href="#数值测试" class="headerlink" title="数值测试"></a>数值测试</h3><table><thead><tr><th align="left">参数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">-eq</td><td align="left">等于则为真</td></tr><tr><td align="left">-ne</td><td align="left">不等于则为真</td></tr><tr><td align="left">-gt</td><td align="left">大于则为真</td></tr><tr><td align="left">-ge</td><td align="left">大于等于则为真</td></tr><tr><td align="left">-lt</td><td align="left">小于则为真</td></tr><tr><td align="left">-le</td><td align="left">小于等于则为真</td></tr></tbody></table><p>实例演示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">num1=100</span><br><span class="line">num2=100</span><br><span class="line">if test $[num1] -eq $[num2]</span><br><span class="line">then</span><br><span class="line">    echo '两个数相等！'</span><br><span class="line">else</span><br><span class="line">    echo '两个数不相等！'</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">两个数相等！</span><br></pre></td></tr></table></figure><p>代码中的 [] 执行基本的算数运算，如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">a=5</span><br><span class="line">b=6</span><br><span class="line"></span><br><span class="line">result=$[a+b] # 注意等号两边不能有空格</span><br><span class="line">echo "result 为： $result"</span><br></pre></td></tr></table></figure><p>结果为:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">result 为： 11</span><br></pre></td></tr></table></figure><h3 id="字符串测试"><a href="#字符串测试" class="headerlink" title="字符串测试"></a>字符串测试</h3><table><thead><tr><th align="left">参数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">=</td><td align="left">等于则为真</td></tr><tr><td align="left">!=</td><td align="left">不相等则为真</td></tr><tr><td align="left">-z 字符串</td><td align="left">字符串的长度为零则为真</td></tr><tr><td align="left">-n 字符串</td><td align="left">字符串的长度不为零则为真</td></tr></tbody></table><p>实例演示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">num1="ru1noob"</span><br><span class="line">num2="runoob"</span><br><span class="line">if test $num1 = $num2</span><br><span class="line">then</span><br><span class="line">    echo '两个字符串相等!'</span><br><span class="line">else</span><br><span class="line">    echo '两个字符串不相等!'</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">两个字符串不相等!</span><br></pre></td></tr></table></figure><h3 id="文件测试"><a href="#文件测试" class="headerlink" title="文件测试"></a>文件测试</h3><table><thead><tr><th align="left">参数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">-e 文件名</td><td align="left">如果文件存在则为真</td></tr><tr><td align="left">-r 文件名</td><td align="left">如果文件存在且可读则为真</td></tr><tr><td align="left">-w 文件名</td><td align="left">如果文件存在且可写则为真</td></tr><tr><td align="left">-x 文件名</td><td align="left">如果文件存在且可执行则为真</td></tr><tr><td align="left">-s 文件名</td><td align="left">如果文件存在且至少有一个字符则为真</td></tr><tr><td align="left">-d 文件名</td><td align="left">如果文件存在且为目录则为真</td></tr><tr><td align="left">-f 文件名</td><td align="left">如果文件存在且为普通文件则为真</td></tr><tr><td align="left">-c 文件名</td><td align="left">如果文件存在且为字符型特殊文件则为真</td></tr><tr><td align="left">-b 文件名</td><td align="left">如果文件存在且为块特殊文件则为真</td></tr></tbody></table><p>实例演示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /bin</span><br><span class="line">if test -e ./bash</span><br><span class="line">then</span><br><span class="line">    echo '文件已存在!'</span><br><span class="line">else</span><br><span class="line">    echo '文件不存在!'</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">文件已存在!</span><br></pre></td></tr></table></figure><p>另外，Shell还提供了与( -a )、或( -o )、非( ! )三个逻辑操作符用于将测试条件连接起来，其优先级为：”!”最高，”-a”次之，”-o”最低。例如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /bin</span><br><span class="line">if test -e ./notFile -o -e ./bash</span><br><span class="line">then</span><br><span class="line">    echo '至少有一个文件存在!'</span><br><span class="line">else</span><br><span class="line">    echo '两个文件都不存在'</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">至少有一个文件存在!</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的流程控制</title>
      <link href="/2018/09/29/linux/shell/8/"/>
      <url>/2018/09/29/linux/shell/8/</url>
      
        <content type="html"><![CDATA[<p>和Java、PHP等语言不一样，shell的流程控制不可缺少</p><h3 id="判断语句"><a href="#判断语句" class="headerlink" title="判断语句"></a>判断语句</h3><h4 id="if"><a href="#if" class="headerlink" title="if"></a>if</h4><p>if 语句语法格式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if condition</span><br><span class="line">then</span><br><span class="line">    command1 </span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN </span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>写成一行（适用于终端命令提示符）：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if [ $(ps -ef | grep -c "ssh") -gt 1 ]; then echo "true"; fi</span><br></pre></td></tr></table></figure><p>末尾的fi就是if倒过来拼写，后面还会遇到类似的。</p><h4 id="if-else"><a href="#if-else" class="headerlink" title="if else"></a>if else</h4><p>if else 语法格式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if condition</span><br><span class="line">then</span><br><span class="line">    command1 </span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN</span><br><span class="line">else</span><br><span class="line">    command</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h4 id="if-else-if-else"><a href="#if-else-if-else" class="headerlink" title="if else-if else"></a>if else-if else</h4><p>if else-if else 语法格式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if condition1</span><br><span class="line">then</span><br><span class="line">    command1</span><br><span class="line">elif condition2 </span><br><span class="line">then </span><br><span class="line">    command2</span><br><span class="line">else</span><br><span class="line">    commandN</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>以下实例判断两个变量是否相等：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a=10</span><br><span class="line">b=20</span><br><span class="line">if [ $a == $b ]</span><br><span class="line">then</span><br><span class="line">   echo "a 等于 b"</span><br><span class="line">elif [ $a -gt $b ]</span><br><span class="line">then</span><br><span class="line">   echo "a 大于 b"</span><br><span class="line">elif [ $a -lt $b ]</span><br><span class="line">then</span><br><span class="line">   echo "a 小于 b"</span><br><span class="line">else</span><br><span class="line">   echo "没有符合的条件"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a 小于 b</span><br></pre></td></tr></table></figure><p>if else语句经常与test命令结合使用，如下所示：</p><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">num1=$[<span class="number">2</span>*<span class="number">3</span>]</span><br><span class="line">num2=$[<span class="number">1</span>+<span class="number">5</span>]</span><br><span class="line"><span class="keyword">if</span> test $[num1] <span class="nomarkup">-eq</span> $[num2]</span><br><span class="line">then</span><br><span class="line">    echo <span class="string">'两个数字相等!'</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    echo <span class="string">'两个数字不相等!'</span></span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">两个数字相等!</span><br></pre></td></tr></table></figure><hr><h3 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h3><h4 id="for循环"><a href="#for循环" class="headerlink" title="for循环"></a>for循环</h4><p>与其他编程语言类似，Shell支持for循环。</p><p>for循环一般格式为：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for var in item1 item2 ... itemN</span><br><span class="line">do</span><br><span class="line">    command1</span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>写成一行：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for var in item1 item2 ... itemN; do command1; command2… done;</span><br></pre></td></tr></table></figure><p>当变量值在列表里，for循环即执行一次所有命令，使用变量名获取列表中的当前取值。命令可为任何有效的shell命令和语句。in列表可以包含替换、字符串和文件名。</p><p>in列表是可选的，如果不用它，for循环使用命令行的位置参数。</p><p>例如，顺序输出当前列表中的数字：</p><p><code>方法1:</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for var in 1 2 3 4 5 </span><br><span class="line">do</span><br><span class="line">    echo "The value is: $var"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p><code>方法2:</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for var in &#123;1..5&#125;</span><br><span class="line">do</span><br><span class="line">    echo "The value is: $var"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p><code>方法3:</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for ((var=1;var&lt;=5;var++))</span><br><span class="line">do</span><br><span class="line">    echo "The value is: $var"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">The value is: 1</span><br><span class="line">The value is: 2</span><br><span class="line">The value is: 3</span><br><span class="line">The value is: 4</span><br><span class="line">The value is: 5</span><br></pre></td></tr></table></figure><p>顺序输出字符串中的字符：</p><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> str <span class="keyword">in</span> <span class="string">'This is a string'</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    echo <span class="variable">$str</span></span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">This is a string</span><br></pre></td></tr></table></figure><h4 id="while-语句"><a href="#while-语句" class="headerlink" title="while 语句"></a>while 语句</h4><p>while循环用于不断执行一系列命令，也用于从输入文件中读取数据；命令通常为测试条件。其格式为：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">while condition</span><br><span class="line">do</span><br><span class="line">    command</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>以下是一个基本的while循环，测试条件是：如果int小于等于5，那么条件返回真。int从0开始，每次循环处理时，int加1。运行上述脚本，返回数字1到5，然后终止。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">int=1</span><br><span class="line">while(( $int&lt;=5 ))</span><br><span class="line">do</span><br><span class="line">    echo $int</span><br><span class="line">    let "int++"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>运行脚本，输出：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td></tr></table></figure><p>以上实例使用了 Bash let 命令，let 命令是 BASH 中用于计算的工具，用于执行一个或多个表达式，变量计算中不需要加上 $ 来表示变量。如果表达式中包含了空格或其他特殊字符，则必须引起来。</p><p>while循环可用于读取键盘信息。下面的例子中，输入信息被设置为变量FILM，按<Ctrl-D>结束循环。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo '按下 &lt;CTRL-D&gt; 退出'</span><br><span class="line">echo -n '输入你最喜欢的网站名: '</span><br><span class="line">while read FILM</span><br><span class="line">do</span><br><span class="line">    echo "是的！$FILM 是一个好网站"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>运行脚本，输出类似下面：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">按下 &lt;CTRL-D&gt; 退出</span><br><span class="line">输入你最喜欢的网站名:菜鸟教程</span><br><span class="line">是的！菜鸟教程 是一个好网站</span><br></pre></td></tr></table></figure><h4 id="无限循环"><a href="#无限循环" class="headerlink" title="无限循环"></a>无限循环</h4><p>无限循环语法格式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">while :</span><br><span class="line">do</span><br><span class="line">    command</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">while true</span><br><span class="line">do</span><br><span class="line">    command</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for (( ; ; ))</span><br></pre></td></tr></table></figure><h3 id="开关语句"><a href="#开关语句" class="headerlink" title="开关语句"></a>开关语句</h3><h4 id="case"><a href="#case" class="headerlink" title="case"></a>case</h4><p>Shell case语句为多选择语句。可以用case语句匹配一个值与一个模式，如果匹配成功，执行相匹配的命令。case语句格式如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">case 值 in</span><br><span class="line">模式1)</span><br><span class="line">    command1</span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN</span><br><span class="line">    ;;</span><br><span class="line">模式2）</span><br><span class="line">    command1</span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><p>case工作方式如上所示。取值后面必须为单词in，每一模式必须以右括号结束。取值可以为变量或常数。匹配发现取值符合某一模式后，其间所有命令开始执行直至 ;;。</p><p>取值将检测匹配的每一个模式。一旦模式匹配，则执行完匹配模式相应命令后不再继续其他模式。如果无一匹配模式，使用星号 * 捕获该值，再执行后面的命令。</p><p>下面的脚本提示输入1到4，与每一种模式进行匹配：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo '输入 1 到 4 之间的数字:'</span><br><span class="line">echo '你输入的数字为:'</span><br><span class="line">read aNum</span><br><span class="line">case $aNum in</span><br><span class="line">    1)  echo '你选择了 1'</span><br><span class="line">    ;;</span><br><span class="line">    2)  echo '你选择了 2'</span><br><span class="line">    ;;</span><br><span class="line">    3)  echo '你选择了 3'</span><br><span class="line">    ;;</span><br><span class="line">    4)  echo '你选择了 4'</span><br><span class="line">    ;;</span><br><span class="line">    *)  echo '你没有输入 1 到 4 之间的数字'</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><p>输入不同的内容，会有不同的结果，例如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">输入 1 到 4 之间的数字:</span><br><span class="line">你输入的数字为:</span><br><span class="line">3</span><br><span class="line">你选择了 3</span><br></pre></td></tr></table></figure><h3 id="跳出循环"><a href="#跳出循环" class="headerlink" title="跳出循环"></a>跳出循环</h3><p>在循环过程中，有时候需要在未达到循环结束条件时强制跳出循环，Shell使用两个命令来实现该功能：break和continue。</p><h4 id="break命令"><a href="#break命令" class="headerlink" title="break命令"></a>break命令</h4><p>break命令允许跳出所有循环（终止执行后面的所有循环）。</p><p>下面的例子中，脚本进入死循环直至用户输入数字大于5。要跳出这个循环，返回到shell提示符下，需要使用break命令。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">while :</span><br><span class="line">do</span><br><span class="line">    echo -n "输入 1 到 5 之间的数字:"</span><br><span class="line">    read aNum</span><br><span class="line">    case $aNum in</span><br><span class="line">        1|2|3|4|5) echo "你输入的数字为 $aNum!"</span><br><span class="line">        ;;</span><br><span class="line">        *) echo "你输入的数字不是 1 到 5 之间的! 游戏结束"</span><br><span class="line">            break</span><br><span class="line">        ;;</span><br><span class="line">    esac</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>执行以上代码，输出结果为：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">输入 1 到 5 之间的数字:3</span><br><span class="line">你输入的数字为 3!</span><br><span class="line">输入 1 到 5 之间的数字:7</span><br><span class="line">你输入的数字不是 1 到 5 之间的! 游戏结束</span><br></pre></td></tr></table></figure><h4 id="continue"><a href="#continue" class="headerlink" title="continue"></a>continue</h4><p>continue命令与break命令类似，只有一点差别，它不会跳出所有循环，仅仅跳出当前循环。</p><p>对上面的例子进行修改：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">while :</span><br><span class="line">do</span><br><span class="line">    echo -n "输入 1 到 5 之间的数字: "</span><br><span class="line">    read aNum</span><br><span class="line">    case $aNum in</span><br><span class="line">        1|2|3|4|5) echo "你输入的数字为 $aNum!"</span><br><span class="line">        ;;</span><br><span class="line">        *) echo "你输入的数字不是 1 到 5 之间的!"</span><br><span class="line">            continue</span><br><span class="line">            echo "游戏结束"</span><br><span class="line">        ;;</span><br><span class="line">    esac</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>运行代码发现，当输入大于5的数字时，该例中的循环不会结束，语句 <strong>echo “游戏结束”</strong> 永远不会被执行。</p><hr><h4 id="esac"><a href="#esac" class="headerlink" title="esac"></a>esac</h4><p>case的语法和C family语言差别很大，它需要一个esac（就是case反过来）作为结束标记，每个case分支用右圆括号，用两个分号表示break。</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的printf命令</title>
      <link href="/2018/09/28/linux/shell/7/"/>
      <url>/2018/09/28/linux/shell/7/</url>
      
        <content type="html"><![CDATA[<p>printf 命令模仿 C 程序库（library）里的 printf() 程序。</p><p>printf 由 POSIX 标准所定义，因此使用 printf 的脚本比使用 echo 移植性好。</p><p>printf 使用引用文本或空格分隔的参数，外面可以在 printf 中使用格式化字符串，还可以制定字符串的宽度、左右对齐方式等。默认 printf 不会像 echo 自动添加换行符，我们可以手动添加 \n。</p><p>printf 命令的语法：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">printf  format-string  [arguments...]</span><br></pre></td></tr></table></figure><p><strong>参数说明：</strong></p><ul><li><strong>format-string:</strong> 为格式控制字符串</li><li><strong>arguments:</strong> 为参数列表。</li></ul><p>实例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">"Hello, Shell"</span></span></span><br><span class="line">Hello, Shell</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">printf</span> <span class="string">"Hello, Shell\n"</span></span></span><br><span class="line">Hello, Shell</span><br><span class="line"><span class="meta">$</span></span><br></pre></td></tr></table></figure><p>接下来,我来用一个脚本来体现printf的强大功能：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">printf "%-10s %-8s %-4s\n" 姓名 性别 体重kg  </span><br><span class="line">printf "%-10s %-8s %-4.2f\n" 郭靖 男 66.1234 </span><br><span class="line">printf "%-10s %-8s %-4.2f\n" 杨过 男 48.6543 </span><br><span class="line">printf "%-10s %-8s %-4.2f\n" 郭芙 女 47.9876</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">姓名     性别   体重kg</span><br><span class="line">郭靖     男      66.12</span><br><span class="line">杨过     男      48.65</span><br><span class="line">郭芙     女      47.99</span><br></pre></td></tr></table></figure><p>%s %c %d %f都是格式替代符</p><p>%-10s 指一个宽度为10个字符（-表示左对齐，没有则表示右对齐），任何字符都会被显示在10个字符宽的字符内，如果不足则自动以空格填充，超过也会将内容全部显示出来。</p><p>%-4.2f 指格式化为小数，其中.2指保留2位小数。</p><p>更多实例：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> format-string为双引号</span></span><br><span class="line">printf "%d %s\n" 1 "abc"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 单引号与双引号效果一样 </span></span><br><span class="line">printf '%d %s\n' 1 "abc" </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 没有引号也可以输出</span></span><br><span class="line">printf %s abcdef</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 格式只指定了一个参数，但多出的参数仍然会按照该格式输出，format-string 被重用</span></span><br><span class="line">printf %s abc def</span><br><span class="line"></span><br><span class="line">printf "%s\n" abc def</span><br><span class="line"></span><br><span class="line">printf "%s %s %s\n" a b c d e f g h i j</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果没有 arguments，那么 %s 用NULL代替，%d 用 0 代替</span></span><br><span class="line">printf "%s and %d \n"</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1 abc</span><br><span class="line">1 abc</span><br><span class="line">abcdefabcdefabc</span><br><span class="line">def</span><br><span class="line">a b c</span><br><span class="line">d e f</span><br><span class="line">g h i</span><br><span class="line">j  </span><br><span class="line"> and 0</span><br></pre></td></tr></table></figure><h3 id="printf的转义序列"><a href="#printf的转义序列" class="headerlink" title="printf的转义序列"></a>printf的转义序列</h3><table><thead><tr><th align="left">序列</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">\a</td><td align="left">警告字符，通常为ASCII的BEL字符</td></tr><tr><td align="left">\b</td><td align="left">后退</td></tr><tr><td align="left">\c</td><td align="left">抑制（不显示）输出结果中任何结尾的换行字符（只在%b格式指示符控制下的参数字符串中有效），而且，任何留在参数里的字符、任何接下来的参数以及任何留在格式字符串中的字符，都被忽略</td></tr><tr><td align="left">\f</td><td align="left">换页（formfeed）</td></tr><tr><td align="left">\n</td><td align="left">换行</td></tr><tr><td align="left">\r</td><td align="left">回车（Carriage return）</td></tr><tr><td align="left">\t</td><td align="left">水平制表符</td></tr><tr><td align="left">\v</td><td align="left">垂直制表符</td></tr><tr><td align="left">\</td><td align="left">一个字面上的反斜杠字符</td></tr><tr><td align="left">\ddd</td><td align="left">表示1到3位数八进制值的字符。仅在格式字符串中有效</td></tr><tr><td align="left">\0ddd</td><td align="left">表示1到3位的八进制值字符</td></tr></tbody></table><p>实例:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">printf</span> <span class="string">"a string, no processing:&lt;%s&gt;\n"</span> <span class="string">"A\nB"</span></span></span><br><span class="line">a string, no processing:&lt;A\nB&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">printf</span> <span class="string">"a string, no processing:&lt;%b&gt;\n"</span> <span class="string">"A\nB"</span></span></span><br><span class="line">a string, no processing:&lt;A</span><br><span class="line"><span class="meta">B&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">printf</span> <span class="string">"www.runoob.com \a"</span></span></span><br><span class="line">www.runoob.com $                  #不换行</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的echo命令</title>
      <link href="/2018/09/27/linux/shell/6/"/>
      <url>/2018/09/27/linux/shell/6/</url>
      
        <content type="html"><![CDATA[<p>Shell 的 echo 指令与 PHP 的 echo 指令类似，都是用于字符串的输出。命令格式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo string</span><br></pre></td></tr></table></figure><p>您可以使用echo实现更复杂的输出格式控制。</p><h3 id="显示普通字符串"><a href="#显示普通字符串" class="headerlink" title="显示普通字符串:"></a>显示普通字符串:</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "It is a test"</span><br></pre></td></tr></table></figure><p>这里的双引号完全可以省略，以下命令与上面实例效果一致：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo It is a test</span><br></pre></td></tr></table></figure><h3 id="显示转义字符"><a href="#显示转义字符" class="headerlink" title="显示转义字符"></a>显示转义字符</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "\"It is a test\""</span><br></pre></td></tr></table></figure><p>结果将是:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">"It is a test"</span><br></pre></td></tr></table></figure><p>同样，双引号也可以省略</p><h3 id="显示变量"><a href="#显示变量" class="headerlink" title="显示变量"></a>显示变量</h3><p>read 命令从标准输入中读取一行,并把输入行的每个字段的值指定给 shell 变量</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line">read name </span><br><span class="line">echo "$name It is a test"</span><br></pre></td></tr></table></figure><p>以上代码保存为 test.sh，name 接收标准输入的变量，结果将是:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@www ~]# sh test.sh</span><br><span class="line">OK                     #标准输入</span><br><span class="line">OK It is a test        #输出</span><br></pre></td></tr></table></figure><h3 id="显示换行"><a href="#显示换行" class="headerlink" title="显示换行"></a>显示换行</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo -e "OK! \n" # -e 开启转义</span><br><span class="line">echo "It is a test"</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">OK!</span><br><span class="line"></span><br><span class="line">It is a test</span><br></pre></td></tr></table></figure><h3 id="显示不换行"><a href="#显示不换行" class="headerlink" title="显示不换行"></a>显示不换行</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line">echo -e "OK! \c" # -e 开启转义 \c 不换行</span><br><span class="line">echo "It is a test"</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">OK! It is a test</span><br></pre></td></tr></table></figure><h3 id="显示结果定向至文件"><a href="#显示结果定向至文件" class="headerlink" title="显示结果定向至文件"></a>显示结果定向至文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "It is a test" &gt; myfile</span><br></pre></td></tr></table></figure><h3 id="原样输出字符串，不进行转义或取变量-用单引号"><a href="#原样输出字符串，不进行转义或取变量-用单引号" class="headerlink" title="原样输出字符串，不进行转义或取变量(用单引号)"></a>原样输出字符串，不进行转义或取变量(用单引号)</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo '$name\"'</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">name\"</span></span><br></pre></td></tr></table></figure><h3 id="显示命令执行结果"><a href="#显示命令执行结果" class="headerlink" title="显示命令执行结果"></a>显示命令执行结果</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo `date`</span><br></pre></td></tr></table></figure><p><strong>注意：</strong> 这里使用的是反引号 <strong>`</strong>, 而不是单引号 <strong>‘</strong>。</p><p>结果将显示当前日期</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Thu Jul 24 10:08:46 CST 2014</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的运算符</title>
      <link href="/2018/09/26/linux/shell/5/"/>
      <url>/2018/09/26/linux/shell/5/</url>
      
        <content type="html"><![CDATA[<p>Shell 和其他编程语言一样，支持多种运算符，包括：</p><ul><li>算数运算符</li><li>关系运算符</li><li>布尔运算符</li><li>字符串运算符</li><li>文件测试运算符</li></ul><p>原生bash不支持简单的数学运算，但是可以通过其他命令来实现，例如 awk 和 expr，expr 最常用。</p><p>expr 是一款表达式计算工具，使用它能完成表达式的求值操作。</p><p>例如，两个数相加(注意使用的是反引号 ` 而不是单引号 ‘)：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">val=`expr 2 + 2`</span><br><span class="line">echo &quot;两数之和为 : $val&quot;</span><br></pre></td></tr></table></figure><p>运行实例</p><p>执行脚本，输出结果如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">两数之和为 : 4</span><br></pre></td></tr></table></figure><p>两点注意：</p><ul><li>表达式和运算符之间要有空格，例如 2+2 是不对的，必须写成 2 + 2，这与我们熟悉的大多数编程语言不一样。</li><li>完整的表达式要被 <code></code> 包含，注意这个字符不是常用的单引号，在 Esc 键下边。</li></ul><h3 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h3><p>下表列出了常用的算术运算符，假定变量 a 为 10，变量 b 为 20：</p><table><thead><tr><th align="left">运算符</th><th align="left">说明</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">+</td><td align="left">加法</td><td align="left"><code>expr $a + $b</code> 结果为 30。</td></tr><tr><td align="left">-</td><td align="left">减法</td><td align="left"><code>expr $a - $b</code> 结果为 -10。</td></tr><tr><td align="left">*</td><td align="left">乘法</td><td align="left"><code>expr $a * $b</code> 结果为  200。</td></tr><tr><td align="left">/</td><td align="left">除法</td><td align="left"><code>expr $b / $a</code> 结果为 2。</td></tr><tr><td align="left">%</td><td align="left">取余</td><td align="left"><code>expr $b % $a</code> 结果为 0。</td></tr><tr><td align="left">=</td><td align="left">赋值</td><td align="left">a=$b 将把变量 b 的值赋给 a。</td></tr><tr><td align="left">==</td><td align="left">相等。用于比较两个数字，相同则返回 true。</td><td align="left">[ $a == $b ] 返回 false。</td></tr><tr><td align="left">!=</td><td align="left">不相等。用于比较两个数字，不相同则返回 true。</td><td align="left">[ $a != $b ] 返回 true。</td></tr></tbody></table><p>注意：条件表达式要放在方括号之间，并且要有空格，例如: [$a==$b] 是错误的，必须写成 [ $a == $b ]。</p><p>算术运算符实例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a=10</span><br><span class="line">b=20</span><br><span class="line"></span><br><span class="line">val=`expr $a + $b`</span><br><span class="line">echo "a + b : $val"</span><br><span class="line"></span><br><span class="line">val=`expr $a - $b`</span><br><span class="line">echo "a - b : $val"</span><br><span class="line"></span><br><span class="line">val=`expr $a * $b`</span><br><span class="line">echo "a * b : $val"</span><br><span class="line"></span><br><span class="line">val=`expr $b / $a`</span><br><span class="line">echo "b / a : $val"</span><br><span class="line"></span><br><span class="line">val=`expr $b % $a`</span><br><span class="line">echo "b % a : $val"</span><br><span class="line"></span><br><span class="line">if [ $a == $b ]</span><br><span class="line">then</span><br><span class="line">  echo "a 等于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a != $b ]</span><br><span class="line">then</span><br><span class="line">  echo "a 不等于 b"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a + b : 30</span><br><span class="line">a - b : -10</span><br><span class="line">a * b : 200</span><br><span class="line">b / a : 2</span><br><span class="line">b % a : 0</span><br><span class="line">a 不等于 b</span><br></pre></td></tr></table></figure><p>注意：</p><ul><li>乘号()前边必须加反斜杠()才能实现乘法运算；</li><li>if…then…fi 是条件语句，后续将会讲解。</li><li>在 MAC 中 shell 的 expr 语法是：$((表达式))，此处表达式中的 “” 不需要转义符号 “&quot; 。</li></ul><h3 id="关系运算符"><a href="#关系运算符" class="headerlink" title="关系运算符"></a>关系运算符</h3><p>关系运算符只支持数字，不支持字符串，除非字符串的值是数字。</p><p>下表列出了常用的关系运算符，假定变量 a 为 10，变量 b 为 20：</p><table><thead><tr><th align="left">运算符</th><th align="left">说明</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">-eq</td><td align="left">检测两个数是否相等，相等返回 true。</td><td align="left">[ $a -eq $b ] 返回 false。</td></tr><tr><td align="left">-ne</td><td align="left">检测两个数是否不相等，不相等返回 true。</td><td align="left">[ $a -ne $b ] 返回 true。</td></tr><tr><td align="left">-gt</td><td align="left">检测左边的数是否大于右边的，如果是，则返回 true。</td><td align="left">[ $a -gt $b ] 返回 false。</td></tr><tr><td align="left">-lt</td><td align="left">检测左边的数是否小于右边的，如果是，则返回 true。</td><td align="left">[ $a -lt $b ] 返回 true。</td></tr><tr><td align="left">-ge</td><td align="left">检测左边的数是否大于等于右边的，如果是，则返回 true。</td><td align="left">[ $a -ge $b ] 返回 false。</td></tr><tr><td align="left">-le</td><td align="left">检测左边的数是否小于等于右边的，如果是，则返回 true。</td><td align="left">[ $a -le $b ] 返回 true。</td></tr></tbody></table><p>关系运算符实例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a=10</span><br><span class="line">b=20</span><br><span class="line"></span><br><span class="line">if [ $a -eq $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a -eq $b : a 等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a -eq $b: a 不等于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a -ne $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a -ne $b: a 不等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a -ne $b : a 等于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a -gt $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a -gt $b: a 大于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a -gt $b: a 不大于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a -lt $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a -lt $b: a 小于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a -lt $b: a 不小于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a -ge $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a -ge $b: a 大于或等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a -ge $b: a 小于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a -le $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a -le $b: a 小于或等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a -le $b: a 大于 b"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">10 -eq 20: a 不等于 b</span><br><span class="line">10 -ne 20: a 不等于 b</span><br><span class="line">10 -gt 20: a 不大于 b</span><br><span class="line">10 -lt 20: a 小于 b</span><br><span class="line">10 -ge 20: a 小于 b</span><br><span class="line">10 -le 20: a 小于或等于 b</span><br></pre></td></tr></table></figure><h3 id="布尔运算符"><a href="#布尔运算符" class="headerlink" title="布尔运算符"></a>布尔运算符</h3><p>下表列出了常用的布尔运算符，假定变量 a 为 10，变量 b 为 20：</p><table><thead><tr><th align="left">运算符</th><th align="left">说明</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">!</td><td align="left">非运算，表达式为 true 则返回 false，否则返回 true。</td><td align="left">[ ! false ] 返回 true。</td></tr><tr><td align="left">-o</td><td align="left">或运算，有一个表达式为 true 则返回 true。</td><td align="left">[ $a -lt 20 -o $b -gt 100 ] 返回 true。</td></tr><tr><td align="left">-a</td><td align="left">与运算，两个表达式都为 true 才返回 true。</td><td align="left">[ $a -lt 20 -a $b -gt 100 ] 返回 false。</td></tr></tbody></table><p>布尔运算符实例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a=10</span><br><span class="line">b=20</span><br><span class="line"></span><br><span class="line">if [ $a != $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a != $b : a 不等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a == $b: a 等于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a -lt 100 -a $b -gt 15 ]</span><br><span class="line">then</span><br><span class="line">  echo "$a 小于 100 且 $b 大于 15 : 返回 true"</span><br><span class="line">else</span><br><span class="line">  echo "$a 小于 100 且 $b 大于 15 : 返回 false"</span><br><span class="line">fi</span><br><span class="line">if [ $a -lt 100 -o $b -gt 100 ]</span><br><span class="line">then</span><br><span class="line">  echo "$a 小于 100 或 $b 大于 100 : 返回 true"</span><br><span class="line">else</span><br><span class="line">  echo "$a 小于 100 或 $b 大于 100 : 返回 false"</span><br><span class="line">fi</span><br><span class="line">if [ $a -lt 5 -o $b -gt 100 ]</span><br><span class="line">then</span><br><span class="line">  echo "$a 小于 5 或 $b 大于 100 : 返回 true"</span><br><span class="line">else</span><br><span class="line">  echo "$a 小于 5 或 $b 大于 100 : 返回 false"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">10 != 20 : a 不等于 b</span><br><span class="line">10 小于 100 且 20 大于 15 : 返回 true</span><br><span class="line">10 小于 100 或 20 大于 100 : 返回 true</span><br><span class="line">10 小于 5 或 20 大于 100 : 返回 false</span><br></pre></td></tr></table></figure><h3 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h3><p>以下介绍 Shell 的逻辑运算符，假定变量 a 为 10，变量 b 为 20:</p><table><thead><tr><th align="left">运算符</th><th align="left">说明</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">&amp;&amp;</td><td align="left">逻辑的 AND</td><td align="left">[[ $a -lt 100 &amp;&amp; $b -gt 100 ]] 返回 false</td></tr><tr><td align="left">||</td><td align="left">逻辑的 OR</td><td align="left">[[ $a -lt 100 || $b -gt 100 ]] 返回 true</td></tr></tbody></table><p>算符实例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a=10</span><br><span class="line">b=20</span><br><span class="line"></span><br><span class="line">if [[ $a -lt 100 &amp;&amp; $b -gt 100 ]]</span><br><span class="line">then</span><br><span class="line">  echo "返回 true"</span><br><span class="line">else</span><br><span class="line">  echo "返回 false"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [[ $a -lt 100 || $b -gt 100 ]]</span><br><span class="line">then</span><br><span class="line">  echo "返回 true"</span><br><span class="line">else</span><br><span class="line">  echo "返回 false"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">返回 false</span><br><span class="line">返回 true</span><br></pre></td></tr></table></figure><h3 id="字符串运算符"><a href="#字符串运算符" class="headerlink" title="字符串运算符"></a>字符串运算符</h3><p>下表列出了常用的字符串运算符，假定变量 a 为 “abc”，变量 b 为 “efg”：</p><table><thead><tr><th align="left">运算符</th><th align="left">说明</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">=</td><td align="left">检测两个字符串是否相等，相等返回 true。</td><td align="left">[ $a = $b ] 返回 false。</td></tr><tr><td align="left">!=</td><td align="left">检测两个字符串是否相等，不相等返回 true。</td><td align="left">[ $a != $b ] 返回 true。</td></tr><tr><td align="left">-z</td><td align="left">检测字符串长度是否为0，为0返回 true。</td><td align="left">[ -z $a ] 返回 false。</td></tr><tr><td align="left">-n</td><td align="left">检测字符串长度是否为0，不为0返回 true。</td><td align="left">[ -n “$a” ] 返回 true。</td></tr><tr><td align="left">$</td><td align="left">检测字符串是否为空，不为空返回 true。</td><td align="left">[ $a ] 返回 true。</td></tr></tbody></table><p>字符串运算符实例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a="abc"</span><br><span class="line">b="efg"</span><br><span class="line"></span><br><span class="line">if [ $a = $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a = $b : a 等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a = $b: a 不等于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a != $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a != $b : a 不等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a != $b: a 等于 b"</span><br><span class="line">fi</span><br><span class="line">if [ -z $a ]</span><br><span class="line">then</span><br><span class="line">  echo "-z $a : 字符串长度为 0"</span><br><span class="line">else</span><br><span class="line">  echo "-z $a : 字符串长度不为 0"</span><br><span class="line">fi</span><br><span class="line">if [ -n "$a" ]</span><br><span class="line">then</span><br><span class="line">  echo "-n $a : 字符串长度不为 0"</span><br><span class="line">else</span><br><span class="line">  echo "-n $a : 字符串长度为 0"</span><br><span class="line">fi</span><br><span class="line">if [ $a ]</span><br><span class="line">then</span><br><span class="line">  echo "$a : 字符串不为空"</span><br><span class="line">else</span><br><span class="line">  echo "$a : 字符串为空"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">abc = efg: a 不等于 b</span><br><span class="line">abc != efg : a 不等于 b</span><br><span class="line">-z abc : 字符串长度不为 0</span><br><span class="line">-n abc : 字符串长度不为 0</span><br><span class="line">abc : 字符串不为空</span><br></pre></td></tr></table></figure><h3 id="文件测试运算符"><a href="#文件测试运算符" class="headerlink" title="文件测试运算符"></a>文件测试运算符</h3><p>文件测试运算符用于检测 Unix 文件的各种属性。</p><p>属性检测描述如下：</p><table><thead><tr><th align="left">操作符</th><th align="left">说明</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">-b file</td><td align="left">检测文件是否是块设备文件，如果是，则返回 true。</td><td align="left">[ -b $file ] 返回 false。</td></tr><tr><td align="left">-c file</td><td align="left">检测文件是否是字符设备文件，如果是，则返回 true。</td><td align="left">[ -c $file ] 返回 false。</td></tr><tr><td align="left"><code>-d file</code></td><td align="left">检测文件是否是目录，如果是，则返回 true。</td><td align="left">[ -d $file ] 返回 false。</td></tr><tr><td align="left"><code>-f file</code></td><td align="left">检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回 true。</td><td align="left">[ -f $file ] 返回 true。</td></tr><tr><td align="left">-g file</td><td align="left">检测文件是否设置了 SGID 位，如果是，则返回 true。</td><td align="left">[ -g $file ] 返回 false。</td></tr><tr><td align="left">-k file</td><td align="left">检测文件是否设置了粘着位(Sticky Bit)，如果是，则返回 true。</td><td align="left">[ -k $file ] 返回 false。</td></tr><tr><td align="left">-p file</td><td align="left">检测文件是否是有名管道，如果是，则返回 true。</td><td align="left">[ -p $file ] 返回 false。</td></tr><tr><td align="left">-u file</td><td align="left">检测文件是否设置了 SUID 位，如果是，则返回 true。</td><td align="left">[ -u $file ] 返回 false。</td></tr><tr><td align="left"><code>-r file</code></td><td align="left">检测文件是否可读，如果是，则返回 true。</td><td align="left">[ -r $file ] 返回 true。</td></tr><tr><td align="left"><code>-w file</code></td><td align="left">检测文件是否可写，如果是，则返回 true。</td><td align="left">[ -w $file ] 返回 true。</td></tr><tr><td align="left"><code>-x file</code></td><td align="left">检测文件是否可执行，如果是，则返回 true。</td><td align="left">[ -x $file ] 返回 true。</td></tr><tr><td align="left"><code>-s file</code></td><td align="left">检测文件是否为空（文件大小是否大于0），不为空返回 true。</td><td align="left">[ -s $file ] 返回 true。</td></tr><tr><td align="left"><code>-e file</code></td><td align="left">检测文件（包括目录）是否存在，如果是，则返回 true。</td><td align="left">[ -e $file ] 返回 true。</td></tr></tbody></table><p>其他检查符：</p><ul><li>-S: 判断某文件是否 socket。</li><li>-L: 检测文件是否存在并且是一个符号链接。</li></ul><p>变量 file 表示文件 /var/www/runoob/test.sh，它的大小为 100 字节，具有 rwx 权限。下面的代码，将检测该文件的各种属性：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">file="/var/www/runoob/test.sh"</span><br><span class="line">if [ -r $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件可读"</span><br><span class="line">else</span><br><span class="line">  echo "文件不可读"</span><br><span class="line">fi</span><br><span class="line">if [ -w $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件可写"</span><br><span class="line">else</span><br><span class="line">  echo "文件不可写"</span><br><span class="line">fi</span><br><span class="line">if [ -x $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件可执行"</span><br><span class="line">else</span><br><span class="line">  echo "文件不可执行"</span><br><span class="line">fi</span><br><span class="line">if [ -f $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件为普通文件"</span><br><span class="line">else</span><br><span class="line">  echo "文件为特殊文件"</span><br><span class="line">fi</span><br><span class="line">if [ -d $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件是个目录"</span><br><span class="line">else</span><br><span class="line">  echo "文件不是个目录"</span><br><span class="line">fi</span><br><span class="line">if [ -s $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件不为空"</span><br><span class="line">else</span><br><span class="line">  echo "文件为空"</span><br><span class="line">fi</span><br><span class="line">if [ -e $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件存在"</span><br><span class="line">else</span><br><span class="line">  echo "文件不存在"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">文件可读</span><br><span class="line">文件可写</span><br><span class="line">文件可执行</span><br><span class="line">文件为普通文件</span><br><span class="line">文件不是个目录</span><br><span class="line">文件不为空</span><br><span class="line">文件存在</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的数组</title>
      <link href="/2018/09/25/linux/shell/4/"/>
      <url>/2018/09/25/linux/shell/4/</url>
      
        <content type="html"><![CDATA[<p>数组中可以存放多个值。Bash Shell 只支持一维数组（不支持多维数组），初始化时不需要定义数组大小（与 PHP 类似）。</p><p>与大部分编程语言类似，数组元素的下标由0开始。</p><p>Shell 数组用括号来表示，元素用”空格”符号分割开，语法格式如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">array_name=(value1 ... valuen)</span><br></pre></td></tr></table></figure><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">my_array=(A B "C" D)</span><br></pre></td></tr></table></figure><p>我们也可以使用下标来定义数组:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">array_name[0]=value0</span><br><span class="line">array_name[1]=value1</span><br><span class="line">array_name[2]=value2</span><br></pre></td></tr></table></figure><h3 id="读取数组"><a href="#读取数组" class="headerlink" title="读取数组"></a>读取数组</h3><p>读取数组元素值的一般格式是：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">&#123;array_name[index]&#125;</span></span><br></pre></td></tr></table></figure><h3 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">my_array=(A B "C" D)</span><br><span class="line"></span><br><span class="line">echo "第一个元素为: $&#123;my_array[0]&#125;"</span><br><span class="line">echo "第二个元素为: $&#123;my_array[1]&#125;"</span><br><span class="line">echo "第三个元素为: $&#123;my_array[2]&#125;"</span><br><span class="line">echo "第四个元素为: $&#123;my_array[3]&#125;"</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod +x test.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./test.sh</span></span><br><span class="line">第一个元素为: A</span><br><span class="line">第二个元素为: B</span><br><span class="line">第三个元素为: C</span><br><span class="line">第四个元素为: D</span><br></pre></td></tr></table></figure><h3 id="获取数组中的所有元素"><a href="#获取数组中的所有元素" class="headerlink" title="获取数组中的所有元素"></a>获取数组中的所有元素</h3><p>使用@ 或 * 可以获取数组中的所有元素，例如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">my_array[0]=A</span><br><span class="line">my_array[1]=B</span><br><span class="line">my_array[2]=C</span><br><span class="line">my_array[3]=D</span><br><span class="line"></span><br><span class="line">echo "数组的元素为: $&#123;my_array[*]&#125;"</span><br><span class="line">echo "数组的元素为: $&#123;my_array[@]&#125;"</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod +x test.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./test.sh</span></span><br><span class="line">数组的元素为: A B C D</span><br><span class="line">数组的元素为: A B C D</span><br></pre></td></tr></table></figure><h3 id="获取数组的长度"><a href="#获取数组的长度" class="headerlink" title="获取数组的长度"></a>获取数组的长度</h3><p>获取数组长度的方法与获取字符串长度的方法相同，例如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">my_array[0]=A</span><br><span class="line">my_array[1]=B</span><br><span class="line">my_array[2]=C</span><br><span class="line">my_array[3]=D</span><br><span class="line"></span><br><span class="line">echo "数组元素个数为: $&#123;#my_array[*]&#125;"</span><br><span class="line">echo "数组元素个数为: $&#123;#my_array[@]&#125;"</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod +x test.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./test.sh</span></span><br><span class="line">数组元素个数为: 4</span><br><span class="line">数组元素个数为: 4</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的参数传递</title>
      <link href="/2018/09/24/linux/shell/3/"/>
      <url>/2018/09/24/linux/shell/3/</url>
      
        <content type="html"><![CDATA[<p>我们可以在执行 Shell 脚本时，向脚本传递参数，脚本内获取参数的格式为：<strong>$n</strong>。<strong>n</strong> 代表一个数字，1 为执行脚本的第一个参数，2 为执行脚本的第二个参数，以此类推……</p><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><p>以下实例我们向脚本传递三个参数，并分别输出，其中 <strong>$0</strong> 为执行的文件名：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "Shell 传递参数实例！";</span><br><span class="line">echo "执行的文件名：$0";</span><br><span class="line">echo "第一个参数为：$1";</span><br><span class="line">echo "第二个参数为：$2";</span><br><span class="line">echo "第三个参数为：$3";</span><br></pre></td></tr></table></figure><p>为脚本设置可执行权限，并执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod +x test.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./test.sh 1 2 3</span></span><br><span class="line">Shell 传递参数实例！</span><br><span class="line">执行的文件名：./test.sh</span><br><span class="line">第一个参数为：1</span><br><span class="line">第二个参数为：2</span><br><span class="line">第三个参数为：3</span><br></pre></td></tr></table></figure><p>另外，还有几个特殊字符用来处理参数：</p><table><thead><tr><th align="left">参数处理</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">$#</td><td align="left">传递到脚本的参数个数</td></tr><tr><td align="left">$*</td><td align="left">以一个单字符串显示所有向脚本传递的参数。 如”$*”用「”」括起来的情况、以”$1 $2 … $n”的形式输出所有参数。</td></tr><tr><td align="left">$$</td><td align="left">脚本运行的当前进程ID号</td></tr><tr><td align="left">$!</td><td align="left">后台运行的最后一个进程的ID号</td></tr><tr><td align="left">$@</td><td align="left">与$*相同，但是使用时加引号，并在引号中返回每个参数。 如”$@”用「”」括起来的情况、以”$1” “$2” … “$n” 的形式输出所有参数。</td></tr><tr><td align="left">$-</td><td align="left">显示Shell使用的当前选项，与set命令功能相同。</td></tr><tr><td align="left">$?</td><td align="left">显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误。</td></tr></tbody></table><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "Shell 传递参数实例！";</span><br><span class="line">echo "第一个参数为：$1";</span><br><span class="line"></span><br><span class="line">echo "参数个数为：$#";</span><br><span class="line">echo "传递的参数作为一个字符串显示：$*";</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod +x test.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./test.sh 1 2 3</span></span><br><span class="line">Shell 传递参数实例！</span><br><span class="line">第一个参数为：1</span><br><span class="line">参数个数为：3</span><br><span class="line">传递的参数作为一个字符串显示：1 2 3</span><br></pre></td></tr></table></figure><p>$* 与 $@ 区别：</p><ul><li>相同点：都是引用所有参数。</li><li>不同点：只有在双引号中体现出来。假设在脚本运行时写了三个参数 1、2、3，，则 “ * “ 等价于 “1 2 3”（传递了一个参数），而 “@” 等价于 “1” “2” “3”（传递了三个参数）。</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "-- \$* 演示 ---"</span><br><span class="line">for i in "$*"; do</span><br><span class="line">    echo $i</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">echo "-- \$@ 演示 ---"</span><br><span class="line">for i in "$@"; do</span><br><span class="line">    echo $i</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod +x test.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./test.sh 1 2 3</span></span><br><span class="line">-- $* 演示 ---</span><br><span class="line">1 2 3</span><br><span class="line">-- $@ 演示 ---</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的字符串</title>
      <link href="/2018/09/23/linux/shell/2/"/>
      <url>/2018/09/23/linux/shell/2/</url>
      
        <content type="html"><![CDATA[<p>字符串是shell编程中最常用最有用的数据类型（除了数字和字符串，也没啥其它类型好用了），字符串可以用单引号，也可以用双引号，也可以不用引号。单双引号的区别跟PHP类似。</p><h3 id="单引号"><a href="#单引号" class="headerlink" title="单引号"></a>单引号</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">str='this is a string'</span><br></pre></td></tr></table></figure><p>单引号字符串的限制：</p><ul><li>单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的；</li><li>单引号字串中不能出现单独一个的单引号（对单引号使用转义符后也不行），但可成对出现，作为字符串拼接使用。</li></ul><h3 id="双引号"><a href="#双引号" class="headerlink" title="双引号"></a>双引号</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">your_name='runoob'</span><br><span class="line">str="Hello, I know you are \"$your_name\"! \n"</span><br><span class="line">echo -e $str</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Hello, I know you are "runoob"!</span><br></pre></td></tr></table></figure><p>双引号的优点：</p><ul><li>双引号里可以有变量</li><li>双引号里可以出现转义字符</li></ul><h3 id="拼接字符串"><a href="#拼接字符串" class="headerlink" title="拼接字符串"></a>拼接字符串</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">your_name="runoob"</span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用双引号拼接</span></span><br><span class="line">greeting="hello, "$your_name" !"</span><br><span class="line">greeting_1="hello, $&#123;your_name&#125; !"</span><br><span class="line">echo $greeting  $greeting_1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用单引号拼接</span></span><br><span class="line">greeting_2='hello, '$your_name' !'</span><br><span class="line">greeting_3='hello, $&#123;your_name&#125; !'</span><br><span class="line">echo $greeting_2  $greeting_3</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hello, runoob ! hello, runoob !</span><br><span class="line">hello, runoob ! hello, $&#123;your_name&#125; !</span><br></pre></td></tr></table></figure><h3 id="获取字符串长度"><a href="#获取字符串长度" class="headerlink" title="获取字符串长度"></a>获取字符串长度</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">string="abcd"</span><br><span class="line">echo $&#123;#string&#125; #输出 4</span><br></pre></td></tr></table></figure><h3 id="提取子字符串"><a href="#提取子字符串" class="headerlink" title="提取子字符串"></a>提取子字符串</h3><p>以下实例从字符串第 <strong>2</strong> 个字符开始截取 <strong>4</strong> 个字符：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">string="runoob is a great site"</span><br><span class="line">echo $&#123;string:1:4&#125; # 输出 unoo</span><br></pre></td></tr></table></figure><h3 id="查找子字符串"><a href="#查找子字符串" class="headerlink" title="查找子字符串"></a>查找子字符串</h3><p>查找字符 <strong>i</strong> 或 <strong>o</strong> 的位置(哪个字母先出现就计算哪个)：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">string="runoob is a great site"</span><br><span class="line">echo `expr index "$string" io`  # 输出 4</span><br></pre></td></tr></table></figure><p><strong>注意：</strong> 以上脚本中 <strong>`</strong> 是反引号，而不是单引号 <strong>‘</strong>，不要看错了哦。</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的变量</title>
      <link href="/2018/09/22/linux/shell/1/"/>
      <url>/2018/09/22/linux/shell/1/</url>
      
        <content type="html"><![CDATA[<h3 id="第一个Shell脚本"><a href="#第一个Shell脚本" class="headerlink" title="第一个Shell脚本"></a>第一个Shell脚本</h3><p>打开文本编辑器(可以使用 vi/vim 命令来创建文件)，新建一个文件 test.sh，扩展名为 sh（sh代表shell），扩展名并不影响脚本执行，见名知意就好，如果你用 php 写 shell 脚本，扩展名就用 php 好了。</p><p>输入一些代码，第一行一般是这样：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">echo "Hello World !"</span><br></pre></td></tr></table></figure><p><code>#! 是一个约定的标记，它告诉系统这个脚本需要什么解释器来执行，即使用哪一种 Shell。</code></p><p>echo 命令用于向窗口输出文本。</p><h3 id="运行-Shell-脚本的两种方法"><a href="#运行-Shell-脚本的两种方法" class="headerlink" title="运行 Shell 脚本的两种方法"></a>运行 Shell 脚本的两种方法</h3><ol><li><p>作为可执行程序</p><p>将上面的代码保存为 test.sh，并 cd 到相应目录：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chmod +x ./test.sh  #使脚本具有执行权限</span><br><span class="line">./test.sh  #执行脚本</span><br></pre></td></tr></table></figure><p>注意，一定要写成 <strong>./test.sh</strong>，而不是 <strong>test.sh</strong>，运行其它二进制的程序也一样，直接写 test.sh，linux 系统会去 PATH 里寻找有没有叫 test.sh 的，而只有 /bin, /sbin, /usr/bin，/usr/sbin 等在 PATH 里，你的当前目录通常不在 PATH 里，所以写成 test.sh 是会找不到命令的，要用 ./test.sh 告诉系统说，就在当前目录找。</p></li><li><p>作为解释器参数</p><p>这种运行方式是，直接运行解释器，其参数就是 shell 脚本的文件名，如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/bin/sh test.sh</span><br><span class="line">/bin/php test.php</span><br></pre></td></tr></table></figure><p>这种方式运行的脚本，可以省略第一行指定解释器信息。</p></li></ol><h3 id="Shell-变量"><a href="#Shell-变量" class="headerlink" title="Shell 变量"></a>Shell 变量</h3><p>定义变量时，变量名不加美元符号（$，PHP语言中变量需要），如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">your_name="runoob.com"</span><br></pre></td></tr></table></figure><p>注意，<code>变量名和等号之间不能有空格</code>，这可能和你熟悉的所有编程语言都不一样。同时，变量名的命名须遵循如下规则：</p><ul><li>命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。</li><li>中间不能有空格，可以使用下划线（_）。</li><li>不能使用标点符号。</li><li>不能使用bash里的关键字（可用help命令查看保留关键字）。</li></ul><p>有效的 Shell 变量名示例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">RUNOOB</span><br><span class="line">LD_LIBRARY_PATH</span><br><span class="line">_var</span><br><span class="line">var2</span><br></pre></td></tr></table></figure><p>无效的变量命名：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">?var=123</span><br><span class="line">user*name=runoob</span><br></pre></td></tr></table></figure><p>除了显式地直接赋值，还可以用语句给变量赋值，如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for file in `ls /etc`</span><br><span class="line">或</span><br><span class="line">for file in $(ls /etc)</span><br></pre></td></tr></table></figure><p>以上语句将 /etc 下目录的文件名循环出来。</p><h4 id="使用变量"><a href="#使用变量" class="headerlink" title="使用变量"></a>使用变量</h4><p>使用一个定义过的变量，只要在变量名前面加美元符号即可，如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">your_name="qinjx"</span><br><span class="line">echo $your_name</span><br><span class="line">echo $&#123;your_name&#125;</span><br></pre></td></tr></table></figure><p>变量名外面的花括号是可选的，加不加都行，加花括号是为了帮助解释器识别变量的边界，比如下面这种情况：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for skill in Ada Coffe Action Java; do</span><br><span class="line">    echo "I am good at $&#123;skill&#125;Script"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>如果不给skill变量加花括号，写成echo “I am good at $skillScript”，解释器就会把$skillScript当成一个变量（其值为空），代码执行结果就不是我们期望的样子了。</p><p>推荐给所有变量加上花括号，这是个好的编程习惯。</p><p>已定义的变量，可以被重新定义，如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">your_name="tom"</span><br><span class="line">echo $your_name</span><br><span class="line">your_name="alibaba"</span><br><span class="line">echo $your_name</span><br></pre></td></tr></table></figure><p>这样写是合法的，但注意，第二次赋值的时候不能写$your_name=”alibaba”，使用变量的时候才加美元符（$）。</p><h4 id="只读变量"><a href="#只读变量" class="headerlink" title="只读变量"></a>只读变量</h4><p>使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。</p><p>下面的例子尝试更改只读变量，结果报错：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">myUrl="http://www.google.com"</span><br><span class="line">readonly myUrl</span><br><span class="line">myUrl="http://www.runoob.com"</span><br></pre></td></tr></table></figure><p>运行脚本，结果如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/bin/sh: NAME: This variable is read only.</span><br></pre></td></tr></table></figure><h4 id="删除变量"><a href="#删除变量" class="headerlink" title="删除变量"></a>删除变量</h4><p>使用 unset 命令可以删除变量。语法：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">unset variable_name</span><br></pre></td></tr></table></figure><p>变量被删除后不能再次使用。unset 命令不能删除只读变量。</p><p>实例: </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line">myUrl="http://www.runoob.com"</span><br><span class="line">unset myUrl</span><br><span class="line">echo $myUrl</span><br></pre></td></tr></table></figure><p>以上实例执行将没有任何输出。</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>后台执行、crontab调度和软连接的使用场景</title>
      <link href="/2018/09/21/linux/5/"/>
      <url>/2018/09/21/linux/5/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 后台执行脚本</li><li>整理 rundeck 视频 部署</li><li>整理 crontab 每隔10s</li><li>整理 软连接 场景 坑</li></ol><h3 id="整理后台执行脚本"><a href="#整理后台执行脚本" class="headerlink" title="整理后台执行脚本"></a>整理后台执行脚本</h3><p>后台执行后命令有三个，分别是：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">./test.sh &amp;</span><br><span class="line">nohup ./test.sh &amp; </span><br><span class="line">nohup ./test.sh &gt; /root/test.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>一般使用第三条</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# nohup ./show.sh &gt;&gt; ./show.log 2&gt;&amp;1 &amp;　　　　#输出重定向</span><br><span class="line">[4] 18637</span><br><span class="line">[root@aliyun ~]# </span><br><span class="line">[root@aliyun ~]# tail -F show.log 　　　　#实时接收输出内容</span><br><span class="line">nohup: ignoring input</span><br><span class="line">Thu Nov 21 17:07:58 CST 2019</span><br><span class="line">Thu Nov 21 17:08:08 CST 2019</span><br><span class="line">Thu Nov 21 17:08:18 CST 2019</span><br><span class="line">Thu Nov 21 17:08:28 CST 2019</span><br><span class="line">Thu Nov 21 17:08:48 CST 2019</span><br></pre></td></tr></table></figure><h3 id="整理-rundeck-视频-部署"><a href="#整理-rundeck-视频-部署" class="headerlink" title="整理 rundeck 视频 部署"></a>整理 rundeck 视频 部署</h3><p><a href="https://www.bilibili.com/video/av35466584?from=search&amp;seid=1197620829255678947" target="_blank" rel="noopener">https://www.bilibili.com/video/av35466584?from=search&amp;seid=1197620829255678947</a></p><h3 id="整理-crontab-每隔10s"><a href="#整理-crontab-每隔10s" class="headerlink" title="整理 crontab 每隔10s"></a>整理 crontab 每隔10s</h3><p>Linux自带的任务调度工具 crontab 的调度单位分别是 分、时、日、周、月 最小的划分粒度是分钟，因此不能解决秒级别的调度问题，</p><p>* 代表每次，如 * / 6 代表每6分钟执行一次</p><p>但是换一种思路，我可以把调度代码包在循环体中，这个循环体执行6次，每次sleep 10s ，加起来就是分钟，即每分钟执行6次，每次间隔10秒</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">for((i=0;i&lt;6;i++));</span><br><span class="line">do</span><br><span class="line">        date</span><br><span class="line">        sleep 10s</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>这样打印出来的结果是间隔10秒</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ./show.sh </span><br><span class="line">Thu Nov 21 17:19:23 CST 2019</span><br><span class="line">Thu Nov 21 17:19:33 CST 2019</span><br><span class="line">Thu Nov 21 17:19:43 CST 2019</span><br><span class="line">Thu Nov 21 17:19:53 CST 2019</span><br><span class="line">Thu Nov 21 17:20:03 CST 2019</span><br><span class="line">Thu Nov 21 17:20:13 CST 2019</span><br></pre></td></tr></table></figure><h3 id="4-整理-软连接-场景-坑"><a href="#4-整理-软连接-场景-坑" class="headerlink" title="4.整理 软连接 场景 坑"></a>4.整理 软连接 场景 坑</h3><p>软连接的使用: ln -s 源文件路径 目标文件路径</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ln -s test.txt test</span><br><span class="line">[root@aliyun ~]# ll</span><br><span class="line">total 24</span><br><span class="line">-rw------- 1 root root       5 Nov 21 17:03 nohup.out</span><br><span class="line">-rw-r--r-- 1 root root      45 Nov 12 23:21 print.sh</span><br><span class="line">-rw-r--r-- 1 root root     196 Nov 21 17:08 show.log</span><br><span class="line">-rwxr--r-- 1 root root      58 Nov 21 17:07 show.sh</span><br><span class="line">drwxr-xr-x 2 root root    4096 Nov 17 10:24 size.log</span><br><span class="line">lrwxrwxrwx 1 root root       8 Nov 21 17:23 test -&gt; test.txt</span><br><span class="line">-rwxr-xr-- 1 root bigdata  198 Nov 18 11:47 test.txt</span><br><span class="line">[root@aliyun ~]#</span><br></pre></td></tr></table></figure><h3 id="使用场景："><a href="#使用场景：" class="headerlink" title="使用场景："></a>使用场景：</h3><ol><li><p>作为源文件的快捷方式存在，好处：升级源文件的时候只需要重新创建软连接，注意：环境变量中不能写源文件的路径，必须写软连接文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun~]# ll</span><br><span class="line">total 5</span><br><span class="line">lrwxrwxrwx 1 root root   8 Nov 21 17:23 mysql -&gt; mysql5.6</span><br><span class="line">drwxr-xr-x 2 root root   6 Nov 20 21:33 mysql5.6  #低版本部署</span><br><span class="line">drwxr-xr-x 2 root root   6 Nov 20 21:33 mysql5.7  #通过软连接来切换升级文件</span><br><span class="line">drwxr-xr-x 3 root root  44 Nov 17 23:13 ruozedata</span><br><span class="line">-rw-r--r-- 1 root root 846 Nov 17 23:12 ruozedata.zip</span><br></pre></td></tr></table></figure></li><li><p>作为数据盘在系统盘中日志写入目录的软连接，好处：日志写入和存储多个文件需要占用大量的磁盘，把日志的存储位置换到了数据盘中</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir /data01/log/  #创建数据盘下的日志目录</span><br><span class="line">mv  /var/log/hbase /data01/log/　　#移动系统盘的日志文件到数据盘</span><br><span class="line">ln -s /data01/log/hbase /var/log/hbase　　#数据盘的日志文件再软连接到系统盘</span><br></pre></td></tr></table></figure></li></ol><p><code>坑</code>：软连接文件创建后的文件和源为文件权限不同，必须注意和修改软连接文件和目标文件的权限</p><p><code>建议</code>：在创建软连接的时候，源文件路径和目标文件路径推介使用绝对路径</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> DataWarehouse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>熟练使用vim、系统命令和程序管理工具</title>
      <link href="/2018/09/20/linux/4/"/>
      <url>/2018/09/20/linux/4/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 vi</li><li>整理 进程 端口号</li><li>整理 连接拒绝 (权限受限)</li><li>整理 高危命令</li><li>常用的 wget yum rpm 压缩</li></ol><h3 id="vim中的常见用法-部分"><a href="#vim中的常见用法-部分" class="headerlink" title="vim中的常见用法(部分)"></a>vim中的常见用法(部分)</h3><table><thead><tr><th>复制</th><th>yy</th></tr></thead><tbody><tr><td>复制多行</td><td>nyy</td></tr><tr><td>当前行向下粘贴</td><td>p</td></tr><tr><td>当前行下上粘贴</td><td>P</td></tr><tr><td>当前位置插入</td><td>i(I)</td></tr><tr><td>当下位置的下一个位置插入</td><td>a(A)</td></tr><tr><td>当前行的下一行插入</td><td>o(O)</td></tr><tr><td>删除当前字符</td><td>x</td></tr><tr><td>删除当前位置到行尾</td><td>D</td></tr><tr><td>删除当前行</td><td>dd</td></tr><tr><td>删除当前行到未行</td><td>dG</td></tr><tr><td>删除n行</td><td>ndd</td></tr><tr><td>删除全部</td><td>gg + dG</td></tr><tr><td>跳转行尾</td><td>Shift + $</td></tr><tr><td>跳转行首</td><td>Shift + ^</td></tr><tr><td>跳转首行</td><td>gg</td></tr><tr><td>跳转未行</td><td>G</td></tr><tr><td>跳转到n行</td><td>:n或者是nG或者是ngg</td></tr><tr><td>撤回一次</td><td>u</td></tr><tr><td>撤回多次</td><td>U</td></tr></tbody></table><p><code>vim编辑中的坑：</code>编辑或者调优配置文件前，一定要备份</p><h3 id="系统命令"><a href="#系统命令" class="headerlink" title="系统命令"></a>系统命令</h3><h4 id="查看磁盘-（df-h）"><a href="#查看磁盘-（df-h）" class="headerlink" title="查看磁盘 （df -h）"></a>查看磁盘 （df -h）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# df -h</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/vda1        40G   11G   27G  29% /</span><br><span class="line">devtmpfs        911M     0  911M   0% /dev</span><br><span class="line">tmpfs           920M     0  920M   0% /dev/shm</span><br><span class="line">tmpfs           920M  332K  920M   1% /run</span><br><span class="line">tmpfs           920M     0  920M   0% /sys/fs/cgroup</span><br><span class="line">tmpfs           184M     0  184M   0% /run/user/0</span><br></pre></td></tr></table></figure><p>了解数据盘的格式：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/dev/vdb1        1T   10G    XXX   X% /data01 数据盘</span><br></pre></td></tr></table></figure><h4 id="查看内存（free-h）"><a href="#查看内存（free-h）" class="headerlink" title="查看内存（free -h）"></a>查看内存（free -h）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# free -h</span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           1.8G        853M         88M        336K        897M        792M</span><br><span class="line">Swap:            0B          0B          0B</span><br></pre></td></tr></table></figure><p>延伸：<a href="http://blog.itpub.net/30089851/viewspace-2131678/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-2131678/</a></p><h4 id="查看负载均衡（top）"><a href="#查看负载均衡（top）" class="headerlink" title="查看负载均衡（top）"></a>查看负载均衡（top）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# top</span><br><span class="line">top - 11:51:16 up 23 days, 12:56,  1 user,  load average: 0.00, 0.01, 0.05</span><br><span class="line">Tasks:  65 total,   1 running,  64 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu(s):  0.0 us,  0.3 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">KiB Mem :  1883724 total,    90012 free,   874596 used,   919116 buff/cache</span><br><span class="line">KiB Swap:        0 total,        0 free,        0 used.   810836 avail Mem </span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND                                       </span><br><span class="line">19516 root       0 -20  126596   9340   6508 S  0.3  0.5  98:38.33 AliYunDun                                     </span><br><span class="line">    1 root      20   0  125124   3344   2112 S  0.0  0.2   0:10.82 systemd                                       </span><br><span class="line">    2 root      20   0       0      0      0 S  0.0  0.0   0:00.00 kthreadd                                      </span><br><span class="line">    3 root      20   0       0      0      0 S  0.0  0.0   0:02.21 ksoftirqd/0                                   </span><br><span class="line">    5 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 kworker/0:0H</span><br></pre></td></tr></table></figure><p>注意：</p><ol><li>负载均衡的数值不能超过10</li><li>如果某服务长期占用cpu或者men，去检查这个进程是在做什么</li><li>如果cpu飙升3000%以上，夯住 ，代码级别，如果不是自己编写的代码，大概率硬件级别–&gt;内存条坏了</li></ol><h4 id="查看进程（ps-ef）"><a href="#查看进程（ps-ef）" class="headerlink" title="查看进程（ps -ef）"></a>查看进程（ps -ef）</h4><p>查看进程常常和 grep 配合使用</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ps -ef|grep ssh</span><br><span class="line">root      4295     1  0 Oct25 ?        00:00:00 /usr/sbin/sshd -D</span><br><span class="line">root     20282  4295  0 11:34 ?        00:00:00 sshd: root@pts/0</span><br><span class="line">root     20318 20284  0 11:56 pts/0    00:00:00 grep --color=auto ssh</span><br></pre></td></tr></table></figure><p>最后一条是自己的进程，可以加上 grep -v grep 去掉这条记录</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ps -ef|grep ssh | grep -v grep进程用户　进程的pid　父id　　　　　　　　　　　　　　　　进程用户的内容(进程所属的目录)</span><br><span class="line">root      4295     1  0 Oct25 ?        00:00:00 /usr/sbin/sshd -D</span><br><span class="line">root     20282  4295  0 11:34 ?        00:00:00 sshd: root@pts/0</span><br></pre></td></tr></table></figure><h4 id="查看端口号（netstat-nlp）"><a href="#查看端口号（netstat-nlp）" class="headerlink" title="查看端口号（netstat -nlp）"></a>查看端口号（netstat -nlp）</h4><p>最常配合查看进程得到的pid号查看端口号</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# netstat -nlp | grep 4295</span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      4295/sshd</span><br></pre></td></tr></table></figure><p><code>注意：</code> 如果查询出来的端口号前面的ip是127.0.0.1或者localhost，属于本地回环ip地址，需要修改相应的配置文件</p><p><code>场景：</code> 在centos部署大数据组件，发现一个错误 Connection refused</p><p>解决思路：</p><ol><li>ping ip 测试ip</li><li>telnet ip port 测试ip和端口号 </li><li>防火墙</li></ol><h4 id="telnet命令安装"><a href="#telnet命令安装" class="headerlink" title="telnet命令安装"></a>telnet命令安装</h4><h5 id="window："><a href="#window：" class="headerlink" title="window："></a>window：</h5><p><img src="https://img2018.cnblogs.com/i-beta/1657732/201911/1657732-20191118121808051-1147850773.png" alt="win10开启telnet命令"></p><h5 id="linux："><a href="#linux：" class="headerlink" title="linux："></a>linux：</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# yum install -y telnet</span><br><span class="line">[root@aliyun ~]# which telnet</span><br><span class="line">/usr/bin/telnet</span><br><span class="line">[root@aliyun ~]# telnet 121.196.220.143 22</span><br><span class="line">Trying 121.196.220.143...</span><br><span class="line">Connected to 121.196.220.143.</span><br><span class="line">Escape character is &apos;^]&apos;.</span><br><span class="line">SSH-2.0-OpenSSH_6.6.1</span><br></pre></td></tr></table></figure><h3 id="三个高危命令"><a href="#三个高危命令" class="headerlink" title="三个高危命令"></a>三个高危命令</h3><table><thead><tr><th>命令</th><th>作用</th></tr></thead><tbody><tr><td><code>rm -rf /</code></td><td>强制无提示删除</td></tr><tr><td><code>vim</code></td><td>编辑生产环境的配置文件不备份</td></tr><tr><td><code>kill -9 $(pgrep -f 匹配关键词)</code></td><td>杀死全部的进程</td></tr></tbody></table><p><code>杀进程之前，先ps 找到相关的进程，搞清楚，哪些是你要杀的，不然造成生产事故</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ps -ef | grep ssh</span><br><span class="line">root      4295     1  0 Oct25 ?        00:00:00 /usr/sbin/sshd -D</span><br><span class="line">root     20282  4295  0 11:34 ?        00:00:00 sshd: root@pts/0</span><br><span class="line">root     20329  4295  0 12:00 ?        00:00:00 sshd: root@pts/1</span><br><span class="line">root     20360  4295  0 12:01 ?        00:00:00 sshd: root@pts/2</span><br><span class="line">root     20380  4295  0 12:02 ?        00:00:00 sshd: root@pts/3</span><br><span class="line">root     20407  4295  0 12:12 ?        00:00:00 sshd: root@pts/4</span><br><span class="line">root     20474  4295  0 12:22 ?        00:00:00 sshd: root@pts/6</span><br><span class="line">root     20502 20476  0 12:30 pts/6    00:00:00 grep --color=auto ssh</span><br><span class="line">[root@aliyun ~]# kill -9 $(pgrep -f ssh)</span><br></pre></td></tr></table></figure><h3 id="常用的程序管理工具"><a href="#常用的程序管理工具" class="headerlink" title="常用的程序管理工具"></a>常用的程序管理工具</h3><h4 id="wget下载安装包"><a href="#wget下载安装包" class="headerlink" title="wget下载安装包"></a>wget下载安装包</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz</span><br></pre></td></tr></table></figure><h4 id="yum包管理"><a href="#yum包管理" class="headerlink" title="yum包管理"></a>yum包管理</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum search xxx</span><br><span class="line">yum install -y xxx</span><br><span class="line">yum remove xxx</span><br></pre></td></tr></table></figure><h4 id="rpm包管理"><a href="#rpm包管理" class="headerlink" title="rpm包管理"></a>rpm包管理</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun conf]# rpm -qa | grep http　　#查看</span><br><span class="line">httpd-2.4.6-90.el7.centos.x86_64</span><br><span class="line">httpd-tools-2.4.6-90.el7.centos.x86_64</span><br><span class="line">[root@aliyun conf]# rpm -e httpd-tools-2.4.6-90.el7.centos.x86_64　　#卸载失败，有依赖文件</span><br><span class="line">error: Failed dependencies:</span><br><span class="line">        httpd-tools = 2.4.6-90.el7.centos is needed by (installed) httpd-2.4.6-90.el7.centos.x86_64</span><br><span class="line">[root@aliyun conf]# rpm -e  --nodeps     httpd-tools-2.4.6-90.el7.centos.x86_64　　#强制跳过依赖检查</span><br></pre></td></tr></table></figure><h4 id="zip压缩解压"><a href="#zip压缩解压" class="headerlink" title="zip压缩解压"></a>zip压缩解压</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zip -r xxx.zip ./*  　　　　#在文件夹里面压缩文件 　　　　</span><br><span class="line">zip -r test.zip test/* 　　#在文件夹外卖压缩文件夹里面的文件unzip test.zip</span><br></pre></td></tr></table></figure><h4 id="tar压缩解压"><a href="#tar压缩解压" class="headerlink" title="tar压缩解压"></a>tar压缩解压</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -xzvf hadoop-2.6.0-cdh5.16.2.tar.gz　　　　#解压缩.ge的tar包</span><br><span class="line">tar -czvf hadoop-2.6.0-cdh5.16.2.tar.gz  hadoop-2.6.0-cdh5.16.2/*　　#压缩.ge的tar包</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> DataWarehouse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>熟悉Linux权限相关命令</title>
      <link href="/2018/09/20/linux/3/"/>
      <url>/2018/09/20/linux/3/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 用户用户组</li><li>整理 <code>sudo</code>命令</li><li>整理 用户无法登录 <code>passwd</code>文件</li><li>权限 <code>rwx------ chmod chown</code> 案例</li><li>其他命令 <code>- su find du</code>等</li></ol><h3 id="用户和用户组"><a href="#用户和用户组" class="headerlink" title="用户和用户组"></a>用户和用户组</h3><p>针对用户的相关文件在：<code>/usr/sbin/user*</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ll /usr/sbin/user*</span><br><span class="line">-rwxr-x---. 1 root root 118192 Nov  6  2016 /usr/sbin/useradd</span><br><span class="line">-rwxr-x---. 1 root root  80360 Nov  6  2016 /usr/sbin/userdel</span><br><span class="line">-rwxr-x---. 1 root root 113840 Nov  6  2016 /usr/sbin/usermod</span><br><span class="line">-rwsr-xr-x  1 root root  11296 Apr 13  2017 /usr/sbin/usernetctl</span><br></pre></td></tr></table></figure><p>针对用户组的相关文件在：<code>/usr/sbin/group*</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ll /usr/sbin/group*</span><br><span class="line">-rwxr-x---. 1 root root 65480 Nov  6  2016 /usr/sbin/groupadd</span><br><span class="line">-rwxr-x---. 1 root root 57016 Nov  6  2016 /usr/sbin/groupdel</span><br><span class="line">-rwxr-x---. 1 root root 57064 Nov  6  2016 /usr/sbin/groupmems</span><br><span class="line">-rwxr-x---. 1 root root 76424 Nov  6  2016 /usr/sbin/groupmod</span><br></pre></td></tr></table></figure><p>可以打印出<code>PATH</code>路径，就会发现<code>/user/sbin</code>已经被添加在了<code>PATH</code>环境中了，可以从主机的任意位置使用这些命令</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# echo $PATH</span><br><span class="line">/opt/module/jdk1.8.0_144/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin</span><br></pre></td></tr></table></figure><p>需求：</p><p>​        1. 添加<code>hadoop</code>用户</p><p>​        2. 删除<code>hadoop</code>用户</p><p>​        3. 重新创建<code>hadoop</code>用户，模拟用户丢失样式，并修正样式</p><p>​        4. 创建<code>bigdata</code>用户组，并把<code>hadoop</code>用户添加进这个用户组</p><p>​        5. 修改<code>bigdata</code>为<code>hadoop</code>的主组</p><ol><li><p>添加<code>hadoop</code>用户</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# useradd hadoop</span><br><span class="line">[root@aliyun ~]# id hadoop</span><br><span class="line">uid=1000(hadoop) gid=1000(hadoop) groups=1000(hadoop)</span><br></pre></td></tr></table></figure></li><li><p>删除<code>hadoop</code>用户</p><p>使用命令帮助查看 <code>userdel</code> 命令</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# userdel --help</span><br><span class="line">Usage: userdel [options] LOGIN</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  -f, --force                   force some actions that would fail otherwise</span><br><span class="line">                                e.g. removal of user still logged in</span><br><span class="line">                                or files, even if not owned by the user</span><br><span class="line">  -h, --help                    display this help message and exit</span><br><span class="line">  -r, --remove                  remove home directory and mail spool</span><br><span class="line">  -R, --root CHROOT_DIR         directory to chroot into</span><br><span class="line">  -Z, --selinux-user            remove any SELinux user mapping for the user</span><br></pre></td></tr></table></figure><p>会发现 <code>-r</code> 选项是删除家目录</p><p>在这里我们选择删除用户的时候不删除家目录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# userdel hadoop</span><br><span class="line">[root@aliyun ~]# id hadoop</span><br><span class="line">id: hadoop: no such user</span><br><span class="line">[root@aliyun home]# cat /etc/passwd | grep ruoze</span><br><span class="line">[root@aliyun home]# cat /etc/group | grep ruoze</span><br></pre></td></tr></table></figure><p>因为<code>hadoop</code>该组只有<code>hadoop</code>用户，当这个用户删除时，组会校验就他自己，会自动删除</p></li><li><p>重新创建<code>hadoop</code>用户，模拟用户丢失样式，并修正样式</p><p>创建<code>hadoop</code>用户</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# useradd hadoop</span><br><span class="line">useradd: warning: the home directory already exists.</span><br><span class="line">Not copying any file from skel directory into it.</span><br><span class="line">Creating mailbox file: File exists</span><br><span class="line">[root@aliyun ~]# id hadoop</span><br><span class="line">uid=1000(hadoop) gid=1000(hadoop) groups=1000(hadoop)</span><br></pre></td></tr></table></figure><p>模拟用户丢失样式</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ ll -a .bash*</span><br><span class="line">-rw-r--r-- 1 hadoop hadoop  18 Dec  7  2016 .bash_logout</span><br><span class="line">-rw-r--r-- 1 hadoop hadoop 193 Dec  7  2016 .bash_profile</span><br><span class="line">-rw-r--r-- 1 hadoop hadoop 231 Dec  7  2016 .bashrc</span><br><span class="line">[hadoop@aliyun ~]$ rm -rf .bash*</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop　　　　#切换用户</span><br><span class="line">Last login: Sun Nov 17 09:29:10 CST 2019 on pts/0</span><br><span class="line">-bash-4.2$ 　　　#用户样式丢失</span><br></pre></td></tr></table></figure><p>修正样式 (这里只有root权限才可以拷贝)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ll  -a /etc/skel/</span><br><span class="line">total 20</span><br><span class="line">drwxr-xr-x.  2 root root 4096 Aug 18  2017 .</span><br><span class="line">drwxr-xr-x. 81 root root 4096 Nov 17 09:27 ..</span><br><span class="line">-rw-r--r--   1 root root   18 Dec  7  2016 .bash_logout</span><br><span class="line">-rw-r--r--   1 root root  193 Dec  7  2016 .bash_profile</span><br><span class="line">-rw-r--r--   1 root root  231 Dec  7  2016 .bashrc</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cp /etc/skel/ .bash* /home/hadoop/</span><br><span class="line">cp: omitting directory ‘/etc/skel/’</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop　　#样式回来了</span><br><span class="line">Last login: Sun Nov 17 09:33:39 CST 2019 on pts/2</span><br><span class="line">[hadoop@aliyun ~]$</span><br></pre></td></tr></table></figure><p>创建<code>bigdata</code>用户组，并把<code>hadoop</code>用户添加进这个用户组</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# groupadd bigdata</span><br><span class="line">[root@aliyun ~]# usermod -a -G bigdata hadoop</span><br><span class="line">[root@aliyun ~]# id hadoop</span><br><span class="line">uid=1000(hadoop) gid=1000(hadoop) groups=1000(hadoop),1001(bigdata)</span><br></pre></td></tr></table></figure><p><em>20200309更新</em>：<code>mysqladmin</code>的属组一里加入<code>hadoop</code>用户</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">usermod -a -G hadoop mysqladmin</span><br></pre></td></tr></table></figure><p>修改<code>bigdata</code>为<code>hadoop</code>的属组</p><p>查看命令帮助发现有一条命令是改变用户的属组的</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-g, --gid GROUP               force use GROUP as new primary group</span><br><span class="line">[root@aliyun ~]# usermod -g bigdata hadoop　　#强制改变属组</span><br><span class="line">[root@aliyun ~]# id hadoop</span><br><span class="line">uid=1000(hadoop) gid=1001(bigdata) groups=1001(bigdata)</span><br></pre></td></tr></table></figure></li></ol><h3 id="sudo命令"><a href="#sudo命令" class="headerlink" title="sudo命令"></a>sudo命令</h3><p><code>sudo</code>命令是让普通用户具备<code>root</code>用户的权限</p><p>添加普通用户具备<code>root</code>权限的文件是：<code>/etc/sudoers</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">90 ## Allow root to run any commands anywhere </span><br><span class="line">91 root    ALL=(ALL)       ALL</span><br><span class="line">92 hadoop  ALL=(root)      NOPASSWD:ALL　　#新添加的内容</span><br></pre></td></tr></table></figure><h3 id="用户无法登录-修改passwd文件"><a href="#用户无法登录-修改passwd文件" class="headerlink" title="用户无法登录 修改passwd文件"></a>用户无法登录 修改<code>passwd</code>文件</h3><p>在模拟用户无法登陆之前，先说明管理用户信息的文件是：<code>/etc/passwd</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# tail -3  /etc/passwd</span><br><span class="line">redis:x:996:994:Redis Database Server:/var/lib/redis:/sbin/nologin</span><br><span class="line">mysqladmin:x:514:101::/usr/local/mysql:/bin/bash</span><br><span class="line">hadoop:x:1000:1001::/home/hadoop:/bin/bash</span><br></pre></td></tr></table></figure><p>需要注意的是最后一个冒号后面是用户的登陆权限</p><p>需求：</p><p>　　1. 模拟用户的登录权限是<code>/bin/false</code>，修改，并登录</p><p>　　2. 模拟用户的登录权限是<code>/sbin/nologin</code>，修改，并登录</p><p><strong>1. 模拟用户的登录权限是<code>/bin/false</code>，修改，并登录</strong></p><ol><li><p>模拟用户的登录权限是<code>/bin/false</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cat /etc/passwd | grep hadoop</span><br><span class="line">hadoop:x:1000:1001::/home/hadoop:/bin/false</span><br></pre></td></tr></table></figure></li><li><p>尝试登录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop</span><br><span class="line">Last login: Sun Nov 17 09:37:25 CST 2019 on pts/2</span><br><span class="line">[root@aliyun ~]# 　　#登录失败</span><br></pre></td></tr></table></figure></li><li><p>查看用户文件权限，并修改</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cat /etc/passwd | grep 'hadoop'</span><br><span class="line">hadoop:x:1000:1001::/home/hadoop:/bin/bash</span><br></pre></td></tr></table></figure></li><li><p>再次登录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop</span><br><span class="line">Last login: Sun Nov 17 09:56:43 CST 2019 on pts/1</span><br><span class="line">[hadoop@aliyun ~]$     #登录成功</span><br></pre></td></tr></table></figure></li></ol><p><strong>2. 模拟用户的登录权限是<code>/sbin/nologin</code>，修改，并登录</strong></p><ol><li><p>模拟用户的登录权限是<code>/sbin/nologin</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cat /etc/passwd | grep hadoop</span><br><span class="line">hadoop:x:1000:1001::/home/hadoop:/sbin/nologin</span><br></pre></td></tr></table></figure></li><li><p>尝试登录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop</span><br><span class="line">Last login: Sun Nov 17 09:59:35 CST 2019 on pts/1</span><br><span class="line">This account is currently not available.</span><br><span class="line">[root@aliyun ~]# 　　#登录失败</span><br></pre></td></tr></table></figure></li><li><p>查看用户文件权限，并修改</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cat /etc/passwd | grep 'hadoop'</span><br><span class="line">hadoop:x:1000:1001::/home/hadoop:/bin/bash</span><br></pre></td></tr></table></figure></li><li><p>再次登录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop</span><br><span class="line">Last login: Sun Nov 17 09:56:43 CST 2019 on pts/1</span><br><span class="line">[hadoop@aliyun ~]$     #登录成功</span><br></pre></td></tr></table></figure></li></ol><h3 id="rwx-chmod-chown-案例"><a href="#rwx-chmod-chown-案例" class="headerlink" title="rwx------ chmod chown 案例"></a><code>rwx------</code> <code>chmod</code> <code>chown</code> 案例</h3><p>查看文件或者目录的读写执行权限</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ll test.txt</span><br><span class="line">-rw-r--r-- 1 root root 12 Nov 12 23:36 test.txt</span><br><span class="line">r: read  4</span><br><span class="line">w: write 2 </span><br><span class="line">x: 执行  1</span><br><span class="line">-: 没权限 0</span><br></pre></td></tr></table></figure><p>　<code>rw-</code>第一组 6 代表文件或文件夹的用户<code>root</code>，读写<br>　<code>r--</code> 第二组 4 代表文件或文件夹的用户组<code>root</code>，读<br>　<code>r--</code> 第三组 4 代表其他组的所属用户对这个文件或文件夹的权限: 读</p><p><code>chmod</code> 命令用来修改文件或者目录的读写执行权限，加 <code>-R</code> 表示递归修改</p><p><code>chown</code> 命令用来修改文件或者目录的属主和属组，加 <code>-R</code> 表示递归修改</p><p>需求：</p><p>​        1. 修改 <code>test.tx</code>t 文件的属组为<code>bigdata</code></p><p>​        2.  <code>test.txt</code> 文件的权限为属主读写执行，属组读执行</p><ol><li><p>修改 <code>test.txt</code> 文件的属组为<code>bigdata</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# chown -R :bigdata test.txt </span><br><span class="line">[root@aliyun ~]# ll test.txt </span><br><span class="line">-rw-r--r-- 1 root bigdata 12 Nov 12 23:36 test.txt</span><br></pre></td></tr></table></figure></li><li><p>修改 <code>test.txt</code> 文件的权限为属主可读写执行，属组可读执行,其他可读</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# chmod -R 754 test.txt </span><br><span class="line">[root@aliyun ~]# ll test.txt </span><br><span class="line">-rwxr-xr-- 1 root bigdata 12 Nov 12 23:36 test.txt</span><br></pre></td></tr></table></figure><p>注意： <code>-R</code> 参数，目前可认为只有<code>chown</code>和<code>chmod</code>命令有，其他都为 <code>-r</code></p></li></ol><h3 id="其他命令-su-find-du"><a href="#其他命令-su-find-du" class="headerlink" title="其他命令 - su find du"></a>其他命令 - <code>su</code> <code>find</code> <code>du</code></h3><p><u><code>su</code>命令用来切换用户，使用<code>su -</code> 用户名的方式，切换的时候把环境也切换了</u></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop　　#su - 用户名</span><br><span class="line">Last login: Sun Nov 17 10:16:08 CST 2019 on pts/2</span><br><span class="line">[hadoop@aliyun ~]$ pwd</span><br><span class="line">/home/hadoop</span><br><span class="line">[root@aliyun ~]# su hadoop　　#su 用户名</span><br><span class="line">[hadoop@aliyun root]$ pwd</span><br><span class="line">/root</span><br></pre></td></tr></table></figure><p>需要注意 <code>.bash_profile</code> 和 <code>.bashrc</code> 两个文件中的环境生效的区别</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">.bash_profile文件 su ruoze不会执行，su - ruoze 都执行</span><br><span class="line">.bashrc文件       su ruoze执行   ，su - ruoze 都执行</span><br></pre></td></tr></table></figure><p><u><code>find</code>命令用来查找文件，在不确定文件名的情况下使用模糊匹配</u></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun root]$ find /home -name '*hadoop*'</span><br><span class="line">/home/hadoop</span><br></pre></td></tr></table></figure><p><u><code>du</code>命令用来查看文件或者目录大小</u></p><p>虽然 ls -l 也可以查看文件或者目录的大小，但是 <code>ls -l</code> 显示的目录大小并不准确</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ll -h  size.log/</span><br><span class="line">total 12K</span><br><span class="line">-rw-r--r-- 1 root root 286K Nov 17 10:24 lastlog</span><br></pre></td></tr></table></figure><p>再使用<code>du -sh</code> 查看一次</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# du -sh size.log/</span><br><span class="line">12K    size.log/</span><br></pre></td></tr></table></figure><p>最后进入<code>size.log</code>文件夹查看文件的大小</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun log]# du -sh lastlog</span><br><span class="line">12K    lastlog</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> DataWarehouse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux对环境变量的理解以及alias、rm、hostory的使用</title>
      <link href="/2018/09/19/linux/2/"/>
      <url>/2018/09/19/linux/2/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 全局环境变量 个人环境变量 <code>which</code>的理解</li><li>整理 别名</li><li>整理 删除</li><li>整理 <code>history</code></li></ol><h3 id="全局环境变量"><a href="#全局环境变量" class="headerlink" title="全局环境变量"></a>全局环境变量</h3><p>全局环境变量的配置文件是：<code>/etc/profile</code></p><p>全局环境变量中一般配置的是共用的程序环境 比如<code>java</code></p><p>下面以<code>java</code>为例子配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# vim /etc/profile</span><br></pre></td></tr></table></figure><p>java的安装路径在 <code>/usr/java</code>下，所以文件中如下配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_144</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br></pre></td></tr></table></figure><p>需要注意的是 <code>$PATH</code>接在<code>$JAVA_HOME</code>的后面，即把<code>$JAVA_HOME</code>放在<code>$PATH</code>的最前面</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/opt/module/jdk1.8.0_144/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/module/jdk1.8.0_144/bin:/root/bin</span><br></pre></td></tr></table></figure><h3 id="个人环境变量"><a href="#个人环境变量" class="headerlink" title="个人环境变量"></a>个人环境变量</h3><p>个人环境变量配置在 <code>~/.bashrc</code> 文件中，这里需要注意的是如果配置在 <code>~/.bash_profile</code>文件中，使用<code>ssh</code>远程连接的时候不会加载 <code>~/.bash_profile</code>，造成一些无法排查的<code>bug</code></p><p>个人环境配置一些独自使用的程序变量，如果配置在用户的个人环境中，其他用户无法访问，比如在<code>hadoop</code>用户下配置 <code>hadoop</code>的环境变量，只有<code>hadoop</code>一个用户能使用</p><h3 id="which的理解"><a href="#which的理解" class="headerlink" title="which的理解"></a>which的理解</h3><p>安装完程序或者配置完变量后，最好的习惯是使用<code>which</code>看一下，检查一下环境是否配置正确，否则可能遇到自以为正确的<code>bug</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# which java</span><br><span class="line">/opt/module/jdk1.8.0_144/bin/java</span><br></pre></td></tr></table></figure><h3 id="别名"><a href="#别名" class="headerlink" title="别名"></a>别名</h3><p>别名的使用可以简写冗长且难以记忆或者难以书写的命令</p><p>格式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alias [-p] [name[=value] ... ]</span><br></pre></td></tr></table></figure><p>例子：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# alias a='ll -a'</span><br><span class="line">[root@aliyun ~]# a</span><br><span class="line">total 80</span><br><span class="line">dr-xr-x---.  6 root root 4096 Nov 15 19:46 .</span><br><span class="line">dr-xr-xr-x. 19 root root 4096 Nov  3 17:29 ..</span><br><span class="line">-rw-------   1 root root 6453 Nov 14 11:16 .bash_history</span><br><span class="line">-rw-r--r--.  1 root root   18 Dec 29  2013 .bash_logout</span><br><span class="line">-rw-r--r--.  1 root root  176 Dec 29  2013 .bash_profile</span><br><span class="line">-rw-r--r--.  1 root root  176 Dec 29  2013 .bashrc</span><br><span class="line">drwx------   3 root root 4096 Aug 18  2017 .cache</span><br></pre></td></tr></table></figure><p>查询主机中已经存在的别名：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# alias</span><br><span class="line">alias cp='cp -i'</span><br><span class="line">alias egrep='egrep --color=auto'</span><br><span class="line">alias fgrep='fgrep --color=auto'</span><br><span class="line">alias grep='grep --color=auto'</span><br><span class="line">alias l.='ls -d .* --color=auto'</span><br><span class="line">alias ll='ls -l --color=auto'</span><br><span class="line">alias ls='ls --color=auto'</span><br><span class="line">alias mv='mv -i'</span><br><span class="line">alias rm='rm -i'</span><br><span class="line">alias which='alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde'</span><br></pre></td></tr></table></figure><p>永久配置别名：</p><p>配置在环境变量中 <code>/etc/profile</code> ，<code>~/.bashrc</code>，<code>~/.bash_profile</code>中，即永久配置别名</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> User specific aliases and <span class="built_in">functions</span></span></span><br><span class="line"></span><br><span class="line">alias rm='rm -i'</span><br><span class="line">alias cp='cp -i'</span><br><span class="line">alias mv='mv -i'</span><br></pre></td></tr></table></figure><h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><table><thead><tr><th>命令</th><th>作用</th></tr></thead><tbody><tr><td><code>mkdir</code></td><td>删除一个空文件</td></tr><tr><td><code>rm -f</code></td><td>直接删除文件</td></tr><tr><td><code>rm -rf</code></td><td>直接删除文件夹</td></tr><tr><td><code>rm -rf</code></td><td>是一个高危的命令</td></tr></tbody></table><p>场景：</p><p><code>shell</code>脚本中，定义变量<code>k = &quot;&quot;</code> 然后<code>rm -rf $k</code> 会默认指定根目录下</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">K=&quot;&quot;rm -rf $K 实际上是rm -rf /</span><br></pre></td></tr></table></figure><p>解决办法是先判断k是否为空</p><h3 id="history"><a href="#history" class="headerlink" title="history"></a>history</h3><p><code>history</code>命令用来查询历史记录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# history | head -10</span><br><span class="line">    1  passwd </span><br><span class="line">    2  ls</span><br><span class="line">    3  cat /etc/hosts</span><br><span class="line">    4  yum install redis</span><br><span class="line">    5  yum install epel-release</span><br><span class="line">    6  yum install redis</span><br><span class="line">    7  df -hT</span><br><span class="line">    8  service redis start</span><br><span class="line">    9  service redis stop</span><br><span class="line">   10  service redis status</span><br></pre></td></tr></table></figure><p>场景：</p><p>莫名其妙的发现主机中的数据没了，可以查看一下历史记录用了哪些命令</p><p>使用 <code>!n</code> 来快速使用一条历史命令</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# !7</span><br><span class="line">df -hT</span><br><span class="line">Filesystem     Type      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/vda1      ext4       40G   11G   27G  29% /</span><br><span class="line">devtmpfs       devtmpfs  911M     0  911M   0% /dev</span><br><span class="line">tmpfs          tmpfs     920M     0  920M   0% /dev/shm</span><br><span class="line">tmpfs          tmpfs     920M  332K  920M   1% /run</span><br><span class="line">tmpfs          tmpfs     920M     0  920M   0% /sys/fs/cgroup</span><br><span class="line">tmpfs          tmpfs     184M     0  184M   0% /run/user/0</span><br></pre></td></tr></table></figure><p><code>history -c</code> 命令可以清空当前窗口的历史输出命令。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# history -c</span><br><span class="line">[root@aliyun ~]# history</span><br><span class="line">1  history</span><br></pre></td></tr></table></figure><p>但是历史记录实际上保存在 <code>~/.bash_history</code>中的</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cat .bash_history | head -10</span><br><span class="line">passwd </span><br><span class="line">ls</span><br><span class="line">cat /etc/hosts</span><br><span class="line">yum install redis</span><br><span class="line">yum install epel-release</span><br><span class="line">yum install redis</span><br><span class="line">df -hT</span><br><span class="line">service redis start</span><br><span class="line">service redis stop</span><br><span class="line">service redis status</span><br></pre></td></tr></table></figure><p>彻底清空该文件夹的方式为：<code>cat /dell/null &gt; .bash_history</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cat /dev/null &gt; .bash_history </span><br><span class="line">[root@aliyun ~]# cat .bash_history</span><br><span class="line">[root@aliyun ~]#</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> DataWarehouse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux操作文件和定位错误</title>
      <link href="/2018/09/18/linux/1/"/>
      <url>/2018/09/18/linux/1/</url>
      
        <content type="html"><![CDATA[<h3 id="置空文件的一些坑"><a href="#置空文件的一些坑" class="headerlink" title="置空文件的一些坑"></a>置空文件的一些坑</h3><ol><li><p>最简单的是直接创建一个空文件:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# touch test.txt | ll test.txt</span><br><span class="line">-rw-r--r-- 1 root root 0 Nov 11 22:29 test.txt</span><br><span class="line">[root@aliyun var]#</span><br></pre></td></tr></table></figure></li><li><p>慎用 <code>echo &quot;&quot; &gt; test.txt</code> 这种方式置空文件</p><p>如果我们使用这种方法置空文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# echo "" &gt; test.txt</span><br><span class="line">[root@aliyun var]#  ll -h test.txt</span><br><span class="line">-rw-r--r-- 1 root root 1 Nov 11 22:31 test.txt</span><br></pre></td></tr></table></figure><p>它不是绝对意义上的为空，文件占有一个字节的大小</p></li><li><p>可以更换为<code>cat /dev/null &gt; test.txt</code> 这种方式置空文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# cat /dev/null &gt; test.txt | ll -h test.txt</span><br><span class="line">-rw-r--r-- 1 root root 0 Nov 11 22:34 test.txt</span><br></pre></td></tr></table></figure><p>真正意义上把文件置空为0个字节</p></li></ol><h3 id="如何定位ERROR"><a href="#如何定位ERROR" class="headerlink" title="如何定位ERROR"></a>如何定位ERROR</h3><ul><li><p>文件内容很小 几十兆<br>上传给<code>windows</code>，用<code>editplus</code>工具打开，在<code>editplus</code>中搜索，定位</p><p>上传下载  <code>yum install -y lrzsz</code></p></li><li><p>文件内容很大 至少几百兆</p><p> 直接定位到<code>ERROR</code>行</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# cat test.log | grep ERROR</span><br><span class="line">10 ERROR : 模拟错误</span><br></pre></td></tr></table></figure></li></ul><h3 id="使用grep定位REEOR上下文"><a href="#使用grep定位REEOR上下文" class="headerlink" title="使用grep定位REEOR上下文"></a>使用<code>grep</code>定位<code>REEOR</code>上下文</h3><ol><li><p>查看<code>ERROR</code>行的前十行（<code>before</code>）</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# cat test.log | grep -B 10 ERROR</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">9</span><br><span class="line">10 ERROR : 模拟错误</span><br></pre></td></tr></table></figure></li><li><p>查看<code>ERROR</code>行的后十行（<code>after</code>）</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# cat test.log | grep -A 10 ERROR</span><br><span class="line">10 ERROR : 模拟错误</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td></tr></table></figure></li><li><p>查看<code>ERROR</code>行的前后二十行</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# cat test.log | grep -C 10 ERROR</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">9</span><br><span class="line">10 ERROR : 模拟错误</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> DataWarehouse </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
