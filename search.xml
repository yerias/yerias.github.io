<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Error: java.io.IOException: Invalid LZO header</title>
      <link href="/2020/04/18/error/6/"/>
      <url>/2020/04/18/error/6/</url>
      
        <content type="html"><![CDATA[<p>在使用Flume传输数据的时候，需要注意几个字段</p><p>我们这里使用的是flume传输到hdfs</p><p>参数：hdfs.fileType 指定的数据传输类型，默认SequenceFile，如果直接传输本文本数据，则会乱码。在传输文本数据的时候它的值要修改为DataStream</p><p>而现在根据我们的错误提示就知道我们使用了Lzo压缩，所以需要把它的值修改为CompressedStream，即可解决问题。</p><hr><p>还需要注意的一个参数是：hdfs.codeC ，在使用flume时，可以将数据压缩输出，它的值可选为gzip, bzip2, lzo, lzop, snappy</p><p>lzop的后缀是lzo</p><p>lzo的后缀是lzp.default</p><hr><p>还是要熟悉一下flume的文档。。。<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#hdfs-sink" target="_blank" rel="noopener">hdfs</a></p>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自定义外部Text数据源</title>
      <link href="/2020/04/16/spark/18/"/>
      <url>/2020/04/16/spark/18/</url>
      
        <content type="html"><![CDATA[<p>这里接着上次的解读jdbc数据源，现在我们自己实现一个text的外部数据源</p><hr><ol><li><p>创建DefaultSource类实现RelationProviderTrait，注意这里的类名必须是DefaultSource，源码中写死了</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultSource</span>  <span class="keyword">extends</span> <span class="title">RelationProvider</span></span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span></span>(sqlContext: <span class="type">SQLContext</span>, parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">BaseRelation</span> = &#123;</span><br><span class="line">      <span class="comment">//拿到client传入的参数path</span></span><br><span class="line">    <span class="keyword">val</span> path = parameters.get(<span class="string">"path"</span>)</span><br><span class="line">      <span class="comment">//判断path是否存在</span></span><br><span class="line">    path <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(p) =&gt;<span class="keyword">new</span> <span class="type">TextDataSourceRelation</span>(sqlContext,p)</span><br><span class="line">      <span class="keyword">case</span> _ =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"path is required ..."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>自定义Relation，继承BashRelation和TableScan，拿到Schema和RDD[Row]</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextDataSourceRelation</span>(<span class="params">context:<span class="type">SQLContext</span>,path:<span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">BaseRelation</span> <span class="keyword">with</span> <span class="title">TableScan</span> <span class="keyword">with</span> <span class="title">Logging</span></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">sqlContext</span></span>: <span class="type">SQLContext</span> = context</span><br><span class="line"></span><br><span class="line">   <span class="comment">//重写StructType接口的方式实现Schema</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">schema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>&#123;</span><br><span class="line">    <span class="type">List</span>(</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"id"</span>,<span class="type">StringType</span>,<span class="literal">true</span>),</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"name"</span>,<span class="type">StringType</span>,<span class="literal">true</span>),</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"sex"</span>,<span class="type">StringType</span>,<span class="literal">true</span>),</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"sal"</span>,<span class="type">DoubleType</span>,<span class="literal">true</span>),</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"comm"</span>,<span class="type">DoubleType</span>,<span class="literal">true</span>)</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写buildScan拿到RDD[Row]</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">      <span class="comment">//拿到文本数据</span></span><br><span class="line">    <span class="keyword">val</span> textRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sqlContext.sparkContext.textFile(path)</span><br><span class="line">      <span class="comment">//拿到每个StructField</span></span><br><span class="line">    <span class="keyword">val</span> schemaField: <span class="type">Array</span>[<span class="type">StructField</span>] = schema.fields</span><br><span class="line">      <span class="comment">//对每行数据逗号切分，并且去掉空格，返回集合</span></span><br><span class="line">    textRDD.map(_.split(<span class="string">","</span>).map(_.trim))</span><br><span class="line">      <span class="comment">//对集合中的每个元素操作，通过zipWithIndex算子可以拿到元素的内容和对应的索引号</span></span><br><span class="line">      .map(row =&gt; row.zipWithIndex.map &#123;</span><br><span class="line">          <span class="comment">//模式匹配，拿到了value和index，然后对其做操作</span></span><br><span class="line">        <span class="keyword">case</span> (value, index) =&gt; &#123;</span><br><span class="line">            <span class="comment">//通过schemaField和index拿到列名</span></span><br><span class="line">          <span class="keyword">val</span> columnName = schemaField(index).name</span><br><span class="line">            <span class="comment">//判断当前的列名是否是sex，并在工具类中做匹配，对value转换类型</span></span><br><span class="line">          <span class="type">Utils</span>.caseTo(<span class="keyword">if</span> (columnName.equalsIgnoreCase(<span class="string">"sex"</span>)) &#123;</span><br><span class="line">              <span class="comment">//如果列名是sex，列下元素是1、2或者3，则返回对应的字符</span></span><br><span class="line">            <span class="keyword">if</span> (value == <span class="string">"1"</span>) &#123;</span><br><span class="line">              <span class="string">"男"</span></span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (value == <span class="string">"2"</span>) &#123;</span><br><span class="line">              <span class="string">"女"</span></span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              <span class="string">"未知"</span></span><br><span class="line">            &#125;</span><br><span class="line">              <span class="comment">//如果列名不是sex，则直接返回元素</span></span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            value</span><br><span class="line">              <span class="comment">//传入dataType的类型，在工具类中做匹配，使value与schema的类型一致</span></span><br><span class="line">          &#125;, schemaField(index).dataType)</span><br><span class="line">      &#125;</span><br><span class="line">        <span class="comment">//结果是个集合，转换成RDD[Row]</span></span><br><span class="line">      &#125;).map(x =&gt; <span class="type">Row</span>.fromSeq(x))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>自定义Utils类</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Utils</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">caseTo</span></span>(value:<span class="type">String</span>,dataType: <span class="type">DataType</span>) =&#123;</span><br><span class="line">      <span class="comment">//模式匹配，转换value的类型</span></span><br><span class="line">    dataType <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> _:<span class="type">DoubleType</span> =&gt; value.toDouble</span><br><span class="line">      <span class="keyword">case</span> _:<span class="type">LongType</span> =&gt; value.toLong</span><br><span class="line">      <span class="keyword">case</span> _:<span class="type">StringType</span> =&gt; value</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>测试</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">"local[2]"</span>)</span><br><span class="line">      .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> textDF: <span class="type">DataFrame</span> = spark.read.format(<span class="string">"com.tunan.spark.sql.extds.text"</span>).load(<span class="string">"tunan-spark-sql/extds"</span>)</span><br><span class="line"></span><br><span class="line">    textDF.printSchema()</span><br><span class="line">    textDF.show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出结果</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- name: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- sex: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- sal: double (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- comm: double (nullable = <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line"><span class="type">ERROR</span> <span class="type">TextDataSourceRelation</span>: 进入buildScan方法</span><br><span class="line">+---+----+----+-------+------+</span><br><span class="line">| id|name| sex|    sal|  comm|</span><br><span class="line">+---+----+----+-------+------+</span><br><span class="line">|  <span class="number">1</span>|张三|  男|<span class="number">10000.0</span>|<span class="number">1000.0</span>|</span><br><span class="line">|  <span class="number">2</span>|李四|  男|<span class="number">12000.0</span>|<span class="number">2000.0</span>|</span><br><span class="line">|  <span class="number">3</span>|王五|  女|<span class="number">12500.0</span>|<span class="number">1000.0</span>|</span><br><span class="line">|  <span class="number">4</span>|赵六|未知|<span class="number">20000.0</span>|<span class="number">2000.0</span>|</span><br><span class="line">|  <span class="number">5</span>|图南|  男|<span class="number">21000.0</span>|<span class="number">1000.0</span>|</span><br><span class="line">|  <span class="number">6</span>|小七|  女|<span class="number">10000.0</span>|<span class="number">1500.0</span>|</span><br><span class="line">+---+----+----+-------+------+</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据列自动推导&amp;数据错误执行模式&amp;UDAF&amp;UDTF&amp;解读Spark SQL执行计划优化</title>
      <link href="/2020/04/16/spark/17/"/>
      <url>/2020/04/16/spark/17/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>数据列自动推导</li><li>数据错误执行模式</li><li>UDAF</li><li>UDTF</li><li>解读Spark SQL执行计划优化</li></ol><h2 id="数据列自动推导"><a href="#数据列自动推导" class="headerlink" title="数据列自动推导"></a>数据列自动推导</h2><ol><li><p>源数据</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">a|b|c</span><br><span class="line"><span class="number">1</span>|<span class="number">2</span>|<span class="number">3</span></span><br><span class="line"><span class="number">4</span>|tunan|<span class="number">6</span></span><br><span class="line"><span class="number">7</span>|<span class="number">8</span>|<span class="number">9.0</span></span><br></pre></td></tr></table></figure></li><li><p>代码处理</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">    .builder()</span><br><span class="line">    .master(<span class="string">"local[2]"</span>)</span><br><span class="line">    .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    .getOrCreate()</span><br><span class="line">   </span><br><span class="line">  <span class="keyword">val</span> csvDF: <span class="type">DataFrame</span> = spark.read</span><br><span class="line">    .format(<span class="string">"csv"</span>)</span><br><span class="line">    .option(<span class="string">"header"</span>,<span class="string">"true"</span>)</span><br><span class="line">    .option(<span class="string">"sep"</span>,<span class="string">"|"</span>)</span><br><span class="line">    .option(<span class="string">"interSchema"</span>,<span class="string">"true"</span>)</span><br><span class="line">    .load(<span class="string">"tunan-spark-sql/data/test.csv"</span>)</span><br><span class="line">   </span><br><span class="line">  csvDF.printSchema()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>打印数据Schema信息</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- a: integer (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- b: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- c: double (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure></li></ol><h2 id="数据错误执行模式"><a href="#数据错误执行模式" class="headerlink" title="数据错误执行模式"></a>数据错误执行模式</h2><p>在Spark中，读取数据时，遇到错误数据或者脏数据时，我们可以使用option设置mode，区分将错误数据是默认处理<code>PERMISSIVE</code>，还是丢弃数据<code>DROPMALFORMED</code>，还是快速失败<code>FAILFAST</code>，这些方法可以在ParseMode.scala</p><ol><li><p>源数据</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">"a"</span>:<span class="number">1</span>,<span class="string">"b"</span>:<span class="number">2</span>,<span class="string">"c"</span>:<span class="number">3</span>&#125;</span><br><span class="line">&#123;<span class="string">"a"</span>:<span class="number">4</span>,:<span class="number">5</span>,<span class="string">"c"</span>:<span class="number">6</span>&#125;</span><br><span class="line">&#123;<span class="string">"a"</span>:<span class="number">7</span>,<span class="string">"b"</span>:<span class="number">8</span>,<span class="string">"c"</span>:<span class="number">9</span>&#125;</span><br></pre></td></tr></table></figure></li><li><p>读数据</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> jsonDF: <span class="type">DataFrame</span> = spark.read.json(<span class="string">"tunan-spark-sql/data/test.json"</span>)</span><br><span class="line">jsonDF.show()</span><br></pre></td></tr></table></figure><p>结果:</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+----------------+----+----+----+</span><br><span class="line">| _corrupt_record|   a|   b|   c|</span><br><span class="line">+----------------+----+----+----+</span><br><span class="line">|            <span class="literal">null</span>|   <span class="number">1</span>|   <span class="number">2</span>|   <span class="number">3</span>|</span><br><span class="line">|&#123;<span class="string">"a"</span>:<span class="number">4</span>,:<span class="number">5</span>,<span class="string">"c"</span>:<span class="number">6</span>&#125;|<span class="literal">null</span>|<span class="literal">null</span>|<span class="literal">null</span>|</span><br><span class="line">|            <span class="literal">null</span>|   <span class="number">7</span>|   <span class="number">8</span>|   <span class="number">9</span>|</span><br><span class="line">+----------------+----+----+----+</span><br></pre></td></tr></table></figure><p>如果没有在option中设置mode选项，默认为<code>PERMISSIVE</code>，通过_corrupt_record列打印出错误信息</p></li><li><p>使用option设置mode为<code>DROPMALFORMED</code>，如果碰到错误的数据，则自动丢弃</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> jsonDF: <span class="type">DataFrame</span> = spark.read.option(<span class="string">"mode"</span>,<span class="string">"DROPMALFORMED"</span>).json(<span class="string">"tunan-spark-sql/data/test.json"</span>)</span><br><span class="line">jsonDF.show()</span><br></pre></td></tr></table></figure><p>结果:</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+---+---+---+</span><br><span class="line">|  a|  b|  c|</span><br><span class="line">+---+---+---+</span><br><span class="line">|  <span class="number">1</span>|  <span class="number">2</span>|  <span class="number">3</span>|</span><br><span class="line">|  <span class="number">7</span>|  <span class="number">8</span>|  <span class="number">9</span>|</span><br><span class="line">+---+---+---+</span><br></pre></td></tr></table></figure></li></ol><h2 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h2><ol><li><p>自定义一个UDAF的class或者object，作为具体的逻辑实现，需要继承<code>UserDefinedAggregateFunction</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AgeAvgUDAF</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span></span>&#123;</span><br><span class="line">    <span class="comment">//输入类型</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"input"</span>,<span class="type">DoubleType</span>,<span class="literal">true</span>)::<span class="type">Nil</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//聚合内部中的buffer类型</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"sums"</span>,<span class="type">DoubleType</span>,<span class="literal">true</span>)::</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"num"</span>,<span class="type">LongType</span>,<span class="literal">true</span>)::<span class="type">Nil</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//输入数据类型</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//输入数据类型是否和输出数据类型相等</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//聚合内部buffer的初始化</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      buffer(<span class="number">0</span>) = <span class="number">0.0</span></span><br><span class="line">      buffer(<span class="number">1</span>) = <span class="number">0</span>L</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分区内更新聚合buffer</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      buffer.update(<span class="number">0</span>,buffer.getDouble(<span class="number">0</span>)+input.getDouble(<span class="number">0</span>))</span><br><span class="line">      buffer.update(<span class="number">1</span>,buffer.getLong(<span class="number">1</span>)+<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分区间合并</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      buffer1.update(<span class="number">0</span>,buffer1.getDouble(<span class="number">0</span>)+buffer2.getDouble(<span class="number">0</span>))</span><br><span class="line">      buffer1.update(<span class="number">1</span>,buffer1.getLong(<span class="number">1</span>)+buffer2.getLong(<span class="number">1</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//最终计算</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">      buffer.getDouble(<span class="number">0</span>)/buffer.getLong(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li><li><p>注册并使用UDAF</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">   <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">     .builder()</span><br><span class="line">     .master(<span class="string">"local[2]"</span>)</span><br><span class="line">     .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">     .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义数据源</span></span><br><span class="line">   <span class="keyword">val</span> list = <span class="keyword">new</span> util.<span class="type">ArrayList</span>[<span class="type">Row</span>]()</span><br><span class="line">   list.add(<span class="type">Row</span>(<span class="string">"zhangsan"</span>,<span class="number">18</span>,<span class="string">"男"</span>))</span><br><span class="line">   list.add(<span class="type">Row</span>(<span class="string">"lisi"</span>,<span class="number">20</span>,<span class="string">"男"</span>))</span><br><span class="line">   list.add(<span class="type">Row</span>(<span class="string">"wangwu"</span>,<span class="number">26</span>,<span class="string">"女"</span>))</span><br><span class="line">   list.add(<span class="type">Row</span>(<span class="string">"翠翠"</span>,<span class="number">18</span>,<span class="string">"女"</span>))</span><br><span class="line">   list.add(<span class="type">Row</span>(<span class="string">"闰土"</span>,<span class="number">8</span>,<span class="string">"男"</span>))</span><br><span class="line"></span><br><span class="line">   <span class="comment">// 自定义Schema</span></span><br><span class="line">   <span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">     <span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">StringType</span>, <span class="literal">true</span>)::</span><br><span class="line">     <span class="type">StructField</span>(<span class="string">"age"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>)::</span><br><span class="line">     <span class="type">StructField</span>(<span class="string">"sex"</span>, <span class="type">StringType</span>, <span class="literal">true</span>)::<span class="type">Nil</span></span><br><span class="line">   )</span><br><span class="line">   </span><br><span class="line"><span class="comment">//创建df</span></span><br><span class="line">   <span class="keyword">val</span> df = spark.createDataFrame(list, schema)</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//创建视图</span></span><br><span class="line">   df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//注册UDAF</span></span><br><span class="line">   spark.udf.register(<span class="string">"age_avg_udaf"</span>,<span class="type">AgeAvgUDAF</span>)</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//使用UDAF</span></span><br><span class="line">   spark.sql(<span class="string">"select sex,age_avg_udaf(age) as ave_age from people group by sex"</span>).show()</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li><li><p>结果展示</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+---+---------+</span><br><span class="line">|sex|  ave_age|</span><br><span class="line">+---+---------+</span><br><span class="line">| 男|    <span class="number">15.33</span>|</span><br><span class="line">| 女|     <span class="number">22.0</span>|</span><br><span class="line">+---+---------+</span><br></pre></td></tr></table></figure></li></ol><h2 id="UDTF"><a href="#UDTF" class="headerlink" title="UDTF"></a>UDTF</h2><p>UDTF还在研究，先搞个简单的案例</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ExplodeUDTF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">"local[2]"</span>)</span><br><span class="line">      .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 自定义schema  </span></span><br><span class="line">    <span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"teacher"</span>, <span class="type">StringType</span>, <span class="literal">true</span>) ::</span><br><span class="line">        <span class="type">StructField</span>(<span class="string">"sources"</span>, <span class="type">StringType</span>, <span class="literal">true</span>) :: <span class="type">Nil</span></span><br><span class="line">    )</span><br><span class="line"><span class="comment">// 自定义数据源</span></span><br><span class="line">    <span class="keyword">val</span> list = <span class="keyword">new</span> util.<span class="type">ArrayList</span>[<span class="type">Row</span>]()</span><br><span class="line">    list.add(<span class="type">Row</span>(<span class="string">"tunan"</span>, <span class="string">"hive,spark,flink"</span>))</span><br><span class="line">    list.add(<span class="type">Row</span>(<span class="string">"xiaoqi"</span>, <span class="string">"cdh,kafka,hbase"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建临时视图</span></span><br><span class="line">    <span class="keyword">val</span> df = spark.createDataFrame(list, schema)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// 使用flatMap拆分</span></span><br><span class="line">    df.flatMap(x =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> line = <span class="keyword">new</span> <span class="type">ListBuffer</span>[(<span class="type">String</span>, <span class="type">String</span>)]()</span><br><span class="line">      <span class="keyword">val</span> sources = x.getString(<span class="number">1</span>).split(<span class="string">","</span>)</span><br><span class="line">      <span class="keyword">for</span> (source &lt;- sources)&#123;</span><br><span class="line">        line.append((x.getString(<span class="number">0</span>),source))</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//返回</span></span><br><span class="line">      line</span><br><span class="line">    &#125;).toDF(<span class="string">"teacher"</span>,<span class="string">"source"</span>).show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+-------+------+</span><br><span class="line">|teacher|source|</span><br><span class="line">+-------+------+</span><br><span class="line">|  tunan|  hive|</span><br><span class="line">|  tunan| spark|</span><br><span class="line">|  tunan| flink|</span><br><span class="line">| xiaoqi|   cdh|</span><br><span class="line">| xiaoqi| kafka|</span><br><span class="line">| xiaoqi| hbase|</span><br><span class="line">+-------+------+</span><br></pre></td></tr></table></figure><h2 id="解读Spark-SQL执行计划优化"><a href="#解读Spark-SQL执行计划优化" class="headerlink" title="解读Spark SQL执行计划优化"></a>解读Spark SQL执行计划优化</h2><ol><li><p>建空表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> sqltest (<span class="keyword">key</span> <span class="keyword">string</span>,<span class="keyword">value</span> <span class="keyword">string</span>)</span><br></pre></td></tr></table></figure></li><li><p>执行SQL</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">explain</span> <span class="keyword">extended</span> <span class="keyword">select</span> a.key*(<span class="number">3</span>*<span class="number">5</span>),b.value <span class="keyword">from</span> sqltest a <span class="keyword">join</span> sqltest b <span class="keyword">on</span> a.key=b.key <span class="keyword">and</span> a.key &gt;<span class="number">3</span>;</span><br></pre></td></tr></table></figure></li><li><p>解读执行计划</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">// 解析逻辑计划，做些简单的解析</span><br><span class="line">== Parsed Logical Plan ==</span><br><span class="line">'Project [unresolvedalias(('a.key * (3 * 5)), None), 'b.value]</span><br><span class="line">+- 'Join Inner, (('a.key = 'b.key) &amp;&amp; ('a.key &gt; 3))</span><br><span class="line">   :- 'SubqueryAlias `a`</span><br><span class="line">   :  +- 'UnresolvedRelation `sqltest`</span><br><span class="line">   +- 'SubqueryAlias `b`</span><br><span class="line">      +- 'UnresolvedRelation `sqltest`</span><br><span class="line"></span><br><span class="line">// 分析逻辑计划，解析出了数据类型，拿到数据库和表，拿到了序列化方式                           </span><br><span class="line">== Analyzed Logical Plan ==</span><br><span class="line">(CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE)): double, value: string</span><br><span class="line">Project [(cast(key<span class="comment">#2 as double) * cast((3 * 5) as double)) AS (CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE))#6, value#5]</span></span><br><span class="line">+- Join Inner, ((key<span class="comment">#2 = key#4) &amp;&amp; (cast(key#2 as int) &gt; 3))</span></span><br><span class="line">   :- SubqueryAlias `a`</span><br><span class="line">   :  +- SubqueryAlias `default`.`sqltest`</span><br><span class="line">   :     +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key<span class="comment">#2, value#3]</span></span><br><span class="line">   +- SubqueryAlias `b`</span><br><span class="line">      +- SubqueryAlias `default`.`sqltest`</span><br><span class="line">         +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key<span class="comment">#4, value#5]</span></span><br><span class="line"></span><br><span class="line">// 优化逻辑计划，数值类型的运算直接拿到结果，解析过滤条件</span><br><span class="line">== Optimized Logical Plan ==</span><br><span class="line">Project [(cast(key<span class="comment">#2 as double) * 15.0) AS (CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE))#6, value#5]</span></span><br><span class="line">+- Join Inner, (key<span class="comment">#2 = key#4)</span></span><br><span class="line">   :- Project [key<span class="comment">#2]</span></span><br><span class="line">   :  +- Filter (isnotnull(key<span class="comment">#2) &amp;&amp; (cast(key#2 as int) &gt; 3))</span></span><br><span class="line">   :     +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key<span class="comment">#2, value#3]</span></span><br><span class="line">   +- Filter ((cast(key<span class="comment">#4 as int) &gt; 3) &amp;&amp; isnotnull(key#4))</span></span><br><span class="line">      +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key<span class="comment">#4, value#5]</span></span><br><span class="line"></span><br><span class="line">//物理计划，join方式为SortMergeJoin，数据使用hashpartitioning保存，扫描表的方式是HiveTableRelation</span><br><span class="line">== Physical Plan ==</span><br><span class="line">*(5) Project [(cast(key#2 as double) * 15.0) AS (CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE))#6, value#5]</span><br><span class="line">+- *(5) SortMergeJoin [key#2], [key#4], Inner</span><br><span class="line">   :- *(2) Sort [key#2 ASC NULLS FIRST], false, 0</span><br><span class="line">   :  +- Exchange hashpartitioning(key<span class="comment">#2, 200)</span></span><br><span class="line">   :     +- *(1) Filter (isnotnull(key#2) &amp;&amp; (cast(key#2 as int) &gt; 3))</span><br><span class="line">   :        +- Scan hive default.sqltest [key<span class="comment">#2], HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#2, value#3]</span></span><br><span class="line">   +- *(4) Sort [key#4 ASC NULLS FIRST], false, 0</span><br><span class="line">      +- Exchange hashpartitioning(key<span class="comment">#4, 200)</span></span><br><span class="line">         +- *(3) Filter ((cast(key#4 as int) &gt; 3) &amp;&amp; isnotnull(key#4))</span><br><span class="line">            +- Scan hive default.sqltest [key<span class="comment">#4, value#5], HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#4, value#5]</span></span><br></pre></td></tr></table></figure><p>可以简单的看做四步，分别是解析逻辑计划、分析逻辑计划、优化逻辑计划、物理执行计划</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM之运行时数据区</title>
      <link href="/2020/04/15/jvm/1/"/>
      <url>/2020/04/15/jvm/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>jvm命令</li><li>jvm的运行时数据区</li><li>jvm会发生哪些ERROR</li><li>从一个class出发理解数据区</li></ol><h2 id="jvm命令"><a href="#jvm命令" class="headerlink" title="jvm命令"></a>jvm命令</h2><h3 id="JVM参数类型"><a href="#JVM参数类型" class="headerlink" title="JVM参数类型"></a>JVM参数类型</h3><ol><li>标准: 稳定的，长期没有变化</li><li>X: 相对变化较少的</li><li>XX: 变化较大，JVM调优重点</li></ol><p>设置参数时，idea指定在VM options里面，命令行直接加在java命令后</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">java -Xss10m -XX:+PrintGCDetails JVMParams</span><br></pre></td></tr></table></figure><h3 id="常见的XX类型的参数"><a href="#常见的XX类型的参数" class="headerlink" title="常见的XX类型的参数"></a>常见的XX类型的参数</h3><ol><li><p>-XX:+PrintGCDetails: 打印GC日志</p></li><li><p>-XX:+PrintFlagsInitial: 打印所有初始的参数信息</p></li><li><p>-XX:+PrintFlagsFinal: 打印所有最终的参数信息</p></li><li><p>-Xms设置堆的最小空间大小。</p></li><li><p>-Xmx设置堆的最大空间大小。</p></li><li><p>-XX:NewSize设置新生代最小空间大小。</p></li><li><p>-XX:MaxNewSize设置新生代最大空间大小。</p></li><li><p>-XX:PermSize设置永久代最小空间大小。</p></li><li><p>-XX:MaxPermSize设置永久代最大空间大小。</p></li><li><p>-Xss设置每个线程的堆栈大小。</p></li><li><p>没有直接设置老年代的参数，但是可以设置堆空间大小和新生代空间大小两个参数来间接控制</p><p> <strong>老年代空间大小=堆空间大小-年轻代大空间大小</strong></p></li></ol><p>例如：</p><p>java -XX:+PrintFlagsFinal -version </p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">uintx MaxHeapSize             := <span class="number">2048917504</span>     &#123;product&#125;</span><br><span class="line">intx MaxInlineLevel            = <span class="number">9</span>              &#123;product&#125;</span><br><span class="line">intx MaxInlineSize       = <span class="number">35</span>             &#123;product&#125;</span><br><span class="line">bool ParGCTrimOverflow      = <span class="keyword">true</span>           &#123;product&#125;</span><br><span class="line">bool ParGCUseLocalOverflow     = <span class="keyword">false</span>          &#123;product&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>上面只显示部分参数，但是能够说明我们需要理解的内容，即 ‘=’ 表示默认值，‘:=’ 表示被修改过的值。同时还有数值类型和布尔类型。</p><h3 id="几个特殊的XX类型参数"><a href="#几个特殊的XX类型参数" class="headerlink" title="几个特殊的XX类型参数"></a>几个特殊的XX类型参数</h3><p>-Xms、-Xmx、-Xss 实际上是XX类型的缩写</p><p>-Xms ==&gt; -XX:InitialHeapSize: 表示为: -Xms10m<br>-Xmx ==&gt; -XX:MaxHeapSize: 表示为: -Xmx10m<br>-Xss ==&gt; -XX:ThreadStackSize</p><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><ol><li><p>查看java进程：jps</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[hadoop<span class="meta">@hadoop</span> ~]$ jps</span><br><span class="line"><span class="number">13612</span> JVMParams</span><br><span class="line"><span class="number">13644</span> Jps</span><br></pre></td></tr></table></figure></li><li><p>查看java进程的参数信息<br>jinfo -flag name pid </p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[hadoop<span class="meta">@hadoop</span> ~]$ jinfo -flag MaxHeapSize <span class="number">13612</span></span><br><span class="line">-XX:MaxHeapSize=<span class="number">2048917504</span></span><br><span class="line">[hadoop<span class="meta">@hadoop</span> ~]$ jinfo -flag InitialHeapSize <span class="number">13612</span></span><br><span class="line">-XX:InitialHeapSize=<span class="number">130023424</span></span><br><span class="line">[hadoop<span class="meta">@hadoop</span> ~]$ jinfo -flag ThreadStackSize <span class="number">13612</span></span><br><span class="line">-XX:ThreadStackSize=<span class="number">1024</span></span><br></pre></td></tr></table></figure><p>怎么理解-XX:MaxHeapSize=2048917504，-XX:InitialHeapSize=130023424 ?</p><p>分析：</p><p>我主机的物理内存为8G，2048917504k = 1.9G，130023424  = 124M</p><p>理论上heap的最大值为物理内存的1/4，最小值为物理内存的1/64</p><p>但是一般情况下，我们会把MaxHeapSize和InitialHeapSize设置相同的值，防止内存抖动</p></li><li><p>查看java进程的默认和设置的参数</p><p>jinfo -flags pid </p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[hadoop<span class="meta">@hadoop</span> ~]$ jinfo -flags  <span class="number">13612</span></span><br><span class="line">Non-<span class="keyword">default</span> VM flags: -XX:CICompilerCount=<span class="number">2</span> ...</span><br><span class="line">Command line:  -Xss10m -XX:+PrintGCDetails ...</span><br></pre></td></tr></table></figure></li></ol><h2 id="jvm的运行时数据区"><a href="#jvm的运行时数据区" class="headerlink" title="jvm的运行时数据区"></a>jvm的运行时数据区</h2><p><img src="https://yerias.github.io/java_img/%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA.jpg" alt="运行时数据区"></p><h3 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h3><p>程序计数器是每个线程私有的</p><p>程序计数器是一块较小的内存空间，它可以看做是当前线程的行号指示器，这在多线程环境下非常有用。使得线程切换后能够恢复到正确的执行位置。</p><p>在java虚拟机规范中，这是唯一一个没有规定任何OutOfMemoryError的地方</p><h3 id="Java虚拟机栈"><a href="#Java虚拟机栈" class="headerlink" title="Java虚拟机栈"></a>Java虚拟机栈</h3><p>java虚拟机栈也是每个线程私有的，它的生命周期和线程相同</p><p>虚拟机栈描述的是java方法执行的线程内存模型: 每个方法被执行的时候，java虚拟机栈都会同步创建一个Frame(栈帧)用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法被调用直到执行完毕的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。</p><p>最常用的就是局部变量表，局部变量表存放了编译期可知的各种java虚拟机的基本数据类型(boolean、byte、char、sort、int、long、float、double)、对象引用(reference类型)和returnAddress类型，这部分在后面讲有详细的解释。</p><p>在java虚拟机规范中，这个区域可能存在两种异常，如果线程请求的栈深度大于虚拟机所允许的深度，会抛出StackOverflowError异常，常见的有循环调用方法名；如果java虚拟机栈容量可以动态扩展，当栈无法申请到足够的内存时就会抛出OutOfMemoryError异常。</p><h3 id="本地方法栈"><a href="#本地方法栈" class="headerlink" title="本地方法栈"></a>本地方法栈</h3><p>本地方法栈是线程私有的</p><p>本地方法栈与虚拟机栈所发挥的作用是非常类似的，其区别只是虚拟机栈为虚拟机执行java方法服务，而本地方法栈则为本地(native)方法服务</p><p>常见的本地方法有getClass、hashCode、clone、notify、notifyAll、wait、sleep等</p><p>与java虚拟机栈一样，本地方法栈也会在栈深度溢出的时候或者栈扩展失败的时候抛出StackOutflowError和OutOfMemoryError异常。</p><h3 id="java堆"><a href="#java堆" class="headerlink" title="java堆"></a>java堆</h3><p>java堆是所有线程共享的，是虚拟机所管理的内存中最大的一块，在虚拟机启动时创建。</p><p>此内存区域的唯一目的是存放对象实例，对象实例包括对象和数组。</p><p>如果在java堆中没有内存完成实例分配，并且堆也无法扩展，java虚拟机将会抛出OutOfMemoryError异常。</p><h3 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h3><p>方法区是所有线程共享的</p><p>它用于存储已被虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据，简单点说就是Class。</p><p>方法区只是java虚拟机的规范，它属于堆的一个逻辑部分，为了和堆区分开，也叫非堆。在jdk8之前，方法区的具体实现叫做永久代，</p><ol><li>由于类及方法的信息大小很难确定，所以内存设置小了会发生OOM，设置大了又浪费</li><li>GC复杂度高，回收效率低</li><li>合并 HotSpot 与 JRockit </li></ol><p>所以在jdk8完全用元空间替换了永久代，元空间直接使用的系统内存。</p><p>在java虚拟机规范中，如果方法区无法满足内存分配需求时，会抛出OouOfMemoryError</p><h3 id="直接内存"><a href="#直接内存" class="headerlink" title="直接内存"></a>直接内存</h3><p>直接内存不是java虚拟机规范中定义的内存区域，但是这部分也会被频繁使用，所以也可能会抛出OutOfMemoryError。</p><h2 id="jvm会发生哪些ERROR"><a href="#jvm会发生哪些ERROR" class="headerlink" title="jvm会发生哪些ERROR"></a>jvm会发生哪些ERROR</h2><h3 id="java堆内存OOM异常测试"><a href="#java堆内存OOM异常测试" class="headerlink" title="java堆内存OOM异常测试"></a>java堆内存OOM异常测试</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">-Xmx10m -Xms10m -XX:+HeapDumpOnOutOfMemoryError</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HeapOOM</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">OOMObject</span></span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        ArrayList&lt;OOMObject&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">            list.add(<span class="keyword">new</span> OOMObject());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">Dumping heap to java_pid184.hprof ...</span><br><span class="line">Heap dump file created [<span class="number">10209413</span> bytes in <span class="number">0.080</span> secs]</span><br></pre></td></tr></table></figure><h3 id="java虚拟机栈和本地方法栈SOF测试"><a href="#java虚拟机栈和本地方法栈SOF测试" class="headerlink" title="java虚拟机栈和本地方法栈SOF测试"></a>java虚拟机栈和本地方法栈SOF测试</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">-Xss2M</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JavaVMStackSOF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> stackLength=<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">stackLeak</span><span class="params">()</span></span>&#123;</span><br><span class="line">        stackLength++;</span><br><span class="line">        stackLeak();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        JavaVMStackSOF stackSOF = <span class="keyword">new</span> JavaVMStackSOF();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            stackSOF.stackLeak();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">            System.out.println(<span class="string">"栈深度："</span>+stackSOF.stackLength);</span><br><span class="line">            <span class="keyword">throw</span> e;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">栈深度：<span class="number">41075</span></span><br><span class="line">Exception in thread <span class="string">"main"</span> java.lang.StackOverflowError</span><br></pre></td></tr></table></figure><h3 id="java虚拟机栈和本地方法栈OOM测试"><a href="#java虚拟机栈和本地方法栈OOM测试" class="headerlink" title="java虚拟机栈和本地方法栈OOM测试"></a>java虚拟机栈和本地方法栈OOM测试</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">-Xss4m</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JavaVMStackOOM</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">dontStop</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">stackleakByThread</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">            <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">                    dontStop();</span><br><span class="line">            &#125;).start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        JavaVMStackOOM oom = <span class="keyword">new</span> JavaVMStackOOM();</span><br><span class="line">        oom.stackleakByThread();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果:</p><p>死机</p><h3 id="方法区和运行时常量池OOM"><a href="#方法区和运行时常量池OOM" class="headerlink" title="方法区和运行时常量池OOM"></a>方法区和运行时常量池OOM</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">-XX:PerSize=<span class="number">6</span>m -XX:MaxPermSize=<span class="number">6</span>M</span><br><span class="line"><span class="keyword">import</span> java.util.HashSet;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RuntimeConstantPoolOOM</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        HashSet&lt;String&gt; set = <span class="keyword">new</span> HashSet&lt;String&gt;();</span><br><span class="line">        <span class="keyword">short</span> i = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">            set.add(String.valueOf(i++).intern());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果:</p><p>jdk8没有测试出来</p><h2 id="从一个class出发理解数据区"><a href="#从一个class出发理解数据区" class="headerlink" title="从一个class出发理解数据区"></a>从一个class出发理解数据区</h2><p><img src="https://yerias.github.io/java_img/class%E7%B1%BB%E5%AF%B9%E6%AF%94%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA.jpg" alt="class类对比运行时数据区"></p><p>如图所示，很容易的理解各区分别保存java代码中的哪些部分</p><p>堆区: 保存的People对象</p><p>栈区: 保存的栈帧，栈帧中保存了引用name和age和引用people</p><p>方法区: 保存的People.class相关的，包括类型信息、常量、静态变量sss</p><p>运行时常量池: 保存name和age字符串内容</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从jdbc的角度解读外部数据源</title>
      <link href="/2020/04/15/spark/16/"/>
      <url>/2020/04/15/spark/16/</url>
      
        <content type="html"><![CDATA[<h2 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h2><p>首先了解三个trait，分别是BaseRelation、TableScan/PrunedScan/PrunedFilteredScan、<del>InsertableRelation</del>、RelationProvider，他们的功能在源码中解读。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//代表了一个抽象的数据源。该数据源由一行行有着已知schema的数据组成（关系表）。</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BaseRelation</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sqlContext</span></span>: <span class="type">SQLContext</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">schema</span></span>: <span class="type">StructType</span><span class="comment">//schema *</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sizeInBytes</span></span>: <span class="type">Long</span> = sqlContext.conf.defaultSizeInBytes</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">needConversion</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">unhandledFilters</span></span>(filters: <span class="type">Array</span>[<span class="type">Filter</span>]): <span class="type">Array</span>[<span class="type">Filter</span>] = filters</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//用于扫描整张表，将数据返回成RDD[Row]。</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Stable</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">TableScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//用于裁剪整张表，将数据返回成RDD[Row]。</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Stable</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">PrunedScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(requiredColumns: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">Row</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//用于裁剪并过滤整张表，将数据返回成RDD[Row]。</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Stable</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">PrunedFilteredScan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(requiredColumns: <span class="type">Array</span>[<span class="type">String</span>], filters: <span class="type">Array</span>[<span class="type">Filter</span>]): <span class="type">RDD</span>[<span class="type">Row</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//插入数据的时候实现，设置overwrite是否为true</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Stable</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">InsertableRelation</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(data: <span class="type">DataFrame</span>, overwrite: <span class="type">Boolean</span>): <span class="type">Unit</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//为自定义的数据源类型生成一个新的Relation对象</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">RelationProvider</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//创建一个新的Relation</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span></span>(sqlContext: <span class="type">SQLContext</span>, parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">BaseRelation</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="jdbc实现"><a href="#jdbc实现" class="headerlink" title="jdbc实现"></a>jdbc实现</h2><p>JdbcRelationProvider (最初也是最终的地方)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdbcRelationProvider</span> <span class="keyword">extends</span> <span class="title">CreatableRelationProvider</span></span></span><br><span class="line"><span class="class"><span class="keyword">with</span> <span class="title">RelationProvider</span> <span class="keyword">with</span> <span class="title">DataSourceRegister</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">shortName</span></span>(): <span class="type">String</span> = <span class="string">"jdbc"</span> <span class="comment">//简称</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span></span>(</span><br><span class="line">        sqlContext: <span class="type">SQLContext</span>,</span><br><span class="line">        parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">BaseRelation</span> = &#123;<span class="comment">//所有options参数以map形式传入</span></span><br><span class="line">        <span class="keyword">val</span> jdbcOptions = <span class="keyword">new</span> <span class="type">JDBCOptions</span>(parameters) <span class="comment">//把参数传入和系统参数匹配</span></span><br><span class="line">        <span class="keyword">val</span> resolver = sqlContext.conf.resolver <span class="comment">//忽略大小写</span></span><br><span class="line">        <span class="keyword">val</span> timeZoneId = sqlContext.conf.sessionLocalTimeZone <span class="comment">//拿到时区</span></span><br><span class="line">        <span class="keyword">val</span> schema = <span class="type">JDBCRelation</span>.getSchema(resolver, jdbcOptions) <span class="comment">//传入参数，拿到schema</span></span><br><span class="line">        <span class="keyword">val</span> parts = <span class="type">JDBCRelation</span>.columnPartition(schema, resolver, timeZoneId,  jdbcOptions) <span class="comment">//拿到分区</span></span><br><span class="line">        <span class="type">JDBCRelation</span>(schema, parts, jdbcOptions)(sqlContext.sparkSession)  <span class="comment">//拿到RDD[R]</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>getSchema </p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSchema</span></span>(resolver: <span class="type">Resolver</span>, jdbcOptions: <span class="type">JDBCOptions</span>): <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> tableSchema = <span class="type">JDBCRDD</span>.resolveTable(jdbcOptions)   <span class="comment">//传入参数，解析table，拿到Schame</span></span><br><span class="line">    jdbcOptions.customSchema <span class="keyword">match</span> &#123; <span class="comment">//模式匹配</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(customSchema) =&gt; <span class="type">JdbcUtils</span>.getCustomSchema( </span><br><span class="line">            tableSchema, customSchema, resolver)<span class="comment">// 返回定制的Schema</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt; tableSchema <span class="comment">//返回直接的Schema</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>resolveTable (阶段一: 拿Schema)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resolveTable</span></span>(options: <span class="type">JDBCOptions</span>): <span class="type">StructType</span> = &#123; <span class="comment">//传入参数，拿到Schame</span></span><br><span class="line">    <span class="keyword">val</span> url = options.url <span class="comment">//拿到url：jdbc:mysql://hadoop:3306/</span></span><br><span class="line">    <span class="keyword">val</span> table = options.tableOrQuery<span class="comment">//拿到table：access_dw.dws_ad_phone_type_dist</span></span><br><span class="line">    <span class="keyword">val</span> dialect = <span class="type">JdbcDialects</span>.get(url) <span class="comment">//拿到方言：MySQLDialect</span></span><br><span class="line">    <span class="keyword">val</span> conn: <span class="type">Connection</span> = <span class="type">JdbcUtils</span>.createConnectionFactory(options)() <span class="comment">//创建连接</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> statement = conn.prepareStatement(dialect.getSchemaQuery(table)) <span class="comment">//拿到sql：com.mysql.jdbc.JDBC42PreparedStatement@5bda157e: SELECT * FROM access_dw.dws_ad_phone_type_dist WHERE 1=0</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            statement.setQueryTimeout(options.queryTimeout) <span class="comment">//设置超时时间</span></span><br><span class="line">            <span class="keyword">val</span> rs = statement.executeQuery() <span class="comment">//执行查询，返回一个查询产生的数据的ResultSet对象</span></span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="type">JdbcUtils</span>.getSchema(rs, dialect, alwaysNullable = <span class="literal">true</span>)  <span class="comment">//传入数据rs，拿到schema，接着下面的内容</span></span><br><span class="line">            &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                rs.close()</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            statement.close()</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        conn.close()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>getSchema (阶段一: 拿Schema)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSchema</span></span>(</span><br><span class="line">    resultSet: <span class="type">ResultSet</span>, <span class="comment">//查询表返回的rs(表结构)</span></span><br><span class="line">    dialect: <span class="type">JdbcDialect</span>, <span class="comment">//MySQL方言</span></span><br><span class="line">    alwaysNullable: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">StructType</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> rsmd = resultSet.getMetaData <span class="comment">//拿到表的元数据</span></span><br><span class="line">  <span class="keyword">val</span> ncols = rsmd.getColumnCount  <span class="comment">//拿到需要的字段的列的数量</span></span><br><span class="line">  <span class="keyword">val</span> fields = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">StructField</span>](ncols) <span class="comment">//创建一个StructField类型的数组，拼接fields</span></span><br><span class="line">  <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">  <span class="keyword">while</span> (i &lt; ncols) &#123; <span class="comment">//循环出每个column</span></span><br><span class="line">    <span class="keyword">val</span> columnName = rsmd.getColumnLabel(i + <span class="number">1</span>) <span class="comment">//返回列名：phoneSystemType</span></span><br><span class="line">    <span class="keyword">val</span> dataType = rsmd.getColumnType(i + <span class="number">1</span>) <span class="comment">//返回数据类型：12</span></span><br><span class="line">    <span class="keyword">val</span> typeName = rsmd.getColumnTypeName(i + <span class="number">1</span>) <span class="comment">//返回数据类型的名称：VARCHAR</span></span><br><span class="line">    <span class="keyword">val</span> fieldSize = rsmd.getPrecision(i + <span class="number">1</span>)  <span class="comment">//返回字段大小：64</span></span><br><span class="line">    <span class="keyword">val</span> fieldScale = rsmd.getScale(i + <span class="number">1</span>)<span class="comment">//返回scale：0</span></span><br><span class="line">    <span class="keyword">val</span> isSigned = &#123; <span class="comment">//判断是否有符号</span></span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        rsmd.isSigned(i + <span class="number">1</span>) <span class="comment">//是否有符号：false</span></span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="comment">// Workaround for HIVE-14684:</span></span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">SQLException</span> <span class="keyword">if</span></span><br><span class="line">        e.getMessage == <span class="string">"Method not supported"</span> &amp;&amp;</span><br><span class="line">          rsmd.getClass.getName == <span class="string">"org.apache.hive.jdbc.HiveResultSetMetaData"</span> =&gt; <span class="literal">true</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> nullable = <span class="keyword">if</span> (alwaysNullable) &#123; <span class="comment">//判断是否可为空</span></span><br><span class="line">      <span class="literal">true</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      rsmd.isNullable(i + <span class="number">1</span>) != <span class="type">ResultSetMetaData</span>.columnNoNulls</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> metadata = <span class="keyword">new</span> <span class="type">MetadataBuilder</span>().putLong(<span class="string">"scale"</span>, fieldScale)</span><br><span class="line">    <span class="keyword">val</span> columnType =</span><br><span class="line">      dialect.getCatalystType(dataType, typeName, fieldSize, metadata).getOrElse(</span><br><span class="line">        getCatalystType(dataType, fieldSize, fieldScale, isSigned)) <span class="comment">// 传入参数拿到类型：StringType</span></span><br><span class="line">    fields(i) = <span class="type">StructField</span>(columnName, columnType, nullable) <span class="comment">//传入列名，数据类型，是否可为空，创建StructField，并加入到fields中</span></span><br><span class="line">    i = i + <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">StructType</span>(fields) <span class="comment">//传入所有的StructField构建StructType，并返回，到这里拿到最终的Schema</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>JDBCRelation (阶段一: 拿Schema)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[sql] <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">JDBCRelation</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    override val schema: <span class="type">StructType</span>, //拿到<span class="type">Schema</span></span></span></span><br><span class="line"><span class="class"><span class="params">    parts: <span class="type">Array</span>[<span class="type">Partition</span>], //得到分区</span></span></span><br><span class="line"><span class="class"><span class="params">    jdbcOptions: <span class="type">JDBCOptions</span></span>)(<span class="params">@transient val sparkSession: <span class="type">SparkSession</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">BaseRelation</span> <span class="title">//实现BaseRelation，必然拿到了Schema</span></span></span><br><span class="line"><span class="class">  <span class="keyword">with</span> <span class="title">PrunedFilteredScan</span> <span class="title">//实现裁剪并且过滤的扫描表</span></span></span><br><span class="line"><span class="class">  <span class="keyword">with</span> <span class="title">InsertableRelation</span> </span>&#123; <span class="comment">//实现插入的模式</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">sqlContext</span></span>: <span class="type">SQLContext</span> = sparkSession.sqlContext</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> needConversion: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//检查JDBCRDD.compileFilter是否可以接受输入过滤器</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">unhandledFilters</span></span>(filters: <span class="type">Array</span>[<span class="type">Filter</span>]): <span class="type">Array</span>[<span class="type">Filter</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (jdbcOptions.pushDownPredicate) &#123;</span><br><span class="line">      filters.filter(<span class="type">JDBCRDD</span>.compileFilter(_, <span class="type">JdbcDialects</span>.get(jdbcOptions.url)).isEmpty)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      filters</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 构建Scan</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(requiredColumns: <span class="type">Array</span>[<span class="type">String</span>], filters: <span class="type">Array</span>[<span class="type">Filter</span>]): <span class="type">RDD</span>[<span class="type">Row</span>] = &#123;<span class="comment">//requiredColumns：需要的列，filters：过滤条件</span></span><br><span class="line">    <span class="comment">// 依赖类型擦除：将RDD[InternalRow]传递回RDD[Row]</span></span><br><span class="line">    <span class="type">JDBCRDD</span>.scanTable(</span><br><span class="line">      sparkSession.sparkContext, <span class="comment">//上下文环境</span></span><br><span class="line">      schema, <span class="comment">//Schema</span></span><br><span class="line">      requiredColumns, <span class="comment">//需要的列</span></span><br><span class="line">      filters, <span class="comment">//过滤条件</span></span><br><span class="line">      parts,   <span class="comment">//分区</span></span><br><span class="line">      jdbcOptions).asInstanceOf[<span class="type">RDD</span>[<span class="type">Row</span>]] <span class="comment">//最终的结果转换成RDD[Row]类型</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(data: <span class="type">DataFrame</span>, overwrite: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    data.write</span><br><span class="line">      .mode(<span class="keyword">if</span> (overwrite) <span class="type">SaveMode</span>.<span class="type">Overwrite</span> <span class="keyword">else</span> <span class="type">SaveMode</span>.<span class="type">Append</span>)</span><br><span class="line">      .jdbc(jdbcOptions.url, jdbcOptions.tableOrQuery, jdbcOptions.asProperties)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> partitioningInfo = <span class="keyword">if</span> (parts.nonEmpty) <span class="string">s" [numPartitions=<span class="subst">$&#123;parts.length&#125;</span>]"</span> <span class="keyword">else</span> <span class="string">""</span></span><br><span class="line">    <span class="comment">// 计划输出中不应包含凭据，表信息就足够了。</span></span><br><span class="line">    <span class="string">s"JDBCRelation(<span class="subst">$&#123;jdbcOptions.tableOrQuery&#125;</span>)"</span> + partitioningInfo</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>scanTable (阶段二: 拿RDD[Row])</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scanTable</span></span>(</span><br><span class="line">    sc: <span class="type">SparkContext</span>, </span><br><span class="line">    schema: <span class="type">StructType</span>,</span><br><span class="line">    requiredColumns: <span class="type">Array</span>[<span class="type">String</span>],</span><br><span class="line">    filters: <span class="type">Array</span>[<span class="type">Filter</span>],</span><br><span class="line">    parts: <span class="type">Array</span>[<span class="type">Partition</span>],</span><br><span class="line">    options: <span class="type">JDBCOptions</span>): <span class="type">RDD</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> url = options.url <span class="comment">//拿到客户端传入的rul</span></span><br><span class="line">    <span class="keyword">val</span> dialect = <span class="type">JdbcDialects</span>.get(url) <span class="comment">//拿到方言</span></span><br><span class="line">    <span class="keyword">val</span> quotedColumns = requiredColumns.map(colName =&gt; dialect.quoteIdentifier(colName)) <span class="comment">//拿到需要的列</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">JDBCRDD</span>(<span class="comment">//传入参数，返回RDD[InternalRow]</span></span><br><span class="line">        sc,</span><br><span class="line">        <span class="type">JdbcUtils</span>.createConnectionFactory(options),</span><br><span class="line">        pruneSchema(schema, requiredColumns),</span><br><span class="line">        quotedColumns,</span><br><span class="line">        filters,</span><br><span class="line">        parts,</span><br><span class="line">        url,</span><br><span class="line">        options)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>JDBCRDD(阶段二: 拿RDD[Row])</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 表示通过JDBC访问的数据库中的表的RDD。</span></span><br><span class="line"><span class="keyword">private</span>[jdbc] <span class="class"><span class="keyword">class</span> <span class="title">JDBCRDD</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    sc: <span class="type">SparkContext</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    getConnection: (</span>) <span class="title">=&gt;</span> <span class="title">Connection</span>,</span></span><br><span class="line"><span class="class">    <span class="title">schema</span></span>: <span class="type">StructType</span>,</span><br><span class="line">    columns: <span class="type">Array</span>[<span class="type">String</span>],</span><br><span class="line">    filters: <span class="type">Array</span>[<span class="type">Filter</span>],</span><br><span class="line">    partitions: <span class="type">Array</span>[<span class="type">Partition</span>],</span><br><span class="line">    url: <span class="type">String</span>,</span><br><span class="line">    options: <span class="type">JDBCOptions</span>)</span><br><span class="line"><span class="keyword">extends</span> <span class="type">RDD</span>[<span class="type">InternalRow</span>](sc, <span class="type">Nil</span>) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 索引与此RDD对应的分区列表。</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = partitions</span><br><span class="line"></span><br><span class="line">    <span class="comment">// `columns` 作为一个字符串注入到SQL查询</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> columnList: <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sb = <span class="keyword">new</span> <span class="type">StringBuilder</span>()</span><br><span class="line">        columns.foreach(x =&gt; sb.append(<span class="string">","</span>).append(x))</span><br><span class="line">        <span class="keyword">if</span> (sb.isEmpty) <span class="string">"1"</span> <span class="keyword">else</span> sb.substring(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// `filters`, 作为一个where语句注入到SQL查询</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> filterWhereClause: <span class="type">String</span> =</span><br><span class="line">    filters</span><br><span class="line">    .flatMap(<span class="type">JDBCRDD</span>.compileFilter(_, <span class="type">JdbcDialects</span>.get(url)))</span><br><span class="line">    .map(p =&gt; <span class="string">s"(<span class="subst">$p</span>)"</span>).mkString(<span class="string">" AND "</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果当前分区有where语句，那么就拼接</span></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getWhereClause</span></span>(part: <span class="type">JDBCPartition</span>): <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span> (part.whereClause != <span class="literal">null</span> &amp;&amp; filterWhereClause.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="string">"WHERE "</span> + <span class="string">s"(<span class="subst">$filterWhereClause</span>)"</span> + <span class="string">" AND "</span> + <span class="string">s"(<span class="subst">$&#123;part.whereClause&#125;</span>)"</span></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (part.whereClause != <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="string">"WHERE "</span> + part.whereClause</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (filterWhereClause.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="string">"WHERE "</span> + filterWhereClause</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="string">""</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对JDBC驱动程序运行SQL查询。</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(thePart: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">        <span class="keyword">var</span> closed = <span class="literal">false</span></span><br><span class="line">        <span class="keyword">var</span> rs: <span class="type">ResultSet</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">var</span> stmt: <span class="type">PreparedStatement</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">var</span> conn: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>() &#123;</span><br><span class="line">            <span class="keyword">if</span> (closed) <span class="keyword">return</span></span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (<span class="literal">null</span> != rs) &#123;</span><br><span class="line">                    rs.close()</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; logWarning(<span class="string">"Exception closing resultset"</span>, e)</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (<span class="literal">null</span> != stmt) &#123;</span><br><span class="line">                    stmt.close()</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; logWarning(<span class="string">"Exception closing statement"</span>, e)</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (<span class="literal">null</span> != conn) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (!conn.isClosed &amp;&amp; !conn.getAutoCommit) &#123;</span><br><span class="line">                        <span class="keyword">try</span> &#123;</span><br><span class="line">                            conn.commit()</span><br><span class="line">                        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                            <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logWarning(<span class="string">"Exception committing transaction"</span>, e)</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                    conn.close()</span><br><span class="line">                &#125;</span><br><span class="line">                logInfo(<span class="string">"closed connection"</span>)</span><br><span class="line">            &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; logWarning(<span class="string">"Exception closing connection"</span>, e)</span><br><span class="line">            &#125;</span><br><span class="line">            closed = <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        context.addTaskCompletionListener[<span class="type">Unit</span>]&#123; context =&gt; close() &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> inputMetrics = context.taskMetrics().inputMetrics</span><br><span class="line">        <span class="keyword">val</span> part = thePart.asInstanceOf[<span class="type">JDBCPartition</span>]</span><br><span class="line">        conn = getConnection()</span><br><span class="line">        <span class="keyword">val</span> dialect = <span class="type">JdbcDialects</span>.get(url)</span><br><span class="line">        <span class="keyword">import</span> scala.collection.<span class="type">JavaConverters</span>._</span><br><span class="line">        dialect.beforeFetch(conn, options.asProperties.asScala.toMap)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 这在通过JDBC读取表/查询之前执行一个通用的SQL语句(或PL/SQL块)。</span></span><br><span class="line">        <span class="comment">// 使用此功能初始化数据库会话环境，例如用于优化和/或故障排除。</span></span><br><span class="line">        options.sessionInitStatement <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="type">Some</span>(sql) =&gt;</span><br><span class="line">            <span class="keyword">val</span> statement = conn.prepareStatement(sql)</span><br><span class="line">            logInfo(<span class="string">s"Executing sessionInitStatement: <span class="subst">$sql</span>"</span>)</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                statement.setQueryTimeout(options.queryTimeout)</span><br><span class="line">                statement.execute() <span class="comment">//最终执行的就是jdbc</span></span><br><span class="line">            &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                statement.close()</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 返回RDD[InternalRow]</span></span><br><span class="line">        <span class="type">CompletionIterator</span>[<span class="type">InternalRow</span>, <span class="type">Iterator</span>[<span class="type">InternalRow</span>]](</span><br><span class="line">            <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context, rowsIterator), close())</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终一套debug走下来，其实就是两步</p><ol><li>第二步通过jdbc查元数据，拿到Schema</li><li>第二步通过jdbc查数据拿到RDD[Row]</li></ol><p><strong>最终的创建DataFrame由框架解决</strong></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RDD转换DadaFrame&amp;使用SQL操作数据源&amp;跨数据源join&amp;SQL与DF与DS的比较&amp;Spark元数据管理: catalog</title>
      <link href="/2020/04/13/spark/15/"/>
      <url>/2020/04/13/spark/15/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>RDD转换DadaFrame</li><li>使用SQL操作数据源</li><li>跨数据源join</li><li>SQL与DF与DS的比较</li><li>Spark元数据管理: catalog</li></ol><h2 id="RDD转换DadaFrame"><a href="#RDD转换DadaFrame" class="headerlink" title="RDD转换DadaFrame"></a>RDD转换DadaFrame</h2><ol><li><p>第一种方式是使用反射来推断包含特定对象类型的RDD的模式</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">reflect</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().master(<span class="string">"local[2]"</span>).appName(<span class="keyword">this</span>.getClass.getSimpleName).getOrCreate()</span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-sql/data/top.txt"</span></span><br><span class="line">        <span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = spark.sparkContext.textFile(in)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = lines.split(<span class="string">","</span>)</span><br><span class="line">            people(words(<span class="number">0</span>), words(<span class="number">1</span>), words(<span class="number">2</span>).toInt)</span><br><span class="line">        &#125;)</span><br><span class="line">        mapRDD.toDF().show()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">people</span>(<span class="params">name:<span class="type">String</span>,subject:<span class="type">String</span>,grade:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure><p>通过反射class的这种方式可以获得Schema创建DataFrame，简单通用，但是在<strong>创建外部数据源的场景下不适用</strong></p></li><li><p>第二种方法是通过编程接口，通过StructType可以构造Schema，然后将其应用于现有的RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">interface</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().master(<span class="string">"local[2]"</span>).appName(<span class="keyword">this</span>.getClass.getSimpleName).getOrCreate()</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-sql/data/top.txt"</span></span><br><span class="line">        <span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = spark.sparkContext.textFile(in)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//在原RDD上创建rowRDD</span></span><br><span class="line">        <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = lines.split(<span class="string">","</span>)</span><br><span class="line">            <span class="type">Row</span>(words(<span class="number">0</span>), words(<span class="number">1</span>), words(<span class="number">2</span>).toDouble)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建和上一步对应的行结构类型的StructType</span></span><br><span class="line">        <span class="keyword">val</span> innerStruct =</span><br><span class="line">            <span class="type">StructType</span>(</span><br><span class="line">            <span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">StringType</span>, <span class="literal">false</span>) ::</span><br><span class="line">            <span class="type">StructField</span>(<span class="string">"subject"</span>, <span class="type">StringType</span>, <span class="literal">false</span>) ::</span><br><span class="line">            <span class="type">StructField</span>(<span class="string">"grade"</span>, <span class="type">DoubleType</span>, <span class="literal">false</span>) :: <span class="type">Nil</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将schema和Rows结合，创建出DF</span></span><br><span class="line">        <span class="keyword">val</span> df = spark.createDataFrame(mapRDD, innerStruct)</span><br><span class="line"></span><br><span class="line">        df.show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>步骤：</p><ol><li>在原RDD上创建rowRDD</li><li>创建和上一步对应的行结构类型的StructType</li><li>将schema和Rows结合，创建出DF</li></ol></li></ol><h2 id="使用SQL操作数据源"><a href="#使用SQL操作数据源" class="headerlink" title="使用SQL操作数据源"></a>使用SQL操作数据源</h2><p>在官网的<a href="http://spark.apache.org/docs/latest/sql-data-sources.html" target="_blank" rel="noopener">Data Sources</a> 下，每个数据源下都有一个Sql选项卡，其中就是对应的SQL采集源数据，并生成对应的SQL视图的代码，如：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">spark-sql (default)&gt; CREATE TEMPORARY VIEW jsonTable</span><br><span class="line">                   &gt; USING org.apache.spark.sql.json</span><br><span class="line">                   &gt; OPTIONS (</span><br><span class="line">                   &gt;   path "file:///home/hadoop/data/people.json"</span><br><span class="line">                   &gt; );</span><br><span class="line">Response code</span><br><span class="line"></span><br><span class="line">spark-sql (default)&gt; SELECT * FROM jsonTable;</span><br><span class="line">agename</span><br><span class="line">NULLMichael</span><br><span class="line">30Andy</span><br><span class="line">19Justin</span><br></pre></td></tr></table></figure><h2 id="跨数据源join"><a href="#跨数据源join" class="headerlink" title="跨数据源join"></a>跨数据源join</h2><p>跨数据源join是Spark非常好用的一个特性，从不同的数据源拿到spark中，再从spark写出去，简直轻而易举。</p><p>下面我们将验证从hive和mysql中分别拿出一个表join(在idea中操作时，需要先连上hive)</p><ol><li><p>jdbc</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> jdbcDF = spark.read</span><br><span class="line">    .format(<span class="string">"jdbc"</span>)</span><br><span class="line">    .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://hadoop/?characterEncoding=utf-8&amp;useSSL=false"</span>)</span><br><span class="line">    .option(<span class="string">"dbtable"</span>, <span class="string">"tunan.dept"</span>)</span><br><span class="line">    .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">    .option(<span class="string">"password"</span>, <span class="string">"root"</span>)</span><br><span class="line">    .load()</span><br></pre></td></tr></table></figure></li><li><p>hive</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> hiveDF = spark.sql(<span class="string">"select * from default.emp"</span>)</span><br></pre></td></tr></table></figure></li><li><p>join</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> joinDF: <span class="type">DataFrame</span> = jdbcDF.join(hiveDF, <span class="string">"deptno"</span>)</span><br></pre></td></tr></table></figure></li><li><p>查看结果</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">&gt;&gt; joinDF.show(<span class="literal">false</span>)</span><br><span class="line">+------+----------+-----+-----+------+---------+----+----------+------+------+</span><br><span class="line">|deptno|     dname|level|empno| ename|      job| jno|      date|   sal| prize|</span><br><span class="line">+------+----------+-----+-----+------+---------+----+----------+------+------+</span><br><span class="line">|    <span class="number">10</span>|<span class="type">ACCOUNTING</span>| <span class="number">1700</span>| <span class="number">7566</span>| <span class="type">JONES</span>|  <span class="type">MANAGER</span>|<span class="number">7839</span>|  <span class="number">1981</span><span class="number">-4</span><span class="number">-2</span>|<span class="number">2975.0</span>|  <span class="literal">null</span>|</span><br><span class="line">|    <span class="number">10</span>|<span class="type">ACCOUNTING</span>| <span class="number">1700</span>| <span class="number">7521</span>|  <span class="type">WARD</span>| <span class="type">SALESMAN</span>|<span class="number">7698</span>| <span class="number">1981</span><span class="number">-2</span><span class="number">-22</span>|<span class="number">1250.0</span>| <span class="number">500.0</span>|</span><br><span class="line">|    <span class="number">20</span>|  <span class="type">RESEARCH</span>| <span class="number">1800</span>| <span class="number">7934</span>|<span class="type">MILLER</span>|    <span class="type">CLERK</span>|<span class="number">7782</span>| <span class="number">1982</span><span class="number">-1</span><span class="number">-23</span>|<span class="number">1300.0</span>|  <span class="literal">null</span>|</span><br><span class="line">|    <span class="number">20</span>|  <span class="type">RESEARCH</span>| <span class="number">1800</span>| <span class="number">7902</span>|  <span class="type">FORD</span>|  <span class="type">ANALYST</span>|<span class="number">7566</span>| <span class="number">1981</span><span class="number">-12</span><span class="number">-3</span>|<span class="number">3000.0</span>|  <span class="literal">null</span>|</span><br><span class="line">....</span><br><span class="line">+------+----------+-----+-----+------+---------+----+----------+------+------+</span><br></pre></td></tr></table></figure></li><li><p>我们并不需要全部的数据，下面我们将经过处理选择我们需要的数据</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 定义视实体类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">EmpDept</span>(<span class="params">deptno:<span class="type">String</span>,dname:<span class="type">String</span>,level:<span class="type">Int</span>,empno:<span class="type">String</span>,ename:<span class="type">String</span>,job:<span class="type">String</span>,jno:<span class="type">String</span>,date:<span class="type">String</span>,sal:<span class="type">Double</span>,prize:<span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Result</span>(<span class="params">empno:<span class="type">String</span>,ename:<span class="type">String</span>,deptno:<span class="type">String</span>,dname:<span class="type">String</span>,prize:<span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//DF转换成DS</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">joinDS</span></span>: <span class="type">Dataset</span>[<span class="type">EmpDept</span>] = joinDF.as[<span class="type">EmpDept</span>]</span><br><span class="line"><span class="comment">//从DS中拿到数据，反射的方式拿到Schema信息</span></span><br><span class="line"><span class="keyword">val</span> mapDS = joinDS.map(x =&gt; <span class="type">Result</span>(x.empno, x.ename, x.deptno, x.dname,x.prize))</span><br></pre></td></tr></table></figure></li><li><p>保存数据</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 保存到文件</span></span><br><span class="line">mapDS.write.format(<span class="string">"orc"</span>).save(<span class="string">"tunan-spark-sql/out"</span>)</span><br><span class="line"><span class="comment">// 保存到MySQL数据库</span></span><br><span class="line">mapDS.write.format(<span class="string">"jdbc"</span>)</span><br><span class="line">.option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://hadoop/?characterEncoding=utf-8&amp;useSSL=false"</span>)</span><br><span class="line">.option(<span class="string">"dbtable"</span>, <span class="string">"tunan.join_result"</span>)</span><br><span class="line">.option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">.option(<span class="string">"password"</span>, <span class="string">"root"</span>)</span><br><span class="line">.mode(<span class="string">"overwrite"</span>)</span><br><span class="line">.save()</span><br></pre></td></tr></table></figure><p>查看结果:</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+-----+------+------+----------+------+</span><br><span class="line">|empno| ename|deptno|     dname| prize|</span><br><span class="line">+-----+------+------+----------+------+</span><br><span class="line">| <span class="number">7566</span>| <span class="type">JONES</span>|    <span class="number">10</span>|<span class="type">ACCOUNTING</span>|  <span class="literal">null</span>|</span><br><span class="line">| <span class="number">7521</span>|  <span class="type">WARD</span>|    <span class="number">10</span>|<span class="type">ACCOUNTING</span>| <span class="number">500.0</span>|</span><br><span class="line">| <span class="number">7934</span>|<span class="type">MILLER</span>|    <span class="number">20</span>|  <span class="type">RESEARCH</span>|  <span class="literal">null</span>|</span><br><span class="line">| <span class="number">7902</span>|  <span class="type">FORD</span>|    <span class="number">20</span>|  <span class="type">RESEARCH</span>|  <span class="literal">null</span>|</span><br><span class="line">...</span><br><span class="line">+-----+------+------+----------+------+</span><br></pre></td></tr></table></figure></li></ol><h2 id="SQL与DF与DS的比较"><a href="#SQL与DF与DS的比较" class="headerlink" title="SQL与DF与DS的比较"></a>SQL与DF与DS的比较</h2><p>小问题：spark.read.load()   这句代码没用指定读取格式，那么它的默认格式是什么？</p><p>现在我们需要对比的是SQL、DF、DS三者对Syntax Errors和Analysis Errors的不同程度的响应</p><p>在上一步中，我们将joinDF转化成了joinDS，现在我们就看看他们在选择需要的列的时候，做了什么样的执行计划</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> selectDF: <span class="type">DataFrame</span> = joinDF.select(<span class="string">"ename"</span>)</span><br><span class="line"><span class="keyword">val</span> selectDS: <span class="type">Dataset</span>[<span class="type">String</span>] = joinDS.map(_.ename)</span><br><span class="line"></span><br><span class="line">println(selectDF.queryExecution.optimizedPlan.numberedTreeString)</span><br><span class="line">println(<span class="string">"-------------"</span>)</span><br><span class="line">println(selectDS.queryExecution.optimizedPlan.numberedTreeString)</span><br></pre></td></tr></table></figure><p>很明显selectDS做出了优化</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="number">00</span> <span class="type">Project</span> [ename#<span class="number">7</span>]</span><br><span class="line"><span class="number">01</span> +- <span class="type">Join</span> <span class="type">Inner</span>, (deptno#<span class="number">0</span> = deptno#<span class="number">13</span>)</span><br><span class="line"><span class="number">02</span>    :- <span class="type">Project</span> [deptno#<span class="number">0</span>]</span><br><span class="line"><span class="number">03</span>    :  +- <span class="type">Filter</span> isnotnull(deptno#<span class="number">0</span>)</span><br><span class="line"><span class="number">04</span>    :     +- <span class="type">Relation</span>[deptno#<span class="number">0</span>,dname#<span class="number">1</span>,level#<span class="number">2</span>] <span class="type">JDBCRelation</span>(tunan.dept) [numPartitions=<span class="number">1</span>]</span><br><span class="line"><span class="number">05</span>    +- <span class="type">Project</span> [ename#<span class="number">7</span>, deptno#<span class="number">13</span>]</span><br><span class="line"><span class="number">06</span>       +- <span class="type">Filter</span> isnotnull(deptno#<span class="number">13</span>)</span><br><span class="line"><span class="number">07</span>          +- <span class="type">HiveTableRelation</span> `<span class="keyword">default</span>`.`emp`, org.apache.hadoop.hive.serde2.<span class="keyword">lazy</span>.<span class="type">LazySimpleSerDe</span>, [empno#<span class="number">6</span>, ename#<span class="number">7</span>, job#<span class="number">8</span>, jno#<span class="number">9</span>, date#<span class="number">10</span>, sal#<span class="number">11</span>, prize#<span class="number">12</span>, deptno#<span class="number">13</span>]</span><br><span class="line"></span><br><span class="line">-------------</span><br><span class="line"></span><br><span class="line"><span class="number">00</span> <span class="type">SerializeFromObject</span> [staticinvoke(<span class="class"><span class="keyword">class</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">unsafe</span>.<span class="title">types</span>.<span class="title">UTF8String</span>, <span class="title">StringType</span>, <span class="title">fromString</span>, <span class="title">input</span>[0, java.lang.<span class="type">String</span>, true], <span class="title">true</span>, <span class="title">false</span>) <span class="title">AS</span> <span class="title">value#47</span>]</span></span><br><span class="line"><span class="class">01 <span class="title">+-</span> <span class="title">MapElements</span> <span class="title">com</span>.<span class="title">tunan</span>.<span class="title">spark</span>.<span class="title">sql</span>.<span class="title">join</span>.<span class="title">JdbcJoinHive$$$Lambda$1097/374205056@10f8e2d2</span>, <span class="title">class</span> <span class="title">com</span>.<span class="title">tunan</span>.<span class="title">spark</span>.<span class="title">sql</span>.<span class="title">join</span>.<span class="title">JdbcJoinHive$EmpDept</span>, [<span class="type">StructField</span>(deptno,<span class="type">StringType</span>,true), <span class="type">StructField</span>(dname,<span class="type">StringType</span>,true), <span class="type">StructField</span>(level,<span class="type">StringType</span>,true), <span class="type">StructField</span>(empno,<span class="type">StringType</span>,true), <span class="type">StructField</span>(ename,<span class="type">StringType</span>,true), <span class="type">StructField</span>(job,<span class="type">StringType</span>,true), <span class="type">StructField</span>(jno,<span class="type">StringType</span>,true), <span class="type">StructField</span>(date,<span class="type">StringType</span>,true), <span class="type">StructField</span>(sal,<span class="type">DoubleType</span>,false), <span class="type">StructField</span>(prize,<span class="type">StringType</span>,true)], <span class="title">obj#46</span></span>: java.lang.<span class="type">String</span></span><br><span class="line"><span class="number">02</span>    +- <span class="type">DeserializeToObject</span> newInstance(<span class="class"><span class="keyword">class</span> <span class="title">com</span>.<span class="title">tunan</span>.<span class="title">spark</span>.<span class="title">sql</span>.<span class="title">join</span>.<span class="title">JdbcJoinHive$EmpDept</span>), <span class="title">obj#45</span></span>: com.tunan.spark.sql.join.<span class="type">JdbcJoinHive</span>$<span class="type">EmpDept</span></span><br><span class="line"><span class="number">03</span>       +- <span class="type">Project</span> [deptno#<span class="number">0</span>, dname#<span class="number">1</span>, level#<span class="number">2</span>, empno#<span class="number">6</span>, ename#<span class="number">7</span>, job#<span class="number">8</span>, jno#<span class="number">9</span>, date#<span class="number">10</span>, sal#<span class="number">11</span>, prize#<span class="number">12</span>]</span><br><span class="line"><span class="number">04</span>          +- <span class="type">Join</span> <span class="type">Inner</span>, (deptno#<span class="number">0</span> = deptno#<span class="number">13</span>)</span><br><span class="line"><span class="number">05</span>             :- <span class="type">Filter</span> isnotnull(deptno#<span class="number">0</span>)</span><br><span class="line"><span class="number">06</span>             :  +- <span class="type">Relation</span>[deptno#<span class="number">0</span>,dname#<span class="number">1</span>,level#<span class="number">2</span>] <span class="type">JDBCRelation</span>(tunan.dept) [numPartitions=<span class="number">1</span>]</span><br><span class="line"><span class="number">07</span>             +- <span class="type">Filter</span> isnotnull(deptno#<span class="number">13</span>)</span><br><span class="line"><span class="number">08</span>                +- <span class="type">HiveTableRelation</span> `<span class="keyword">default</span>`.`emp`, org.apache.hadoop.hive.serde2.<span class="keyword">lazy</span>.<span class="type">LazySimpleSerDe</span>, [empno#<span class="number">6</span>, ename#<span class="number">7</span>, job#<span class="number">8</span>, jno#<span class="number">9</span>, date#<span class="number">10</span>, sal#<span class="number">11</span>, prize#<span class="number">12</span>, deptno#<span class="number">13</span>]</span><br></pre></td></tr></table></figure><p>由此我们可以根据在Spark SQL应用中，选择列的时候，SQL、DF、DS三者做一个比较</p><table><thead><tr><th></th><th>SQL</th><th>DF</th><th>DS</th></tr></thead><tbody><tr><td>Syntax Errors</td><td>runtime</td><td>compile</td><td>compile</td></tr><tr><td>Analysis Errors</td><td>runtime</td><td>runtime</td><td>compile</td></tr></tbody></table><p>在执行SQL的时候，无论是语法错误还是运行错误，都无法在编译时就提前暴露出来</p><p>在执行DF的时候，算子如果写错了，会提前暴露出来，但是写的列名只有在运行的时候才会检查是否正确</p><p>在执行DS的时候，由于case class反射的机制，算子和列名都可以提前到代码编写时就检测到错误</p><p><strong>所以最优的执行顺序为 DS &gt; DF &gt; SQL</strong></p><h3 id="面试题：RDD、DS、DF的区别"><a href="#面试题：RDD、DS、DF的区别" class="headerlink" title="面试题：RDD、DS、DF的区别"></a>面试题：RDD、DS、DF的区别</h3><ol><li>RDD不支持SQL</li><li>DF每一行都是Row类型，不能直接访问字段，必须解析才行</li><li>DS每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获 得每一行的信息</li><li>DataFrame与Dataset均支持spark sql的操作，比如select，group by之类，还 能注册临时表/视窗，进行sql语句操作</li><li>可以看出，Dataset在需要访问列中的某个字段时是非常方便的，然而，如果要 写一些适配性很强的函数时，如果使用Dataset，行的类型又不确定，可能是各种case class，无法实现适配，这时候用DataFrame即Dataset[Row]就能比较好的解决问题。</li></ol><h3 id="面试题：RDD和Dataset-DataFrame中的Persist的默认缓存级别"><a href="#面试题：RDD和Dataset-DataFrame中的Persist的默认缓存级别" class="headerlink" title="面试题：RDD和Dataset/DataFrame中的Persist的默认缓存级别"></a>面试题：RDD和Dataset/DataFrame中的Persist的默认缓存级别</h3><ul><li>Dataset 中的缓存级别是 MEMORY_AND_DISK</li><li>RDD 中的缓存级别是 MEMORY_ONLY</li></ul><h3 id="面试题：Spark-RDD和Spark-SQL的的cache有什么区别"><a href="#面试题：Spark-RDD和Spark-SQL的的cache有什么区别" class="headerlink" title="面试题：Spark RDD和Spark SQL的的cache有什么区别"></a>面试题：Spark RDD和Spark SQL的的cache有什么区别</h3><ul><li><p>Spark RDD的cache是lazy的，需要action才会执行cache操作</p></li><li><p>Spark SQL的cache是egaer的，马上就cache了</p></li></ul><h2 id="Spark元数据管理-catalog"><a href="#Spark元数据管理-catalog" class="headerlink" title="Spark元数据管理: catalog"></a>Spark元数据管理: catalog</h2><p>拿到catalog</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().master(<span class="string">"local[2]"</span>).appName(<span class="keyword">this</span>.getClass.getSimpleName).enableHiveSupport().getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> catalog: <span class="type">Catalog</span> = spark.catalog</span><br></pre></td></tr></table></figure><ol><li><p>展示所有数据库</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dbList: <span class="type">Dataset</span>[<span class="type">Database</span>] = catalog.listDatabases()</span><br><span class="line">dbList.show(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure></li><li><p>当前数据库</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">catalog.currentDatabase</span><br></pre></td></tr></table></figure></li><li><p>只展示名字</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line">dbList.map(_.name).show()</span><br></pre></td></tr></table></figure></li><li><p>展示指定库的所有表</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">catalog.setCurrentDatabase(<span class="string">"offline_dw"</span>)</span><br><span class="line">catalog.listTables().show(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure></li><li><p>过滤表</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> listTable = catalog.listTables()</span><br><span class="line">listTable.filter(<span class="symbol">'name</span> === <span class="string">"dws_country_traffic"</span>).show(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure></li><li><p>判断某个表是否缓存</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">catalog.isCached(<span class="string">"dws_country_traffic"</span>)</span><br><span class="line">catalog.cacheTable(<span class="string">"dws_country_traffic"</span>)</span><br><span class="line">catalog.isCached(<span class="string">"dws_country_traffic"</span>)</span><br><span class="line">catalog.uncacheTable(<span class="string">"dws_country_traffic"</span>)</span><br></pre></td></tr></table></figure><p><strong>注意：catalog的cacheTable是lazy的</strong></p></li><li><p>展示所有函数</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">catalog.listFunctions().show(<span class="number">1000</span>，<span class="literal">false</span>)</span><br></pre></td></tr></table></figure></li><li><p>注册函数，再次展示</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">spark.udf.register(<span class="string">"udf_string_length"</span>,(word:<span class="type">String</span>) =&gt; &#123;</span><br><span class="line">    word.split(<span class="string">","</span>).length</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">catalog.listFunctions().filter(<span class="symbol">'name</span> === <span class="string">"udf_string_length"</span>).show(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkCore的调优之开发调优</title>
      <link href="/2020/04/10/spark/14/"/>
      <url>/2020/04/10/spark/14/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>调优概述</li><li>原则一：避免创建重复的RDD</li><li>原则二：尽可能复用同一个RDD</li><li>原则三：对多次使用的RDD进行持久化</li><li>原则四：尽量避免使用shuffle类算子</li><li>原则五：使用map-side预聚合的shuffle操作</li><li>原则六：使用高性能的算子</li><li>原则七：广播大变量</li><li>原则八：使用Kryo优化序列化性能</li><li>原则九：优化数据结构</li><li>原则十：Data Locality本地化级别</li></ul><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。</p><p>然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。</p><p>Spark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。</p><p>笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。</p><p>本文作为Spark性能优化指南的基础篇，主要讲解开发调优以及资源调优。</p><h2 id="调优概述"><a href="#调优概述" class="headerlink" title="调优概述"></a>调优概述</h2><p>Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。</p><h2 id="原则一：避免创建重复的RDD"><a href="#原则一：避免创建重复的RDD" class="headerlink" title="原则一：避免创建重复的RDD"></a>原则一：避免创建重复的RDD</h2><p>通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。</p><p>我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。</p><p>一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。</p><p>一个简单的例子</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。</span></span><br><span class="line"><span class="comment">// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。</span></span><br><span class="line"><span class="comment">// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd2.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。</span></span><br><span class="line"><span class="comment">// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。</span></span><br><span class="line"><span class="comment">// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。</span></span><br><span class="line"><span class="comment">// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><h2 id="原则二：尽可能复用同一个RDD"><a href="#原则二：尽可能复用同一个RDD" class="headerlink" title="原则二：尽可能复用同一个RDD"></a>原则二：尽可能复用同一个RDD</h2><p>除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。</p><p>一个简单的例子</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 错误的做法。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。</span></span><br><span class="line"><span class="comment">// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line"><span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; rdd2 = rdd1.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分别对rdd1和rdd2执行了不同的算子操作。</span></span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd2.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。</span></span><br><span class="line"><span class="comment">// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 其实在这种情况下完全可以复用同一个RDD。</span></span><br><span class="line"><span class="comment">// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。</span></span><br><span class="line"><span class="comment">// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd1.map(tuple._2...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。</span></span><br><span class="line"><span class="comment">// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。</span></span><br><span class="line"><span class="comment">// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。</span></span><br></pre></td></tr></table></figure><h2 id="原则三：对多次使用的RDD进行持久化"><a href="#原则三：对多次使用的RDD进行持久化" class="headerlink" title="原则三：对多次使用的RDD进行持久化"></a>原则三：对多次使用的RDD进行持久化</h2><p>当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。</p><p>Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。</p><p>因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。</p><p>对多次使用的RDD进行持久化的代码示例</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"><span class="comment">// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。</span></span><br><span class="line"><span class="comment">// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。</span></span><br><span class="line"><span class="comment">// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).cache()</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。</span></span><br><span class="line"><span class="comment">// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。</span></span><br><span class="line"><span class="comment">// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。</span></span><br><span class="line"><span class="comment">// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><p>对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。</p><h3 id="Spark的持久化级别"><a href="#Spark的持久化级别" class="headerlink" title="Spark的持久化级别"></a>Spark的持久化级别</h3><table><thead><tr><th>持久化级别</th><th>含义解释</th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。</td></tr><tr><td>MEMORY_AND_DISK</td><td>使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</td></tr><tr><td>MEMORY_ONLY_SER</td><td>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>MEMORY_AND_DISK_SER</td><td>基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>DISK_ONLY</td><td>使用未序列化的Java对象格式，将数据全部写入磁盘文件中。</td></tr><tr><td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等.</td><td>对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。</td></tr></tbody></table><h3 id="如何选择一种最合适的持久化策略"><a href="#如何选择一种最合适的持久化策略" class="headerlink" title="如何选择一种最合适的持久化策略"></a>如何选择一种最合适的持久化策略</h3><ul><li>默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。</li><li>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</li><li>如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。</li><li>通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</li></ul><h2 id="原则四：尽量避免使用shuffle类算子"><a href="#原则四：尽量避免使用shuffle类算子" class="headerlink" title="原则四：尽量避免使用shuffle类算子"></a>原则四：尽量避免使用shuffle类算子</h2><p>如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。</p><p>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。</p><p>因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p><p>Broadcast与map进行join代码示例</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 传统的join操作会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Broadcast+map的join操作，不会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 使用Broadcast将一个数据量较小的RDD作为广播变量。</span></span><br><span class="line"><span class="keyword">val</span> rdd2Data = rdd2.collect()</span><br><span class="line"><span class="keyword">val</span> rdd2DataBroadcast = sc.broadcast(rdd2Data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span></span><br><span class="line"><span class="comment">// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。</span></span><br><span class="line"><span class="comment">// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.map(rdd2DataBroadcast...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span></span><br><span class="line"><span class="comment">// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span></span><br></pre></td></tr></table></figure><h2 id="原则五：使用map-side预聚合的shuffle操作"><a href="#原则五：使用map-side预聚合的shuffle操作" class="headerlink" title="原则五：使用map-side预聚合的shuffle操作"></a>原则五：使用map-side预聚合的shuffle操作</h2><p>如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。</p><p>所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。</p><p>比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</p><p><img src="https://yerias.github.io/spark_img/shuffle1.png" alt="shuffle1"></p><p><img src="https://yerias.github.io/spark_img/shuffle2.png" alt="shuffle2"></p><h2 id="原则六：使用高性能的算子"><a href="#原则六：使用高性能的算子" class="headerlink" title="原则六：使用高性能的算子"></a>原则六：使用高性能的算子</h2><p>除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。</p><h3 id="使用reduceByKey-aggregateByKey替代groupByKey"><a href="#使用reduceByKey-aggregateByKey替代groupByKey" class="headerlink" title="使用reduceByKey/aggregateByKey替代groupByKey"></a>使用reduceByKey/aggregateByKey替代groupByKey</h3><p>详情见“原则五：使用map-side预聚合的shuffle操作”。</p><h3 id="使用mapPartitions替代普通map"><a href="#使用mapPartitions替代普通map" class="headerlink" title="使用mapPartitions替代普通map"></a>使用mapPartitions替代普通map</h3><p>mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！</p><h3 id="使用foreachPartitions替代foreach"><a href="#使用foreachPartitions替代foreach" class="headerlink" title="使用foreachPartitions替代foreach"></a>使用foreachPartitions替代foreach</h3><p>原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。</p><h3 id="使用filter之后进行coalesce操作"><a href="#使用filter之后进行coalesce操作" class="headerlink" title="使用filter之后进行coalesce操作"></a>使用filter之后进行coalesce操作</h3><p>通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。</p><h3 id="使用repartitionAndSortWithinPartitions替代repartition与sort类操作"><a href="#使用repartitionAndSortWithinPartitions替代repartition与sort类操作" class="headerlink" title="使用repartitionAndSortWithinPartitions替代repartition与sort类操作"></a>使用repartitionAndSortWithinPartitions替代repartition与sort类操作</h3><p>repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。</p><h2 id="原则七：广播大变量"><a href="#原则七：广播大变量" class="headerlink" title="原则七：广播大变量"></a>原则七：广播大变量</h2><p>有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。</p><p>在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。</p><p>因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。</p><p>广播大变量的代码示例</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 以下代码在算子函数中，使用了外部的变量。</span></span><br><span class="line"><span class="comment">// 此时没有做任何特殊操作，每个task都会有一份list1的副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line">rdd1.map(list1...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下代码将list1封装成了Broadcast类型的广播变量。</span></span><br><span class="line"><span class="comment">// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。</span></span><br><span class="line"><span class="comment">// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。</span></span><br><span class="line"><span class="comment">// 每个Executor内存中，就只会驻留一份广播变量副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line"><span class="keyword">val</span> list1Broadcast = sc.broadcast(list1)</span><br><span class="line">rdd1.map(list1Broadcast...)</span><br></pre></td></tr></table></figure><h2 id="原则八：使用Kryo优化序列化性能"><a href="#原则八：使用Kryo优化序列化性能" class="headerlink" title="原则八：使用Kryo优化序列化性能"></a>原则八：使用Kryo优化序列化性能</h2><p>在Spark中，主要有三个地方涉及到了序列化：</p><ul><li>在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。</li><li>将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。</li><li>使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。</li></ul><p>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。</p><p>以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建SparkConf对象。</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class="line"><span class="comment">// 设置序列化器为KryoSerializer。</span></span><br><span class="line">conf.set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line"><span class="comment">// 注册要序列化的自定义类型。</span></span><br><span class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</span><br></pre></td></tr></table></figure><h2 id="原则九：优化数据结构"><a href="#原则九：优化数据结构" class="headerlink" title="原则九：优化数据结构"></a>原则九：优化数据结构</h2><p>Java中，有三种类型比较耗费内存：</p><ul><li>对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。</li><li>字符串，每个字符串内部都有一个字符数组以及长度等额外信息。</li><li>集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。</li></ul><p>因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。</p><p>但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。</p><h2 id="原则十：Data-Locality本地化级别"><a href="#原则十：Data-Locality本地化级别" class="headerlink" title="原则十：Data Locality本地化级别"></a>原则十：Data Locality本地化级别</h2><p><strong>PROCESS_LOCAL</strong>：进程本地化，代码和数据在同一个进程中，也就是在同一个executor中；计算数据的task由executor执行，数据在executor的BlockManager中；性能最好</p><p><strong>NODE_LOCAL</strong>：节点本地化，代码和数据在同一个节点中；比如说，数据作为一个HDFS block块，就在节点上，而task在节点上某个executor中运行；或者是，数据和task在一个节点上的不同executor中；数据需要在进程间进行传输<br><strong>NO_PREF</strong>：对于task来说，数据从哪里获取都一样，没有好坏之分<br><strong>RACK_LOCAL</strong>：机架本地化，数据和task在一个机架的两个节点上；数据需要通过网络在节点之间进行传输<br><strong>ANY</strong>：数据和task可能在集群中的任何地方，而且不在一个机架中，性能最差</p><p>spark.locality.wait，默认是3s</p><p>Spark在Driver上，对Application的每一个stage的task，进行分配之前，都会计算出每个task要计算的是哪个分片数据，RDD的某个partition；Spark的task分配算法，优先，会希望每个task正好分配到它要计算的数据所在的节点，这样的话，就不用在网络间传输数据；</p><p>但是可能task没有机会分配到它的数据所在的节点，因为可能那个节点的计算资源和计算能力都满了；所以呢，这种时候，通常来说，Spark会等待一段时间，默认情况下是3s钟（不是绝对的，还有很多种情况，对不同的本地化级别，都会去等待），到最后，实在是等待不了了，就会选择一个比较差的本地化级别，比如说，将task分配到靠它要计算的数据所在节点，比较近的一个节点，然后进行计算。</p><p>但是对于第二种情况，通常来说，肯定是要发生数据传输，task会通过其所在节点的BlockManager来获取数据，BlockManager发现自己本地没有数据，会通过一个getRemote()方法，通过TransferService（网络数据传输组件）从数据所在节点的BlockManager中，获取数据，通过网络传输回task所在节点。</p><p>对于我们来说，当然不希望是类似于第二种情况的了。最好的，当然是task和数据在一个节点上，直接从本地executor的BlockManager中获取数据，纯内存，或者带一点磁盘IO；如果要通过网络传输数据的话，那么实在是，性能肯定会下降的，大量网络传输，以及磁盘IO，都是性能的杀手。</p><p><strong>什么时候要调节这个参数？</strong><br>观察日志，spark作业的运行日志，推荐大家在测试的时候，先用client模式，在本地就直接可以看到比较全的日志。<br>日志里面会显示，starting task。。。，PROCESS LOCAL、NODE LOCAL，观察大部分task的数据本地化级别。</p><p>如果大多都是PROCESS_LOCAL，那就不用调节了<br>如果是发现，好多的级别都是NODE_LOCAL、ANY，那么最好就去调节一下数据本地化的等待时长<br>调节完，应该是要反复调节，每次调节完以后，再来运行，观察日志<br>看看大部分的task的本地化级别有没有提升；看看，整个spark作业的运行时间有没有缩短</p><p>但是注意别本末倒置，本地化级别倒是提升了，但是因为大量的等待时长，spark作业的运行时间反而增加了，那就还是不要调节了。</p><p>spark.locality.wait，默认是3s；可以改成6s，10s</p><p>默认情况下，下面3个的等待时长，都是跟上面那个是一样的，都是3s</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">spark.locality.wait.process<span class="comment">//建议60s</span></span><br><span class="line">spark.locality.wait.node<span class="comment">//建议30s</span></span><br><span class="line">spark.locality.wait.rack<span class="comment">//建议20s</span></span><br></pre></td></tr></table></figure><hr><p>转载自: <a href="https://tech.meituan.com/spark-tuning-basic.html" target="_blank" rel="noopener">https://tech.meituan.com/spark-tuning-basic.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark源码之解读spark-shell脚本</title>
      <link href="/2020/04/10/spark/13/"/>
      <url>/2020/04/10/spark/13/</url>
      
        <content type="html"><![CDATA[<p>该篇文章主要分析一下Spark源码中启动spark-shell脚本的处理逻辑，从spark-shell一步步深入进去看看任务提交的整体流程</p><ol><li><p>spark-shell脚本解读</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 初始化cygwin=<span class="literal">false</span></span></span><br><span class="line">cygwin=false</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查你的系统是否属于cygwin</span></span><br><span class="line">case "$(uname)" in</span><br><span class="line">  CYGWIN*) cygwin=true;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置shell的模式为POSIX标准模式</span></span><br><span class="line">set -o posix</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检测是否设置过SPARK_HOME环境变量</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">export _SPARK_CMD_USAGE="Usage: ./bin/spark-shell [options]</span><br><span class="line">Scala REPL options:</span><br><span class="line">  -I &lt;file&gt;                   preload &lt;file&gt;, enforcing line-by-line interpretation"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">scala默认不会使用java classpath，需要手动设置一下让scala使用java</span></span><br><span class="line">SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true"</span><br><span class="line"></span><br><span class="line">function main() &#123;</span><br><span class="line"><span class="meta">  #</span><span class="bash"> 判断是否是cygwin</span></span><br><span class="line">  if $cygwin; then</span><br><span class="line">    # 关闭echo回显，设置读操作最少1个字符</span><br><span class="line">    stty -icanon min 1 -echo &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">    export SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Djline.terminal=unix"</span><br><span class="line">    # 启动spark-submit 执行org.apache.spark.repl.Main类，并设置应用的名字，传递参数</span><br><span class="line">    "$&#123;SPARK_HOME&#125;"/bin/spark-submit --class org.apache.spark.repl.Main --name "Spark shell" "$@"</span><br><span class="line">    # 开启echo回显</span><br><span class="line">    stty icanon echo &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">  else</span><br><span class="line">    export SPARK_SUBMIT_OPTS</span><br><span class="line">    # 启动spark-submit 执行org.apache.spark.repl.Main类，并设置应用的名字，传递参数</span><br><span class="line">    "$&#123;SPARK_HOME&#125;"/bin/spark-submit --class org.apache.spark.repl.Main --name "Spark shell" "$@"</span><br><span class="line">  fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">exit_status=127</span><br><span class="line">saved_stty=""</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> restore stty settings (<span class="built_in">echo</span> <span class="keyword">in</span> particular)</span></span><br><span class="line">function restoreSttySettings() &#123;</span><br><span class="line">  stty $saved_stty</span><br><span class="line">  saved_stty=""</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 判断是否恢复终端设置</span></span><br><span class="line">function onExit() &#123;</span><br><span class="line">  if [[ "$saved_stty" != "" ]]; then</span><br><span class="line">    restoreSttySettings</span><br><span class="line">  fi</span><br><span class="line">  exit $exit_status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 捕获INT中断信号，然就执行onExit方法</span></span><br><span class="line">trap onExit INT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 保存了当前的终端配置</span></span><br><span class="line">saved_stty=$(stty -g 2&gt;/dev/null)</span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果收到退出命令，就恢复stty状态</span></span><br><span class="line">if [[ ! $? ]]; then</span><br><span class="line">  saved_stty=""</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 调用main方法，并传递所有的参数</span></span><br><span class="line">main "$@"</span><br><span class="line"></span><br><span class="line">exit_status=$?</span><br><span class="line">onExit</span><br></pre></td></tr></table></figure></li><li><p>上面启动了spark-submit，接下来我们解读该脚本</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 检查是否设置了<span class="variable">$&#123;SPARK_HOME&#125;</span></span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在Python 3.3+中禁用字符串的随机哈希</span></span><br><span class="line">export PYTHONHASHSEED=0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动spark-class，并传递了org.apache.spark.deploy.SparkSubmit作为第一个参数，然后把前面Spark-shell的参数都传给spark-class</span></span><br><span class="line">exec "$&#123;SPARK_HOME&#125;"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"</span><br></pre></td></tr></table></figure></li><li><p>在spark-submit中又启动了spark-class，继续解读</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 检查/设置SPARK_HOME</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置一些环境变量</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;"/bin/load-spark-env.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 找到bin/java,并赋值给RUNNER变量</span></span><br><span class="line">if [ -n "$&#123;JAVA_HOME&#125;" ]; then</span><br><span class="line">  RUNNER="$&#123;JAVA_HOME&#125;/bin/java"</span><br><span class="line">else</span><br><span class="line">  if [ "$(command -v java)" ]; then</span><br><span class="line">    RUNNER="java"</span><br><span class="line">  else</span><br><span class="line">    echo "JAVA_HOME is not set" &gt;&amp;2</span><br><span class="line">    exit 1</span><br><span class="line">  fi  </span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 拿到Spark的Jar包</span></span><br><span class="line">if [ -d "$&#123;SPARK_HOME&#125;/jars" ]; then</span><br><span class="line">  SPARK_JARS_DIR="$&#123;SPARK_HOME&#125;/jars"</span><br><span class="line">else</span><br><span class="line">  SPARK_JARS_DIR="$&#123;SPARK_HOME&#125;/assembly/target/scala-$SPARK_SCALA_VERSION/jars"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ ! -d "$SPARK_JARS_DIR" ] &amp;&amp; [ -z "$SPARK_TESTING$SPARK_SQL_TESTING" ]; then</span><br><span class="line">  echo "Failed to find Spark jars directory ($SPARK_JARS_DIR)." 1&gt;&amp;2</span><br><span class="line">  echo "You need to build Spark with the target \"package\" before running this program." 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">else</span><br><span class="line">  LAUNCH_CLASSPATH="$SPARK_JARS_DIR/*"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果需要，将启动程序构建目录添加到类路径。</span></span><br><span class="line">if [ -n "$SPARK_PREPEND_CLASSES" ]; then</span><br><span class="line">  LAUNCH_CLASSPATH="$&#123;SPARK_HOME&#125;/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line">if [[ -n "$SPARK_TESTING" ]]; then</span><br><span class="line">  unset YARN_CONF_DIR</span><br><span class="line">  unset HADOOP_CONF_DIR</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 封装了真正的执行的spark的类</span></span><br><span class="line">build_command() &#123;</span><br><span class="line">  "$RUNNER" -Xmx128m -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@"</span><br><span class="line">  printf "%d\0" $?</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 关闭posix模式，因为它不允许进程替换</span></span><br><span class="line">set +o posix</span><br><span class="line"></span><br><span class="line">CMD=()</span><br><span class="line"><span class="meta">#</span><span class="bash"> 首先循环读取ARG参数，加入到CMD中</span></span><br><span class="line">while IFS= read -d '' -r ARG; do</span><br><span class="line">  CMD+=("$ARG")</span><br><span class="line"><span class="meta">#</span><span class="bash"> 执行Spark的类</span></span><br><span class="line">done &lt; &lt;(build_command "$@")</span><br><span class="line">COUNT=$&#123;#CMD[@]&#125;</span><br><span class="line">LAST=$((COUNT - 1))</span><br><span class="line">LAUNCHER_EXIT_CODE=$&#123;CMD[$LAST]&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then</span><br><span class="line">  echo "$&#123;CMD[@]&#125;" | head -n-1 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $LAUNCHER_EXIT_CODE != 0 ]; then</span><br><span class="line">  exit $LAUNCHER_EXIT_CODE</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">CMD=("$&#123;CMD[@]:0:$LAST&#125;")</span><br><span class="line">exec "$&#123;CMD[@]&#125;"</span><br></pre></td></tr></table></figure></li></ol><p>可以看到虽然是执行了spark-shell，但是最终执行的是<code>org.apache.spark.launcher.Main</code>类，也就是说spark-shell的最底层是使用java来启动的</p><p>他们的执行流程大致如下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark-shell&#123;</span><br><span class="line">spark-submit&#123;</span><br><span class="line">spark-class&#123;</span><br><span class="line">   build_command() &#123;</span><br><span class="line">   "$RUNNER" -Xmx128m -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@"</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban配置Plugin实现Spark作业提交(非Shell)</title>
      <link href="/2020/04/10/azkaban/2/"/>
      <url>/2020/04/10/azkaban/2/</url>
      
        <content type="html"><![CDATA[<p>第一步，我们要打开azkaban的<a href="https://github.com/azkaban/azkaban/tree/master/az-hadoop-jobtype-plugin/src/jobtypes" target="_blank" rel="noopener">官网</a>，配置一些文件和参数，如图所示</p><p><img src="https://yerias.github.io/azkaban_img/az%E9%85%8D%E7%BD%AESpark%E6%8F%90%E4%BA%A4.jpg" alt="az配置Spark提交"></p><p>将<code>spark</code>、<code>common.properties</code>、<code>commonprivate.properties</code>拷贝到服务器中对应的目录，最终的文件展示如下</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">[hadoop<span class="meta">@hadoop</span> jobtypes]$ tree</span><br><span class="line">.</span><br><span class="line">├── commonprivate.properties</span><br><span class="line">├── common.properties</span><br><span class="line">└── spark</span><br><span class="line">    ├── plugin.properties</span><br><span class="line">    └── <span class="keyword">private</span>.properties</span><br></pre></td></tr></table></figure><ol><li><p>配置commonprivate.properties中hadoop.home和spark.home指定的家目录</p></li><li><p>配置common.properties中hadoop.home和spark.home指定的家目录</p></li><li><p>修改private.properties文件中的参数(临时方案，可行)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">jobtype.classpath=$&#123;hadoop.classpath&#125;:$&#123;spark.home&#125;/conf:$&#123;spark.home&#125;/lib<span class="comment">/*</span></span><br><span class="line"><span class="comment">===&gt;</span></span><br><span class="line"><span class="comment">jobtype.classpath=hadoop.classpath:$&#123;spark.home&#125;/conf:$&#123;spark.home&#125;/lib/*</span></span><br></pre></td></tr></table></figure><p>这么做的原因是我们以上的文件中没有配置hadoop.classpath，官方也没有说明hadoop.classpath应该配置什么参数，目前修改掉引用不影响程序的使用。</p></li></ol><p>第二步，在conf/azkaban.properties文件下增加一个配置，主机名:端口号(随意修改)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">azkaban.webserver.url=https:<span class="comment">//hadoop:8666</span></span><br></pre></td></tr></table></figure><p>第三步，提交作业，所配的参数需要参考<a href="https://github.com/azkaban/azkaban/blob/master/az-hadoop-jobtype-plugin/src/main/java/azkaban/jobtype/SparkJobArg.java" target="_blank" rel="noopener">官网</a></p><p>测试案例:</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">config:</span><br><span class="line">    user.to.proxy: hadoop</span><br><span class="line">nodes:</span><br><span class="line">  - name: sparkwc</span><br><span class="line">    <span class="class"><span class="keyword">type</span></span>: spark</span><br><span class="line">    config:</span><br><span class="line">      <span class="class"><span class="keyword">class</span></span>: com.data.spark.wc.<span class="type">SparkWC</span></span><br><span class="line">      master: yarn</span><br><span class="line">      deploy-mode: client</span><br><span class="line">      executor-memory: <span class="number">512</span>M</span><br><span class="line">      driver-memory: <span class="number">512</span>M</span><br><span class="line">      conf.spark.testing.memory: <span class="number">471859200</span></span><br><span class="line">      execution-jar: tunan-spark-utils<span class="number">-1.0</span>.jar</span><br><span class="line">      jars: tunan-spark-core<span class="number">-1.0</span>.jar</span><br><span class="line">      params: hdfs:<span class="comment">//hadoop:9000/input/wc.txt hdfs://hadoop:9000/out</span></span><br></pre></td></tr></table></figure><p>注意配置文件中的 jar 没有写路径，这么提交<strong>需要把 jar 包和配置文件一起打成zip包</strong>，提交到AZ的Web界面</p><p>第四步，查看结果</p><p><em>20200413更新：</em>  数据下标越界问题：hadoop下的/share/hadoop/common/lib/paranamer-2.3.jar过时，使用–jars传spark下的/jars/paranamer-2.8.jar</p>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark中的序列化</title>
      <link href="/2020/04/10/spark/12/"/>
      <url>/2020/04/10/spark/12/</url>
      
        <content type="html"><![CDATA[<p>在写Spark应用时，常常会碰到序列化的问题。例如，在Driver端的程序中创建了一个对象，而在各个Executor端会用到这个对象——由于Driver端的代码和Executor端的代码在不同的JVM中，甚至在不同的节点上，因此必然要有相应</p><h2 id="Java框架进行序列化"><a href="#Java框架进行序列化" class="headerlink" title="Java框架进行序列化"></a>Java框架进行序列化</h2><p>在默认情况下，Spark会使用Java的ObjectOutputStream框架对对象进行序列化，并且可以与任何实现java.io.Serializable的类一起工作。您还可以通过扩展java.io.Externalizable来更紧密地控制序列化的性能。Java序列化是灵活的，但通常相当慢，并且会导致许多类的大型序列化格式。</p><p>测试代码：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">String</span>, name: <span class="type">String</span>, age: <span class="type">Int</span>, gender: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">SerializationDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"SerializationDemo"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> stduentArr = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Student</span>]()</span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">1000000</span>) &#123;</span><br><span class="line">            stduentArr += (<span class="type">Student</span>(i + <span class="string">""</span>, i + <span class="string">"a"</span>, <span class="number">10</span>, <span class="string">"male"</span>))</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> <span class="type">JavaSerialization</span> = sc.parallelize(stduentArr)</span><br><span class="line">        <span class="type">JavaSerialization</span>.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>).count()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="type">Thread</span>.sleep(<span class="number">10000</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在Web界面查看：33.2M</p><h2 id="Kryo框架进行序列化"><a href="#Kryo框架进行序列化" class="headerlink" title="Kryo框架进行序列化"></a>Kryo框架进行序列化</h2><p>Spark还可以使用Kryo库（Spark 2.x）来更快地序列化对象。Kryo比Java（通常多达10倍）要快得多，也更紧凑，但是不支持所有可串行化类型，并且要求您提前注册您将在程序中使用的类，以获得最佳性能。</p><ol><li><p>不注册使用的类测试</p><p>在conf中配置spark.serializer = org.apache.spark.serializer.KryoSerializer来使用kryo序列化</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">String</span>, name: <span class="type">String</span>, age: <span class="type">Int</span>, gender: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">SerializationDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">        .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">        .setAppName(<span class="string">"SerializationDemo"</span>)</span><br><span class="line">        .set(<span class="string">"spark.serializer"</span>,<span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> stduentArr = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Student</span>]()</span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">1000000</span>) &#123;</span><br><span class="line">            stduentArr += (<span class="type">Student</span>(i + <span class="string">""</span>, i + <span class="string">"a"</span>, <span class="number">10</span>, <span class="string">"male"</span>))</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> <span class="type">JavaSerialization</span> = sc.parallelize(stduentArr)</span><br><span class="line">        <span class="type">JavaSerialization</span>.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>).count()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="type">Thread</span>.sleep(<span class="number">10000</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在Web界面查看：53.2M</p><p>这是因为使用Kryo时，不将使用的类注册，往往会得到比java序列化占用更大的内存</p></li><li><p>注册使用的类测试</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">String</span>, name: <span class="type">String</span>, age: <span class="type">Int</span>, gender: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">SerializationDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">        .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">        .setAppName(<span class="string">"SerializationDemo"</span>)</span><br><span class="line">        .set(<span class="string">"spark.serializer"</span>,<span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line">        .registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">Student</span>])) <span class="comment">// 将自定义的类注册到Kryo</span></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> stduentArr = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Student</span>]()</span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">1000000</span>) &#123;</span><br><span class="line">            stduentArr += (<span class="type">Student</span>(i + <span class="string">""</span>, i + <span class="string">"a"</span>, <span class="number">10</span>, <span class="string">"male"</span>))</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> <span class="type">JavaSerialization</span> = sc.parallelize(stduentArr)</span><br><span class="line">        <span class="type">JavaSerialization</span>.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>).count()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="type">Thread</span>.sleep(<span class="number">10000</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在Web界面查看：21.7 M</p><p>在conf中注册</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure></li></ol><h2 id="总结及拓展"><a href="#总结及拓展" class="headerlink" title="总结及拓展"></a>总结及拓展</h2><p>Kryo serialization 性能和序列化大小都比默认提供的 Java serialization 要好，但是<strong>使用Kryo需要将自定义的类先注册进去</strong>，使用起来比 Java serialization麻烦。自从Spark 2.x 以来，我们在使用简单类型、简单类型数组或字符串类型的简单类型来调整RDDs时，在内部使用Kryo序列化器。</p><p>通过查找sparkcontext初始化的源码，可以发现某些类型已经在sparkcontext初始化的时候被注册进去。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Component which configures serialization, compression and encryption for various Spark</span></span><br><span class="line"><span class="comment"> * components, including automatic selection of which [[Serializer]] to use for shuffles.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">SerializerManager</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    defaultSerializer: <span class="type">Serializer</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    conf: <span class="type">SparkConf</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    encryptionKey: <span class="type">Option</span>[<span class="type">Array</span>[<span class="type">Byte</span>]]</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(defaultSerializer: <span class="type">Serializer</span>, conf: <span class="type">SparkConf</span>) = <span class="keyword">this</span>(defaultSerializer, conf, <span class="type">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> kryoSerializer = <span class="keyword">new</span> <span class="type">KryoSerializer</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> stringClassTag: <span class="type">ClassTag</span>[<span class="type">String</span>] = implicitly[<span class="type">ClassTag</span>[<span class="type">String</span>]]</span><br><span class="line">    <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> primitiveAndPrimitiveArrayClassTags: <span class="type">Set</span>[<span class="type">ClassTag</span>[_]] = &#123;</span><br><span class="line">        <span class="keyword">val</span> primitiveClassTags = <span class="type">Set</span>[<span class="type">ClassTag</span>[_]](</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Boolean</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Byte</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Char</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Double</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Float</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Int</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Long</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Null</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Short</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">val</span> arrayClassTags = primitiveClassTags.map(_.wrap)</span><br><span class="line">        primitiveClassTags ++ arrayClassTags</span><br></pre></td></tr></table></figure><p>也就是说，Boolean、Byte、Char、Double、Float、Int、Long、Null、Short这些类型修饰的属性，自动使用kryo序列化。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark之分组TopN模块</title>
      <link href="/2020/04/10/spark/10/"/>
      <url>/2020/04/10/spark/10/</url>
      
        <content type="html"><![CDATA[<p>在Spark中，分组TopN好写，但是如果想写出性能好的代码却也很难。下面我们将通过写TopN的方式，找出问题，解决问题。</p><ol><li><p>直接reduceByKey完成分组求和排序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> in = <span class="string">"file:///home/hadoop/data/site.log"</span></span><br><span class="line">    <span class="comment">//连接SparkMaster</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">        ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)<span class="comment">//((domain,url),1)</span></span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = mapRDD.reduceByKey(_ + _).groupBy(x =&gt; x._1._1).mapValues( x=&gt; x.toList.sortBy(x =&gt; -x._2).map(x =&gt; (x._1._1,x._1._2,x._2)).take(<span class="number">2</span>))</span><br><span class="line">    result.foreach(println)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该方法虽然直接，但是在reduceByKey和groupBy分别进过了shuffle，而且x.toList是一个非常吃内存的操作，如果数据量大，直接OOM</p></li><li><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> in = <span class="string">"tunan-spark-core/data/site.log"</span></span><br><span class="line">    <span class="comment">//连接SparkMaster</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">        ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> domains = <span class="type">Array</span>(<span class="string">"www.google.com"</span>, <span class="string">"www.ruozedata.com"</span>, <span class="string">"www.baidu.com"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (domain &lt;- domains)&#123;</span><br><span class="line">        mapRDD.filter(x =&gt; x._1._1.equals(domain)).reduceByKey(_+_).sortBy(x =&gt; -x._2).take(<span class="number">2</span>).foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>核心思想：把需要分组分类的数据提前拿出来，在filter中过滤，每次执行一个分组，虽然减少了一次shuffle，但是我们不可能每次都把需要的数据都能提前拿到数据</p></li><li><p>使用ditinct.collect返回的数组替换人为创建的数组</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> in = <span class="string">"tunan-spark-core/data/site.log"</span></span><br><span class="line">    <span class="comment">//连接SparkMaster</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">        ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> domains = mapRDD.map(x =&gt; x._1._1).distinct().collect()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (domain &lt;- domains)&#123;</span><br><span class="line">        mapRDD.filter( x =&gt; domain.equals(x._1._1)).reduceByKey(_+_).sortBy(x =&gt; -x._2).take(<span class="number">2</span>).foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>有人说distinct性能不好，但是我们这里使用去重的是domain，这个数据量并不是很大，可以勉强接受，现在每次都使用for循环来处理数据，能不能更加优化一下呢</p></li><li><p>使用分区执行替换for循环</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> in = <span class="string">"tunan-spark-core/data/site.log"</span></span><br><span class="line">    <span class="comment">//连接SparkMaster</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">        ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> domains = mapRDD.map(x =&gt; x._1._1).distinct().collect()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapPartRDD = mapRDD.reduceByKey(<span class="keyword">new</span> <span class="type">MyPartitioner</span>(domains), _ + _).mapPartitions(partition =&gt; &#123;</span><br><span class="line">        partition.toList.sortBy(x =&gt; -x._2).take(<span class="number">2</span>).iterator</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    mapPartRDD.foreach(println)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>自定义的分区类</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span>(<span class="params">domains:<span class="type">Array</span>[<span class="type">String</span>]</span>) <span class="keyword">extends</span> <span class="title">Partitioner</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> map = mutable.<span class="type">HashMap</span>[<span class="type">String</span>,<span class="type">Int</span>]()</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until (domains.length))&#123;</span><br><span class="line">        map(domains(i)) = i</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = domains.length</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> domain = key.asInstanceOf[(<span class="type">String</span>, <span class="type">String</span>)]._1</span><br><span class="line">        map(domain)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这么做的好处是原本一起计算的RDD，现在每个分区里面去计算了，虽然toList内存占用大，但是还凑合，最终的版本就是把toList替换掉。</p></li><li><p>使用TreeSet替换toList实现最终的排序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-core/data/site.log"</span></span><br><span class="line">        <span class="comment">//连接SparkMaster</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">            ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> domains = mapRDD.map(x =&gt; x._1._1).distinct().collect()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> ord: <span class="type">Ordering</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = <span class="keyword">new</span> <span class="type">Ordering</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)]() &#123;</span><br><span class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: ((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>), y: ((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)): <span class="type">Int</span> = &#123;</span><br><span class="line">                <span class="keyword">if</span> (!x._1.equals(y._1) &amp;&amp; x._2 == y._2) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//  降序排</span></span><br><span class="line">                y._2 - x._2</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> treeSort = mapRDD.reduceByKey(<span class="keyword">new</span> <span class="type">MyPartitioner</span>(domains), _ + _).mapPartitions(partition =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> set = mutable.<span class="type">TreeSet</span>.empty(ord)</span><br><span class="line">            partition.foreach(x =&gt; &#123;</span><br><span class="line">                set.add(x)</span><br><span class="line">                <span class="keyword">if</span> (set.size &gt; <span class="number">2</span>) &#123;</span><br><span class="line">                    set.remove(set.lastKey) <span class="comment">//移除最后一个</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            set.toIterator</span><br><span class="line">        &#125;).collect()</span><br><span class="line">        treeSort.foreach(println)</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>使用TreeSet实现自定义排序器，使之每次维护的只有需要的极少量数据，这样占用内存少，效率最高。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkSQL&amp;DataFrame的read和write&amp;SparkSQL做统计分析&amp;UDF函数&amp;存储格式的转换</title>
      <link href="/2020/04/03/spark/11/"/>
      <url>/2020/04/03/spark/11/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>SparkSQL</li><li>DataFrame的read和write</li><li>SparkSQL做统计分析</li><li>UDF函数</li><li>存储格式的转换</li></ol><h2 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h2><h3 id="认识SparkSQL"><a href="#认识SparkSQL" class="headerlink" title="认识SparkSQL"></a>认识SparkSQL</h3><ol><li><p>SparkSQL的进化之路</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.0以前：</span><br><span class="line">   Shark</span><br><span class="line">1.1.x开始：</span><br><span class="line">   SparkSQL(只是测试性的) SQL</span><br><span class="line">1.3.x:</span><br><span class="line">   SparkSQL(正式版本)+Dataframe</span><br><span class="line">1.5.x:</span><br><span class="line">SparkSQL 钨丝计划</span><br><span class="line">1.6.x：</span><br><span class="line">   SparkSQL+DataFrame+DataSet(测试版本)</span><br><span class="line">2.x.x:</span><br><span class="line">   SparkSQL+DataFrame+DataSet(正式版本)</span><br><span class="line">   SparkSQL:还有其他的优化</span><br><span class="line">   StructuredStreaming(DataSet)</span><br></pre></td></tr></table></figure></li><li><p>什么是SparkSQL?</p><p>spark SQL是spark的一个模块，主要用于进行结构化数据的处理。它提供的最核心的编程抽象就是DataFrame。</p></li><li><p>SparkSQL的作用</p><p>提供一个编程抽象（DataFrame） 并且作为分布式 SQL 查询引擎</p><p>DataFrame：它可以根据很多源进行构建，包括：<strong>结构化的数据文件，hive中的表，外部的关系型数据库，以及RDD</strong></p></li><li><p>运行原理</p><p>将 Spark SQL 转化为 RDD， 然后提交到集群执行</p></li><li><p>特点</p><ol><li>容易整合</li><li>统一的数据访问方式</li><li>兼容 Hive</li><li>标准的数据连接</li></ol></li><li><p>spark-sql</p><p>spark-sql是一个Spark专属的SQL命令行交互工具，在使用spark-sql之前要把hive-site.xml 拷贝到Spark/Conf下，spark-sql和spark-shell用法一样，但是在引入外部依赖的时候，spark-sql需要用–jars和–driver-class-path同时引入依赖才不会报错</p></li><li><p>持久化</p><p>在spark-sql中的持久化Table命令是: cache table xxx，清除持久化 uncache table xxx</p><p>spark-SQL中的cache和uncache都是eager的，立即执行的</p><p><strong>考点：RDD和SparkSQL的cache有什么区别？</strong></p><ul><li>RDD中的cache是lazy的 spark-SQL中的cache是eager的</li></ul></li><li><p>遗留问题</p><p>–files/–jars    传进去的东西清不掉</p></li></ol><h3 id="SparkSession"><a href="#SparkSession" class="headerlink" title="SparkSession"></a>SparkSession</h3><p>SparkSession是Spark 2.0引如的新概念。SparkSession为用户提供了统一的切入点，来让用户学习spark的各项功能。 </p><p>在spark的早期版本中，SparkContext是spark的主要切入点，由于RDD是主要的API，我们通过sparkcontext来创建和操作RDD。对于每个其他的API，我们需要使用不同的context。例如，对于Streming，我们需要使用StreamingContext；对于sql，使用sqlContext；对于Hive，使用hiveContext。但是随着DataSet和DataFrame的API逐渐成为标准的API，就需要为他们建立接入点。所以在spark2.0中，引入SparkSession作为DataSet和DataFrame API的切入点，SparkSession封装了SparkConf、SparkContext和SQLContext。为了向后兼容，SQLContext和HiveContext也被保存下来。 </p><p>SparkSession实质上是SQLContext和HiveContext的组合（未来可能还会加上StreamingContext），所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。</p><p>特点：</p><ol><li><p><strong>为用户提供一个统一的切入点使用Spark 各项功能</strong></p></li><li><p><strong>允许用户通过它调用 DataFrame 和 Dataset 相关 API 来编写程序</strong></p></li><li><p><strong>减少了用户需要了解的一些概念，可以很容易的与 Spark 进行交互</strong></p></li><li><p><strong>与 Spark 交互之时不需要显示的创建 SparkConf, SparkContext 以及 SQlContext，这些对象已经封闭在 SparkSession 中</strong></p></li></ol><h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><p>在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame与RDD的主要区别在于，前者带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得Spark SQL得以洞察更多的结构信息，从而对藏于DataFrame背后的数据源以及作用于DataFrame之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在stage层面进行简单、通用的流水线优化。</p><p><img src="https://yerias.github.io/spark_img/RDD%E5%92%8CDataFrame%E7%9A%84%E5%AD%98%E5%82%A8%E5%86%85%E5%AE%B9%E6%AF%94%E8%BE%83.png" alt="RDD和DataFrame的存储内容比较"></p><h2 id="DataFrame的read和write"><a href="#DataFrame的read和write" class="headerlink" title="DataFrame的read和write"></a>DataFrame的read和write</h2><h3 id="json"><a href="#json" class="headerlink" title="json"></a>json</h3><ol><li><p>数据的读取[DataFrameReader]</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">rdd2df</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-sql/data/people.json"</span></span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"><span class="comment">//  读取json数据</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.format(<span class="string">"json"</span>).load(in)</span><br><span class="line">        <span class="comment">//  使用$"" 导入隐式转换</span></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        <span class="comment">//  可以使用UDF</span></span><br><span class="line">        df.select($<span class="string">"name"</span>,$<span class="string">"age"</span>).show(<span class="number">2</span>,<span class="literal">false</span>)</span><br><span class="line">        <span class="comment">//  不可以使用UDF 适合大部分场景</span></span><br><span class="line">        df.select(<span class="string">"name"</span>,<span class="string">"age"</span>).show()</span><br><span class="line">        <span class="comment">//  不推介，写着复杂</span></span><br><span class="line">        df.select(df(<span class="string">"name"</span>),df(<span class="string">"age"</span>)).show(<span class="number">2</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>select方法用于选择要输出的列，推介使用 $”col” 和 “col” 的方法</p><ol><li>使用select可以选取打印的列，空值为null</li><li>show()默认打印20条数据，可以指定条数</li><li>truncate默认为true，截取长度，可以设置为false</li></ol><p>select方法有三种不同的写法，fliter也有</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.select(df(<span class="string">"name"</span>),df(<span class="string">"age"</span>)).filter(<span class="symbol">'name</span> === <span class="string">"Andy"</span>).show()<span class="comment">//推介使用</span></span><br><span class="line">df.select(df(<span class="string">"name"</span>),df(<span class="string">"age"</span>)).filter(df(<span class="string">"name"</span>) === <span class="string">"Andy"</span>).show()</span><br><span class="line">df.select(df(<span class="string">"name"</span>),df(<span class="string">"age"</span>)).filter(<span class="string">"name = 'Andy'"</span>).show()</span><br></pre></td></tr></table></figure><p>printSchema()方法可以查看数据的Schema信息</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.printSchema()</span><br><span class="line">------------------------------------------------</span><br><span class="line">root</span><br><span class="line"> |-- age: long (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- name: string (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure></li><li><p>数据的存储[DataFrameWriter]</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> selectDf: <span class="type">DataFrame</span> = df.select($<span class="string">"name"</span>, $<span class="string">"age"</span>)</span><br><span class="line"><span class="comment">//  写出json数据</span></span><br><span class="line">selectDf.write.format(<span class="string">"json"</span>).mode(<span class="string">"overwrite"</span>).save(out)</span><br></pre></td></tr></table></figure><p>这里需要知道的一个概念是Save Modes</p><p>Save操作可以选择使用SaveMode，它指定目标如果存在，如何处理现有数据。重要的是要认识到，这些保存模式不利用任何锁定，也不是原子性的。此外，在执行覆盖时，在写入新数据之前将删除数据。</p><table><thead><tr><th align="left">Scala/Java</th><th align="left">Any Language</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><code>SaveMode.ErrorIfExists</code> (default)</td><td align="left"><code>&quot;error&quot; or &quot;errorifexists&quot;</code> (default)</td><td align="left">在将DataFrame保存到数据源时，如果数据已经存在，则会抛出error。</td></tr><tr><td align="left"><code>SaveMode.Append</code></td><td align="left"><code>&quot;append&quot;</code></td><td align="left">在将DataFrame保存到数据源时，如果数据/表已经存在，则DataFrame的内容将被append到现有数据中。</td></tr><tr><td align="left"><code>SaveMode.Overwrite</code></td><td align="left"><code>&quot;overwrite&quot;</code></td><td align="left">overwrite模式意味着在将DataFrame保存到数据源时，如果数据/表已经存在，则现有数据将被DataFrame的内容覆盖。</td></tr><tr><td align="left"><code>SaveMode.Ignore</code></td><td align="left"><code>&quot;ignore&quot;</code></td><td align="left">ignore模式意味着在将DataFrame保存到数据源时，如果数据已经存在，则save操作不保存DataFrame的内容，也不更改现有数据。这类似于SQL中的<code>CREATE TABLE IF NOT EXISTS</code>。</td></tr></tbody></table></li></ol><h3 id="text"><a href="#text" class="headerlink" title="text"></a>text</h3><ol><li><p>数据的读取</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">text2df</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-sql/data/people.txt"</span></span><br><span class="line">        <span class="keyword">val</span> out = <span class="string">"tunan-spark-sql/out"</span></span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">        <span class="comment">//DataFrame不能直接split，且调用map返回的是一个Dataset</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.format(<span class="string">"text"</span>).load(in)</span><br><span class="line">        <span class="keyword">val</span> mapDF: <span class="type">Dataset</span>[(<span class="type">String</span>, <span class="type">String</span>)] = df.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.toString().split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">        mapDF.show()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//DataFrame转换为RDD后，再toDF，返回的是一个DataFrame</span></span><br><span class="line">        <span class="keyword">val</span> mapRDD2DF: <span class="type">DataFrame</span> = df.rdd.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.toString().split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;).toDF()</span><br><span class="line">        mapRDD2DF.show()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用textFile方法读取文本文件直接返回的是一个Dataset</span></span><br><span class="line">        <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(in)</span><br><span class="line">        <span class="keyword">val</span> mapDs: <span class="type">Dataset</span>[(<span class="type">String</span>, <span class="type">String</span>)] = ds.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">        mapDs.show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>文本数据读进来的一行在一个字段里面，所以要使用map算子，在map中split</p><ol><li>直接read.format()读进来的是DataFrame，map中不能直接split</li><li>DataFrame通过.rdd的方式转换成RDD，map中也不能直接split</li><li>通过read.textFile()的方式读进来的是Dataset，map中可以split</li></ol></li><li><p>数据的存储</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.format(<span class="string">"text"</span>).load(in)</span><br><span class="line"><span class="keyword">val</span> mapDF = df.map(row =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> words = row.toString().split(<span class="string">","</span>)</span><br><span class="line">    <span class="comment">// 拼接成一列</span></span><br><span class="line">    words(<span class="number">0</span>) +<span class="string">","</span>+words(<span class="number">1</span>)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">mapDF.write.format(<span class="string">"text"</span>).mode(<span class="string">"overwrite"</span>).save(out)</span><br></pre></td></tr></table></figure><p>文本数据写出去的时候</p><ol><li>不支持int类型，如果存在int类型，会报错，解决办法是toString，转换成字符串</li><li>只能作为一列输出，如果是多列，会报错，解决办法是拼接起来，组成一列</li></ol><p><strong>文本数据压缩输出，只要是Spark支持的压缩的格式，都可以指定</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapDF.write</span><br><span class="line">    .format(<span class="string">"text"</span>)</span><br><span class="line">    <span class="comment">// 添加压缩操作</span></span><br><span class="line">    .option(<span class="string">"compression"</span>,<span class="string">"gzip"</span>)</span><br><span class="line">    .mode(<span class="string">"overwrite"</span>)</span><br><span class="line">    .save(out)</span><br></pre></td></tr></table></figure></li></ol><h3 id="csv"><a href="#csv" class="headerlink" title="csv"></a>csv</h3><ol><li><p>数据的读取</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">csv2df</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-sql/data/people.csv"</span></span><br><span class="line">        <span class="keyword">val</span> out = <span class="string">"tunan-spark-sql/out"</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read</span><br><span class="line">            .format(<span class="string">"csv"</span>)</span><br><span class="line">            .option(<span class="string">"header"</span>, <span class="string">"true"</span>)</span><br><span class="line">            .option(<span class="string">"sep"</span>, <span class="string">";"</span>)</span><br><span class="line">            .option(<span class="string">"interSchema"</span>,<span class="string">"true"</span>)</span><br><span class="line">            .load(in)</span><br><span class="line">        df.show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>csv读取数据注意使用几个参数</p><ol><li>指定表头：<code>option(&quot;header&quot;, &quot;true&quot;)</code></li><li>指定分隔符：<code>option(&quot;sep&quot;, &quot;;&quot;)</code></li><li>类型自动推测：<code>option(&quot;interSchema&quot;,&quot;true&quot;)</code></li></ol></li></ol><h3 id="jdbc"><a href="#jdbc" class="headerlink" title="jdbc"></a>jdbc</h3><p>在操作jdbc之前要导入两个依赖，一个是mysql-jdbc，用来连接mysql，一个是config，用来解决硬编码的问题</p><p>依赖：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.typesafe<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>config<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.38<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>application.conf文件</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">db.<span class="keyword">default</span>.driver=<span class="string">"com.mysql.jdbc.Driver"</span></span><br><span class="line">db.<span class="keyword">default</span>.url=<span class="string">"jdbc:mysql://hadoop/listener?characterEncoding=utf-8&amp;useSSL=false"</span></span><br><span class="line">db.<span class="keyword">default</span>.user=<span class="string">"root"</span></span><br><span class="line">db.<span class="keyword">default</span>.password=<span class="string">"root"</span></span><br><span class="line">db.<span class="keyword">default</span>.source=<span class="string">"dws_ad_phone_type_dist"</span></span><br><span class="line">db.<span class="keyword">default</span>.target=<span class="string">"dws_ad_phone_type_dist_1"</span></span><br><span class="line">db.<span class="keyword">default</span>.db=<span class="string">"access_dw"</span></span><br><span class="line"></span><br><span class="line"># Connection Pool settings</span><br><span class="line">db.<span class="keyword">default</span>.poolInitialSize=<span class="number">10</span></span><br><span class="line">db.<span class="keyword">default</span>.poolMaxSize=<span class="number">20</span></span><br><span class="line">db.<span class="keyword">default</span>.connectionTimeoutMillis=<span class="number">1000</span></span><br></pre></td></tr></table></figure><ol><li><p>数据的读取</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">mysql2df</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取配置文件中的值，db.default开头</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="type">ConfigFactory</span>.load()</span><br><span class="line">        <span class="keyword">val</span> driver = conf.getString(<span class="string">"db.default.driver"</span>)</span><br><span class="line">        <span class="keyword">val</span> url = conf.getString(<span class="string">"db.default.url"</span>)</span><br><span class="line">        <span class="keyword">val</span> user = conf.getString(<span class="string">"db.default.user"</span>)</span><br><span class="line">        <span class="keyword">val</span> password = conf.getString(<span class="string">"db.default.password"</span>)</span><br><span class="line">        <span class="keyword">val</span> source = conf.getString(<span class="string">"db.default.source"</span>)</span><br><span class="line">        <span class="keyword">val</span> target = conf.getString(<span class="string">"db.default.target"</span>)</span><br><span class="line">        <span class="keyword">val</span> db = conf.getString(<span class="string">"db.default.db"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取数据库的内容</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read</span><br><span class="line">            .format(<span class="string">"jdbc"</span>)</span><br><span class="line">            .option(<span class="string">"url"</span>, url)</span><br><span class="line">            .option(<span class="string">"dbtable"</span>, <span class="string">s"<span class="subst">$db</span>.<span class="subst">$source</span>"</span>)<span class="comment">//库名.源表</span></span><br><span class="line">            .option(<span class="string">"user"</span>, user)</span><br><span class="line">            .option(<span class="string">"password"</span>, password)</span><br><span class="line">            .option(<span class="string">"driver"</span>, driver)</span><br><span class="line">            .load()</span><br><span class="line">        <span class="comment">//使用DataFrame创建临时表提供spark.sql查询</span></span><br><span class="line">        df.createOrReplaceTempView(<span class="string">"phone_type_dist"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//spark.sql写SQL返回一个DataFrame</span></span><br><span class="line">        <span class="keyword">val</span> sqlDF: <span class="type">DataFrame</span> = spark.sql(<span class="string">"select * from phone_type_dist where phoneSystemType = 'IOS'"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>使用df.createOrReplaceTempView()方法创建一个DataFrame数据生成的临时表，提供spark.sql()使用SQL操作数据，返回的也是一个DataFrame</li></ul></li><li><p>数据的存储</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//接着上面返回的sqlDF: DataFrame</span></span><br><span class="line">sqlDF.write</span><br><span class="line">    .format(<span class="string">"jdbc"</span>)</span><br><span class="line">    .option(<span class="string">"url"</span>, url)</span><br><span class="line">    .option(<span class="string">"dbtable"</span>, <span class="string">s"<span class="subst">$db</span>.<span class="subst">$target</span>"</span>)<span class="comment">//库名.目标表</span></span><br><span class="line">    .option(<span class="string">"user"</span>, user)</span><br><span class="line">    .option(<span class="string">"password"</span>, password)</span><br><span class="line">    .option(<span class="string">"driver"</span>,driver)</span><br><span class="line">    .save()</span><br></pre></td></tr></table></figure></li></ol><h2 id="SparkSQL做统计分析"><a href="#SparkSQL做统计分析" class="headerlink" title="SparkSQL做统计分析"></a>SparkSQL做统计分析</h2><ol><li><p>数据</p></li><li><p>需求：求每个国家的每个域名的访问流量排名前2</p></li><li><p>SQL实现</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GroupTopN</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">//读取数据</span></span><br><span class="line">        <span class="keyword">val</span> ds = spark.read.textFile(in)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        <span class="comment">//为生成需要的表格做准备</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = ds.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">3</span>), words(<span class="number">12</span>), words(<span class="number">15</span>).toLong)</span><br><span class="line">        &#125;).toDF(<span class="string">"country"</span>, <span class="string">"domain"</span>, <span class="string">"traffic"</span>)</span><br><span class="line"></span><br><span class="line">        df.createOrReplaceTempView(<span class="string">"access"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 每个国家的域名流量前2</span></span><br><span class="line">        <span class="keyword">val</span> topNSQL=<span class="string">""</span><span class="string">"select</span></span><br><span class="line"><span class="string">                      |*</span></span><br><span class="line"><span class="string">                      |from (</span></span><br><span class="line"><span class="string">                      |select</span></span><br><span class="line"><span class="string">                      |t.*,row_number() over(partition by country order by sum_traffic desc) r</span></span><br><span class="line"><span class="string">                      |from</span></span><br><span class="line"><span class="string">                      |(</span></span><br><span class="line"><span class="string">                      |select country,domain,sum(traffic) as sum_traffic from access group by country,domain</span></span><br><span class="line"><span class="string">                      |) t</span></span><br><span class="line"><span class="string">                      |) rt</span></span><br><span class="line"><span class="string">                      |where rt.r &lt;=2 "</span><span class="string">""</span>.stripMargin</span><br><span class="line"></span><br><span class="line">        spark.sql(topNSQL).show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>如果只要求traffic的降序，可以使用API直接写出来</p><p>分组，求和，别名，降序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//traffic降序排序</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">df.groupBy(<span class="string">"country"</span>,<span class="string">"domain"</span>).agg(sum(<span class="string">"traffic"</span>).as(<span class="string">"sum_traffic"</span>)).sort($<span class="string">"sum_traffic"</span>.desc).show()</span><br></pre></td></tr></table></figure><p><strong>注意看源码中案例仿写</strong></p></li><li><p>结果展示</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+----------------+-----------------+-----------+---+</span><br><span class="line">|         country|           domain|sum_traffic|  r|</span><br><span class="line">+----------------+-----------------+-----------+---+</span><br><span class="line">|            中国| www.bilibili.com|   <span class="number">24265886</span>|  <span class="number">1</span>|</span><br><span class="line">|            中国|www.ruozedata.com|    <span class="number">4187637</span>|  <span class="number">2</span>|</span><br><span class="line">|          利比亚| www.bilibili.com|      <span class="number">22816</span>|  <span class="number">1</span>|</span><br><span class="line">|          利比亚|  ruoze.ke.qq.com|      <span class="number">15970</span>|  <span class="number">2</span>|</span><br><span class="line">|            加纳| www.bilibili.com|     <span class="number">138659</span>|  <span class="number">1</span>|</span><br><span class="line">|            加纳|www.ruozedata.com|      <span class="number">17988</span>|  <span class="number">2</span>|</span><br><span class="line">|        利比里亚| www.bilibili.com|      <span class="number">20593</span>|  <span class="number">1</span>|</span><br><span class="line">|        利比里亚|  ruoze.ke.qq.com|       <span class="number">7466</span>|  <span class="number">2</span>|</span><br><span class="line">+----------------+-----------------+-----------+---+</span><br></pre></td></tr></table></figure></li></ol><h2 id="UDF函数"><a href="#UDF函数" class="headerlink" title="UDF函数"></a>UDF函数</h2><ol><li><p>数据</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">大狗小破车,渣团,热刺,我纯</span><br><span class="line">桶子利物浦</span><br><span class="line">二娃南大王,西班牙人</span><br></pre></td></tr></table></figure></li><li><p>需求：求出每个人的爱好的个数</p></li><li><p>SQLs实现</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LoveLength</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取文本内容</span></span><br><span class="line">        <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(in)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//文本转换成DF</span></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = ds.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.split(<span class="string">"\t"</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;).toDF(<span class="string">"name"</span>, <span class="string">"love"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建UDF</span></span><br><span class="line">        spark.udf.register(<span class="string">"length"</span>, (love: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">            love.split(<span class="string">","</span>).length</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//DF创建临时表</span></span><br><span class="line">        df.createOrReplaceTempView(<span class="string">"udf_love"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//在sql中使用UDF函数</span></span><br><span class="line">        spark.sql(<span class="string">"select name,love,length(love) as love_length from udf_love"</span>).show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>上面是使用SQL的解决方案，还可以使用API的方法</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//自定义的udf需要返回值</span></span><br><span class="line"><span class="keyword">val</span> loveLengthUDF: <span class="type">UserDefinedFunction</span> = spark.udf.register(<span class="string">"length"</span>, (love: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">    love.split(<span class="string">","</span>).length</span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment">//df.select中传入UDF函数</span></span><br><span class="line">df.select($<span class="string">"name"</span>,$<span class="string">"love"</span>,loveLengthUDF($<span class="string">"love"</span>)).show()</span><br></pre></td></tr></table></figure></li><li><p>结果展示</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+----+---------------------+----------------+</span><br><span class="line">|大狗|小破车,渣团,热刺,我纯   |               <span class="number">4</span>|</span><br><span class="line">|桶子|               利物浦 |               <span class="number">1</span>|</span><br><span class="line">|二娃|      南大王,西班牙人  |               <span class="number">2</span>|</span><br><span class="line">+----+---------------------+----------------+</span><br></pre></td></tr></table></figure></li></ol><h2 id="存储格式的转换"><a href="#存储格式的转换" class="headerlink" title="存储格式的转换"></a>存储格式的转换</h2><p>Spark读text文件进行清洗，清洗完以后直接以我们想要的列式存储格式输出，如果按以前的方式要经过很多复杂的步骤</p><p>用Spark的时候只需要在<code>df.write.format(&quot;orc&quot;).mode().save()</code>中指定格式即可，如orc，现在就很方便了，想转成什么格式，只要format支持就ok</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">text2orc</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-sql/data/people.txt"</span></span><br><span class="line">        <span class="keyword">val</span> out = <span class="string">"tunan-spark-sql/out"</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> df = spark.read.textFile(in)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//对文本文件做处理</span></span><br><span class="line">        df.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>),words(<span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">            .toDF(<span class="string">"name"</span>,<span class="string">"age"</span>)<span class="comment">//这一步解决了数据没有表头的问题</span></span><br><span class="line">            .write</span><br><span class="line">            .mode(<span class="string">"overwrite"</span>)<span class="comment">//save mode</span></span><br><span class="line">            .format(<span class="string">"orc"</span>)<span class="comment">//save format</span></span><br><span class="line">            .save(out)<span class="comment">//save path</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>20200416更新：</em> df.write.format(“…”).option(“compression”,”…”)   ==&gt; 存储格式+压缩格式</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>经典案例&amp;多目录输出&amp;计数器&amp;持久化&amp;广播变量</title>
      <link href="/2020/04/02/spark/9/"/>
      <url>/2020/04/02/spark/9/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>经典案例</li><li>多目录输出</li><li>计数器</li><li>持久化</li><li>广播变量</li></ol><h2 id="经典案例"><a href="#经典案例" class="headerlink" title="经典案例"></a>经典案例</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 用户     节目            展示 点击</span></span><br><span class="line"><span class="comment"> * 001,一起看|电视剧|军旅|亮剑,1,1</span></span><br><span class="line"><span class="comment"> * 001,一起看|电视剧|军旅|亮剑,1,0</span></span><br><span class="line"><span class="comment"> * 002,一起看|电视剧|军旅|士兵突击,1,1</span></span><br><span class="line"><span class="comment"> * ==&gt;</span></span><br><span class="line"><span class="comment"> * 001,一起看,2,1</span></span><br><span class="line"><span class="comment"> * 001,电视剧,2,1</span></span><br><span class="line"><span class="comment"> * 001,军旅,2,1</span></span><br><span class="line"><span class="comment"> * 001,亮剑,2,1</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">exercise02</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> linesRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"tunan-spark-core/data/test2.txt"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用map返回的是一个数组，我不要数组，就使用flatMap</span></span><br><span class="line">        <span class="keyword">import</span> com.tunan.spark.utils.<span class="type">ImplicitAspect</span>.rdd2RichRDD</span><br><span class="line">        <span class="keyword">val</span> map2RDD: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), (<span class="type">Int</span>, <span class="type">Int</span>))] = linesRDD.flatMap(line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">","</span>)</span><br><span class="line">            <span class="keyword">val</span> programs: <span class="type">Array</span>[<span class="type">String</span>] = words(<span class="number">1</span>).split(<span class="string">"\\|"</span>)</span><br><span class="line">            <span class="keyword">val</span> mapRDD: <span class="type">Array</span>[((<span class="type">String</span>, <span class="type">String</span>), (<span class="type">Int</span>, <span class="type">Int</span>))] = programs.map(program =&gt; ((words(<span class="number">0</span>), program), (words(<span class="number">2</span>).toInt, words(<span class="number">3</span>).toInt)))</span><br><span class="line">            mapRDD</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="keyword">val</span> groupRDD: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Iterable</span>[(<span class="type">Int</span>, <span class="type">Int</span>)])] = map2RDD.groupByKey()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//这里是mapValues很好的一个使用案例</span></span><br><span class="line">        <span class="keyword">val</span> mapVRDD: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), (<span class="type">Int</span>, <span class="type">Int</span>))] = groupRDD.mapValues(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> imps: <span class="type">Int</span> = x.map(_._1).sum</span><br><span class="line">            <span class="keyword">val</span> check: <span class="type">Int</span> = x.map(_._2).sum</span><br><span class="line">            (imps, check)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//格式化输出</span></span><br><span class="line">        mapVRDD.map(x =&gt; &#123;</span><br><span class="line">            (x._1._1,x._1._2,x._2._1,x._2._1)</span><br><span class="line">        &#125;).print()</span><br><span class="line"></span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="多目录输出"><a href="#多目录输出" class="headerlink" title="多目录输出"></a>多目录输出</h2><ol><li><p>实现多目录输出自定义类</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.<span class="type">NullWritable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.lib.<span class="type">MultipleTextOutputFormat</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyMultipleTextOutputFormat</span> <span class="keyword">extends</span> <span class="title">MultipleTextOutputFormat</span>[<span class="type">Any</span>,<span class="type">Any</span>] </span>&#123;</span><br><span class="line">    <span class="comment">//生成最终生成的key的类型，这里不要，给Null</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">generateActualKey</span></span>(key: <span class="type">Any</span>, value: <span class="type">Any</span>): <span class="type">Any</span> = <span class="type">NullWritable</span>.get()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//生成最终生成的value的类型，这里是String</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">generateActualValue</span></span>(key: <span class="type">Any</span>, value: <span class="type">Any</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">        value.asInstanceOf[<span class="type">String</span>]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//生成文件名</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">generateFileNameForKeyValue</span></span>(key: <span class="type">Any</span>, value: <span class="type">Any</span>, name: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="string">s"<span class="subst">$key</span>/<span class="subst">$name</span>"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>主类，使用<code>saveAsHadoopFile(path, keyClass, valueClass, fm.runtimeClass.asInstanceOf[Class[F]])</code>方法保存数据，指定参数</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MultipleDirectory</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> out = <span class="string">"tunan-spark-core/out"</span></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">        <span class="type">CheckHDFSOutPath</span>.ifExistsDeletePath(sc.hadoopConfiguration,out)</span><br><span class="line">        <span class="comment">//读取数组，转换成键值对的格式</span></span><br><span class="line">        <span class="keyword">val</span> lines = sc.textFile(<span class="string">"tunan-spark-core/ip/access-result/*"</span>)</span><br><span class="line">        <span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = lines.map(line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = line.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">12</span>), line)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">//多目录保存文件</span></span><br><span class="line">        mapRDD.saveAsHadoopFile(out,classOf[<span class="type">String</span>],classOf[<span class="type">String</span>],classOf[<span class="type">MyMultipleTextOutputFormat</span>])</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>结果</p><p><img src="https://yerias.github.io/spark_img/%E5%A4%9A%E7%9B%AE%E5%BD%95%E8%BE%93%E5%87%BA.jpg" alt="多目录输出"></p></li></ol><p>在spark程序中，当一个传递给Spark操作(例如map和reduce)的函数在远程节点上面运行时，Spark操作实际上操作的是这个函数所用变量的一个<strong>独立副本</strong>。这些变量会被复制到每台机器上，并且这些变量在远程机器上的所有更新都不会传递回驱动程序。通常跨任务的读写变量是低效的，但是，Spark还是为两种常见的使用模式提供了两种有限的共享变量：广播变量（broadcast variable）和累加器（accumulator）</p><h2 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h2><h3 id="为什么要定义计数器？"><a href="#为什么要定义计数器？" class="headerlink" title="为什么要定义计数器？"></a>为什么要定义计数器？</h3><p>在spark应用程序中，我们经常会有这样的需求，如<em>异常监控</em>，<em>调试</em>，<em>记录符合某特性的数据的数目</em>，这种需求都需要用到计数器，如果一个变量不被声明为一个累加器，那么它将在被改变时不会再driver端进行全局汇总，即在分布式运行时每个task运行的只是原始变量的一个副本，并不能改变原始变量的值，但是当这个变量被声明为累加器后，该变量就会有分布式计数的功能。</p><h3 id="图解计数器"><a href="#图解计数器" class="headerlink" title="图解计数器"></a>图解计数器</h3><p>错误的图解</p><p><img src="https://yerias.github.io/spark_img/%E7%B4%AF%E5%8A%A0%E5%99%A8%E9%94%99%E8%AF%AF%E7%9A%84%E5%9B%BE%E8%A7%A3.png" alt="累加器错误的图解"></p><p>正确的图解</p><p><img src="https://yerias.github.io/spark_img/%E7%B4%AF%E5%8A%A0%E5%99%A8%E6%AD%A3%E7%A1%AE%E7%9A%84%E5%9B%BE%E8%A7%A3.png" alt="累加器正确的图解"></p><p>计数器种类很多，但是经常用的就是两种，<code>longAccumulator</code>和<code>collectionAccumulator</code></p><p><strong>需要注意的是计数器是lazy的，只有触发action才会进行计数，在不持久化的情况下重复触发action，计数器会重复累加</strong></p><h3 id="LongAccumulator"><a href="#LongAccumulator" class="headerlink" title="LongAccumulator"></a>LongAccumulator</h3><p>Accumulators 是只能通过associative和commutative操作“added”的变量，因此可以有效地并行支持。它们可用于实现计数器(如MapReduce)和Spark本身支持数字类型的累加器，程序员还<strong>可以添加对新类型的支持</strong>。</p><p><code>longAccumulator</code>通过累加的方式计数</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyLongAccumulator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">        <span class="keyword">var</span> acc = sc.longAccumulator(<span class="string">"计数"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> forRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">            <span class="comment">// 计数器做累加</span></span><br><span class="line">            acc.add(<span class="number">1</span>L)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">// action操作 </span></span><br><span class="line">        forRDD.count()</span><br><span class="line">       </span><br><span class="line">        println(acc.value)<span class="comment">// 9</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用<code>longAccumulator</code>做计数的时候要小心重复执行action导致的acc.value的变化</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyLongAccumulatorV2</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">        <span class="comment">//生成计数器</span></span><br><span class="line">        <span class="keyword">val</span> acc = sc.longAccumulator(<span class="string">"计数"</span>)</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">8</span>)</span><br><span class="line">        <span class="keyword">val</span> forRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">            <span class="comment">//计数器做累加</span></span><br><span class="line">            acc.add(<span class="number">1</span>L)</span><br><span class="line">        &#125;)</span><br><span class="line">        forRDD.count()</span><br><span class="line">        println(acc.value)<span class="comment">//8</span></span><br><span class="line">        forRDD.count()</span><br><span class="line">        println(acc.value)<span class="comment">//16</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于重复执行了count()，累加器的数量成倍增长，解决这种错误累加也很简单，就是在count之前调用forRDD的cache方法(或persist)，这样在count后数据集就会被缓存下来，reduce操作就会读取缓存的数据集，而无需从头开始计算。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyLongAccumulator</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">        <span class="comment">//生成计数器</span></span><br><span class="line">        <span class="keyword">val</span> acc = sc.longAccumulator(<span class="string">"计数"</span>)</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">8</span>)</span><br><span class="line">        <span class="keyword">val</span> forRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">            <span class="comment">//计数器做累加</span></span><br><span class="line">            acc.add(<span class="number">1</span>L)</span><br><span class="line">        &#125;)</span><br><span class="line">        forRDD.cache().count()</span><br><span class="line">        println(acc.value)<span class="comment">//8</span></span><br><span class="line">        forRDD.count()</span><br><span class="line">        println(acc.value)<span class="comment">//8</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="CollectionAccumulator"><a href="#CollectionAccumulator" class="headerlink" title="CollectionAccumulator"></a>CollectionAccumulator</h3><p><code>collectionAccumulator</code>，集合计数器，计数器中保存的是集合元素，通过泛型指定。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：id后三位相同的加入计数器</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyCollectionAccumulator</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sc  = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//生成集合计数器</span></span><br><span class="line">        <span class="keyword">val</span> acc = sc.collectionAccumulator[<span class="type">People</span>](<span class="string">"集合计数器"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//生成RDD</span></span><br><span class="line">        <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">People</span>] = sc.parallelize(<span class="type">Array</span>(<span class="type">People</span>(<span class="string">"tunan"</span>, <span class="number">100000</span>), <span class="type">People</span>(<span class="string">"xiaoqi"</span>, <span class="number">100001</span>), <span class="type">People</span>(<span class="string">"张三"</span>, <span class="number">100222</span>), <span class="type">People</span>(<span class="string">"李四"</span>, <span class="number">100003</span>)))</span><br><span class="line"></span><br><span class="line"> <span class="comment">//map操作</span></span><br><span class="line">        rdd.map(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> id2 = x.id.toString.reverse</span><br><span class="line">            <span class="comment">//满足条件就加入计数器，</span></span><br><span class="line">            <span class="keyword">if</span> (id2(<span class="number">0</span>) == id2(<span class="number">1</span>) &amp;&amp; id2(<span class="number">0</span>) ==id2(<span class="number">2</span>))&#123;</span><br><span class="line">                acc.add(x)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).count()<span class="comment">//触发action</span></span><br><span class="line"></span><br><span class="line">        println(acc.value)<span class="comment">//[People(张三,100222), People(tunan,100000)]</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">People</span>(<span class="params">name:<span class="type">String</span>,id:<span class="type">Long</span></span>)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>注意事项：</strong></p><ol><li><p>计数器在Driver端定义赋初始值，计数器只能在Driver端读取最后的值，在Excutor端更新。</p></li><li><p>计数器不是一个调优的操作，因为如果不这样做，结果是错的</p></li></ol><h2 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h2><p>Spark中最重要的功能之一是跨操作在内存中持久化数据集。持久化一个RDD时，每个节点在内存中存储它计算的任何分区，并在该数据集(或从中派生的数据集)的其他操作中重构它们。这使得将来的操作要快得多(通常超过10倍)。缓存是迭代算法和快速交互使用的关键工具。</p><p>可以使用其上的persist()或cache()方法将RDD标记为持久的。第一次在操作中计算它时，它将保存在节点的内存中。Spark的缓存是容错的——如果一个RDD的任何分区丢失了，它将使用最初创建它的转换自动重新计算。</p><p>持久化的存储级别很多，常用的是MEMORY_ONLY、MEMORY_ONLY_SER、MEMORY_AND_DISK</p><table><thead><tr><th align="left">Storage Level</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left">MEMORY_ONLY</td><td align="left">将RDD作为不序列化的Java对象存储在JVM中。如果RDD不适合内存，那么一些分区将不会被缓存，而是在需要它们时动态地重新计算。这是默认级别。</td></tr><tr><td align="left">MEMORY_AND_DISK</td><td align="left">将RDD作为不序列化的Java对象存储在JVM中。如果RDD不适合内存，那么将不适合的分区存储在磁盘上，并在需要时从那里读取它们。</td></tr><tr><td align="left">MEMORY_ONLY_SER (Java and Scala)</td><td align="left">将RDD存储为序列化的Java对象(每个分区一个字节数组)。这通常比反序列化对象更节省空间，特别是在使用快速序列化器时，但读取时需要更多cpu。</td></tr></tbody></table><h3 id="如何选择它们？"><a href="#如何选择它们？" class="headerlink" title="如何选择它们？"></a><strong>如何选择它们？</strong></h3><p>Storage Level的选择是内存和CPU的权衡</p><ol><li>内存多：MEMORY_ONLY (不进行序列化)</li><li>CPU跟的上：MEMORY_ONLY_SER (进行了序列化，推介)</li><li>不建议写Disk</li></ol><p>使用cache()和persist()进行持久化操作，它们都是<strong>lazy</strong>的，需要action才能触发，默认使用MEMORY_ONLY</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> forRDD.cache</span></span><br><span class="line">res18: forRDD.type = MapPartitionsRDD[9] at map at &lt;console&gt;:27</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> forRDD.count</span></span><br><span class="line">res19: Long = 8</span><br></pre></td></tr></table></figure><p>结果可以在Web UI的<strong>Storage</strong>中查看</p><p>如果需要<strong>清除缓存</strong>，使用unpersist()，清除缓存数据是立即执行的</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> forRDD.unpersist()</span></span><br><span class="line">res8: forRDD.type = MapPartitionsRDD[3] at map at &lt;console&gt;:28</span><br></pre></td></tr></table></figure><p><strong>怎么修改存储级别？</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> forRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="comment">//计数器做累加</span></span><br><span class="line">    acc.add(<span class="number">1</span>L)</span><br><span class="line">&#125;).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>).count()</span><br></pre></td></tr></table></figure><p>StorageLevel是个object，需要的级别都可以从里面拿出来</p><p><strong>考点：cache和persist有什么区别？</strong></p><ul><li>cache调用的persist，persist调用的persist(storage level)</li></ul><p><strong>问题：序列化和非序列化有什么区别？</strong></p><ul><li>序列化将对象转换成字节数组了，节省空间，占CPU</li></ul><h3 id="Removing-Data"><a href="#Removing-Data" class="headerlink" title="Removing Data"></a>Removing Data</h3><p>Spark自动监视每个节点上的缓存使用情况，并以最近最少使用(LRU)的方式删除旧的数据分区。如果想要手动删除一个RDD，而不是等待它从缓存中消失，那么可以使用RDD.unpersist()方法。</p><p>伪代码以及画图表示出什么是LRU？</p><h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><h3 id="为什么要将变量定义成广播变量？"><a href="#为什么要将变量定义成广播变量？" class="headerlink" title="为什么要将变量定义成广播变量？"></a>为什么要将变量定义成广播变量？</h3><p>如果我们要在分布式计算里面分发大对象，例如：<em>字典</em>，<em>集合</em>，<em>黑白名单</em>等，这个都会由Driver端进行分发，一般来讲，如果这个变量不是广播变量，那么每个task就会分发一份，这在<strong>task数目十分多的情况下Driver的带宽会成为系统的瓶颈，而且会大量消耗task服务器上的资源</strong>，如果将这个变量声明为广播变量，那么知识每个executor拥有一份，这个executor启动的task会共享这个变量，节省了通信的成本和服务器的资源。</p><h3 id="广播变量图解"><a href="#广播变量图解" class="headerlink" title="广播变量图解"></a>广播变量图解</h3><p>错误的，不使用广播变量</p><p><img src="https://yerias.github.io/spark_img/%E4%B8%8D%E4%BD%BF%E7%94%A8%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F.png" alt="不使用广播变量"></p><p>正确的，使用广播变量的情况</p><p><img src="https://yerias.github.io/spark_img/%E4%BD%BF%E7%94%A8%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F.png" alt="使用广播变量"></p><h3 id="小表广播案例"><a href="#小表广播案例" class="headerlink" title="小表广播案例"></a>小表广播案例</h3><p>使用广播变量的场景很多， 我们都知道spark 一种常见的优化方式就是小表广播， 使用 map join 来代替 reduce join， 我们通过把小的数据集广播到各个节点上，节省了一次特别 expensive 的 shuffle 操作。</p><p>比如driver 上有一张数据量很小的表， 其他节点上的task 都需要 lookup 这张表， 那么 driver 可以先把这张表 copy 到这些节点，这样 task 就可以在本地查表了。</p><ol><li><p>Fact table  航线(起点机场, 终点机场, 航空公司, 起飞时间)</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SEA,JFK,DL,7:00</span><br><span class="line">SFO,LAX,AA,7:05</span><br><span class="line">SFO,JFK,VX,7:05</span><br><span class="line">JFK,LAX,DL,7:10</span><br><span class="line">LAX,SEA,DL,7:10</span><br></pre></td></tr></table></figure></li><li><p>Dimension table 机场(简称, 全称, 城市, 所处城市简称)</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">JFK,John F. Kennedy International Airport,New York,NY</span><br><span class="line">LAX,Los Angeles International Airport,Los Angeles,CA</span><br><span class="line">SEA,Seattle-Tacoma International Airport,Seattle,WA</span><br><span class="line">SFO,San Francisco International Airport,San Francisco,CA</span><br></pre></td></tr></table></figure></li><li><p>Dimension table  航空公司(简称,全称)</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">AA,American Airlines</span><br><span class="line">DL,Delta Airlines</span><br><span class="line">VX,Virgin America</span><br></pre></td></tr></table></figure></li><li><p>思路：将机场维度表和航空公司维度表进行广播，生成Map，航线事实表从广播变量中通过key拿到value(计算在每个executor上)</p></li><li><p>代码</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BroadcastApp</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Fact table  航线(起点机场, 终点机场, 航空公司, 起飞时间)</span></span><br><span class="line">        <span class="keyword">val</span> flights = sc.textFile(<span class="string">"tunan-spark-core/broadcast/flights.txt"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Dimension table 机场(简称, 全称, 城市, 所处城市简称)</span></span><br><span class="line">        <span class="keyword">val</span> airports: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"tunan-spark-core/broadcast/airports.txt"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Dimension table  航空公司(简称,全称)</span></span><br><span class="line">        <span class="keyword">val</span> airlines = sc.textFile(<span class="string">"tunan-spark-core/broadcast/airlines.txt"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 最终统计结果：</span></span><br><span class="line"><span class="comment">         * 出发城市           终点城市           航空公司名称         起飞时间</span></span><br><span class="line"><span class="comment">         * Seattle           New York       Delta Airlines          7:00</span></span><br><span class="line"><span class="comment">         * San Francisco     Los Angeles    American Airlines       7:05</span></span><br><span class="line"><span class="comment">         * San Francisco     New York       Virgin America          7:05</span></span><br><span class="line"><span class="comment">         * New York          Los Angeles    Delta Airlines          7:10</span></span><br><span class="line"><span class="comment">         * Los Angeles       Seattle        Delta Airlines          7:10</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//广播Dimension Table airport，生成Map</span></span><br><span class="line">        <span class="keyword">val</span> airportsBC = sc.broadcast(airports.map(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = x.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">2</span>))</span><br><span class="line">        &#125;).collectAsMap())</span><br><span class="line"></span><br><span class="line">        <span class="comment">//广播Dimension Table airlines，生成Map</span></span><br><span class="line">        <span class="keyword">val</span> airlinesBC = sc.broadcast(airlines.map(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = x.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;).collectAsMap())</span><br><span class="line"></span><br><span class="line">        <span class="comment">//通过key获取value</span></span><br><span class="line">        flights.map(lines =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = lines.split(<span class="string">","</span>)</span><br><span class="line">            <span class="keyword">val</span> a = airportsBC.value.get(words(<span class="number">0</span>)).get</span><br><span class="line">            <span class="keyword">val</span> b = airportsBC.value.get(words(<span class="number">1</span>)).get</span><br><span class="line">            <span class="keyword">val</span> c = airlinesBC.value.get(words(<span class="number">2</span>)).get</span><br><span class="line">            a+<span class="string">"    "</span>+b+<span class="string">"    "</span>+c+<span class="string">"    "</span>+words(<span class="number">3</span>)</span><br><span class="line">        &#125;).foreach(println)</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>结果</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">New York    Los Angeles     Delta Airlines    7:10</span><br><span class="line">Los Angeles     Seattle    Delta Airlines    7:10</span><br><span class="line">Seattle    New York    Delta Airlines    7:00</span><br><span class="line">San Francisco   Los Angeles     American Airlines 7:05</span><br><span class="line">San Francisco   New York    Virgin America    7:05</span><br></pre></td></tr></table></figure></li></ol><h3 id="为什么只能-broadcast-只读的变量"><a href="#为什么只能-broadcast-只读的变量" class="headerlink" title="为什么只能 broadcast 只读的变量"></a>为什么只能 broadcast 只读的变量</h3><p>这就涉及一致性的问题，如果变量可以被更新，那么一旦变量被某个节点更新，其他节点要不要一块更新？如果多个节点同时在更新，更新顺序是什么？怎么做同步？ 仔细想一下， 每个都很头疼， spark 目前就索性搞成了只读的。  因为分布式强一致性真的很蛋疼</p><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ol><li><p>变量一旦被定义为一个广播变量，那么这个变量只能读，不能修改</p></li><li><p>能不能将一个RDD使用广播变量广播出去？因为RDD是不存储数据的。<strong>可以将RDD的结果广播出去。</strong></p></li><li><p>广播变量只能在Driver端定义，<strong>不能在Executor端定义。</strong></p></li><li><p>在Driver端可以修改广播变量的值，<strong>在Executor端无法修改广播变量的值。</strong></p></li><li><p>如果executor端用到了Driver的变量，<strong>不使用广播变量在Executor有多少task就有多少Driver端的变量副本。</strong></p></li><li><p>如果Executor端用到了Driver的变量，<strong>使用广播变量在每个Executor中只有一份Driver端的变量副本。</strong></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark之短信告警</title>
      <link href="/2020/03/31/spark/8/"/>
      <url>/2020/03/31/spark/8/</url>
      
        <content type="html"><![CDATA[<p>下面的案例继续延续Spark监控中的邮件监控，在监控中检测到数据异常，需要发送邮件告警</p><p>发送邮件工具类</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">MsgUtils</span> </span>&#123;</span><br><span class="line">    public static void send(<span class="type">String</span> recivers, <span class="type">String</span> title, <span class="type">String</span> content) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">Properties</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>();</span><br><span class="line">        properties.setProperty(<span class="string">"mail.host"</span>,<span class="string">"smtp.qq.com"</span>);</span><br><span class="line">        properties.setProperty(<span class="string">"mail.transport.protocol"</span>,<span class="string">"smtp"</span>);</span><br><span class="line">        properties.setProperty(<span class="string">"mail.smtp.auth"</span>, <span class="string">"true"</span>);</span><br><span class="line">        properties.setProperty(<span class="string">"mail.smtp.ssl.enable"</span>,<span class="string">"false"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">MailSSLSocketFactory</span> factory = <span class="keyword">new</span> <span class="type">MailSSLSocketFactory</span>();</span><br><span class="line">        factory.setTrustAllHosts(<span class="literal">true</span>);</span><br><span class="line">        properties.put(<span class="string">"mail.smtp.ssl.socketFactory"</span>, factory);</span><br><span class="line"></span><br><span class="line">        <span class="type">Authenticator</span> authenticator = <span class="keyword">new</span> <span class="type">Authenticator</span>() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">protected</span> <span class="type">PasswordAuthentication</span> getPasswordAuthentication() &#123;</span><br><span class="line">                <span class="type">String</span> username = <span class="string">"发送者qq邮箱"</span>;</span><br><span class="line">                <span class="type">String</span> password = <span class="string">"发送者qq授权码"</span>;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">PasswordAuthentication</span>(username, password);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="type">Session</span> session = <span class="type">Session</span>.getInstance(properties, authenticator);</span><br><span class="line"></span><br><span class="line">        <span class="type">MimeMessage</span> message = <span class="keyword">new</span> <span class="type">MimeMessage</span>(session);</span><br><span class="line">        <span class="type">InternetAddress</span> from = <span class="keyword">new</span> <span class="type">InternetAddress</span>(<span class="string">"发送者qq邮箱"</span>);</span><br><span class="line">        message.setFrom(from);</span><br><span class="line">        <span class="type">InternetAddress</span>[] tos = <span class="type">InternetAddress</span>.parse(recivers);</span><br><span class="line">        message.setRecipients(<span class="type">Message</span>.<span class="type">RecipientType</span>.<span class="type">TO</span>, tos);</span><br><span class="line">        message.setSubject(title);</span><br><span class="line">        message.setContent(content, <span class="string">"text/html;charset=UTF-8"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">Transport</span>.send(message);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(<span class="type">String</span>[] args) <span class="keyword">throws</span> <span class="type">Exception</span>&#123;</span><br><span class="line">        send(<span class="string">"接收邮箱"</span>, <span class="string">"测试"</span>, <span class="string">"测试内容"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Spark监控中调用</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">MsgUtils</span>.send(<span class="string">"接收者邮箱,接收者邮箱"</span>, <span class="string">"ERROR：数据异常"</span>, <span class="string">s"jobID: <span class="subst">$jobId</span> 数据异常，请马上检查: <span class="subst">$&#123;listener.toString&#125;</span>s"</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark之监控模块</title>
      <link href="/2020/03/31/spark/7/"/>
      <url>/2020/03/31/spark/7/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>Spark自带监控</li><li>Spark接口监控</li><li>Spark自定义监控</li></ol><h2 id="Spark自带监控"><a href="#Spark自带监控" class="headerlink" title="Spark自带监控"></a><a href="http://spark.apache.org/docs/latest/monitoring.html#spark-history-server-configuration-options" target="_blank" rel="noopener">Spark自带监控</a></h2><p>第一种监控方式是Spark自带的，由于Spark Web UI界面只在sc的生命周期内有效，所以我们需要存储日志，在Spark sc 生命周期结束后重构UI界面。</p><p>首先看官方文档配置，这里只是简单配置</p><ol><li><p>修改spark.default.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">#开启日志存储</span><br><span class="line">spark.eventLog.enabled true</span><br><span class="line">#指定日志存储的HDFS目录</span><br><span class="line">spark.eventLog.dir hdfs://hadoop:9000/spark-logs</span><br><span class="line">#开启日志存储7天自动删除</span><br><span class="line">spark.history.fs.cleaner.enabled true</span><br></pre></td></tr></table></figure></li><li><p>修改spark.env.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">#指定日志恢复目录，就是上面的日志存储目录</span><br><span class="line">SPARK_HISTORY_OPTS = "-Dspark.history.fs.logDirectory=hdfs://hadoop:9000/spark-logs"</span><br></pre></td></tr></table></figure></li><li><p>在 sc 的生命周期外打开历史UI界面</p></li></ol><h2 id="Spark接口监控"><a href="#Spark接口监控" class="headerlink" title="Spark接口监控"></a><a href="http://spark.apache.org/docs/latest/monitoring.html#rest-api" target="_blank" rel="noopener">Spark接口监控</a></h2><p>首先看官方文档配置，这里只是简单介绍</p><p>查看application列表：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">http:<span class="comment">//hadoop:18080/api/v1/applications</span></span><br></pre></td></tr></table></figure><p>查看application的所有job</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">http:<span class="comment">//hadoop:18080/history/application_1585632916452_0002/jobs/</span></span><br></pre></td></tr></table></figure><h2 id="Spark自定义监控"><a href="#Spark自定义监控" class="headerlink" title="Spark自定义监控"></a><a href="http://spark.apache.org/docs/latest/monitoring.html#executor-task-metrics" target="_blank" rel="noopener">Spark自定义监控</a></h2><p>metrics: 数据信息</p><p>spark 提供了一系列整个任务生命周期中各个阶段变化的事件监听机制，通过这一机制可以在任务的各个阶段做一些自定义的各种动作。SparkListener便是这些阶段的事件监听接口类 通过实现这个类中的各种方法便可实现自定义的事件处理动作。</p><p>自定义监听sparListener后的注册方式有两种：</p><p><strong>方法1：</strong>conf 配置中指定</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">.setAppName(getClass.getSimpleName)</span><br><span class="line">.setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line"><span class="comment">//监听类注册</span></span><br><span class="line">.set(<span class="string">"spark.extraListeners"</span>, <span class="string">"com.tunan.spark.listener.MySparkListener"</span>)</span><br><span class="line">.getOrCreate()</span><br></pre></td></tr></table></figure><p><strong>方法2：</strong>sparkContext 类中指定</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc.addSparkListener(<span class="keyword">new</span> <span class="type">MySparkAppListener</span>)</span><br></pre></td></tr></table></figure><h3 id="SparkListerner"><a href="#SparkListerner" class="headerlink" title="SparkListerner"></a>SparkListerner</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//SparkListener 下各个事件对应的函数名非常直白，即如字面所表达意思。</span></span><br><span class="line"><span class="comment">//想对哪个阶段的事件做一些自定义的动作，变继承SparkListener实现对应的函数即可</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkListener</span> <span class="keyword">extends</span> <span class="title">SparkListenerInterface</span> </span>&#123;</span><br><span class="line">  <span class="comment">//阶段完成时触发的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStageCompleted</span></span>(stageCompleted: <span class="type">SparkListenerStageCompleted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//阶段提交时触发的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStageSubmitted</span></span>(stageSubmitted: <span class="type">SparkListenerStageSubmitted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//任务启动时触发的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskStart</span></span>(taskStart: <span class="type">SparkListenerTaskStart</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//下载任务结果的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskGettingResult</span></span>(taskGettingResult: <span class="type">SparkListenerTaskGettingResult</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//任务结束的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskEnd</span></span>(taskEnd: <span class="type">SparkListenerTaskEnd</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//job启动的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onJobStart</span></span>(jobStart: <span class="type">SparkListenerJobStart</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//job结束的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onJobEnd</span></span>(jobEnd: <span class="type">SparkListenerJobEnd</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//环境变量被更新的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onEnvironmentUpdate</span></span>(environmentUpdate: <span class="type">SparkListenerEnvironmentUpdate</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//块管理被添加的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBlockManagerAdded</span></span>(blockManagerAdded: <span class="type">SparkListenerBlockManagerAdded</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBlockManagerRemoved</span></span>(</span><br><span class="line">      blockManagerRemoved: <span class="type">SparkListenerBlockManagerRemoved</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//取消rdd缓存的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onUnpersistRDD</span></span>(unpersistRDD: <span class="type">SparkListenerUnpersistRDD</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//app启动的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onApplicationStart</span></span>(applicationStart: <span class="type">SparkListenerApplicationStart</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//app结束的事件 [以下各事件也如同函数名所表达各个阶段被触发的事件不在一一标注]</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onApplicationEnd</span></span>(applicationEnd: <span class="type">SparkListenerApplicationEnd</span>): <span class="type">Unit</span> = &#123; &#125; </span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorMetricsUpdate</span></span>(</span><br><span class="line">      executorMetricsUpdate: <span class="type">SparkListenerExecutorMetricsUpdate</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorAdded</span></span>(executorAdded: <span class="type">SparkListenerExecutorAdded</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorRemoved</span></span>(executorRemoved: <span class="type">SparkListenerExecutorRemoved</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorBlacklisted</span></span>(</span><br><span class="line">      executorBlacklisted: <span class="type">SparkListenerExecutorBlacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorUnblacklisted</span></span>(</span><br><span class="line">      executorUnblacklisted: <span class="type">SparkListenerExecutorUnblacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onNodeBlacklisted</span></span>(</span><br><span class="line">      nodeBlacklisted: <span class="type">SparkListenerNodeBlacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onNodeUnblacklisted</span></span>(</span><br><span class="line">      nodeUnblacklisted: <span class="type">SparkListenerNodeUnblacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBlockUpdated</span></span>(blockUpdated: <span class="type">SparkListenerBlockUpdated</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onOtherEvent</span></span>(event: <span class="type">SparkListenerEvent</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h3><p>首先看官方文档配置，这里只是简单案例</p><ol><li>自定义监控类，继承SparkListener</li><li>重写onTaskEnd方法，拿到taskMetrics</li><li>从taskMetrics获取各种数据信息</li><li>注册到被监听的类</li></ol><p><strong>第1-3步代码</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySparkListener</span>(<span class="params">conf:<span class="type">SparkConf</span></span>) <span class="keyword">extends</span> <span class="title">SparkListener</span> <span class="keyword">with</span> <span class="title">Logging</span></span>&#123;</span><br><span class="line">    <span class="comment">//监听每个Task结束</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskEnd</span></span>(taskEnd: <span class="type">SparkListenerTaskEnd</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//获取应用程序名称</span></span><br><span class="line">        <span class="keyword">val</span> appName = conf.get(<span class="string">"spark.app.name"</span>)</span><br><span class="line">        <span class="comment">//获取作业的taskMetrics</span></span><br><span class="line">        <span class="keyword">val</span> metrics = taskEnd.taskMetrics</span><br><span class="line">        <span class="comment">//使用对象接收参数</span></span><br><span class="line">        <span class="keyword">val</span> metricsObject = <span class="type">Metrics</span>(appName,taskEnd.stageId, taskEnd.taskInfo.taskId, metrics.inputMetrics.bytesRead, metrics.shuffleReadMetrics.totalBytesRead, metrics.outputMetrics.bytesWritten, metrics.shuffleWriteMetrics.bytesWritten)</span><br><span class="line">        <span class="comment">//输出字符串类型的metricsObject</span></span><br><span class="line">        logError(metricsObject.toString)</span><br><span class="line">        <span class="comment">//输出Json类型的metricsObject</span></span><br><span class="line">        logError(<span class="type">Json</span>(<span class="type">DefaultFormats</span>).write(metricsObject))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//定义case class对象</span></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Metrics</span>(<span class="params">appName:<span class="type">String</span>,stageId:<span class="type">Long</span>,taskId:<span class="type">Long</span>,bytesRead:<span class="type">Long</span>,bytesWritten:<span class="type">Long</span>,shuffleReadMetrics:<span class="type">Long</span>,shuffleWriteMetrics:<span class="type">Long</span></span>)</span>&#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = <span class="string">s"appName:<span class="subst">$appName</span>,stageId:<span class="subst">$stageId</span>,taskId:<span class="subst">$taskId</span>,bytesRead:<span class="subst">$bytesRead</span>,bytesWritten:<span class="subst">$bytesWritten</span>,shuffleReadMetrics:<span class="subst">$shuffleReadMetrics</span>,shuffleWriteMetrics:<span class="subst">$shuffleWriteMetrics</span>"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>第四步代码</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//输入、输出路径</span></span><br><span class="line">        <span class="keyword">val</span> (in,out) = (args(<span class="number">0</span>),args(<span class="number">1</span>))</span><br><span class="line">        <span class="comment">//配置conf</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">            .setAppName(getClass.getSimpleName)</span><br><span class="line">            .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">            <span class="comment">//监听类注册</span></span><br><span class="line">            .set(<span class="string">"spark.extraListeners"</span>, <span class="string">"com.tunan.spark.listener.MySparkListener"</span>)</span><br><span class="line">        <span class="comment">//拿到sc</span></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="comment">//删除输出目录</span></span><br><span class="line">        <span class="type">CheckHDFSOutPath</span>.ifExistsDeletePath(<span class="keyword">new</span> <span class="type">Configuration</span>(),out)</span><br><span class="line">        <span class="comment">//操作算子</span></span><br><span class="line">        <span class="keyword">val</span> result = sc.textFile(in).flatMap(_.split(<span class="string">"\t"</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">        <span class="comment">//保存文件</span></span><br><span class="line">        result.saveAsTextFile(out)</span><br><span class="line">        <span class="comment">//关闭sc</span></span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Spark自定义监控案例"><a href="#Spark自定义监控案例" class="headerlink" title="Spark自定义监控案例"></a>Spark自定义监控案例</h2><p>把监控参数写入到MySQL</p><p>需求：应用程序名字、jobID号、stageID号、taskID号、读取数据量、写入数据量、shuffle读取数据量、shuffle写入数据量。</p><ol><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> wc2mysql(</span><br><span class="line">app_name <span class="built_in">varchar</span>(<span class="number">32</span>),</span><br><span class="line">job_id <span class="built_in">bigint</span>,</span><br><span class="line">stage_id <span class="built_in">bigInt</span>,</span><br><span class="line">task_id <span class="built_in">bigint</span>,</span><br><span class="line">file_read_byte <span class="built_in">bigint</span>,</span><br><span class="line">file_write_byte <span class="built_in">bigint</span>,</span><br><span class="line">shuffle_read_byte <span class="built_in">bigint</span>,</span><br><span class="line">shuffle_write_byte <span class="built_in">bigint</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p>实现监控类</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySparkListenerV2</span>(<span class="params">conf: <span class="type">SparkConf</span></span>) <span class="keyword">extends</span> <span class="title">SparkListener</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义JobID</span></span><br><span class="line">    <span class="keyword">var</span> jobId:<span class="type">Long</span> = _</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onJobStart</span></span>(jobStart: <span class="type">SparkListenerJobStart</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//初始化</span></span><br><span class="line">        jobId = jobStart.jobId</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"============准备插入数据============"</span>)</span><br><span class="line">    <span class="comment">//监听每个Task结束</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskEnd</span></span>(taskEnd: <span class="type">SparkListenerTaskEnd</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//获取应用程序名称</span></span><br><span class="line">        <span class="keyword">val</span> appName = conf.get(<span class="string">"spark.app.name"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取作业的taskMetrics</span></span><br><span class="line">        <span class="keyword">val</span> metrics = taskEnd.taskMetrics</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用对象接收参数</span></span><br><span class="line">        <span class="keyword">val</span> listener = <span class="type">Listener</span>(appName, jobId, taskEnd.stageId, taskEnd.taskInfo.taskId, metrics.inputMetrics.bytesRead, metrics.outputMetrics.bytesWritten, metrics.shuffleReadMetrics.totalBytesRead, metrics.shuffleWriteMetrics.bytesWritten)</span><br><span class="line"></span><br><span class="line"><span class="comment">//结果插入到MySQL</span></span><br><span class="line">        <span class="type">ListenerCURD</span>.insert(listener)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//发送监控邮件</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"true"</span> == conf.get(<span class="string">"spark.send.mail.enabled"</span>))&#123;</span><br><span class="line">            <span class="type">MsgUtils</span>.send(<span class="string">"971118017@qq.com"</span>, <span class="string">"ERROR：数据异常"</span>, <span class="string">s"jobID: <span class="subst">$jobId</span> 数据异常，请马上检查: <span class="subst">$&#123;listener.toString&#125;</span>"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    println(<span class="string">"============成功插入数据============"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>实现监控数据写入MySQL</p><p>使用的是scalikejdbc框架实现的，使用具体方法在我的<a href="http://yerias.github.io/2020/03/16/scala/2/">博客</a></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Listener</span>(<span class="params">app_name: <span class="type">String</span>, job_id: <span class="type">Long</span>, stage_id: <span class="type">Long</span>, task_id: <span class="type">Long</span>, file_read_byte: <span class="type">Long</span>, file_write_byte: <span class="type">Long</span>, shuffle_read_byte: <span class="type">Long</span>, shuffle_write_byte: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">ListenerCURD</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Before</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//初始化配置</span></span><br><span class="line">        <span class="type">DBs</span>.setupAll()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(listener: <span class="type">Listener</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="type">Before</span>()</span><br><span class="line">        <span class="comment">//事物插入</span></span><br><span class="line">        <span class="type">DB</span>.localTx &#123;</span><br><span class="line">            <span class="keyword">implicit</span> session =&gt; &#123;</span><br><span class="line">                <span class="type">SQL</span>(<span class="string">"insert into wc2mysql(app_name,job_id,stage_id,task_id,file_read_byte,file_write_byte,shuffle_read_byte,shuffle_write_byte) values(?,?,?,?,?,?,?,?)"</span>)</span><br><span class="line">                    .bind(listener.app_name,listener.job_id, listener.stage_id, listener.task_id, listener.file_read_byte, listener.file_write_byte, listener.shuffle_read_byte, listener.shuffle_write_byte)</span><br><span class="line">                    .update()</span><br><span class="line">                    .apply()</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">After</span>()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">After</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//关闭资源</span></span><br><span class="line">        <span class="type">DBs</span>.closeAll()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>启动被监控类</p><p>被监控的类还是我们上面的WordCount的类，关键在于在SparkConf()中注册</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//配置conf</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">.setAppName(getClass.getSimpleName)</span><br><span class="line">.setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line"><span class="comment">//监听类注册</span></span><br><span class="line">.set(<span class="string">"spark.extraListeners"</span>, <span class="string">"com.tunan.spark.listener.MySparkListenerV2"</span>)</span><br></pre></td></tr></table></figure></li><li><p>在数据库中查询</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> wc2mysql;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark术语&amp;Spark提交&amp;YARN上的提交模式&amp;窄依赖&amp;宽依赖</title>
      <link href="/2020/03/30/spark/6/"/>
      <url>/2020/03/30/spark/6/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>Spark术语</li><li>Spark提交</li><li>YARN上提交模式</li><li>宽依赖</li><li>窄依赖</li></ol><h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><p>下表总结了关于集群概念的术语:</p><table><thead><tr><th><strong>Term</strong></th><th>Meaning</th></tr></thead><tbody><tr><td>Application</td><td>Spark上的应用程序。由一个<strong>driver program</strong>和集群上的<strong>executors</strong>组成。</td></tr><tr><td>Application jar</td><td>一个包含用户的Spark应用程序的jar包</td></tr><tr><td>Driver program</td><td>运行应用程序main()函数并创建SparkContext的进程</td></tr><tr><td>Cluster manager</td><td>用于获取集群资源的外部服务(例如，standalone manager、Mesos、YARN)</td></tr><tr><td>Deploy mode</td><td>区别driver process在何处运行。在“cluster”模式下，框架启动集群内部的驱动程序。在“client”模式下，提交者启动集群外部的驱动程序。</td></tr><tr><td>Worker node</td><td>可以在集群中运行application的任何节点</td></tr><tr><td>Executor</td><td>在Worker node上被application启动的进程，它运行任务并将数据保存在内存或磁盘存储器中。每个application都有自己的Executor。</td></tr><tr><td>Task</td><td>将被发送给一个执行者的工作单元</td></tr><tr><td>Job</td><td>由多个任务组成的并行计算，这些任务在响应一个Spark操作时产生(如保存、收集)</td></tr><tr><td>Stage</td><td>每个作业被分成更小的任务集，称为阶段，这些阶段相互依赖(类似于MapReduce中的map和reduce阶段)</td></tr></tbody></table><h2 id="Spark提交"><a href="#Spark提交" class="headerlink" title="Spark提交"></a><a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">Spark提交</a></h2><p>注意：在使用Spark提交之前，一定要在环境变量中配置<code>HADOOP_CONF_DIR</code>，否则hadoop的环境引不进来</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HADOOP_CONF_DIR=XXX</span><br></pre></td></tr></table></figure><p>Spark支持的部署模式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class &lt;main-class&gt; \</span><br><span class="line">  --master &lt;master-url&gt; \</span><br><span class="line">  --deploy-mode &lt;deploy-mode&gt; \</span><br><span class="line">  --conf &lt;key&gt;=&lt;value&gt; \</span><br><span class="line">  ... # other options</span><br><span class="line">  &lt;application-jar&gt; \</span><br><span class="line">  [application-arguments]</span><br></pre></td></tr></table></figure><p>一些常用的选项是：</p><ul><li><code>--class</code>：您的应用程序的入口点（例如<code>org.apache.spark.examples.SparkPi</code>）</li><li><code>--master</code>：集群的<a href="http://spark.apache.org/docs/latest/submitting-applications.html#master-urls" target="_blank" rel="noopener">主URL</a>（例如<code>spark://23.195.26.187:7077</code>）</li><li><code>--deploy-mode</code>：将驱动程序部署在工作节点（<code>cluster</code>）上还是作为外部客户端（<code>client</code>）本地部署（默认值：<code>client</code>）</li><li><code>--conf</code>：键值格式的任意Spark配置属性。对于包含空格的值，将“ key = value”用引号引起来（如图所示）。</li><li><code>application-jar</code>：jar包的路径，包括您的应用程序和所有依赖项。URL必须在群集内部全局可见，例如，<code>hdfs://</code>路径或<code>file://</code>所有节点上都存在的路径。</li><li><code>application-arguments</code>：参数传递给您的主类的main方法（如果有）</li></ul><p>其他常用的选项：</p><ul><li><p><code>--num-executors</code>：executors的数量</p></li><li><p><code>--executor-memory</code>：每个executor的内存数量</p></li><li><p><code>--total-executor-cores 100</code>：executor的总的core数</p></li><li><p><code>--jars</code>：指定需要依赖的jar包，多个jar包逗号分隔，application中直接引用</p></li><li><p><code>--files</code>：需要依赖的文件，在application中使用SparkFiles.get(“file”)取出，同时需要放在resources目录下</p></li></ul><p>注意：local模式默认读写HDFS数据 读本地要加<code>file://</code></p><h2 id="提交模式"><a href="#提交模式" class="headerlink" title="提交模式"></a>提交模式</h2><h3 id="cliet模式"><a href="#cliet模式" class="headerlink" title="cliet模式"></a>cliet模式</h3><ol><li>Driver运行在Client</li><li>AM职责就是去YARN上申请资源</li><li>Driver会和请求到的container/executor进行通信</li><li>Driver是不能退出的</li></ol><p><img src="https://yerias.github.io/spark_img/Client%E6%A8%A1%E5%BC%8F.jpg" alt="Client模式"></p><p><strong>Client模式控制台能看到日志</strong></p><h3 id="cluster模式"><a href="#cluster模式" class="headerlink" title="cluster模式"></a>cluster模式</h3><ol><li><p>Driver运行位置在AM</p></li><li><p>Client提交上去了  它退出对整个作业没影响</p></li><li><p>AM(申请资源)+Driver（调度DAG，分发任务）</p><p><img src="https://yerias.github.io/spark_img/Cluster%E6%A8%A1%E5%BC%8F.jpg" alt="Cluster模式"></p></li></ol><p><strong>控制台不能看到日志，不支持Spark-shell(Spark-SQL) ，交互性操作的都不能</strong></p><h2 id="窄依赖"><a href="#窄依赖" class="headerlink" title="窄依赖"></a>窄依赖</h2><ol><li>一个父RDD的分区至多被一个子RDD的某个分区使用一次</li><li>一个父RDD的分区和一个子RDD的分区是唯一映射 典型的map</li><li>多个父RDD的分区和一个子RDD的分区是唯一映射 典型的union</li></ol><p><img src="https://yerias.github.io/spark_img/%E7%AA%84%E4%BE%9D%E8%B5%96.jpg" alt="窄依赖"></p><p>在窄依赖中有个特殊的join是不经过shuffle 的</p><p>这个特殊的join的存在有三个条件：</p><ol><li>RDD1的分区数 = RDD2的分区数</li><li>RDD1的分区数 = Join的分区数</li><li>RDD2的分区数 = Join的分区数 </li></ol><p>我们看一个案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * rdd1、rdd2、join 三者的分区数相同，不经过shuffle</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>((<span class="string">"香蕉"</span>,<span class="number">20</span>), (<span class="string">"苹果"</span>,<span class="number">50</span>), (<span class="string">"菠萝"</span>,<span class="number">30</span>), (<span class="string">"猕猴桃"</span>, <span class="number">50</span>)),<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">List</span>((<span class="string">"草莓"</span>,<span class="number">90</span>), (<span class="string">"苹果"</span>,<span class="number">25</span>), (<span class="string">"菠萝"</span>,<span class="number">25</span>), (<span class="string">"猕猴桃"</span>, <span class="number">30</span>), (<span class="string">"西瓜"</span>, <span class="number">45</span>)),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.reduceByKey(_ + _)</span><br><span class="line"><span class="keyword">val</span> rdd4 = rdd2.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> joinRDD = rdd3.join(rdd4,<span class="number">2</span>)</span><br><span class="line">joinRDD.collect()</span><br></pre></td></tr></table></figure><p>再看Application的DAG图，从两个 reduceByKey 到 join 是一个 stage 中的，说明没有产生 shuffle</p><p><img src="https://yerias.github.io/spark_img/%E7%89%B9%E6%AE%8A%E7%9A%84Jion.jpg" alt="特殊的Jion"></p><h2 id="宽依赖"><a href="#宽依赖" class="headerlink" title="宽依赖"></a>宽依赖</h2><ol><li>一个父RDD的分区会被子RDD的分区使用多次</li></ol><p><img src="https://yerias.github.io/spark_img/%E5%AE%BD%E4%BE%9D%E8%B5%96.jpg" alt="宽依赖"></p><p>除了前面那种是三个条件满足的，其他的 join 都是宽依赖</p><p>我们使RDD1的分区数和RDD2的分区数相等，但是 join的分区数不相等</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * rdd1、rdd2的分区数不同，但是和join的分区数不同，会经过shuffle</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>((<span class="string">"香蕉"</span>,<span class="number">20</span>), (<span class="string">"苹果"</span>,<span class="number">50</span>), (<span class="string">"菠萝"</span>,<span class="number">30</span>), (<span class="string">"猕猴桃"</span>, <span class="number">50</span>)),<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">List</span>((<span class="string">"草莓"</span>,<span class="number">90</span>), (<span class="string">"苹果"</span>,<span class="number">25</span>), (<span class="string">"菠萝"</span>,<span class="number">25</span>), (<span class="string">"猕猴桃"</span>, <span class="number">30</span>), (<span class="string">"西瓜"</span>, <span class="number">45</span>)),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.reduceByKey(_ + _)</span><br><span class="line"><span class="keyword">val</span> rdd4 = rdd2.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> joinRDD = rdd3.join(rdd4,<span class="number">4</span>)</span><br><span class="line">joinRDD.collect()</span><br></pre></td></tr></table></figure><p>我们看DAG图，产生了stage，也就是经过了shuffle</p><p><img src="https://yerias.github.io/spark_img/%E7%89%B9%E6%AE%8A%E7%9A%84Jion%E5%AE%BD%E4%BE%9D%E8%B5%96.jpg" alt="特殊的Jion宽依赖"></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark疯狂踩坑系列</title>
      <link href="/2020/03/30/error/5/"/>
      <url>/2020/03/30/error/5/</url>
      
        <content type="html"><![CDATA[<p>如果WEB UI界面或者程序日志里面看不到错误，使用以下方式查看日志</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn logs -applicationId application_1585536649766_xxxx</span><br></pre></td></tr></table></figure><p>错误1</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Error: Could not find or load main class org.apache.spark.deploy.yarn.ApplicationMaster</span><br></pre></td></tr></table></figure><p>解决办法：</p><p>检查spark-defaults.conf中的配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark.yarn.jars=hdfs://hadoop:9000/spark-yarn/jars/*.jar</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark.yarn.archive  hdfs://hadoop000:8020/tmp/spark-archive/spark2.4.5.zip</span><br></pre></td></tr></table></figure><p>以上两种配置方式不可以错乱</p><p>错误2</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">java.lang.NoClassDefFoundError: org/lionsoul/ip2region/DbConfig</span><br></pre></td></tr></table></figure><p>解决办法</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--jars /home/hadoop/lib/ip2region-1.7.2.jar</span><br></pre></td></tr></table></figure><p>错误3</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">java.lang.NullPointerException</span><br><span class="line">at com.tunan.spark.utils.IpParseUtil.IpParse(IpParseUtil.java:19)</span><br></pre></td></tr></table></figure><p>解决办法</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--files /home/hadoop/lib/ip2region.db</span><br></pre></td></tr></table></figure><hr><p>代码中拿出文件有两种方式</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String dbPath = GetIPRegion.class.getResource("/ip2region.db").getPath();</span><br><span class="line">String dbPath = SparkFiles.get(<span class="string">"/ip2region.db"</span>);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark使用Yarn模式解决Jar乱飞情况</title>
      <link href="/2020/03/29/spark/5/"/>
      <url>/2020/03/29/spark/5/</url>
      
        <content type="html"><![CDATA[<ol><li><p>在本地创建zip文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在spark的jars目录下</span></span><br><span class="line">zip spark.zip ./*</span><br></pre></td></tr></table></figure></li><li><p>HDFS上创建存放spark jar的目录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -mkdir -p  /spark-yarn/jars</span><br></pre></td></tr></table></figure></li><li><p>将$SPARK_HOME/jars下的spark.zip包上传至刚建的HDFS路径</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop jars]$ hadoop fs -put ./spark.zip /spark-yarn/jars/</span><br></pre></td></tr></table></figure></li><li><p>在 spark-defaults.conf中添加(也可以在启动的时候–conf指定)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark.yarn.archive=hdfs://hadoop:9000/spark-yarn/jars/spark.zip</span><br></pre></td></tr></table></figure></li><li><p>查看Spark log</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn log -applicationID xxx</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MR编程时，Driver传递的参数Mapper显示为NULL</title>
      <link href="/2020/03/26/error/4/"/>
      <url>/2020/03/26/error/4/</url>
      
        <content type="html"><![CDATA[<p>在进行MR编程时，除了需要拿到HDFS上面的数据，有时候还需要Driver和Mapper或者Reducer之间进行参数传递</p><p>先看看我碰到的问题</p><p><img src="https://yerias.github.io/java_img/14.png" alt=""></p><p><img src="https://yerias.github.io/java_img/15.png" alt=""></p><p>在Driver中配置向Conf中配置了参数，在Mapper中从Context中拿出来的却是null值，<strong>问题出现在Job.getInstance() 中没有把Conf传递进去。</strong></p><p>所以以后要注意，在创建Job的时候要把Conf也放进去。</p><p>还需要注意的是Conf中的设值必须在Job.getInstance()上面完成</p>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM之内存模型</title>
      <link href="/2020/03/26/jvm/4/"/>
      <url>/2020/03/26/jvm/4/</url>
      
        <content type="html"><![CDATA[<p>Java内存模型其实就是围绕着在并发过程中如果解决原子性、有序性和可见性的通信规则</p><p><img src="https://yerias.github.io/java_img/1.jpg" alt="线程、主内存、工作内存三者之间的关系"></p><h2 id="主内存与工作内存"><a href="#主内存与工作内存" class="headerlink" title="主内存与工作内存"></a>主内存与工作内存</h2><p>Java内存模型的主要目的就是定义程序中各种变量的访问规则，即关注在虚拟机中把变量存储到内存和从内存中取出变量这样的底层细节。</p><p>此处的变量指的是包括了实例字段，静态字段和构成数组对象的元素。但不包括局部变量和方法参数，因为后者是线程私有的。</p><ol><li><p>主内存和工作内存的关系？</p><p>Java内存模型规定了所有的变量都存储在主内存中，每条线程还有自己的工作内存，线程的工作内存保存了被该线程使用的变量的主内存副本。线程对变量的所有操作(读取、赋值等)都必须在工作内存中进行，而不能直接读写主内存中的数据。不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递需要通过主内存来完成。</p></li><li><p>主内存和工作内存如何交互？</p><p>主内存和工作内存如何交互？即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步回主内存。Java内存模型定义了8种操作来完成。分别是</p><ul><li>lock(锁定)</li><li>unlock(解锁)</li><li>read(读取)</li><li>load(载入)</li><li>use(使用)</li><li>assign(赋值)</li><li>store(存储)</li><li>write(写入)</li></ul><p>他们每一种操作都是原子的、不可再分的(double和long类型，32位主机会拆分成两次执行)</p><p>他们规定了八种执行规则，但这不是我们关心的重点，作为开发者更需要了解的是先行发生原则–用来确定一个操作在并发环境下是否安全</p></li></ol><h2 id="Volatile变量的特殊规则"><a href="#Volatile变量的特殊规则" class="headerlink" title="Volatile变量的特殊规则"></a>Volatile变量的特殊规则</h2><p>Volatile是Java虚拟机提供的最轻量级的<strong>同步机制</strong></p><p>为什么说他是最轻量级的同步机制？因为它只能保证可见性、有序性，而不能保证原子性。虽然应用场景有限，但是快，能保证其他线程的立即可见性。</p><p>当一个变量被定义成volatile之后，它具备两项特性：</p><ol><li><p><strong>保证此变量对所有线程的可见性，</strong>这里的可见性是指一条线程修改了这个变量的值，新值对于其他线程来说是立即得知的。由于volatile不能保证原子性(比如运算是分两步来做的)，只能在以下两条规则的场景中进行运算。</p><ul><li>运算结果不依赖变量的当前值。</li><li>变量不需要与其他的状态变量共同参与不变约束。</li></ul><p>简单点说，就是自己玩自己的，典型的引用场景就是作位状态标志的修饰符。</p></li><li><p><strong>通过加入内存屏障禁止指令重排序</strong>，指令重排序优化是编辑器做的一种代码执行顺序的优化，只关注结果，不关注过程，但是这么做可能让程序逻辑混乱，比如本来应该在后面执行的代码，跑到前面来执行了，这时候就要使用volatile禁止指令重排序，从而保证代码的执行顺序和程序的顺序相同。</p></li></ol><h2 id="可见性、有序性、原子性"><a href="#可见性、有序性、原子性" class="headerlink" title="可见性、有序性、原子性"></a>可见性、有序性、原子性</h2><ol><li><p>原子性</p><p>Java内存模型直接保证的原子性变量操作包括read、load、assign、use、store、write这六个，我们大致可以认为，对基本数据类型的访问、读写都是具备原子性的。</p><p>对于其他场景的原子性保证，Java内存模型提供了lock和unlock操作满足这种需求，反应到代码中就是synchronized关键字，因此<strong>synchronized是具备原子性的</strong>。</p></li><li><p>可见性</p><p>可见性就是指当一个线程修改了共享变量的值时，其他线程能够立即得知这个修改。</p><p><strong>volatile</strong>的特殊规则保证了新值能够立即从工作内存同步到主内存，以及每次使用前从主内存刷新。</p><p><strong>synchronized</strong>是通过”对一个变量执行unlock操作之前，必须先把此变量同步回主内存中”实现的。</p><p><strong>final</strong>关键字修饰的字段在构造器中初始化完成，并且构造器没有把”this”的引用逃逸出去，那么在其他线程中就能看见final修饰字段的值。</p></li><li><p>有序性</p><p>java程序中天然的有序性可以总结为一句话: 如果在本线程内观察，所有的操作都是有序的；如果在一个线程中观察另一个线程，所有的操作都是无序的。前半句是指 “线程内变量为串行语义”，后半句是指 “指令重排序” 现象和 “工作内存和主内存同步延迟” 现象。</p><p><strong>volatile</strong>关键字本身就包含了禁止指令重排序的语义。</p><p><strong>synchronized</strong>是由 “一个变量在同一时刻只允许一条线程对其进行lock操作” 这条规则获得的。 </p></li></ol><h2 id="先行发生原则"><a href="#先行发生原则" class="headerlink" title="先行发生原则"></a>先行发生原则</h2><p>先行发生是Java内存模型定义的两项操作之间的顺序关系，如果说操作A先行发生于操作B，其实就是说在发生操作B之前，操作A产生的影响被操作B观察到，”影响” 包括修改了内存中共享变量的值、发生了消息、调用了方法等。</p><p>Java内存模型中 “天然的” 先行发生关系包括以下八种，如果两个操作之间的关系不在此列，并且无法从下列规则推导出来，则他们就没有<strong>顺序型保障</strong>，虚拟机可以对它们进行任意的<strong>重排序</strong>。 </p><ol><li>程序次序规则</li><li>管程锁定规则</li><li>volatile变量规则</li><li>线程启动规则</li><li>线程终止规则</li><li>线程中断规则</li><li>对象终结规则</li><li>传递性</li></ol><ul><li>“时间上的先后顺序” 与 “先行发生” 之间有什么不同?</li></ul><p>时间先后顺序与先行发生原则之间基本没有因果关系，所以我们衡量并发安全问题的时候不要受时间顺序的干扰，一切必须以先行发生原则为准。</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark之排序模块</title>
      <link href="/2020/03/25/spark/4/"/>
      <url>/2020/03/25/spark/4/</url>
      
        <content type="html"><![CDATA[<p>目录</p><ol><li>算子排序</li><li>面向对象排序</li><li>隐式转换排序</li><li>Ordering.on排序</li></ol><p>Spark中的排序模块，顾名思义，这篇文章都是说如何排序</p><h2 id="算子排序"><a href="#算子排序" class="headerlink" title="算子排序"></a>算子排序</h2><p>的确，在Spark中有很多算子可以排序，可以给数组排序，可以给键值对排序，我们会使用算子引入排序，然后再重点介绍如何使用隐式转换达到排序的效果。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">(<span class="string">"西瓜 20 100"</span>, <span class="string">"苹果 10 500"</span>, <span class="string">"香蕉 10 30"</span>, <span class="string">"菠萝 30 200"</span>)</span><br></pre></td></tr></table></figure><p>现在我们有一行数据，我们如何使它按价格降序排序？</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//TODO List中每个东西的含义：名称name、价格price、库存amount</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="string">"西瓜 20 100"</span>, <span class="string">"苹果 10 500"</span>, <span class="string">"香蕉 10 30"</span>, <span class="string">"菠萝 30 200"</span>))</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//拆分map拿出来 ==&gt; 返回tuple(productData) ==&gt; 排序sortBy</span></span><br><span class="line"><span class="keyword">val</span> mapRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    (name, price, amount)</span><br><span class="line">&#125;)</span><br><span class="line">mapRDD.sortBy(-_._2).collect().foreach(println);</span><br></pre></td></tr></table></figure><p>也许你会认为很简单，那么现在要求按价格降序排序，价格相同库存降序排序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//拆分 ==&gt; 返回tuple() ==&gt; 排序sortBy(x =&gt; (-x._2,-x._3))</span></span><br><span class="line"><span class="keyword">val</span> mapRDD2 = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    (name, price, amount)</span><br><span class="line">&#125;)</span><br><span class="line">mapRDD2.sortBy(x=&gt;(-x._2,-x._3)).collect().foreach(println);</span><br></pre></td></tr></table></figure><p>的确，使用算子排序很简单，但是简洁的代码，能让你回想起几个月前的这几行是什么意思吗？</p><h2 id="面向对象排序"><a href="#面向对象排序" class="headerlink" title="面向对象排序"></a>面向对象排序</h2><p>现在我们引入面向对象的方式来排序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//TODO 面向对象的方式实现</span></span><br><span class="line"><span class="keyword">val</span> productRDD: <span class="type">RDD</span>[<span class="type">ProductCaseClass</span>] = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Products</span>(name, price, amount)</span><br><span class="line">&#125;)</span><br><span class="line">productRDD.sortBy(x=&gt;x).collect().foreach(println)</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Products</span>(<span class="params">val name: <span class="type">String</span>, val price: <span class="type">Double</span>, val amount: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">Ordered</span>[<span class="type">Products</span>]</span></span><br><span class="line"><span class="class">    <span class="keyword">with</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = <span class="string">s"<span class="subst">$name</span> | <span class="subst">$price</span> | <span class="subst">$amount</span>"</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">Products</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        that.price.toInt-<span class="keyword">this</span>.price.toInt</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>你是不是傻？case class 自己实现了序列化，实现了toString、equals、hashCode，用起来还不需要new，创建class干啥？</p><p>于是再次改造</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> productRDD: <span class="type">RDD</span>[<span class="type">ProductCaseClass</span>] = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    <span class="type">ProductCaseClass</span>(name, price, amount)</span><br><span class="line">&#125;)</span><br><span class="line">productRDD.sortBy(x=&gt;x).collect().foreach(println)</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ProductCaseClass</span>(<span class="params">name:<span class="type">String</span>,price:<span class="type">Double</span>,amount:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Ordered</span>[<span class="type">ProductCaseClass</span>]</span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> =  <span class="string">s"case class:  <span class="subst">$name</span> | <span class="subst">$price</span> | <span class="subst">$amount</span>"</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">ProductCaseClass</span>): <span class="type">Int</span> = that.price.toInt - <span class="keyword">this</span>.price.toInt</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="隐式转换排序"><a href="#隐式转换排序" class="headerlink" title="隐式转换排序"></a>隐式转换排序</h2><p>于是呼~ 不爽，现在只给你一个最普通的类，给我增强出带排序功能的类。这不是隐式转换吗？</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//一个最普通的类,不实现ordered排序的功能</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductsInfo</span>(<span class="params">val name: <span class="type">String</span>, val price: <span class="type">Double</span>, val amount: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = <span class="string">s"ProductsInfo:  <span class="subst">$name</span> | <span class="subst">$price</span> | <span class="subst">$amount</span>"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>那么我们该如何改造它？</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> productRDD: <span class="type">RDD</span>[<span class="type">ProductsInfo</span>] = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ProductsInfo</span>(name, price, amount)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>我就把数据切分出来，返回一个<code>ProductsInfo</code>，其他啥都不干</p><p>现在最重要的就是实现隐式转换，将ProductsInfo增加排序的功能，排序的规则还要自定义</p><ol><li><p>隐式方法/转换 </p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">ProductsInfo2Orderding</span></span>(productsInfo:<span class="type">ProductsInfo</span>):<span class="type">Ordered</span>[<span class="type">ProductsInfo</span>] = <span class="keyword">new</span> <span class="type">Ordered</span>[<span class="type">ProductsInfo</span>] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">ProductsInfo</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span> (that.price-productsInfo.price&gt;<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="number">1</span></span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (that.price-productsInfo.price==<span class="number">0</span> &amp;&amp; that.amount-productsInfo.amount &gt;<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="number">1</span></span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="number">-1</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>隐式变量</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> <span class="type">ProductsInfo2Orderding</span>:<span class="type">Ordering</span>[<span class="type">ProductsInfo</span>] = <span class="keyword">new</span> <span class="type">Ordering</span>[<span class="type">ProductsInfo</span>] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">ProductsInfo</span>, y: <span class="type">ProductsInfo</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        y.amount - x.amount</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>隐式对象</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">ProductsInfo22Orderding</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[<span class="type">ProductsInfo</span>]</span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">ProductsInfo</span>, y: <span class="type">ProductsInfo</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        y.amount - x.amount</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>这三种方式都可以达到增强<code>ProductsInfo</code>的功能，他们都离不开一个公式：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">x2y</span></span>(普通的x):牛逼的y = <span class="keyword">new</span> 牛逼的y(普通的x)</span><br></pre></td></tr></table></figure><p>上面的隐式转换就是这个公式的直接应用</p><h2 id="Ordering-on排序"><a href="#Ordering-on排序" class="headerlink" title="Ordering.on排序"></a>Ordering.on排序</h2><p>你也许会说，够了吧，这么多种方式排序了，累都累死人了，那么我告诉有一种方法，不需要case class、不需要class、不需要隐式转换，只有一行代码就能排序，学不学？</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> productRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    (name, price, amount)<span class="comment">//请注意，我们这里productRDD返回的是一个tuple</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>实现排序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * (Double,Int) : 定义排序规则的返回值类型,可以是class</span></span><br><span class="line"><span class="comment"> * (String,Double,Int) : 进来数据的类型</span></span><br><span class="line"><span class="comment"> * (x =&gt; (-x._2,-x._3)) : 定义排序的规则</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> order = <span class="type">Ordering</span>[(<span class="type">Double</span>,<span class="type">Int</span>)].on[(<span class="type">String</span>,<span class="type">Double</span>,<span class="type">Int</span>)](x =&gt; (-x._2,-x._3))</span><br></pre></td></tr></table></figure><p>是不是看懵逼了？使用<code>productRDD</code>调用<code>sortBy()</code>就输出了排序后的结果</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">productRDD.sortBy(x =&gt; x).print()</span><br></pre></td></tr></table></figure><p>结果:</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">(菠萝,<span class="number">30.0</span>,<span class="number">200</span>)</span><br><span class="line">(西瓜,<span class="number">20.0</span>,<span class="number">100</span>)</span><br><span class="line">(苹果,<span class="number">10.0</span>,<span class="number">500</span>)</span><br><span class="line">(香蕉,<span class="number">10.0</span>,<span class="number">30</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>执行Hive SQL/MR 报错：Current usage: 77.8mb of 512.0mb physical memory used; 1.1gb of 1.0gb virtual memory used. Killing container.</title>
      <link href="/2020/03/25/error/3/"/>
      <url>/2020/03/25/error/3/</url>
      
        <content type="html"><![CDATA[<p>从错误消息中，可以看到使用的虚拟内存超过了当前1.0gb的限制。这可以通过两种方式解决：</p><p><strong>禁用虚拟内存限制检查</strong></p><p>YARN只会忽略该限制；为此，请将其添加到您的<code>yarn-site.xml</code>：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether virtual memory limits will be enforced for containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>此设置的默认值为<code>true</code>。</p><p><strong>增加虚拟内存与物理内存的比率</strong></p><p>在<code>yarn-site.xml</code>更改中，此值将高于当前设置</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Ratio between virtual memory to physical memory when setting memory limits for containers. Container allocations are expressed in terms of physical memory, and virtual memory usage is allowed to exceed this allocation by this ratio.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>默认是 <code>2.1</code></p><p>还可以增加分配给容器的物理内存量。</p><p>注意：更改配置后不要忘记重新启动yarn。</p><hr><p>如果不能修改集群配置，我们可以参考这么做：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">-Dmapreduce.map.memory.mb=<span class="number">4096</span></span><br></pre></td></tr></table></figure><hr><p>在运行MR的作业中我们需要关心一下几个参数：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">yarn.nodemanager.resource.memory-mb</span><br><span class="line">yarn.scheduler.minimum-allocation-mb</span><br><span class="line">yarn.scheduler.maximum-allocation-mb</span><br><span class="line">yarn.nodemanager.vmem-check-enabled</span><br><span class="line">yarn.nodemanager.vmem-pmem-ratio</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapJoin，文件在HDFS上Idea报错：File does not exist: /xxx/yyy.txt#yyy.txt</title>
      <link href="/2020/03/25/error/2/"/>
      <url>/2020/03/25/error/2/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.io.FileNotFoundException: File does not exist: /data/dept.txt#dept.txt</span><br></pre></td></tr></table></figure><p>先去HDFS上确定文件是否存在，文件不存在，put文件上去，再次运行</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://192.168.91.10:9000/data/emp.txt</span><br></pre></td></tr></table></figure><p>有是Path找不到，再去HDFS上检查这个文件是否存在，文件不存在，再次put上去，然后运行</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INFO DFSClient: Could not obtain BP-1292531802-192.168.181.10-1583457649867:blk_1073746058_5236 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.91.10:50010,DS-f8088840-5065-4320-8b51-4563ccec125a,DISK] Dead nodes:  DatanodeInfoWithStorage[192.168.91.10:50010,DS-f8088840-5065-4320-8b51-4563ccec125a,DISK]. Will get new block locations from namenode and retry...</span><br><span class="line">WARN DFSClient: DFS chooseDataNode: got # 2 IOException, will wait for 3263.5374223592803 msec.</span><br><span class="line">WARN BlockReaderFactory: I/O error constructing remote block reader.</span><br><span class="line">java.net.ConnectException: Connection timed out: no further information</span><br></pre></td></tr></table></figure><p>继续报错，连接不到DataNode的块，去命令行输入jps发现进程没挂，然后我怀疑是因为我的host没有配置hadoop的ip映射关系，于是配上后，继续运行</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java.lang.Exception: java.io.FileNotFoundException: dept.txt (系统找不到指定的文件。)</span><br><span class="line">Caused by: java.io.FileNotFoundException: dept.txt (系统找不到指定的文件。)</span><br></pre></td></tr></table></figure><p>不出所望的再次报错，这次的日志多，上下文分析一下</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">WARN FileUtil: Command &apos;G:\hadoop-2.7.2\bin\winutils.exe symlink E:\Java\hadoop\hadoop-client\dept.txt \tmp\hadoop-Tunan\mapred\local\1585106809398\dept.txt&apos; failed 1 with: CreateSymbolicLink error (1314): ???????????</span><br><span class="line">WARN LocalDistributedCacheManager: Failed to create symlink: \tmp\hadoop-Tunan\mapred\local\1585106809398\dept.txt &lt;- E:\Java\hadoop\hadoop-client/dept.txt</span><br></pre></td></tr></table></figure><p>由于我们给hdfs上面文件的路径创建了一个链接，也就是symlink，我们发现这个两个警告是symlink创建失败，于是使用管理员的身份启动Idea，解决</p><hr><p>需要注意是除了管理员身份启动Idea，还需要在在程序的main方法下开启symlink的功能，才能在<code>new FileInputStream(new File(&quot;dept.txt&quot;))</code>中使用</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">FileSystem.enableSymlinks();</span><br></pre></td></tr></table></figure><p>还需要注意的是本地操作HDFS的一些配置</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"hadoop"</span>);</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://192.168.91.10:9000"</span>);</span><br><span class="line">conf.set(<span class="string">"dfs.client.use.datanode.hostname"</span>, <span class="string">"true"</span>);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark之Transformations&amp;Action</title>
      <link href="/2020/03/24/spark/2/"/>
      <url>/2020/03/24/spark/2/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>Transformations</li><li>Action</li></ol><h2 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h2><p>Transformations的特点是lazy的，和Scala中的lazy该念一致：延迟/懒加载，也就是不会立刻执行，只有等待遇到第一个action才会去提交作业到Spark上</p><h3 id="转换算子"><a href="#转换算子" class="headerlink" title="转换算子"></a>转换算子</h3><p><strong>map</strong> 作用到每一个元素</p><p>输入:任意类型的函数，输出:泛型U类型的函数，返回RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure><p><strong>mapPartitions</strong> 作用到每一个分区</p><p>输入:一个可迭代的类型T，输出:一个可迭代的类型U，返回RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure><p><strong>mapPartitionsWithIndex</strong> 作用到每一个分区并打印分区数</p><p>输入:分区索引，可迭代的类型T，输出:可迭代的类型U，返回RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    f: (<span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure><p><strong>glom()</strong> 按分区返回数组</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">glom</span></span>(): <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">T</span>]]</span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD.glom().collect().foreach(f=&gt;f.foreach(x =&gt; println(_)))</span><br></pre></td></tr></table></figure><p><strong>filter()</strong> 过滤</p><p>输入:输入一个函数T，输出:一个布尔值，返回一个RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Boolean</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p><strong>sample()</strong> 取样</p><p>输入:是否放回的布尔值，抽出来的概率，返回一个RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(</span><br><span class="line">      withReplacement: <span class="type">Boolean</span>,</span><br><span class="line">      fraction: <span class="type">Double</span>,</span><br><span class="line">      seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p><strong>distinct(x)</strong> 去重 ==&gt; numPartitions可指定分区</p><p>输入的必须是RDD，返回的也是一个RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dist: <span class="type">RDD</span>[<span class="type">Int</span>] = listRDD3.distinct()</span><br></pre></td></tr></table></figure><p><strong>coalesce(x)</strong> 重点(小文件相关场景大量使用):   ==&gt; reduce数量决定最终输出的文件数，coalesce的作用是减少到指定分区数(x)，减少分区是窄依赖<br>==&gt; Spark作业遇到shuffle 会切分stage<br>输入一个分区数，返回一个重分区后的RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coalesce</span></span>(numPartitions: <span class="type">Int</span>, shuffle: <span class="type">Boolean</span> = <span class="literal">false</span>,</span><br><span class="line">               partitionCoalescer: <span class="type">Option</span>[<span class="type">PartitionCoalescer</span>] = <span class="type">Option</span>.empty)</span><br><span class="line">              (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>)</span><br><span class="line">      : <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD2.coalesce(<span class="number">1</span>).getNumPartitions</span><br></pre></td></tr></table></figure><h3 id="双value算子"><a href="#双value算子" class="headerlink" title="双value算子"></a>双value算子</h3><p><strong>zip()</strong> 拉链 ==&gt; 不同分区和不同元素都不能用</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zip</span></span>[<span class="type">A1</span> &gt;: <span class="type">A</span>, <span class="type">B</span>, <span class="type">That</span>](that: <span class="type">GenIterable</span>[<span class="type">B</span>])(<span class="keyword">implicit</span> bf: <span class="type">CanBuildFrom</span>[<span class="type">Repr</span>, (<span class="type">A1</span>, <span class="type">B</span>), <span class="type">That</span>]): <span class="type">That</span></span><br></pre></td></tr></table></figure><p><strong>zipWithIndex()</strong> 打印拉链所在的分区</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zipWithIndex</span></span>[<span class="type">A1</span> &gt;: <span class="type">A</span>, <span class="type">That</span>](<span class="keyword">implicit</span> bf: <span class="type">CanBuildFrom</span>[<span class="type">Repr</span>, (<span class="type">A1</span>, <span class="type">Int</span>), <span class="type">That</span>]): <span class="type">That</span></span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> name = <span class="type">List</span>(<span class="string">"张三"</span>, <span class="string">"李四"</span>, <span class="string">"王五"</span>)</span><br><span class="line"><span class="keyword">val</span> age = <span class="type">List</span>(<span class="number">19</span>, <span class="number">26</span>, <span class="number">38</span>)</span><br><span class="line"><span class="keyword">val</span> zipRDD: <span class="type">List</span>[((<span class="type">String</span>, <span class="type">Int</span>), <span class="type">Int</span>)] = name.zip(age).zipWithIndex</span><br></pre></td></tr></table></figure><p><strong>union()</strong> 并集 ==&gt; 分区数相加</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">union</span></span>[<span class="type">B</span> &gt;: <span class="type">A</span>, <span class="type">That</span>](that: <span class="type">GenSeq</span>[<span class="type">B</span>])(<span class="keyword">implicit</span> bf: <span class="type">CanBuildFrom</span>[<span class="type">Repr</span>, <span class="type">B</span>, <span class="type">That</span>]): <span class="type">That</span></span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line"><span class="keyword">val</span> list2 = <span class="type">List</span>(<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">8</span>)</span><br><span class="line"><span class="keyword">val</span> ints: <span class="type">List</span>[<span class="type">Int</span>] = list1.union(list2)</span><br></pre></td></tr></table></figure><p><strong>intersection()</strong> 交集</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">intersect</span></span>[<span class="type">B</span> &gt;: <span class="type">A</span>](that: <span class="type">GenSeq</span>[<span class="type">B</span>]): <span class="type">Repr</span></span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> inter: <span class="type">List</span>[<span class="type">Int</span>] = list1.intersect(list2)</span><br></pre></td></tr></table></figure><p><strong>subtract()</strong> 差集</p><p>输入的必须是RDD，返回的也是一个RDD</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def subtract(other: RDD[T]): RDD[T]</span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sub: <span class="type">RDD</span>[<span class="type">Int</span>] = listRDD2.subtract(listRDD3)</span><br></pre></td></tr></table></figure><p><strong>cartesian()</strong> 笛卡尔积</p><p>输入的必须是RDD，返回的也是一个RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cartesian</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](other: <span class="type">RDD</span>[<span class="type">U</span>]): <span class="type">RDD</span>[(<span class="type">T</span>, <span class="type">U</span>)]</span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> car = listRDD2.cartesian(listRDD3)</span><br></pre></td></tr></table></figure><h3 id="kv算子"><a href="#kv算子" class="headerlink" title="kv算子"></a>kv算子</h3><p><strong>mapValues</strong> 得到所有ky的函数</p><p>输入:一个函数V，输出:一个值U，返回key为K，value为U的键函数对RDD</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapValues</span></span>[<span class="type">U</span>](f: <span class="type">V</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">U</span>)]</span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapRDD.groupByKey().mapValues(_.sum).print()</span><br></pre></td></tr></table></figure><p><strong>sortBy(x)</strong> 降序指定-x，指定任意参数</p><p>输入键值对，指定排序的值，默认升序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortBy</span></span>[<span class="type">K</span>](</span><br><span class="line">      f: (<span class="type">T</span>) =&gt; <span class="type">K</span>,</span><br><span class="line">      ascending: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">      numPartitions: <span class="type">Int</span> = <span class="keyword">this</span>.partitions.length)</span><br><span class="line">      (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">K</span>], ctag: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p><strong>sortByKey(true|false)</strong> 只能根据key排序</p><p>默认升序为true，可指定降序为false</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortByKey</span></span>(ascending: <span class="type">Boolean</span> = <span class="literal">true</span>, numPartitions: <span class="type">Int</span> = self.partitions.length)</span><br><span class="line">      : <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapRDD.map(x=&gt;(x._2,x._1)).sortByKey(<span class="literal">false</span>).map(x=&gt;(x._2,x._1)).print()</span><br></pre></td></tr></table></figure><p><strong>groupByKey</strong> 返回的kv对中的函数可迭代<br>    ==&gt;每个数据都经过shuffle，到reduce聚合，数据量大</p><p>可指定分区数，返回一个PariRDD，包含一个Key和一个可迭代的Value</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapRDD.groupByKey().mapValues(_.sum).print()</span><br></pre></td></tr></table></figure><p><strong>reduceByKey()</strong> 对value做指定的操作，直接返回函数<br>    ==&gt;map端有Combiner先进行了一次预聚合操作，减少了网络IO传输的数据量，所以比groupByKey快<br>    ==&gt;groupByKey的shuffle数据量明显多于reduceByKey，所以建议使用reduceByKey</p><p>输入两个值，输出一个值，返回一个PariRDD，包含一个明确的Key和一个明确的Value</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure><p><strong>join()</strong></p><p>两个RDDjoin，返回一个PariRDD包含一个key，两个Value</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> mapRDD2 = sc.parallelize(<span class="type">List</span>((<span class="string">"zhaoliu"</span>, <span class="number">18</span>), (<span class="string">"zhangsan"</span>, <span class="number">22</span>), (<span class="string">"list"</span>, <span class="number">21</span>), (<span class="string">"wangwu"</span>, <span class="number">26</span>)))</span><br><span class="line"><span class="keyword">val</span> mapRDD3 = sc.parallelize(<span class="type">List</span>((<span class="string">"hongqi"</span>, <span class="string">"男"</span>), (<span class="string">"zhangsan"</span>, <span class="string">"男"</span>), (<span class="string">"list"</span>, <span class="string">"女"</span>), (<span class="string">"wangwu"</span>, <span class="string">"男"</span>)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))]</span><br></pre></td></tr></table></figure><p><strong>leftOuterJoin</strong></p><p>两个RDDjoin，返回一个PariRDD包含一个key，一个确定的左表Value值，一个Option类型的右表Value值，即可能为空</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leftOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">Option</span>[<span class="type">W</span>]))]</span><br></pre></td></tr></table></figure><p><strong>rightOuterJoin</strong></p><p>两个RDDjoin，返回一个PariRDD包含一个key，一个确定的右表Value值，一个Option类型的左表Value值，即可能为空</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rightOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Option</span>[<span class="type">V</span>], <span class="type">W</span>))]</span><br></pre></td></tr></table></figure><p><strong>fullOuterJoin</strong></p><p>两个RDDjoin，返回一个PariRDD包含一个key，一个Option类型的右表Value值，一个Option类型的左表Value值，即都可能为空</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fullOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Option</span>[<span class="type">V</span>], <span class="type">Option</span>[<span class="type">W</span>]))]</span><br></pre></td></tr></table></figure><p><strong>cogroup</strong> </p><p>作用和join类似，不同的是返回的结果是可迭代的，而join返回的是值，原因是join底层调用了cogroup</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W</span>]))]</span><br></pre></td></tr></table></figure><p>面试题：Spark Core 不使用distinct去重</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> listRDD3 = sc.parallelize(<span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">listRDD3.map(x=&gt;(x,<span class="literal">null</span>)).reduceByKey((x,y)=&gt;x).map(_._1).print()</span><br></pre></td></tr></table></figure><h2 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h2><p><strong>first()</strong></p><p>返回第一个元素，等于take(1)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">first</span></span>(): <span class="type">T</span></span><br></pre></td></tr></table></figure><p><strong>take()</strong>    </p><p>拿出指定的前N个元素,返回一个数组，结果为原始顺序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">take</span></span>(num: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p><strong>count()</strong></p><p>返回元素数量，是个Long型</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span> = sc.runJob(<span class="keyword">this</span>, <span class="type">Utils</span>.getIteratorSize _).sum</span><br></pre></td></tr></table></figure><p><strong>sum</strong> </p><p>求和，返回一个Double型</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(): <span class="type">Double</span></span><br></pre></td></tr></table></figure><p><strong>max</strong> </p><p>返回最大值，结果通过隐式转换排序过</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max</span></span>()(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">T</span></span><br></pre></td></tr></table></figure><p><strong>min</strong></p><p>返回最小值，结果通过隐式转换排序过</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">min</span></span>()(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">T</span></span><br></pre></td></tr></table></figure><p><strong>top()</strong></p><p>先排降序再返回前N个元素组成的<strong>数组</strong>，字典序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p>案例：升序排序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD.top(<span class="number">3</span>)(<span class="type">Ordering</span>.by(x =&gt; -x)).foreach(println)</span><br></pre></td></tr></table></figure><p><strong>takeOrdered</strong></p><p>先排降序再返回N个元素组成的<strong>数组</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeOrdered</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p>案例：升序排序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD.takeOrdered(<span class="number">3</span>)(<span class="type">Ordering</span>.by(x =&gt; x)).foreach(println)</span><br></pre></td></tr></table></figure><p><strong>reduce</strong></p><p>聚合，输入两个元素输出一个元素，类型相同</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(f: (<span class="type">T</span>, <span class="type">T</span>) =&gt; <span class="type">T</span>): <span class="type">T</span></span><br></pre></td></tr></table></figure><p><strong>foreach</strong></p><p>循环输出</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def foreach(f: T =&gt; Unit): Unit</span><br></pre></td></tr></table></figure><p><strong>foreachPartition</strong></p><p>分区循环输出</p><p>输入的是一个可迭代的类型T，输出Unit</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreachPartition</span></span>(f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Unit</span>): <span class="type">Unit</span></span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD.foreachPartition(x=&gt;x.foreach(println))</span><br></pre></td></tr></table></figure><p><strong>countByKey</strong></p><p>根据key统计个数，用作<strong>检测数据倾斜</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countByKey</span></span>(): <span class="type">Map</span>[<span class="type">K</span>, <span class="type">Long</span>]</span><br></pre></td></tr></table></figure><p><strong>lookup</strong></p><p>根据map中的键来取出相应的值的，</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lookup</span></span>(key: <span class="type">K</span>): <span class="type">Seq</span>[<span class="type">V</span>]</span><br></pre></td></tr></table></figure><p>案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapRDD.lookup(<span class="string">"zhangsan"</span>).foreach(println)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>编译Spark&amp;Idea配置Spark环境&amp;RDD五大特点&amp;Spark参数管理&amp;数据的读写</title>
      <link href="/2020/03/24/spark/1/"/>
      <url>/2020/03/24/spark/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>编译Spark</li><li>Idea配置Spark环境</li><li>RDD五大特点</li><li>Spark参数管理</li><li>数据的读写</li></ol><h2 id="编译Spark"><a href="#编译Spark" class="headerlink" title="编译Spark"></a>编译Spark</h2><p>作为一个Spark玩的6的攻城狮，第一步就是要学会如何编译Spark</p><ol><li><p>下载spark源码: <a href="http://spark.apache.org/" target="_blank" rel="noopener">官网</a>或者<a href="https://github.com/apache/spark" target="_blank" rel="noopener">github</a></p></li><li><p>查看官网<a href="看官方文档http://spark.apache.org/docs/latest/building-spark.html">编译文档</a>，切记注意版本号，不同版本号编译方式区别很大</p></li><li><p>修改相关配置</p><ol><li><p>注释掉make-distribution.sh脚本中的128行左右一下，使用固定的版本替代</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">VERSION=2.4.5</span><br><span class="line">SCALA_VERSION=2.12.10</span><br><span class="line">SPARK_HADOOP_VERSION=2.6.0-cdh5.16.2</span><br><span class="line">SPARK_HIVE=1</span><br></pre></td></tr></table></figure><p>替代==&gt;</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">#VERSION=$("$MVN" help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v "INFO"\</span><br><span class="line">#    | grep -v "WARNING"\</span><br><span class="line">#    | tail -n 1)</span><br><span class="line">#SCALA_VERSION=$("$MVN" help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v "INFO"\</span><br><span class="line">#    | grep -v "WARNING"\</span><br><span class="line">#    | tail -n 1)</span><br><span class="line">#SPARK_HADOOP_VERSION=$("$MVN" help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v "INFO"\</span><br><span class="line">#    | grep -v "WARNING"\</span><br><span class="line">#    | tail -n 1)</span><br><span class="line">#140 #SPARK_HIVE=$("$MVN" help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v "INFO"\</span><br><span class="line">#    | grep -v "WARNING"\</span><br><span class="line">#    | fgrep --count "<span class="tag">&lt;<span class="name">id</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">id</span>&gt;</span>";\</span><br><span class="line">#    # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\</span><br><span class="line">#    # because we use "set -o pipefail"</span><br><span class="line">#    echo -n)</span><br></pre></td></tr></table></figure></li><li><p>修改maven仓库地址，在253行左右，pom.xml文件</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">Maven Repository</span><br><span class="line"><span class="comment">&lt;!--&lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt; --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"></span><br><span class="line">加上cdh的下载地址</span><br><span class="line"><span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">releases</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">releases</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>false<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol></li><li><p>开始编译</p><p>maven编译前执行</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">export MAVEN_OPTS=<span class="string">"-Xmx2g -XX:ReservedCodeCacheSize=1g"</span></span><br></pre></td></tr></table></figure><p>make-distribution.sh编译不需要执行，我们这里使用make-distribution.sh编译，它的脚本自动执行了这句</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">./dev/change-scala-version.sh <span class="number">2.12</span></span><br><span class="line">./dev/make-distribution.sh --name <span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.16</span><span class="number">.2</span> --tgz -Pyarn -Phive -Phive-thriftserver -Pscala-<span class="number">2.12</span> -Phadoop-<span class="number">2.6</span> -Dhadoop.version=<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.16</span><span class="number">.2</span></span><br></pre></td></tr></table></figure></li><li><p>查看生成的jar包</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">/home/hadoop/app/spark-<span class="number">2.4</span><span class="number">.5</span>/spark-<span class="number">2.4</span><span class="number">.5</span>-bin-<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.16</span><span class="number">.2</span>.tgz</span><br></pre></td></tr></table></figure></li></ol><h2 id="Idea配置Spark环境"><a href="#Idea配置Spark环境" class="headerlink" title="Idea配置Spark环境"></a>Idea配置Spark环境</h2><ol><li><p>idae引入依赖</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.source</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.target</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>2.6.0-cdh5.16.2<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.12.10<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.tools.version</span>&gt;</span>2.12<span class="tag">&lt;/<span class="name">scala.tools.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">spark.version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_$&#123;scala.tools.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>配置阿里云和cdh的仓库地址</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>false<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>central<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>aliyun maven<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">layout</span>&gt;</span>default<span class="tag">&lt;/<span class="name">layout</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 是否开启发布版构件下载 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">releases</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">releases</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 是否开启快照版构件下载 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>false<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="RDD五大特点"><a href="#RDD五大特点" class="headerlink" title="RDD五大特点"></a>RDD五大特点</h2><p>先看官方介绍:</p><p>弹性分布式数据集(RDD)，是Spark的基本抽象。表示可以并行操作的不可变的、分区的元素集合。这个类包含所有RDDS上可用的基本操作，如<code>map</code>、<code>filter</code>和<code>filter</code>。</p><p><code>[[org.apache.spark.rdd.PairRDDFunctions]]</code>，包含仅在键值对的RDDs上可用的操作，如<code>groupByKey</code> 和<code>join</code> ; </p><p><code>[[org.apache.spark.rdd.DoubleRDDFunctions]]</code> ，包含仅在双精度的RDDs上可用的操作;</p><p><code>[[org.apache.spark.rdd.SequenceFileRDDFunctions]]</code>，包含可在RDDs上使用的操作，这些操作可以保存为序列文件。</p><p>所有的操作都可以通过隐式转换的方式在任何正确类型的RDD上自动使用(例如RDD[(Int, Int)]);</p><p>RDD: resilient distributed dataset(弹性分布式数据集)，</p><ol><li><p>弹性表示容错</p></li><li><p>分布式表示分区</p></li><li><p>数据集表示集</p></li></ol><p><strong>每个RDD有五个主要特征:</strong></p><ol><li>一个RDD由很多partition构成(block块对应partition)，在spark中，有多少partition就对应有多少个task来执行。</li><li>对RDD做计算，相当于对RDD的每个partition或split做计算</li><li>RDD之间有依赖关系，可溯源，容错机制</li><li>如果RDD里面存的数据是key-value形式，则可以进行重新分区</li><li>最优位置计算，也就是数据的本地性，移动计算而不是移动数据。</li></ol><p>RDD是一个顶级的抽象类，它有五个抽象方法，分别实现了这五个特性</p><ol><li><p>分区</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Implemented by subclasses to return the set of partitions in this RDD. This method will only</span></span><br><span class="line"><span class="comment"> * be called once, so it is safe to implement a time-consuming computation in it.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * The partitions in this array must satisfy the following property:</span></span><br><span class="line"><span class="comment"> *   `rdd.partitions.zipWithIndex.forall &#123; case (partition, index) =&gt; partition.index == index &#125;`</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>]</span><br></pre></td></tr></table></figure></li><li><p>计算</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * :: DeveloperApi ::</span></span><br><span class="line"><span class="comment">  * Implemented by subclasses to compute a given partition.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li><li><p>依赖</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only</span></span><br><span class="line"><span class="comment">  * be called once, so it is safe to implement a time-consuming computation in it.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = deps</span><br></pre></td></tr></table></figure></li><li><p>重新分区</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/** Optionally overridden by subclasses to specify how they are partitioned. */</span></span><br><span class="line"><span class="meta">@transient</span> <span class="keyword">val</span> partitioner: <span class="type">Option</span>[<span class="type">Partitioner</span>] = <span class="type">None</span></span><br></pre></td></tr></table></figure></li><li><p>最优位置</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Optionally overridden by subclasses to specify placement preferences.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPreferredLocations</span></span>(split: <span class="type">Partition</span>): <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Nil</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="Spark参数管理"><a href="#Spark参数管理" class="headerlink" title="Spark参数管理"></a>Spark参数管理</h2><ol><li><p>如果想要定义自己的参数传递到spark中去，一定要以<code>spark.</code>开头</p></li><li><p>如果想要获取spark中的参数的值，使用<code>sc.getConf.get(key)</code></p></li></ol><h2 id="数据的读写"><a href="#数据的读写" class="headerlink" title="数据的读写"></a>数据的读写</h2><ol><li><p>读本地数据：<br><code>sc.textFile(&quot;file://&quot;)</code> 需要添加<code>file://</code></p></li><li><p>读hdfs数据：<br><code>sc.textFile(&quot;&quot;)</code>  默认读hdfs 不需要加前缀<br>注意：textFile() 可以使用通配符匹配目录、指定文件、指定文件夹</p></li><li><p>wholeTextFiles()</p><p>如果使用的是wholeTextFiles() ，它会返回路径+内容</p></li><li><p>写本地</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">rdd.saveAsTestFile(<span class="string">"path"</span>)</span><br></pre></td></tr></table></figure></li><li><p>写HDFS需要配置</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"hadoop"</span>);</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://192.168.91.10:9000"</span>);</span><br><span class="line">conf.set(<span class="string">"dfs.client.use.datanode.hostname"</span>, <span class="string">"true"</span>);</span><br></pre></td></tr></table></figure></li><li><p>压缩写</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">saveAsTestFile(out,classOf[<span class="type">BZip2Codec</span>])</span><br></pre></td></tr></table></figure></li><li><p>对象写</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span>(<span class="params">name:<span class="type">String</span>,age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">p1</span> </span>= (<span class="string">"张三"</span>,<span class="number">18</span>)</span><br><span class="line"><span class="keyword">val</span> p2 = (<span class="string">"李四"</span>,<span class="number">21</span>)</span><br><span class="line">parallelize(<span class="type">List</span>(p1,p2)).saveAsObjectFile(<span class="string">"out"</span>)</span><br></pre></td></tr></table></figure></li><li><p>对象读</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc.objectFile[<span class="type">Persion</span>](<span class="string">"out"</span>).collect().foreach(println)</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala之闭包&amp;柯里化</title>
      <link href="/2020/03/23/scala/4/"/>
      <url>/2020/03/23/scala/4/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>闭包</li><li>方法与函数的区别</li><li>柯里化</li></ol><h2 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h2><p>说到柯里化必先说起闭包，我们先不关心闭包和柯里化是什么，而是看一个transformation</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> init:<span class="type">Int</span> = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> i = list.foldLeft[<span class="type">Int</span>](init)((x,y) =&gt; &#123;</span><br><span class="line">    println(<span class="string">s"init = <span class="subst">$init</span> | x = <span class="subst">$x</span> | y = <span class="subst">$y</span>"</span> )</span><br><span class="line">    x+y</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">println(i)</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">init = <span class="number">10</span> | x = <span class="number">10</span> | y = <span class="number">1</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">11</span> | y = <span class="number">2</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">13</span> | y = <span class="number">3</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">16</span> | y = <span class="number">4</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">20</span> | y = <span class="number">5</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">25</span> | y = <span class="number">6</span></span><br><span class="line"><span class="number">31</span></span><br></pre></td></tr></table></figure><p>从结果来看 <code>foldLeft</code> 需要三个参数，<code>init</code>初始值，变量x，变量y，然后将他们累加(不考虑其他运算规则)</p><p>现在我们来看看闭包的解释：</p><ol><li><p>闭包是一个函数，返回值依赖于声明在函数外部的一个或多个变量。</p></li><li><p>闭包通常来讲可以简单的认为是可以访问一个函数里面局部变量的另外一个函数。</p></li></ol><p><code>val i = list.foldLeft[Int](init)((x,y))</code> 是一个闭包，返回值依赖声明在函数外的<code>init</code></p><p>如果我们把<code>foldLeft[Int](init)((x,y))</code> 拆分成两个函数的话，一个是有明确参数引用的 <code>foldLeft[Int](init)</code>， 另外一个是匿名函数<code>((x,y))</code>，如果我们观察仔细的话，会发现上面的代码中，第一次输出的结果 <code>init = x</code>  我们就可以简单的认为匿名函数<code>((x,y))</code>访问了<code>flodLeft</code>中的局部变量<code>init</code> ,事实上，<code>x</code>第一次的值就是拿的<code>init</code> 。</p><p>我们现在可以总结闭包就是在函数外面声明了一个变量，在函数中引用了这个变量，就称之为闭包，其他函数可以依赖闭包(函数)中的这个变量。由于闭包是把外部变量包了进来，所以这个变量的生命周期和闭包的生命周期一致。</p><p>最后在看一个案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> factor = <span class="number">3</span></span><br><span class="line"><span class="keyword">val</span> multiplier = (i:<span class="type">Int</span>) =&gt; i * factor</span><br><span class="line">println(multiplier(<span class="number">3</span>))</span><br></pre></td></tr></table></figure><p>没错，<strong>函数的返回值依赖于声明在函数外部的一个或多个变量就是闭包。</strong></p><h2 id="方法与函数的区别"><a href="#方法与函数的区别" class="headerlink" title="方法与函数的区别"></a>方法与函数的区别</h2><p>还需要讲一下方法与函数区别，因为柯里化指的是将原来接受两个参数的方法变成新的接受一个参数的函数的过程。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">method</span> </span>= (x:<span class="type">Int</span>,y:<span class="type">Int</span>) =&gt; x+y</span><br><span class="line">method: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span><span class="comment">//方法</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> func = (x:<span class="type">Int</span>,y:<span class="type">Int</span>) =&gt; x+y</span><br><span class="line">func: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1307</span>/<span class="number">1309775952</span>@<span class="number">32</span>a1aabf<span class="comment">//函数</span></span><br></pre></td></tr></table></figure><ol><li>首先应该要知道=号右边的内容 <code>(x: Int, y: Int) =&gt; x + y</code>是一个函数体。</li><li>方法只能用def修饰，函数可以用def修饰，也可以用val修饰。</li><li>当函数用def来接收之后，不再显示为function，转换为方法。</li><li>方法可以省略参数，函数不可以。</li><li>函数可以作为方法的参数。</li></ol><h2 id="柯里化"><a href="#柯里化" class="headerlink" title="柯里化"></a>柯里化</h2><p>理解闭包后，我们看一个复杂的闭包案例：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">addBase</span></span>(x:<span class="type">Int</span>) = (y:<span class="type">Int</span>) =&gt; x+y</span><br><span class="line">addBase: (x: <span class="type">Int</span>)<span class="type">Int</span> =&gt; <span class="type">Int</span></span><br></pre></td></tr></table></figure><p>首先我们看到<code>addBase</code>是一个方法，<code>(y:Int) =&gt; x+y</code>是一个函数体，下面我们试试执行<code>addBase(x:Int)</code></p><p>会返回什么?</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; addBase(<span class="number">3</span>)</span><br><span class="line">res27: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1308</span>/<span class="number">761477414</span>@<span class="number">553</span>f7b1e</span><br></pre></td></tr></table></figure><p>返回了一个函数，现在我们明白了，<code>addBase(x:Int)</code>是一个<strong>方法</strong>，在传入具体的值后，返回了一个具体的<strong>函数</strong></p><p>到现在 “=” 号后面的 <code>(y:Int) =&gt; x+y</code> 这段还没有使用到，但是我们知道 “=” 号后面是一个函数体，给函数中传入y 返回 x+y? 我们试试。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> addThree = addBase(<span class="number">3</span>)<span class="comment">//使用个变量接收函数</span></span><br><span class="line">addThree: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1308</span>/<span class="number">761477414</span>@<span class="number">65e1</span>c98c</span><br><span class="line">scala&gt; addThree(<span class="number">4</span>)<span class="comment">//传入4</span></span><br><span class="line">res29: <span class="type">Int</span> = <span class="number">7</span><span class="comment">//输出7</span></span><br></pre></td></tr></table></figure><p>等等。。。这就是一个闭包啊，x=3是外部传入的给函数<code>addBase</code>的，这时的<code>addBase</code>就是函数，它的内部维护了一个变量x=3，这时另外一个函数<code>addThree</code> 在输出x+y前，访问了<code>addBase</code>中的x=3。</p><p>完全符合闭包的定义规则，<strong>函数的返回值依赖于声明在函数外部的一个或多个变量就是闭包</strong>。</p><p>上面的<code>addBase</code>方法使我们一次传入一个3一次传入一个4，那么有没有办法一次传入呢？</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; addBase(<span class="number">3</span>)(<span class="number">4</span>)</span><br><span class="line">res34: <span class="type">Int</span> = <span class="number">7</span></span><br></pre></td></tr></table></figure><p>没错，这就是柯里化，<strong>柯里化指的是将原来接受两个参数的方法变成新的接受一个参数的函数的过程。并且新的函数返回一个以原有第二个参数作为参数的函数。</strong></p><p>我们发现了上面闭包的代码其实就是柯里化的过程</p><p>现在我们总结下柯里化是什么?</p><ol><li><p>柯里化指的是将原来接受两个参数的方法变成新的接受一个参数的函数的过程。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">sum1</span></span>(x:<span class="type">Int</span>): <span class="type">Int</span> =&gt; <span class="type">Int</span> = (y:<span class="type">Int</span>)=&gt;x+y</span><br><span class="line">sum1: (x: <span class="type">Int</span>)<span class="type">Int</span> =&gt; <span class="type">Int</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> res1 = sum1(<span class="number">3</span>)</span><br><span class="line">res1: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1321</span>/<span class="number">962341885</span>@<span class="number">41</span>d283de</span><br><span class="line"></span><br><span class="line">scala&gt; res1(<span class="number">4</span>)</span><br><span class="line">res38: <span class="type">Int</span> = <span class="number">7</span></span><br></pre></td></tr></table></figure><p>==&gt;上面是接受两个参数，下面是接受一个参数</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">sum2</span></span>(x:<span class="type">Int</span>)(y:<span class="type">Int</span>): <span class="type">Int</span> = x+y</span><br><span class="line">sum2: (x: <span class="type">Int</span>)(y: <span class="type">Int</span>)<span class="type">Int</span></span><br><span class="line"></span><br><span class="line">scala&gt; sum2(<span class="number">2</span>)(<span class="number">3</span>)</span><br><span class="line">res39: <span class="type">Int</span> = <span class="number">5</span></span><br></pre></td></tr></table></figure></li><li><p>新的函数返回一个以原有第二个参数作为参数的函数。</p><p>意思就是原来要分两步做的事情，现在分一步就做好了，底层自动调用和返回。在这个过程中，使用了闭包。</p></li></ol><p>懵逼之余。。。返回最开始的<code>foldLeft</code>案例，看<code>foldLeft</code> 方法源码：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foldLeft</span></span>[<span class="type">B</span>](z: <span class="type">B</span>)(<span class="meta">@deprecatedName</span>(<span class="symbol">'f</span>) op: (<span class="type">B</span>, <span class="type">A</span>) =&gt; <span class="type">B</span>): <span class="type">B</span></span><br></pre></td></tr></table></figure><p>这其实就是一个柯里化应用</p><ol><li><p>折叠操作是一个递归的过程，将上一次的计算结果代入到函数中 </p></li><li><p>作为结果的参数在<code>foldLeft</code>是第一个参数，下个参数在<code>foldRight</code>是第二个参数</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> i = list.foldLeft[<span class="type">Int</span>](init)(_+_)</span><br></pre></td></tr></table></figure><p>==&gt;<code>init</code> = 10 作为初始值只用一次，然后(_+_)累加，累加的结果放在第一个参数位置</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">x = <span class="number">10</span> | y = <span class="number">1</span></span><br><span class="line">x = <span class="number">11</span> | y = <span class="number">2</span></span><br><span class="line">x = <span class="number">13</span> | y = <span class="number">3</span></span><br><span class="line">x = <span class="number">16</span> | y = <span class="number">4</span></span><br><span class="line">x = <span class="number">20</span> | y = <span class="number">5</span></span><br><span class="line">x = <span class="number">25</span> | y = <span class="number">6</span></span><br></pre></td></tr></table></figure></li></ol><p><strong>总结</strong></p><ol><li><p>柯里化技术在提高适用性还是在延迟执行或者固定易变因素等方面有着重要重要的作用，加上scala语言本身就是推崇简洁编码，使得同样功能的函数在定义与转换的时候会更加灵活多样。另外在Spark的源码中有大量运用scala柯里化技术的情况，需要掌握好该技术才能看得懂相关的源代码。</p></li><li><p>在scala柯里化中，闭包也发挥着重要的作用。所谓的闭包就是变量出了函数的定义域外在其他代码块还能其作用，这样的情况称之为闭包。就上述讨论的案例而言，如果没有闭包作用，那么转换后函数其实返回的匿名函数是无法在与第一个参数x相关结合的，自然也就无法保证其所实现的功能是跟原来一致的。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark之WC产生多少个RDD</title>
      <link href="/2020/03/20/spark/3/"/>
      <url>/2020/03/20/spark/3/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>WC产生多少个RDD</li></ol><h2 id="WC产生多少个RDD"><a href="#WC产生多少个RDD" class="headerlink" title="WC产生多少个RDD"></a>WC产生多少个RDD</h2><p>一句标准的WC产生了多少个RDD？</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> result = sc.textFile(<span class="string">"E:\\Java\\spark\\tunan-spark\\tunan-spark-core\\data\\wc.txt"</span>).flatMap(_.split(<span class="string">"\t"</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">result.saveAsTextFile(<span class="string">"out"</span>)</span><br></pre></td></tr></table></figure><ol><li><p>使用<code>toDebugString</code>方法查看RDD的数量</p><p>result.toDebugString(不包括<code>saveAsTextFile</code>方法)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">(<span class="number">2</span>) <span class="type">ShuffledRDD</span>[<span class="number">4</span>] at reduceByKey at wordcount.scala:<span class="number">11</span> []</span><br><span class="line"> +-(<span class="number">2</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at wordcount.scala:<span class="number">11</span> []</span><br><span class="line">    |  <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at flatMap at wordcount.scala:<span class="number">11</span> []</span><br><span class="line">    |  <span class="type">E</span>:\<span class="type">Java</span>\spark\tunan-spark\tunan-spark-core\data\wc.txt <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at wordcount.scala:<span class="number">11</span> []</span><br><span class="line">    |  <span class="type">E</span>:\<span class="type">Java</span>\spark\tunan-spark\tunan-spark-core\data\wc.txt <span class="type">HadoopRDD</span>[<span class="number">0</span>] at textFile at wordcount.scala:<span class="number">11</span> []</span><br></pre></td></tr></table></figure><p>上面的方法中：textFile算子中有HadoopRDD和MapPartitionsRDD；flatMap方法有MapPartitionsRDD；map方法有MapPartitionsRDD；reduceByKey方法有ShuffledRDD。</p><p>这里一共是5个RDD，如果加上saveAsTextFile方法中的一个MapPartitionsRDD，则一共是6个RDD，如果加上sort方法也是一样的算法。</p></li><li><p>查看源码的方式计算RDD的数量</p><p><code>sc.textFile(&quot;...&quot;)</code></p><p>textFile的作用是从HDFS、本地文件系统(在所有节点上可用)或任何hadoop支持的文件系统URI读取文本文件，并将其作为字符串的RDD返回。</p><p>path：受支持的文件系统上的文本文件的路径</p><p>minPartitions：建议的结果RDD的最小分区数，默认值是2</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(</span><br><span class="line">    path: <span class="type">String</span>,</span><br><span class="line">    minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[<span class="type">String</span>] = withScope &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    hadoopFile(path, classOf[<span class="type">TextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>],</span><br><span class="line">               minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在textFile值调用了hadoopFile方法，该方法传入了<code>path</code>，<code>TextInputFormat</code>(也就是mapreduce中的FileInputFormat方法，特点是按行读取)，<code>LongWritable</code>(mapreduce计算中的key，记录的是offset)，<code>Text</code>(mapreduce计算中的key，记录的是每行的内容)，<code>minPartitions</code>(分区数)，然后返回一个tuple，tuple记录的是key和value，我们这里做了一个处理，<code>.map(pair =&gt; pair._2.toString)</code>方法让结果只有内容，而忽略掉了offset。</p><p>继续看hadoopFile的源码</p><p>使用任意的InputFormat获取Hadoop文件的RDD，返回一个RDD类型的包含(offset，value)的元组</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hadoopFile</span></span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">      path: <span class="type">String</span>,<span class="comment">//目录下的输入数据文件，路径可以用逗号分隔路径作为输入列表</span></span><br><span class="line">      inputFormatClass: <span class="type">Class</span>[_ &lt;: <span class="type">InputFormat</span>[<span class="type">K</span>, <span class="type">V</span>]],<span class="comment">//要读取的数据的存储格式</span></span><br><span class="line">      keyClass: <span class="type">Class</span>[<span class="type">K</span>],<span class="comment">//key的类型</span></span><br><span class="line">      valueClass: <span class="type">Class</span>[<span class="type">V</span>],<span class="comment">//value的类型</span></span><br><span class="line">      minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]<span class="comment">//分区数，默认值是2</span></span><br><span class="line">= withScope &#123;assertNotStopped()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这是一种强制加载hdfs-site.xml的方法</span></span><br><span class="line">    <span class="type">FileSystem</span>.getLocal(hadoopConfiguration)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//一个Hadoop的配置文件大概10 KB,这是相当大的,所以广播它</span></span><br><span class="line">    <span class="keyword">val</span> confBroadcast = broadcast(<span class="keyword">new</span> <span class="type">SerializableConfiguration</span>(hadoopConfiguration))</span><br><span class="line">    <span class="keyword">val</span> setInputPathsFunc = (jobConf: <span class="type">JobConf</span>) =&gt; <span class="type">FileInputFormat</span>.setInputPaths(jobConf, path)<span class="comment">//设置作业环境</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">HadoopRDD</span>(<span class="comment">//创建一个HadoopRDD</span></span><br><span class="line">      <span class="keyword">this</span>,</span><br><span class="line">      confBroadcast,<span class="comment">//广播配置</span></span><br><span class="line">      <span class="type">Some</span>(setInputPathsFunc),<span class="comment">//作业环境也许可能出错，所以使用Some()</span></span><br><span class="line">      inputFormatClass,<span class="comment">//读取文件的格式化类</span></span><br><span class="line">      keyClass,<span class="comment">//key类型</span></span><br><span class="line">      valueClass,<span class="comment">//value类型</span></span><br><span class="line">      minPartitions).setName(path)<span class="comment">//分片数</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>因为Hadoop的RecordReader类为每条记录使用相同的Writable对象，直接保存或者直接使用aggregation或者shuffle将会产生很多对同一个对象的引用，所以我们保存、排序或者聚合操作writable对象前，要使用map方法做一个映射。</p><p>回到上一步的textFile方法中，Hadoop的返回值是一个包含offset和value的元组，我们只需要内容，所以使用map方法做一个映射，只拿元祖中的value即可</p><p><code>.map(pair =&gt; pair._2.toString)</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)<span class="comment">//初始化检查</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.map(cleanF))==&gt;(context, pid, iter) ==&gt; hadoopRDD, 分区<span class="type">ID</span>, 迭代器</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该map方法又创建一个MapPartitionsRDD，将map算子应用于每个分区的子RDD。这应用到了RDD五大特性之一的，对每个RDD做计算，实际上是对每个RDD的partition或者split做计算。由于MapPartitionsRDD较为复杂，暂不解析。</p><p><strong><em>到此，textFile产生了两个RDD，分别是HadoopRDD和MapPartitionsRDD。共两个RDD</em></strong></p></li></ol><p>   <code>.flatMap(_.split(&quot;\t&quot;))</code></p><p>   flatMap首先作用在每一个元素上，然后将结果扁平化，最后返回一个新的RDD</p>   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.flatMap(cleanF))<span class="comment">//对RDD的所有分区做计算</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>   <strong><em>到此，flatMap产生了一个RDD，是MapPartitionsRDD。共三个RDD</em></strong></p><p>   <code>.map((_, 1))</code></p><p>   将map作用在每一个元素上，然后返回一个新的RDD</p>   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.map(cleanF))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>   <strong><em>到此，map产生了一个RDD，是MapPartitionsRDD。共四个RDD</em></strong></p><p>   <code>.reduceByKey(_ + _)</code></p><p>   使用联合和交换reduce函数合并每个键的值。<strong>在将结果发送到reduce之前，这也将在每个mapper上本地执行合并，类似于MapReduce中的“combiner”。</strong>输出将使用现有分区器/并行度级别进行哈希分区。</p>   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">    reduceByKey(defaultPartitioner(self), func)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>   继续查看<code>reduceByKey(defaultPartitioner(self), func)</code>方法</p>   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(partitioner: <span class="type">Partitioner</span>, func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">    combineByKeyWithClassTag[<span class="type">V</span>]((v: <span class="type">V</span>) =&gt; v, func, func, partitioner)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>   继续查看<code>combineByKeyWithClassTag[V]((v: V) =&gt; v, func, func, partitioner)</code>方法</p><p>   这是一个具体干活的方法，它使用一组自定义聚合函数组合每个键的元素。将RDD[(K, V)]转换为RDD[(K, C)]类型的结果，用于“组合类型”C。这是一个复杂的方法，暂不做解析。</p>   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKeyWithClassTag</span></span>[<span class="type">C</span>](</span><br><span class="line">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    partitioner: <span class="type">Partitioner</span>,</span><br><span class="line">    mapSideCombine: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">    serializer: <span class="type">Serializer</span> = <span class="literal">null</span>)(<span class="keyword">implicit</span> ct: <span class="type">ClassTag</span>[<span class="type">C</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = self.withScope &#123;</span><br><span class="line">    require(mergeCombiners != <span class="literal">null</span>, <span class="string">"mergeCombiners must be defined"</span>) <span class="comment">// required as of Spark 0.9.0</span></span><br><span class="line">    <span class="keyword">if</span> (keyClass.isArray) &#123;</span><br><span class="line">        <span class="keyword">if</span> (mapSideCombine) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Cannot use map-side combining with array keys."</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (partitioner.isInstanceOf[<span class="type">HashPartitioner</span>]) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"HashPartitioner cannot partition array keys."</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> aggregator = <span class="keyword">new</span> <span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">        self.context.clean(createCombiner),</span><br><span class="line">        self.context.clean(mergeValue),</span><br><span class="line">        self.context.clean(mergeCombiners))</span><br><span class="line">    <span class="keyword">if</span> (self.partitioner == <span class="type">Some</span>(partitioner)) &#123;</span><br><span class="line">        self.mapPartitions(iter =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> context = <span class="type">TaskContext</span>.get()</span><br><span class="line">            <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context, aggregator.combineValuesByKey(iter, context))</span><br><span class="line">        &#125;, preservesPartitioning = <span class="literal">true</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](self, partitioner)<span class="comment">//创建ShuffledRDD</span></span><br><span class="line">        .setSerializer(serializer)</span><br><span class="line">        .setAggregator(aggregator)</span><br><span class="line">        .setMapSideCombine(mapSideCombine)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>   <strong><em>到此，reduceByKey产生了一个RDD，是ShuffledRDD。共五个RDD</em></strong></p><p>   <code>.saveAsTextFile(&quot;...&quot;)</code></p><p>   将每个元素使用字符串表示形式，将此RDD保存为文本文件。</p>   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsTextFile</span></span>(path: <span class="type">String</span>): <span class="type">Unit</span> = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> nullWritableClassTag = implicitly[<span class="type">ClassTag</span>[<span class="type">NullWritable</span>]]</span><br><span class="line">    <span class="keyword">val</span> textClassTag = implicitly[<span class="type">ClassTag</span>[<span class="type">Text</span>]]</span><br><span class="line">    <span class="keyword">val</span> r = <span class="keyword">this</span>.mapPartitions &#123; iter =&gt;<span class="comment">//注意这里</span></span><br><span class="line">        <span class="keyword">val</span> text = <span class="keyword">new</span> <span class="type">Text</span>()</span><br><span class="line">        iter.map &#123; x =&gt;</span><br><span class="line">            text.set(x.toString)</span><br><span class="line">            (<span class="type">NullWritable</span>.get(), text)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">RDD</span>.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, <span class="literal">null</span>)</span><br><span class="line">    .saveAsHadoopFile[<span class="type">TextOutputFormat</span>[<span class="type">NullWritable</span>, <span class="type">Text</span>]](path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>   通过每次将一个分区的数据以流的方式传入到HDFS中再关闭流</p>   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanedF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>(<span class="comment">//创建MapPartitionsRDD</span></span><br><span class="line">    <span class="keyword">this</span>,</span><br><span class="line">    (context: <span class="type">TaskContext</span>, index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedF(iter),</span><br><span class="line">    preservesPartitioning)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>   <del><strong><em>到此，saveAsTextFile产生了一个RDD，是MapPartitionsRDD。共六个RDD</em></strong></del></p><p>   <em>20200325更新：</em></p><p>   在最后的saveAsTextFile()算子中，我们忽略了一个RDD，它就是是<code>PairRDDFunctions</code>，这个RDD是通过RDD隐式转换过来的</p>   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">rddToPairRDDFunctions</span></span>[<span class="type">K</span>, <span class="type">V</span>](rdd: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)])</span><br><span class="line">(<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>], vt: <span class="type">ClassTag</span>[<span class="type">V</span>], ord: <span class="type">Ordering</span>[<span class="type">K</span>] = <span class="literal">null</span>): <span class="type">PairRDDFunctions</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">PairRDDFunctions</span>(rdd)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>   <strong><em>修正后的 RDD数量是 7个</em></strong></p><ol start="3"><li>总结：使用toDebugString方法，简单的看到了生成了多少个RDD，通过阅读源码的方式，详细了解到了生成了多少个RDD，他们分别做了什么事情。我们这个流程生成了<del>6</del>7个RDD，如果对结果进行排序，也是相同的方法可以看到答案。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala之var和val的比较&amp;lazy懒加载</title>
      <link href="/2020/03/19/scala/3/"/>
      <url>/2020/03/19/scala/3/</url>
      
        <content type="html"><![CDATA[<h3 id="1：内容是否可变：val修饰的是不可变的，var修饰是可变的"><a href="#1：内容是否可变：val修饰的是不可变的，var修饰是可变的" class="headerlink" title="1：内容是否可变：val修饰的是不可变的，var修饰是可变的"></a>1：内容是否可变：val修饰的是不可变的，var修饰是可变的</h3><p>下面看一段代码，你猜是否有错误</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ValAndVar</span> </span>&#123;</span><br><span class="line">    <span class="comment">//val 修饰由于不可变性必须初始化</span></span><br><span class="line">    <span class="keyword">val</span> <span class="type">LOVE</span>:<span class="type">String</span> = _</span><br><span class="line">    <span class="keyword">var</span> <span class="type">SEX</span>:<span class="type">String</span>  = _</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> name = <span class="string">"tunan"</span></span><br><span class="line">        <span class="keyword">var</span> age = <span class="number">18</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//val 修饰由于不可变性不能重新赋值</span></span><br><span class="line">        name = <span class="string">"zhangsan"</span></span><br><span class="line">        age = <span class="number">19</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>真实的结果:</p><p><img src="https://yerias.github.io/scala_img/val%E5%92%8Cvar%E7%9A%84%E5%8F%AF%E5%8F%98%E6%80%A7%E6%AF%94%E8%BE%83.jpg" alt="val和var的可变性比较"></p><ol><li>val是不可变的，所以修饰的变量必须初始化</li><li>val是不可变的，所以修饰的变量不能重新赋值</li><li>val是不可变的，所以是多线程安全的</li><li>val是不可变的，不用担心会改变它修饰的对象的状态</li><li>val是不可变的，增强了代码的可读性，不用担心它的内容发生变化</li><li>var是可变的，可以增强代码的灵活性，和val互补</li></ol><h3 id="2：val修饰的变量在编译后类似于java中的中的变量被final修饰"><a href="#2：val修饰的变量在编译后类似于java中的中的变量被final修饰" class="headerlink" title="2：val修饰的变量在编译后类似于java中的中的变量被final修饰"></a>2：val修饰的变量在编译后类似于java中的中的变量被final修饰</h3><ol><li><p>先看源代码</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ValAndVar</span> </span>&#123;</span><br><span class="line">    <span class="keyword">val</span> <span class="type">LOVE</span>:<span class="type">String</span> = <span class="string">"篮球"</span></span><br><span class="line">    <span class="keyword">var</span> <span class="type">SEX</span>:<span class="type">String</span>  = _</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> name:<span class="type">String</span> = <span class="string">"tunan"</span></span><br><span class="line">        <span class="keyword">var</span> age:<span class="type">Int</span> = <span class="number">18</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>再看反编译后的代码(只保留了我们想要的部分)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ValAndVar$</span> </span>&#123;</span><br><span class="line">  public static <span class="type">ValAndVar</span>$ <span class="type">MODULE</span>$;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">String</span> <span class="type">LOVE</span>;</span><br><span class="line">    </span><br><span class="line">  public void main(<span class="type">String</span>[] args) &#123;</span><br><span class="line">    <span class="type">String</span> name = <span class="string">"tunan"</span>;</span><br><span class="line">    int age = <span class="number">18</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们发现这段代码很诡异，scala中的类变量，在字节码层面转换成了 parivate final ，而main方法中的变量却没有添加final修饰，这是否证明编译器有问题？</p><p>答案是否定的，对于val或者final都只是给编译器用的，编译器如果发现你给此变量重新赋值会抛出错误。同时字节码(bytecode)不具备表达一个局部变量是不可变(immutable)的能力。</p><p>所以就有了现在结果。</p></li></ol><h3 id="3：lazy修饰符可以修饰变量，但是这个变量必须是val修饰的"><a href="#3：lazy修饰符可以修饰变量，但是这个变量必须是val修饰的" class="headerlink" title="3：lazy修饰符可以修饰变量，但是这个变量必须是val修饰的"></a>3：lazy修饰符可以修饰变量，但是这个变量必须是val修饰的</h3><ol><li><p>在证明lazy修饰的变量必须是val之前，我们先看看lazy是什么？</p><p>Scala中使用关键字lazy来定义惰性变量，实现延迟加载(懒加载)。<br>惰性变量只能是不可变变量，并且只有在调用惰性变量时，才会去实例化这个变量。</p><p>在Java中，一般使用get和set实现延迟加载(懒加载)，而在Scala中对延迟加载这一特性提供了语法级别的支持:</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> name = initName()</span><br></pre></td></tr></table></figure><p>使用lazy关键字修饰变量后，只有在使用该变量时，才会调用其实例化方法。也就是说在定义name=initName()时并不会调用initName()方法，只有在后面的代码中使用变量name时才会调用initName()方法。</p><p>如果<strong>不使用lazy关键字对变量修饰</strong>，那么变量name是立即实例化的，下面将通过一组案例对比认识：</p><p><code>不使用lazy修饰的方法：</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LazyDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initName</span></span>:<span class="type">String</span>=&#123;</span><br><span class="line">        println(<span class="string">"初始化initName"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"返回intName"</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="comment">//        lazy val name = initName</span></span><br><span class="line">        <span class="keyword">val</span> name = initName<span class="comment">//程序走到这里，就打印了initName的输出语句</span></span><br><span class="line">        println(<span class="string">"hello，欢迎来到图南之家"</span>)</span><br><span class="line">        println(name)<span class="comment">//程序走到这里，打印initName的返回值</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的name没有使用lazy关键字进行修饰，所以name是立即实例化的。</p><p>结果：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">初始化initName</span><br><span class="line">hello，欢迎来到图南之家</span><br><span class="line">返回intName</span><br></pre></td></tr></table></figure><p><code>使用lazy修饰后的方法：</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LazyDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initName</span></span>:<span class="type">String</span>=&#123;</span><br><span class="line">        println(<span class="string">"初始化initName"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"返回intName"</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">lazy</span> <span class="keyword">val</span> name = initName<span class="comment">//不调用initName方法，即不打印initName中的输出语句</span></span><br><span class="line"><span class="comment">//        val name = initName</span></span><br><span class="line">        println(<span class="string">"hello，欢迎来到图南之家"</span>)<span class="comment">//打印main方法中的输出语句</span></span><br><span class="line">        println(name)<span class="comment">//打印initName的输出语句，打印返回值</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在声明name时，并没有立即调用实例化方法initName(),而是在使用name时，才会调用实例化方法,并且无论调用多少次，实例化方法只会执行一次。</p><p>结果：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">hello，欢迎来到图南之家</span><br><span class="line">初始化initName</span><br><span class="line">返回intName</span><br></pre></td></tr></table></figure></li><li><p>证明lazy只能修饰的变量只能使用val</p><p>我们发现name都是使用val修饰的，如果我们使用var修饰会怎么样呢？</p><p><img src="https://yerias.github.io/scala_img/lazy%E6%87%92%E5%8A%A0%E8%BD%BD.jpg" alt="lazy懒加载"></p><p>我们发现报错：<code>&#39;lazy&#39; modifier allowed only with value definitions</code></p><p>实际上就是认为<code>lazy</code>修饰的变量只能<code>val</code>修饰</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>执行Saprk or Scala程序:找不到或者无法加载主类 xxx</title>
      <link href="/2020/03/19/error/1/"/>
      <url>/2020/03/19/error/1/</url>
      
        <content type="html"><![CDATA[<p>使用百度和谷歌，测试了广大程序员给出的各种解决办法，包括更换jdk版本(之前也是jdk8，小版本不同)，更换scala 的版本(2.12大版本内更换小版本，因为我的spark2.4.5需要的scala版本是2.12)，更换idea的输出路径，重构代码，清理idea的缓存，删除依赖重新下载，重建项目，导入其他同学的项目，皆出现<code>找不到或者无法加载主类 xxx</code>异常</p><p>最后重装idea，解决！</p>]]></content>
      
      
      <categories>
          
          <category> Error </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Error </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA的包装类解析</title>
      <link href="/2020/03/17/java/11/"/>
      <url>/2020/03/17/java/11/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>什么是包装类</li><li>自动装箱和自动拆箱</li><li>包装类可以为null，而基本类型不可以</li><li>包装类型可用于泛型，而基本类型不可以</li><li>基本类型比包装类型更高效</li><li>两个包装类型的值可以相同，但却可以不相等</li></ol><h2 id="什么是包装类"><a href="#什么是包装类" class="headerlink" title="什么是包装类"></a>什么是包装类</h2><p>Java的每个基本类型都有对应的包装类型，比如说int的包装类型是Integer，double的包装类型是Double。</p><h2 id="自动装箱和自动拆箱"><a href="#自动装箱和自动拆箱" class="headerlink" title="自动装箱和自动拆箱"></a>自动装箱和自动拆箱</h2><p>既然有了基本类型和包装类型，肯定有些是要在他们之间进行转换。把基本类型转换成包装类型的过程叫做装箱。反之，把包装类型转换成基本类型的过程叫做拆箱。</p><p>在Java5之前，开发人员要进行手动拆装箱</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Integer integer2 = <span class="keyword">new</span> Integer(<span class="number">10</span>);</span><br><span class="line"><span class="keyword">int</span> j = integer2.intValue();</span><br></pre></td></tr></table></figure><p>java5引入了自动拆装箱的功能，减少了开发人员的工作</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Integer chenmo  = <span class="number">10</span>;  <span class="comment">// 自动装箱</span></span><br><span class="line"><span class="keyword">int</span> wanger = chenmo;     <span class="comment">// 自动拆箱</span></span><br></pre></td></tr></table></figure><p>使用反编译工作编译后的结果如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Integer integer3 = Integer.valueOf(<span class="number">10</span>);</span><br><span class="line"><span class="keyword">int</span> k = integer3.intValue();</span><br></pre></td></tr></table></figure><p>也就说自动装箱是调用了<code>Integer.valueOf()</code>完成的，自动拆箱是通过调用<code>integer.intValue()</code>完成的。</p><p>理解了自动拆装箱的原理后，我们来看一道面试题</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">int</span> b = <span class="number">100</span>;</span><br><span class="line">System.out.println(a ==b);</span><br><span class="line"></span><br><span class="line">Integer c = <span class="number">100</span>;</span><br><span class="line">Integer d = <span class="number">100</span>;</span><br><span class="line">System.out.println(c == d);</span><br><span class="line"></span><br><span class="line">Integer e = <span class="number">200</span>;</span><br><span class="line">Integer f = <span class="number">200</span>;</span><br><span class="line">System.out.println(e == f);</span><br><span class="line"></span><br><span class="line">System.out.println( a == c);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> h = <span class="number">200</span>;</span><br><span class="line">System.out.println( e == h);</span><br></pre></td></tr></table></figure><p>在看这段代码之前，我们要明白的是 == 号，基础类型比较的是值，引用类型比较的是内存地址</p><p>第一段代码，很好理解，基础类型比较的是值</p><p>第二段代码是包装类，这里需要引入一个缓冲池(IntegerCache )的概念，JVM把-128到127的数值存到了内存中，需要的时候直接从内存拿，而不是重新创建一个对象</p><p>第三段代码也很容易理解，如果-128到127是从缓冲池中拿，那么超过这个范围的，自然就是堆中创建了</p><p>第四段是基本类型和包装类型做比较，这时候包装类型会先转成基本类型，然后再比较</p><p>第五段代码同上，没有在堆中创建的对象这一步</p><p>结果即是：<code>true、true、false、true、true</code></p><p>之前我们就已经知道了自动装箱是Integer.valueOf()方法，我们现在看看它的源码，如果做到-128到127是缓冲池中拿，而超过了则需要在堆中创建。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Integer <span class="title">valueOf</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//如果需要包装的数值大于IntegerCache.low 并且小于IntegerCache.high，就在IntegerCache.cache中拿，否则就new一个Integer，从这里看cache应该是个数组，保存了-128到127的数值</span></span><br><span class="line">    <span class="keyword">if</span> (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high)</span><br><span class="line">       <span class="comment">// 数据中的位置是0 - 255 ==&gt; -127 - 128 ==&gt; -IntegerCache.low的位置的值是0</span></span><br><span class="line">        <span class="keyword">return</span> IntegerCache.cache[i + (-IntegerCache.low)];</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Integer(i);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">IntegerCache</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> low = -<span class="number">128</span>;<span class="comment">//最小值-128</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> high;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> Integer cache[];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//静态代码块，初始化时就加载好了</span></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="comment">// 最大值可以通过配置文件设置</span></span><br><span class="line">        <span class="keyword">int</span> h = <span class="number">127</span>;<span class="comment">//最大值127</span></span><br><span class="line">        <span class="comment">//从jvm的配置中拿到自定义缓冲池最大值的参数</span></span><br><span class="line">        String integerCacheHighPropValue =</span><br><span class="line">            sun.misc.VM.getSavedProperty(<span class="string">"java.lang.Integer.IntegerCache.high"</span>);</span><br><span class="line">        <span class="comment">//integerCacheHighPropValue的值默认是null</span></span><br><span class="line">        <span class="keyword">if</span> (integerCacheHighPropValue != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">//拿到传入的最大值</span></span><br><span class="line">                <span class="keyword">int</span> i = parseInt(integerCacheHighPropValue);</span><br><span class="line">                i = Math.max(i, <span class="number">127</span>);</span><br><span class="line">                <span class="comment">// 最大值不能超过Integer定义的最大值</span></span><br><span class="line">                h = Math.min(i, Integer.MAX_VALUE - (-low) -<span class="number">1</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span>( NumberFormatException nfe) &#123;</span><br><span class="line">                <span class="comment">// 如果属性不能被解析成int型，就忽略它。</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        high = h;<span class="comment">//赋值给high</span></span><br><span class="line"><span class="comment">//(high - low) + 1 = 256</span></span><br><span class="line">        cache = <span class="keyword">new</span> Integer[(high - low) + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span> j = low;<span class="comment">// 遍历 0 - 256</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; cache.length; k++)</span><br><span class="line">            <span class="comment">// -127开始累加，并且都放到cache中，注意k是从0开始的</span></span><br><span class="line">            cache[k] = <span class="keyword">new</span> Integer(j++);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// range [-128, 127] must be interned (JLS7 5.1.7)</span></span><br><span class="line">        <span class="keyword">assert</span> IntegerCache.high &gt;= <span class="number">127</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">IntegerCache</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>看到这里也就明白了，-128-127是如何实现的，超过这个范围为什么是在堆中创建了，在这个源码中，还有一个地方没有解释，那就是参数<code>java.lang.Integer.IntegerCache.high</code>，这个参数可以加载到手动传入的值，从而扩大或者缩小缓冲池的最大值</p><p>如果我们设置这个值的大小为200：<code>-Djava.lang.Integer.IntegerCache.high=200</code>，那么我们就能看到下面的代码的结果是true</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Integer e = <span class="number">200</span>;</span><br><span class="line">Integer f = <span class="number">200</span>;</span><br><span class="line">System.out.println(e == f);</span><br></pre></td></tr></table></figure><p>看完上面的分析之后，我希望大家记住一点：<strong>当需要进行自动装箱时，如果数字在 -128 至 127 之间时，会直接使用缓存中的对象，而不是重新创建一个对象</strong>。</p><p>注意：缓冲池只有Integer类型有</p><h2 id="包装类可以为null，而基本类型不可以"><a href="#包装类可以为null，而基本类型不可以" class="headerlink" title="包装类可以为null，而基本类型不可以"></a>包装类可以为null，而基本类型不可以</h2><p>别小看这一点区别，它使得包装类型可以应用于 POJO 中，而基本类型则不行。</p><p>那为什么 POJO 的属性必须要用包装类型呢？</p><p>对于基本数据类型，数据库的查询结果可能是 null，如果使用基本类型的话，因为要自动拆箱（将包装类型转为基本类型，比如说把 Integer 对象转换成 int 值），就会抛出 <code>NullPointerException</code> 的异常。因为基础类型的值只能是数值。</p><h2 id="包装类型可用于泛型，而基本类型不可以"><a href="#包装类型可用于泛型，而基本类型不可以" class="headerlink" title="包装类型可用于泛型，而基本类型不可以"></a>包装类型可用于泛型，而基本类型不可以</h2><p>我们先尝试定义一个List</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;<span class="keyword">int</span>&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br></pre></td></tr></table></figure><p>报错：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Syntax error, insert <span class="string">"Dimensions"</span> to complete ReferenceTypeList&lt;Integer&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br></pre></td></tr></table></figure><p>原因是泛型只能使用Object 类及其子类，所以包装类型可用于泛型，而基本类型不可以</p><h2 id="基本类型比包装类型更高效"><a href="#基本类型比包装类型更高效" class="headerlink" title="基本类型比包装类型更高效"></a>基本类型比包装类型更高效</h2><p>基本数据类型在栈中直接存储具体的数值，而包装类型则存储在堆中，栈中存放的是引用</p><p><img src="https://yerias.github.io/java_img/13.png" alt=""></p><p>很显然，相对于基本数据类型而言，包装类型需要占用更多的内存空间，假如没有基本数据类型的话，对于数值这类经常能用到的数据来说，每次都要通过new来创建包装类型就显得非常笨重。</p><h2 id="两个包装类型的值可以相同，但却可以不相等"><a href="#两个包装类型的值可以相同，但却可以不相等" class="headerlink" title="两个包装类型的值可以相同，但却可以不相等"></a>两个包装类型的值可以相同，但却可以不相等</h2><p>两个包装类型的值可以相同，但却不相等</p><p>不相等是因为两个包装类型在使用“==”进行判断的时候，判断的是其指向的内存地址是否相等。包装类的值如果是在堆中创建出的话，因为内存地址不同，所以返回的是false。</p><p>而值相同是因为包装类型在做<code>equals</code>比较的时候，都先拆箱成了基础类型，然后再做比较，即比较的是内容，所以为true。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (obj <span class="keyword">instanceof</span> Integer) &#123;</span><br><span class="line">        <span class="keyword">return</span> value == ((Integer)obj).intValue();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala之使用ScalikeJDBC操作MySQL</title>
      <link href="/2020/03/16/scala/2/"/>
      <url>/2020/03/16/scala/2/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>简介</li><li>配置</li><li>操作数据库</li></ol><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>ScalikeJDBC是一款给Scala开发者使用的简介访问类库，它是基于SQL的，使用者只需要关注SQL逻辑的编写，所有的数据库操作都交给ScalikeJDBC。这个类库内置包含了JDBCAPI，并且给用户提供了简单易用并且非常灵活的API。并且，QueryDSl（通用查询查询框架）使你的代码类型安全，半年过去可重复使用。我们可以在生产环境大胆地使用这款DB访问类库。</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><ol><li><p>解决依赖</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.11.8<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scalikejdbc.version</span>&gt;</span>3.3.2<span class="tag">&lt;/<span class="name">scalikejdbc.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mysql.jdbc.version</span>&gt;</span>5.1.38<span class="tag">&lt;/<span class="name">mysql.jdbc.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--Scala相关依赖--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--scalikejdbc相关依赖--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scalikejdbc<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scalikejdbc_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scalikejdbc.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scalikejdbc<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scalikejdbc-config_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scalikejdbc.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;mysql.jdbc.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>解决配置</p><p>在<code>src</code>的<code>main</code>目录下配置一个<code>resource</code>文件夹，文件夹下再创建一个<code>application.conf</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">db.default.driver="com.mysql.jdbc.Driver"</span><br><span class="line">db.default.url="jdbc:mysql://hadoop001/ruoze_d6?characterEncoding=utf-8"</span><br><span class="line">db.default.user="root"</span><br><span class="line">db.default.password="123456"</span><br><span class="line"></span><br><span class="line"><span class="comment"># Connection Pool settings</span></span><br><span class="line">db.default.poolInitialSize=10</span><br><span class="line">db.default.poolMaxSize=20</span><br><span class="line">db.default.connectionTimeoutMillis=1000</span><br></pre></td></tr></table></figure></li></ol><h2 id="操作数据库"><a href="#操作数据库" class="headerlink" title="操作数据库"></a>操作数据库</h2><ol><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> Employer(</span><br><span class="line">    <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">10</span>),</span><br><span class="line">    age <span class="built_in">varchar</span>(<span class="number">4</span>),</span><br><span class="line">    salary  <span class="built_in">varchar</span>(<span class="number">10</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure></li><li><p>scala编程实现增删改查操作</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.wsk.bigdata.scala.scalikejdbc</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scalikejdbc._</span><br><span class="line"><span class="keyword">import</span> scalikejdbc.config._</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义样例类获取数据</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Employer</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span>, salary: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">JdbcTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">DBs</span>.setupAll()<span class="comment">//初始化配置</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//数据</span></span><br><span class="line">    <span class="keyword">val</span> employers = <span class="type">List</span>(<span class="type">Employer</span>(<span class="string">"zhangsan"</span>, <span class="number">20</span>, <span class="number">18000</span>), <span class="type">Employer</span>(<span class="string">"zhangliu"</span>, <span class="number">50</span>, <span class="number">300000</span>), <span class="type">Employer</span>(<span class="string">"lisi"</span>, <span class="number">22</span>, <span class="number">22000</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//批量插入</span></span><br><span class="line">    insert(employers)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//查询出结果</span></span><br><span class="line">    <span class="keyword">val</span> results = select()</span><br><span class="line">    <span class="keyword">for</span> (employer &lt;- results) &#123;</span><br><span class="line">      println(employer.name, employer.age, employer.salary)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//修改</span></span><br><span class="line">    update(<span class="number">1000</span>, <span class="string">"zhangsan"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//根据条件删除</span></span><br><span class="line">    deleteByname(<span class="string">"zhangliu"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//删除所有</span></span><br><span class="line">    deleteAll()</span><br><span class="line"></span><br><span class="line">   <span class="comment">//关闭资源</span></span><br><span class="line">    <span class="type">DBs</span>.closeAll()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">//插入数据</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(employers: <span class="type">List</span>[<span class="type">Employer</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//事物插入</span></span><br><span class="line">    <span class="type">DB</span>.localTx &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      <span class="keyword">for</span> (employer &lt;- employers) &#123;</span><br><span class="line">        <span class="type">SQL</span>(<span class="string">"insert into wsktest(name,age,salary) values(?,?,?)"</span>)</span><br><span class="line">          .bind(employer.name, employer.age, employer.salary)</span><br><span class="line">          .update()<span class="comment">//更新操作</span></span><br><span class="line">                .apply()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  <span class="comment">//查询操作</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">select</span></span>(): <span class="type">List</span>[<span class="type">Employer</span>] = &#123;</span><br><span class="line">    <span class="type">DB</span>.readOnly &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      <span class="type">SQL</span>(<span class="string">"select * from wsktest"</span>)</span><br><span class="line">        .map(rs =&gt; <span class="type">Employer</span>(rs.string(<span class="string">"name"</span>), rs.int(<span class="string">"age"</span>), rs.long(<span class="string">"salary"</span>)))</span><br><span class="line">        .list()  <span class="comment">//结果转换成list</span></span><br><span class="line">        .apply() </span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//更新操作</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(age: <span class="type">Int</span>, name: <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="type">DB</span>.autoCommit &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      <span class="type">SQL</span>(<span class="string">"update wsktest set age = ? where name = ?"</span>)</span><br><span class="line">        .bind(age, name)</span><br><span class="line">        .update()<span class="comment">//更新操作</span></span><br><span class="line">        .apply()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//根据条件删除</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteByname</span></span>(name: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">DB</span>.autoCommit &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      <span class="type">SQL</span>(<span class="string">"delete from wsktest where name = ?"</span>)</span><br><span class="line">        .bind(name)<span class="comment">//更新操作</span></span><br><span class="line">        .update()</span><br><span class="line">        .apply()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//删除所有</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteAll</span></span>(): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="type">DB</span>.autoCommit &#123; </span><br><span class="line">        <span class="keyword">implicit</span> session =&gt;</span><br><span class="line">      <span class="type">SQL</span>(<span class="string">"delete from wsktest "</span>)</span><br><span class="line">        .update()<span class="comment">//更新操作</span></span><br><span class="line">        .apply()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Curator的介绍&amp;使用</title>
      <link href="/2020/03/15/zookeeper/2/"/>
      <url>/2020/03/15/zookeeper/2/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>简介</li><li>基于Curator的Zookeeper基本用法</li><li>监听器</li><li>分布式锁</li><li>Leader选举</li></ol><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Apache Curator是一个比较完善的ZooKeeper客户端框架，通过封装的一套高级API 简化了ZooKeeper的操作。通过查看官方文档，可以发现Curator主要解决了三类问题：</p><ul><li>封装ZooKeeper client与ZooKeeper server之间的连接处理</li><li>提供了一套Fluent风格的操作API</li><li>提供ZooKeeper各种应用场景(recipe， 比如：分布式锁服务、集群领导选举、共享计数器、缓存机制、分布式队列等)的抽象封装</li></ul><h3 id="Curator主要从以下几个方面降低了zk使用的复杂性："><a href="#Curator主要从以下几个方面降低了zk使用的复杂性：" class="headerlink" title="Curator主要从以下几个方面降低了zk使用的复杂性："></a>Curator主要从以下几个方面降低了zk使用的复杂性：</h3><ul><li>重试机制:提供可插拔的重试机制, 它将给捕获所有可恢复的异常配置一个重试策略，并且内部也提供了几种标准的重试策略(比如指数补偿)</li><li>连接状态监控: Curator初始化之后会一直对zk连接进行监听，一旦发现连接状态发生变化将会作出相应的处理</li><li>zk客户端实例管理:Curator会对zk客户端到server集群的连接进行管理，并在需要的时候重建zk实例，保证与zk集群连接的可靠性</li><li>各种使用场景支持:Curator实现了zk支持的大部分使用场景（甚至包括zk自身不支持的场景），这些实现都遵循了zk的最佳实践，并考虑了各种极端情况</li></ul><h2 id="基于Curator的Zookeeper基本用法"><a href="#基于Curator的Zookeeper基本用法" class="headerlink" title="基于Curator的Zookeeper基本用法"></a>基于Curator的Zookeeper基本用法</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CuratorBase</span> </span>&#123;</span><br><span class="line">    <span class="comment">//会话超时时间</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> SESSION_TIMEOUT = <span class="number">30</span> * <span class="number">1000</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//连接超时时间</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> CONNECTION_TIMEOUT = <span class="number">3</span> * <span class="number">1000</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//ZooKeeper服务地址</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String CONNECT_ADDR = <span class="string">"192.168.1.1:2100,192.168.1.1:2101,192.168.1.:2102"</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//创建连接实例</span></span><br><span class="line">    <span class="keyword">private</span> CuratorFramework client = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;  </span><br><span class="line">        <span class="comment">//1 重试策略：初试时间为1s 重试10次</span></span><br><span class="line">        RetryPolicy retryPolicy = <span class="keyword">new</span> ExponentialBackoffRetry(<span class="number">1000</span>, <span class="number">10</span>);</span><br><span class="line">        <span class="comment">//2 通过工厂创建连接</span></span><br><span class="line">        CuratorFramework client = CuratorFrameworkFactory.builder()</span><br><span class="line">                    .connectString(CONNECT_ADDR).connectionTimeoutMs(CONNECTION_TIMEOUT)</span><br><span class="line">                    .sessionTimeoutMs(SESSION_TIMEOUT)</span><br><span class="line">                    .retryPolicy(retryPolicy)</span><br><span class="line"><span class="comment">//命名空间           .namespace("super")</span></span><br><span class="line">                    .build();</span><br><span class="line">        <span class="comment">//3 开启连接</span></span><br><span class="line">        cf.start();</span><br><span class="line">        </span><br><span class="line">        System.out.println(States.CONNECTED);</span><br><span class="line">        System.out.println(cf.getState());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建永久节点</span></span><br><span class="line">        client.create().forPath(<span class="string">"/curator"</span>,<span class="string">"/curator data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//创建永久有序节点</span></span><br><span class="line">        client.create().withMode(CreateMode.PERSISTENT_SEQUENTIAL).forPath(<span class="string">"/curator_sequential"</span>,<span class="string">"/curator_sequential data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//创建临时节点</span></span><br><span class="line">        client.create().withMode(CreateMode.EPHEMERAL)</span><br><span class="line">            .forPath(<span class="string">"/curator/ephemeral"</span>,<span class="string">"/curator/ephemeral data"</span>.getBytes());</span><br><span class="line"> </span><br><span class="line">        <span class="comment">//创建临时有序节点</span></span><br><span class="line">        client.create().withMode(CreateMode.EPHEMERAL_SEQUENTIAL) .forPath(<span class="string">"/curator/ephemeral_path1"</span>,<span class="string">"/curator/ephemeral_path1 data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        client.create().withProtection().withMode(CreateMode.EPHEMERAL_SEQUENTIAL).forPath(<span class="string">"/curator/ephemeral_path2"</span>,<span class="string">"/curator/ephemeral_path2 data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//测试检查某个节点是否存在</span></span><br><span class="line">        Stat stat1 = client.checkExists().forPath(<span class="string">"/curator"</span>);</span><br><span class="line">        Stat stat2 = client.checkExists().forPath(<span class="string">"/curator2"</span>);</span><br><span class="line">        </span><br><span class="line">        System.out.println(<span class="string">"'/curator'是否存在： "</span> + (stat1 != <span class="keyword">null</span> ? <span class="keyword">true</span> : <span class="keyword">false</span>));</span><br><span class="line">        System.out.println(<span class="string">"'/curator2'是否存在： "</span> + (stat2 != <span class="keyword">null</span> ? <span class="keyword">true</span> : <span class="keyword">false</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取某个节点的所有子节点</span></span><br><span class="line">        System.out.println(client.getChildren().forPath(<span class="string">"/"</span>));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//获取某个节点数据</span></span><br><span class="line">        System.out.println(<span class="keyword">new</span> String(client.getData().forPath(<span class="string">"/curator"</span>)));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//设置某个节点数据</span></span><br><span class="line">        client.setData().forPath(<span class="string">"/curator"</span>,<span class="string">"/curator modified data"</span>.getBytes());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建测试节点</span></span><br><span class="line">        client.create().orSetData().creatingParentContainersIfNeeded()</span><br><span class="line">            .forPath(<span class="string">"/curator/del_key1"</span>,<span class="string">"/curator/del_key1 data"</span>.getBytes());</span><br><span class="line"> </span><br><span class="line">        client.create().orSetData().creatingParentContainersIfNeeded()</span><br><span class="line">        .forPath(<span class="string">"/curator/del_key2"</span>,<span class="string">"/curator/del_key2 data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        client.create().forPath(<span class="string">"/curator/del_key2/test_key"</span>,<span class="string">"test_key data"</span>.getBytes());</span><br><span class="line">              </span><br><span class="line">        <span class="comment">//删除该节点</span></span><br><span class="line">        client.delete().forPath(<span class="string">"/curator/del_key1"</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//级联删除子节点</span></span><br><span class="line">        client.delete().guaranteed().deletingChildrenIfNeeded().forPath(<span class="string">"/curator/del_key2"</span>);</span><br><span class="line">    ｝ </span><br><span class="line">｝</span><br></pre></td></tr></table></figure><ul><li><code>orSetData()</code>方法：如果节点存在则Curator将会使用给出的数据设置这个节点的值，相当于 setData() 方法</li><li><code>creatingParentContainersIfNeeded()</code>方法：如果指定节点的父节点不存在，则Curator将会自动级联创建父节点</li><li><code>guaranteed()</code>方法：如果服务端可能删除成功，但是client没有接收到删除成功的提示，Curator将会在后台持续尝试删除该节点</li><li><code>deletingChildrenIfNeeded()</code>方法：如果待删除节点存在子节点，则Curator将会级联删除该节点的子节点</li></ul><p>事务管理：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">* 事务管理：碰到异常，事务会回滚</span><br><span class="line">     * <span class="meta">@throws</span> Exception</span><br><span class="line">     */</span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testTransaction</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">//定义几个基本操作</span></span><br><span class="line">        CuratorOp createOp = client.transactionOp().create()</span><br><span class="line">                .forPath(<span class="string">"/curator/one_path"</span>,<span class="string">"some data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        CuratorOp setDataOp = client.transactionOp().setData()</span><br><span class="line">                .forPath(<span class="string">"/curator"</span>,<span class="string">"other data"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        CuratorOp deleteOp = client.transactionOp().delete()</span><br><span class="line">                .forPath(<span class="string">"/curator"</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//事务执行结果</span></span><br><span class="line">        List&lt;CuratorTransactionResult&gt; results = client.transaction()</span><br><span class="line">                .forOperations(createOp,setDataOp,deleteOp);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//遍历输出结果</span></span><br><span class="line">        <span class="keyword">for</span>(CuratorTransactionResult result : results)&#123;</span><br><span class="line">            System.out.println(<span class="string">"执行结果是： "</span> + result.getForPath() + <span class="string">"--"</span> + result.getType());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//因为节点“/curator”存在子节点，所以在删除的时候将会报错，事务回滚</span></span><br></pre></td></tr></table></figure><h2 id="监听器"><a href="#监听器" class="headerlink" title="监听器"></a>监听器</h2><p>Curator提供了三种Watcher(Cache)来监听结点的变化：</p><ul><li><strong>Path Cache</strong>：监视一个路径下1）孩子结点的创建、2）删除，3）以及结点数据的更新。产生的事件会传递给注册的PathChildrenCacheListener。</li><li><strong>Node Cache</strong>：监视一个结点的创建、更新、删除，并将结点的数据缓存在本地。</li><li><strong>Tree Cache</strong>：Path Cache和Node Cache的“合体”，监视路径下的创建、更新、删除事件，并缓存路径下所有孩子结点的数据。</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 在注册监听器的时候，如果传入此参数，当事件触发时，逻辑由线程池处理</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        ExecutorService pool = Executors.newFixedThreadPool(<span class="number">2</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 监听数据节点的变化情况</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">final</span> NodeCache nodeCache = <span class="keyword">new</span> NodeCache(client, <span class="string">"/zk-huey/cnode"</span>, <span class="keyword">false</span>);</span><br><span class="line">        nodeCache.start(<span class="keyword">true</span>);</span><br><span class="line">        nodeCache.getListenable().addListener(</span><br><span class="line">            <span class="keyword">new</span> NodeCacheListener() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">nodeChanged</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    System.out.println(<span class="string">"Node data is changed, new data: "</span> + </span><br><span class="line">                        <span class="keyword">new</span> String(nodeCache.getCurrentData().getData()));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;, </span><br><span class="line">            pool</span><br><span class="line">        );</span><br><span class="line">        </span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 监听子节点的变化情况</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">final</span> PathChildrenCache childrenCache = <span class="keyword">new</span> PathChildrenCache(client, <span class="string">"/zk-huey"</span>, <span class="keyword">true</span>);</span><br><span class="line">        childrenCache.start(StartMode.POST_INITIALIZED_EVENT);</span><br><span class="line">        childrenCache.getListenable().addListener(</span><br><span class="line">            <span class="keyword">new</span> PathChildrenCacheListener() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">childEvent</span><span class="params">(CuratorFramework client, PathChildrenCacheEvent event)</span></span></span><br><span class="line"><span class="function">                        <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">switch</span> (event.getType()) &#123;</span><br><span class="line">                        <span class="keyword">case</span> CHILD_ADDED:</span><br><span class="line">                            System.out.println(<span class="string">"CHILD_ADDED: "</span> + event.getData().getPath());</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        <span class="keyword">case</span> CHILD_REMOVED:</span><br><span class="line">                            System.out.println(<span class="string">"CHILD_REMOVED: "</span> + event.getData().getPath());</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        <span class="keyword">case</span> CHILD_UPDATED:</span><br><span class="line">                            System.out.println(<span class="string">"CHILD_UPDATED: "</span> + event.getData().getPath());</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        <span class="keyword">default</span>:</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            pool</span><br><span class="line">        );</span><br><span class="line">        </span><br><span class="line">        client.setData().forPath(<span class="string">"/zk-huey/cnode"</span>, <span class="string">"world"</span>.getBytes());</span><br><span class="line">        </span><br><span class="line">        Thread.sleep(<span class="number">10</span> * <span class="number">1000</span>);</span><br><span class="line">        pool.shutdown();</span><br><span class="line">        client.close();</span><br></pre></td></tr></table></figure><h2 id="分布式锁"><a href="#分布式锁" class="headerlink" title="分布式锁"></a>分布式锁</h2><p>分布式编程时，比如最容易碰到的情况就是应用程序在线上多机部署，于是当多个应用同时访问某一资源时，就需要某种机制去协调它们。例如，现在一台应用正在rebuild缓存内容，要临时锁住某个区域暂时不让访问；又比如调度程序每次只想一个任务被一台应用执行等等。</p><p>下面的程序会启动两个线程t1和t2去争夺锁，拿到锁的线程会占用5秒。运行多次可以观察到，有时是t1先拿到锁而t2等待，有时又会反过来。Curator会用我们提供的lock路径的结点作为全局锁，这个结点的数据类似这种格式：[<em>c</em>64e0811f-9475-44ca-aa36-c1db65ae5350-lock-0000000005]，每次获得锁时会生成这种串，释放锁时清空数据。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.curator.framework.CuratorFramework;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.CuratorFrameworkFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.recipes.locks.InterProcessMutex;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.retry.RetryNTimes;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Curator framework's distributed lock test.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CuratorDistrLockTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Zookeeper info */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ZK_ADDRESS = <span class="string">"192.168.1.100:2181"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ZK_LOCK_PATH = <span class="string">"/zktest"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 1.Connect to zk</span></span><br><span class="line">        CuratorFramework client = CuratorFrameworkFactory.newClient(</span><br><span class="line">                ZK_ADDRESS,</span><br><span class="line">                <span class="keyword">new</span> RetryNTimes(<span class="number">10</span>, <span class="number">5000</span>)</span><br><span class="line">        );</span><br><span class="line">        client.start();</span><br><span class="line">        System.out.println(<span class="string">"zk client start successfully!"</span>);</span><br><span class="line"></span><br><span class="line">        Thread t1 = <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">            doWithLock(client);</span><br><span class="line">        &#125;, <span class="string">"t1"</span>);</span><br><span class="line">        Thread t2 = <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">            doWithLock(client);</span><br><span class="line">        &#125;, <span class="string">"t2"</span>);</span><br><span class="line"></span><br><span class="line">        t1.start();</span><br><span class="line">        t2.start();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">doWithLock</span><span class="params">(CuratorFramework client)</span> </span>&#123;</span><br><span class="line">        InterProcessMutex lock = <span class="keyword">new</span> InterProcessMutex(client, ZK_LOCK_PATH);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (lock.acquire(<span class="number">10</span> * <span class="number">1000</span>, TimeUnit.SECONDS)) &#123;</span><br><span class="line">                System.out.println(Thread.currentThread().getName() + <span class="string">" hold lock"</span>);</span><br><span class="line">                Thread.sleep(<span class="number">5000L</span>);</span><br><span class="line">                System.out.println(Thread.currentThread().getName() + <span class="string">" release lock"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                lock.release();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Leader选举"><a href="#Leader选举" class="headerlink" title="Leader选举"></a>Leader选举</h2><p>当集群里的某个服务down机时，我们可能要从slave结点里选出一个作为新的master，这时就需要一套能在分布式环境中自动协调的Leader选举方法。Curator提供了LeaderSelector监听器实现Leader选举功能。同一时刻，只有一个Listener会进入takeLeadership()方法，说明它是当前的Leader。注意：<strong>当Listener从takeLeadership()退出时就说明它放弃了“Leader身份”</strong>，这时Curator会利用Zookeeper再从剩余的Listener中选出一个新的Leader。autoRequeue()方法使放弃Leadership的Listener有机会重新获得Leadership，如果不设置的话放弃了的Listener是不会再变成Leader的。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.curator.framework.CuratorFramework;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.CuratorFrameworkFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.recipes.leader.LeaderSelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.recipes.leader.LeaderSelectorListener;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.framework.state.ConnectionState;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.retry.RetryNTimes;</span><br><span class="line"><span class="keyword">import</span> org.apache.curator.utils.EnsurePath;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Curator framework's leader election test.</span></span><br><span class="line"><span class="comment"> * Output:</span></span><br><span class="line"><span class="comment"> *  LeaderSelector-2 take leadership!</span></span><br><span class="line"><span class="comment"> *  LeaderSelector-2 relinquish leadership!</span></span><br><span class="line"><span class="comment"> *  LeaderSelector-1 take leadership!</span></span><br><span class="line"><span class="comment"> *  LeaderSelector-1 relinquish leadership!</span></span><br><span class="line"><span class="comment"> *  LeaderSelector-0 take leadership!</span></span><br><span class="line"><span class="comment"> *  LeaderSelector-0 relinquish leadership! </span></span><br><span class="line"><span class="comment"> *      ...</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CuratorLeaderTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Zookeeper info */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ZK_ADDRESS = <span class="string">"192.168.1.100:2181"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ZK_PATH = <span class="string">"/zktest"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        LeaderSelectorListener listener = <span class="keyword">new</span> LeaderSelectorListener() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">takeLeadership</span><span class="params">(CuratorFramework client)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                System.out.println(Thread.currentThread().getName() + <span class="string">" take leadership!"</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// takeLeadership() method should only return when leadership is being relinquished.</span></span><br><span class="line">                Thread.sleep(<span class="number">5000L</span>);</span><br><span class="line"></span><br><span class="line">                System.out.println(Thread.currentThread().getName() + <span class="string">" relinquish leadership!"</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">stateChanged</span><span class="params">(CuratorFramework client, ConnectionState state)</span> </span>&#123;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">            registerListener(listener);</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">            registerListener(listener);</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">            registerListener(listener);</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        Thread.sleep(Integer.MAX_VALUE);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">registerListener</span><span class="params">(LeaderSelectorListener listener)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 1.Connect to zk</span></span><br><span class="line">        CuratorFramework client = CuratorFrameworkFactory.newClient(</span><br><span class="line">                ZK_ADDRESS,</span><br><span class="line">                <span class="keyword">new</span> RetryNTimes(<span class="number">10</span>, <span class="number">5000</span>)</span><br><span class="line">        );</span><br><span class="line">        client.start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.Ensure path</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">new</span> EnsurePath(ZK_PATH).ensure(client.getZookeeperClient());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.Register listener</span></span><br><span class="line">        LeaderSelector selector = <span class="keyword">new</span> LeaderSelector(client, ZK_PATH, listener);</span><br><span class="line">        selector.autoRequeue();</span><br><span class="line">        selector.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><p>转载：<a href="https://www.cnblogs.com/erbing/p/9799098.html" target="_blank" rel="noopener">https://www.cnblogs.com/erbing/p/9799098.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Zookeeper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Zookeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA的String类源码解析</title>
      <link href="/2020/03/11/java/10/"/>
      <url>/2020/03/11/java/10/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>源码的角度解析String不可变</li><li>String Pool 的角度解析String不可变</li><li>String对象不可变性的优缺点</li><li>String对象是否真的不可变</li><li>从源码的角度解析StringBuilder的可变</li><li>从源码的角度解析StringBuffer和StringBuilder的异同</li><li>编译器对String做出了哪些优化</li></ol><h2 id="从源码的角度解析String不可变"><a href="#从源码的角度解析String不可变" class="headerlink" title="从源码的角度解析String不可变"></a>从源码的角度解析String不可变</h2><p>所谓的不可变类是指这个类的实例一旦创建完成后，就不能改变其成员变量值。</p><p>String类中每一个看起来会修改String值的方法，实际上都是创建了一个全新的String对象</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">upcase</span><span class="params">(String s)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> s.toUpperCase();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    String name = <span class="string">"tunan"</span>;</span><br><span class="line">    System.out.println(name);<span class="comment">//tunan</span></span><br><span class="line"></span><br><span class="line">    String name2 = upcase(name);</span><br><span class="line">    System.out.println(name2);<span class="comment">//TUNAN</span></span><br><span class="line"></span><br><span class="line">    System.out.println(name);<span class="comment">//tunan</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当把name传给upcase()方法的时候，实际上传递的是一个引用的拷贝。而该引用所指的对象其实一直待在单一的物理位置上，从未动过。</p><p>回到upcase()的定义，传入其中的引用有了名字s，只有upcase()运行的时候，局部引用s才存在。一旦upcase()运行结束，s就消失了。当然了，upcase()的返回值，其实只是最终结果的引用。这足已说明，upcase()返回的引用已经指向了一个新的对象name2，而原本的name则还在原地。</p><p>既然String类型的变量name没有变过，我们从源码的角度去看为什么没有改变</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">String</span></span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span>, <span class="title">Comparable</span>&lt;<span class="title">String</span>&gt;, <span class="title">CharSequence</span> </span>&#123;</span><br><span class="line">    <span class="comment">//使用字节数组存储字符串</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">char</span> value[];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//存储hash值</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> hash; <span class="comment">// Default to 0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以清楚的看到String类是一个final类，但这并不是String不可变的真正原因，继续看String实现了CharSequence接口</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">CharSequence</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">length</span><span class="params">()</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">char</span> <span class="title">charAt</span><span class="params">(<span class="keyword">int</span> index)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="function">CharSequence <span class="title">subSequence</span><span class="params">(<span class="keyword">int</span> start, <span class="keyword">int</span> end)</span></span>;</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>CharSequence是一个接口，它只包括length(), charAt(int index), subSequence(int start, int end)这几个API接口。同时除了String实现了CharSequence之外，StringBuffer和StringBuilder也实现了CharSequence接口。 </p><p>也就是说，CharSequence其实也就是定义了字符串操作的接口，其他具体的实现是由String、StringBuilder、StringBuffer完成的，String、StringBuilder、StringBuffer都可以转化为CharSequence类型。</p><p>继续看String类，<code>private final char value[];</code>这个final类型的字符型变量才是真正存储字符串的容器，也正是因为这个变量是final的，才真正决定了字符串不可变，也许你不相信，你可以说Stirng类也是final修饰的，也是不可变的，那么如果StringBuilder和StringBuffer也是final修饰的呢？</p><h2 id="String-Pool-的角度解析String不可变"><a href="#String-Pool-的角度解析String不可变" class="headerlink" title="String Pool 的角度解析String不可变"></a>String Pool 的角度解析String不可变</h2><p>JVM为了提升性能和减少内存开销，避免字符串的重复创建，其维护了一块特殊的内存空间，这就是我们今天要讨论的核心，即字符串池（String Pool）。</p><p>我们知道，在Java中有两种创建字符串对象的方式：</p><ol><li><p>采用字面值的方式赋值 </p></li><li><p>采用new关键字新建一个字符串对象。</p><p>这两种方式在性能和内存占用方面存在着差别。</p></li></ol><p>方式一：采用字面值的方式赋值，例如：</p><p><img src="https://yerias.github.io/java_img/1.png" alt=""></p><p>采用字面值的方式创建一个字符串时，JVM首先会去字符串池中查找是否存在”aaa”这个对象，如果不存在，则在字符串池中创建”aaa”这个对象，然后将池中”aaa”这个对象的引用地址返回给字符串常量str，这样str会指向池中”aaa”这个字符串对象；如果存在，则不创建任何对象，直接将池中”aaa”这个对象的地址返回，赋给字符串常量。</p><p><img src="https://yerias.github.io/java_img/3.jpg" alt=""></p><p> 在本例中，执行：str == str2 ，会得到以下结果：</p><p><img src="https://yerias.github.io/java_img/2.png" alt=""></p><p>这是因为，创建字符串对象str2时，字符串池中已经存在”aaa”这个对象，直接把对象”aaa”的引用地址返回给str2，这样str2指向了池中”aaa”这个对象，也就是说str和str2指向了同一个对象，因此语句System.out.println(str == str2)输出：true。</p><p>方式二：采用new关键字新建一个字符串对象，例如：</p><p><img src="https://yerias.github.io/java_img/4.png" alt=""></p><p>采用new关键字新建一个字符串对象时，JVM首先在字符串池中查找有没有”aaa”这个字符串对象，如果有，则不在池中再去创建”aaa”这个对象了，直接在堆中创建一个”aaa”字符串对象，然后将堆中的这个”aaa”对象的地址返回赋给引用str3，这样，str3就指向了堆中创建的这个”aaa”字符串对象；如果没有，则首先在字符串池中创建一个”aaa”字符串对象，然后再在堆中创建一个”aaa”字符串对象，然后将堆中这个”aaa”字符串对象的地址返回赋给str3引用，这样，str3指向了堆中创建的这个”aaa”字符串对象。</p><p><img src="https://yerias.github.io/java_img/6.jpg" alt=""></p><p>在这个例子中，执行：str3 == str4，得到以下结果：</p><p><img src="https://yerias.github.io/java_img/5.png" alt=""></p><p>因为，采用new关键字创建对象时，每次new出来的都是一个新的对象，也即是说引用str3和str4指向的是两个不同的对象，因此语句System.out.println(str3 == str4)输出：false。</p><p>字符串池的实现有一个前提条件：String对象是不可变的。因为这样可以保证多个引用可以同时指向字符串池中的同一个对象。如果字符串是可变的，那么一个引用操作改变了对象的值，对其他引用会有影响，这样显然是不合理的。</p><p><strong>Java语言规范（Java Language Specification）</strong>中对字符串做出了如下说明：每一个字符串常量都是指向一个字符串类实例的引用。字符串对象有一个固定值。字符串常量，或者一般的说，常量表达式中的字符串都被使用方法 String.intern进行保留来共享唯一的实例。</p><p>以上是Java语言规范中的原文，比较官方，用更通俗易懂的语言翻译过来主要说明了三点：</p><ol><li>每一个字符串常量都指向字符串池中或者堆内存中的一个字符串实例。</li><li>字符串对象值是固定的，一旦创建就不能再修改。</li><li>字符串常量或者常量表达式中的字符串都被方法String.intern()在字符串池中保留了唯一的实例。</li></ol><p><img src="https://yerias.github.io/java_img/7.jpg" alt=""></p><p>​        其他包</p><p>​            <img src="https://yerias.github.io/java_img/8.png" alt=""></p><p>​        结果</p><p>​                <img src="https://yerias.github.io/java_img/9.png" alt=""></p><p>结论：</p><ul><li>同一个包下同一个类中的字符串常量的引用指向同一个字符串对象；</li><li>同一个包下不同的类中的字符串常量的引用指向同一个字符串对象；</li><li>不同的包下不同的类中的字符串常量的引用仍然指向同一个字符串对象；</li><li>由常量表达式计算出的字符串是在编译时进行计算,然后被当作常量；</li><li>在运行时通过连接计算出的字符串是新创建的，因此是不同的；</li><li>通过计算生成的字符串显示调用intern方法后产生的结果与原来存在的同样内容的字符串常量是一样的。</li></ul><p>从上面的例子可以看出，字符串常量在<strong>编译时</strong>计算和在<strong>运行时</strong>计算，其执行过程是不同的，得到的结果也是不同的。我们来看看下面这段代码：</p><p><img src="https://yerias.github.io/java_img/10.png" alt=""></p><p> 代码输出如下：</p><p><img src="https://yerias.github.io/java_img/11.png" alt=""></p><p>为什么出现上面的结果呢？这是因为，字符串字面量拼接操作是在Java编译器编译期间就执行了，也就是说编译器编译时，直接把”java”、”language”和”specification”这三个字面量进行”+”操作得到一个”javalanguagespecification” 常量，并且直接将这个常量放入字符串池中，这样做实际上是一种优化，将3个字面量合成一个，避免了创建多余的字符串对象。而字符串引用的”+”运算是在Java运行期间执行的，即str + str2 + str3在程序执行期间才会进行计算，它会在堆内存中重新创建一个拼接后的字符串对象。总结来说就是：字面量”+”拼接是在编译期间进行的，拼接后的字符串存放在字符串池中；而字符串引用的”+”拼接运算实在运行时进行的，新创建的字符串存放在堆中。</p><p><strong>到这里我们也能理解了什么是字符串的不可变性</strong>，其本质是在字符串池中开辟了一块空间，字符串的地址不变，字符串变量重新赋值感觉是字符串变了，其实是在字符串池中开辟了另外一块空间，并且字符串的引用重新指向新的空间地址，而原来的字符串内容和内存地址在字符串池中没有改变过。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String name = <span class="string">"aaa"</span>;</span><br><span class="line">name = <span class="string">"bbb"</span>;</span><br><span class="line">System.out.println(name);</span><br></pre></td></tr></table></figure><p><img src="https://yerias.github.io/java_img/12.jpg" alt=""></p><p>字符串池的位置是在堆中，那么GC的时候<strong>字符串如何保证不被GC？</strong><br>为了优化空间，运行时实例创建的全局字符串常量池中有一个表，总是为池中每个唯一的字符串对象维护一个引用。这就意味着它们一直引用着字符串常量池中的对象，所以，在常量池中的这些字符串不会被垃圾收集器回收。</p><p>总结：字符串是常量，字符串池中的每个字符串对象只有唯一的一份，可以被多个引用所指向，避免了重复创建内容相同的字符串；通过字面值赋值创建的字符串对象存放在字符串池中，通过关键字new出来的字符串对象存放在堆中。</p><h2 id="String对象不可变性的优缺点"><a href="#String对象不可变性的优缺点" class="headerlink" title="String对象不可变性的优缺点"></a>String对象不可变性的优缺点</h2><p><strong>1.字符串常量池的需要</strong>.<br>字符串常量池可以将一些字符常量放在常量池中重复使用，避免每次都重新创建相同的对象、节省存储空间。但如果字符串是可变的，此时相同内容的String还指向常量池的同一个内存空间，当某个变量改变了该内存的值时，其他遍历的值也会发生改变。所以不符合常量池设计的初衷。</p><p><strong>2. 线程安全考虑</strong>。<br>同一个字符串实例可以被多个线程共享。这样便不用因为线程安全问题而使用同步。字符串自己便是线程安全的。</p><p><strong>3. 类加载器要用到字符串，不可变性提供了安全性，以便正确的类被加载</strong>。譬如你想加载java.sql.Connection类，而这个值被改成了myhacked.Connection，那么会对你的数据库造成不可知的破坏。</p><p><strong>4. 支持hash映射和缓存。</strong><br>因为字符串是不可变的，所以在它创建的时候hashcode就被缓存了，不需要重新计算。这就使得字符串很适合作为Map中的键，字符串的处理速度要快过其它的键对象。这就是HashMap中的键往往都使用字符串。</p><p>缺点：</p><ol><li>如果有对String对象值改变的需求，那么会创建大量的String对象(使用StringBuffer或者StringBuilder替代)。</li></ol><h2 id="String对象是否真的不可变"><a href="#String对象是否真的不可变" class="headerlink" title="String对象是否真的不可变"></a>String对象是否真的不可变</h2><p>String对象的不可变，其根本是内存地址的不可变，这在字符串池中有解析</p><p>虽然String对象将value设置为final，并且还通过各种机制保证其成员变量不可改变。但是还是可以通过反射机制的手段改变其值。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建字符串"Hello World"， 并赋给引用s</span></span><br><span class="line">String s = <span class="string">"hello world"</span>;</span><br><span class="line">System.out.println(<span class="string">"s = "</span> +s);</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取String类中的value字段(private final char value[];)</span></span><br><span class="line">Field valueFieldOfString  = String.class.getDeclaredField("value");</span><br><span class="line"></span><br><span class="line"><span class="comment">//改变value属性的访问权限</span></span><br><span class="line">valueFieldOfString .setAccessible(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取s对象上的value属性的值</span></span><br><span class="line"><span class="keyword">char</span>[] value = (<span class="keyword">char</span>[]) valueFieldOfString.get(s);</span><br><span class="line"></span><br><span class="line"><span class="comment">//改变value所引用的数组中的第5个字符</span></span><br><span class="line">value[<span class="number">5</span>] =<span class="string">'_'</span>;</span><br><span class="line">System.out.println(<span class="string">"s = "</span> +s);</span><br></pre></td></tr></table></figure><p>打印结果为：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">s = Hello World</span><br><span class="line">s = Hello_World</span><br></pre></td></tr></table></figure><p>发现String的值已经发生了改变。也就是说，通过反射是可以修改所谓的“不可变”对象的</p><h2 id="从源码的角度解析StringBuilder的可变"><a href="#从源码的角度解析StringBuilder的可变" class="headerlink" title="从源码的角度解析StringBuilder的可变"></a>从源码的角度解析StringBuilder的可变</h2><p>StringBuilder可以动态构造字符串，并且是线程不安全的，我们从源码的角度解析StringBuilder为什么可以动态构造字符串。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">StringBuilder</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">AbstractStringBuilder</span></span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span>, <span class="title">CharSequence</span></span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">    <span class="comment">//默认char容量16</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">StringBuilder</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(<span class="number">16</span>);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//指定了则使用父类的构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">StringBuilder</span><span class="params">(<span class="keyword">int</span> capacity)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(capacity);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>我们首先看到StringBuilder也是final修饰的， 和String一样，不仅如此StringBuffer也是final修饰的，下面将不再解释，它继承了AbstractStringBuilder，并且和String、StringBuffer一样，都实现了CharSequence接口</p><p>看构造方法默认容量是16，指定了容量则使用父类的构造方法，我们现在去看下父类中如何实现的</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">AbstractStringBuilder</span> <span class="keyword">implements</span> <span class="title">Appendable</span>, <span class="title">CharSequence</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 使用字节数组存储字符串</span></span><br><span class="line">    <span class="keyword">char</span>[] value;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 记录存储的字符串的长度</span></span><br><span class="line">    <span class="keyword">int</span> count;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 空构造方法</span></span><br><span class="line">    AbstractStringBuilder() &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 传入初始值的构造方法</span></span><br><span class="line">    AbstractStringBuilder(<span class="keyword">int</span> capacity) &#123;</span><br><span class="line">        value = <span class="keyword">new</span> <span class="keyword">char</span>[capacity];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>父类的构造方法中是new了一个指定长度的char字节数组，这说明StringBuilder底层也是使用字符数组保存字符串的，需要注意的是value的定义，和String类中的实现不同，这里没有private和final修饰，正是因为这点，所以StringBuilder是可变的，StringBuilder的value字节数组可以动态的改变大小。</p><p>我们已经知道了StringBuilder为什么可变，还需要注意的是它的append方法，该方法直接决定了StringBuilder如何追加字符串。也是和StringBuffer唯一不同的地方</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StringBuilder <span class="title">append</span><span class="params">(String str)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>.append(str);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>直接重写的父类方法</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> AbstractStringBuilder <span class="title">append</span><span class="params">(String str)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (str == <span class="keyword">null</span>)<span class="comment">//检查是否空</span></span><br><span class="line">        <span class="keyword">return</span> appendNull();</span><br><span class="line">    <span class="keyword">int</span> len = str.length();<span class="comment">//获得字符串长度</span></span><br><span class="line">    ensureCapacityInternal(count + len);<span class="comment">//检查容量/库容</span></span><br><span class="line">    str.getChars(<span class="number">0</span>, len, value, count);<span class="comment">//拷贝内容</span></span><br><span class="line">    count += len;<span class="comment">//增加长度</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;<span class="comment">//返回</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 扩容</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">newCapacity</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// overflow-conscious code</span></span><br><span class="line">    <span class="keyword">int</span> newCapacity = (value.length &lt;&lt; <span class="number">1</span>) + <span class="number">2</span>;<span class="comment">//扩容为原字节数组长度的两倍+2，注意不是count</span></span><br><span class="line">    <span class="keyword">if</span> (newCapacity - minCapacity &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        newCapacity = minCapacity;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (newCapacity &lt;= <span class="number">0</span> || MAX_ARRAY_SIZE - newCapacity &lt; <span class="number">0</span>)</span><br><span class="line">        ? hugeCapacity(minCapacity)</span><br><span class="line">        : newCapacity;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们发现append方法的底层是对字符数组内容的复制，并且容量不够时，是扩容为原字节数组长度的两倍+2，是字节数组，不是容量</p><h2 id="从源码的角度解析StringBuffer和StringBuilder的异同"><a href="#从源码的角度解析StringBuffer和StringBuilder的异同" class="headerlink" title="从源码的角度解析StringBuffer和StringBuilder的异同"></a>从源码的角度解析StringBuffer和StringBuilder的异同</h2><p>StringBuffer和StringBuilder的所有实现一模一样，包括继承的父类，实现的接口，扩容机制，value的定义，正是这些特性让他们两很像，同时也都支持动态构造字符串。</p><p>我们知道StringBuffer和StringBuilder最大的不同是线程安全性的问题，StringBuffer在所有以StringBuilder为基础的代码上，在重写父类的方法的同时加了synchronized修饰，保证了线程的安全</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//下面只是节选一些StringBuffer中的函数</span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(<span class="keyword">char</span> ch)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(<span class="keyword">char</span>[] chars)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(<span class="keyword">char</span>[] chars, <span class="keyword">int</span> start, <span class="keyword">int</span> length)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(Object obj)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(String string)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(StringBuffer sb)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(CharSequence s)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">append</span><span class="params">(CharSequence s, <span class="keyword">int</span> start, <span class="keyword">int</span> end)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">insert</span><span class="params">(<span class="keyword">int</span> index, <span class="keyword">char</span> ch)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">insert</span><span class="params">(<span class="keyword">int</span> index, <span class="keyword">char</span>[] chars)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">insert</span><span class="params">(<span class="keyword">int</span> index, <span class="keyword">char</span>[] chars, <span class="keyword">int</span> start, <span class="keyword">int</span> length)</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> StringBuffer     <span class="title">insert</span><span class="params">(<span class="keyword">int</span> index, String string)</span></span></span><br></pre></td></tr></table></figure><h2 id="编译器对String做出了哪些优化"><a href="#编译器对String做出了哪些优化" class="headerlink" title="编译器对String做出了哪些优化"></a>编译器对String做出了哪些优化</h2><p>String的不可变性会带来一定的效率问题。为String对象重载的 “+” 操作符就是一个例子。重载的意思是，一个操作符在应用于特定的类时，被赋予了特殊的意义。</p><p>我们用一段代码来验证 “+” 用来拼接String</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String mongo = <span class="string">"mongo"</span>;</span><br><span class="line">String s = <span class="string">"abc"</span> + mongo + <span class="string">"def"</span> +<span class="number">47</span>;</span><br><span class="line">System.out.println(s);<span class="comment">//abcmongodef47</span></span><br></pre></td></tr></table></figure><p>我们猜想一下字符串s的工作方式，它可能有一个append方法，首先s的内容是abc，然后新建一个字符串的内容是abcmongo，继续新建内容是abcmongodef的字符串，最后新建abcmongodef47的字符串，也许你会说为什么不是 “abc” + mongo + “def” +47 一起生成一个字符串然后赋值给s，但是我们不要忘记字符串String，它是一个类。</p><p>这种设计方法可以行的通，但是为了最终生成的String，产生了一大堆的需要GC的中间对象。这样的性能是非常糟糕的。</p><p>那么String是如何做优化的？我们使用JDK自带的工具javap来反编译以上代码，-c表示生成JVM字节码，删除没用的部分，剩下的内容如下</p><p><code>javap -c StringTest</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Compiled from <span class="string">"StringTest.java"</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">string</span>.<span class="title">StringTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(java.lang.String[])</span></span>;</span><br><span class="line">    Code:</span><br><span class="line">       0: ldc           #2                  // String mongo</span><br><span class="line">       <span class="number">2</span>: astore_1<span class="comment">// store mongo</span></span><br><span class="line">       3: new           #3                  // StringBuilder</span><br><span class="line">       <span class="number">6</span>: dup</span><br><span class="line">       7: invokespecial #4                  // StringBuilder."&lt;init&gt;":()V</span><br><span class="line">      10: ldc           #5                  // String abc</span><br><span class="line">      12: invokevirtual #6                  // StringBuilder.append：abc</span><br><span class="line">      <span class="number">15</span>: aload_1<span class="comment">// load mongo</span></span><br><span class="line">      16: invokevirtual #6                  // StringBuilder.append：mongo</span><br><span class="line">      19: ldc           #7                  // String def</span><br><span class="line">      21: invokevirtual #6                  // StringBuilder.append：def</span><br><span class="line">      <span class="number">24</span>: bipush        <span class="number">47</span></span><br><span class="line">      26: invokevirtual #8                  // StringBuilder.append：47</span><br><span class="line">      29: invokevirtual #9                  // StringBuilder.toString：abcmongodef47</span><br><span class="line">      <span class="number">32</span>: astore_2<span class="comment">// store s = abcmongodef47</span></span><br><span class="line">      33: getstatic     #10                 // Field System.out:Ljava/io/PrintStream;</span><br><span class="line">      <span class="number">36</span>: aload_2</span><br><span class="line">      37: invokevirtual #11                 // PrintStream.println:(Ljava/lang/String;)V</span><br><span class="line">      <span class="number">40</span>: <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>即使看不懂编译语句也不重要，我们需要注意的重点是：编译器自动引入了java.lang.StringBuilder类，虽然我们在源码中并没有使用StringBuilder类，但是编译器却自动使用了它，因为它更加高效。</p><p>现在也许你会觉得可以随意的使用String对象，反正编译器会自动优化性能，<strong>可是我们千万要记住一点，在循环的内部拼接字符串，并不会起到优化的效果。</strong></p><p>下面的程序采用两种方式生成String：方法一使用多个String对象；方法二中使用了StringBuilder。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WitherStringBuilder</span> </span>&#123;</span><br><span class="line"><span class="comment">//方法一</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">implicit</span><span class="params">(String[] fields)</span></span>&#123;</span><br><span class="line">        String result=<span class="string">""</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; fields.length; i++) &#123;</span><br><span class="line">            result += fields[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//方法二</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">explicit</span><span class="params">(String[] fields)</span></span>&#123;</span><br><span class="line">        StringBuilder result = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; fields.length; i++) &#123;</span><br><span class="line">            result.append(fields[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行javap -c WitherStringBuilder，可以看到两个方法对应的(简化过的)字节码，首先是implicit()方法：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> java.lang.<span class="function">String <span class="title">implicit</span><span class="params">(java.lang.String[])</span></span>;</span><br><span class="line">  Code:</span><br><span class="line">     0: ldc           #2                  // String</span><br><span class="line">     <span class="number">2</span>: astore_2</span><br><span class="line">     <span class="number">3</span>: iconst_0</span><br><span class="line">     <span class="number">4</span>: istore_3</span><br><span class="line">     <span class="number">5</span>: iload_3</span><br><span class="line">     <span class="number">6</span>: aload_1</span><br><span class="line">     <span class="number">7</span>: arraylength</span><br><span class="line">     <span class="number">8</span>: if_icmpge     <span class="number">38</span></span><br><span class="line">    11: new           #3                  // StringBuilder</span><br><span class="line">    <span class="number">14</span>: dup</span><br><span class="line">    15: invokespecial #4                  // StringBuilder."&lt;init&gt;":()</span><br><span class="line">    <span class="number">18</span>: aload_2</span><br><span class="line">    19: invokevirtual #5                  // StringBuilder.append:()</span><br><span class="line">    <span class="number">22</span>: aload_1</span><br><span class="line">    <span class="number">23</span>: iload_3</span><br><span class="line">    <span class="number">24</span>: aaload</span><br><span class="line">    25: invokevirtual #5                  //StringBuilder.append:()</span><br><span class="line">    28: invokevirtual #6                  // StringBuilder.toString:()</span><br><span class="line">    <span class="number">31</span>: astore_2</span><br><span class="line">    <span class="number">32</span>: iinc          <span class="number">3</span>, <span class="number">1</span></span><br><span class="line">    <span class="number">35</span>: goto          <span class="number">5</span></span><br><span class="line">    <span class="number">38</span>: aload_2</span><br><span class="line">    <span class="number">39</span>: areturn</span><br></pre></td></tr></table></figure><p>注意从第8行到第35行构成一个循环体。</p><p>第8行：对堆栈中的操作数进行 “大于或等于的整数比较运算”，循环结束时跳到第38行。</p><p>第35行：返回循环体的起始点(第5行)。</p><p>要注意的重点是：StringBuilder是在循环之内构造的，这意味着每经过循环一次，就会创建一个新的StringBuilder对象。这样的操作没有任何优化可言。</p><p>下面是explicit()方法对应的字节码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> java.lang.<span class="function">String <span class="title">explicit</span><span class="params">(java.lang.String[])</span></span>;</span><br><span class="line">  Code:</span><br><span class="line">     0: new           #3                  // StringBuilder</span><br><span class="line">     <span class="number">3</span>: dup</span><br><span class="line">     4: invokespecial #4                  // StringBuilder."&lt;init&gt;":()</span><br><span class="line">     <span class="number">7</span>: astore_2</span><br><span class="line">     <span class="number">8</span>: iconst_0</span><br><span class="line">     <span class="number">9</span>: istore_3</span><br><span class="line">    <span class="number">10</span>: iload_3</span><br><span class="line">    <span class="number">11</span>: aload_1</span><br><span class="line">    <span class="number">12</span>: arraylength</span><br><span class="line">    <span class="number">13</span>: if_icmpge     <span class="number">30</span></span><br><span class="line">    <span class="number">16</span>: aload_2</span><br><span class="line">    <span class="number">17</span>: aload_1</span><br><span class="line">    <span class="number">18</span>: iload_3</span><br><span class="line">    <span class="number">19</span>: aaload</span><br><span class="line">    20: invokevirtual #5                  // StringBuilder.append:()</span><br><span class="line">    <span class="number">23</span>: pop</span><br><span class="line">    <span class="number">24</span>: iinc          <span class="number">3</span>, <span class="number">1</span></span><br><span class="line">    <span class="number">27</span>: goto          <span class="number">10</span></span><br><span class="line">    <span class="number">30</span>: aload_2</span><br><span class="line">    31: invokevirtual #6                  // StringBuilder.toString:()</span><br><span class="line">    <span class="number">34</span>: areturn</span><br></pre></td></tr></table></figure><p>可以看到，不仅循环部分的代码更加简短，而且它只生成了一个StringBuilder对象。所以遇到循环内拼接字符串时在循环体的外部定义StringBuilder()可以大大提升程序的性能。当然，如果字符串操作简单的话，那么就可以信赖编译器的优化。</p><p>而且显示地创建StringBuilder还允许你预先为其指定大小。如果你已经知道最终的字符串大概多长，那预先指定StringBuilder的大小还可以避免多次重新分配缓冲。</p><hr><p>参考书籍：《Java编程思想(第4版)》</p><p>参考文章：</p><p><a href="https://www.cnblogs.com/kissazi2/p/3648671.html" target="_blank" rel="noopener">https://www.cnblogs.com/kissazi2/p/3648671.html</a></p><p><a href="https://www.cnblogs.com/xudong-bupt/p/3961159.html" target="_blank" rel="noopener">https://www.cnblogs.com/xudong-bupt/p/3961159.html</a></p><p><a href="https://www.cnblogs.com/fangfuhai/p/5500065.html" target="_blank" rel="noopener">https://www.cnblogs.com/fangfuhai/p/5500065.html</a></p><p><a href="https://www.cnblogs.com/jaylon/p/5721571.html" target="_blank" rel="noopener">https://www.cnblogs.com/jaylon/p/5721571.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nginx的简介&amp;安装&amp;常用操作</title>
      <link href="/2020/03/10/nginx/1/"/>
      <url>/2020/03/10/nginx/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>Nginx的功能介绍</li><li>Nginx的安装</li><li>Nginx常用操作</li></ol><h2 id="Nginx的功能介绍"><a href="#Nginx的功能介绍" class="headerlink" title="Nginx的功能介绍"></a>Nginx的功能介绍</h2><p>Nginx是一个轻量级、高性能、稳定性高、并发性好的HTTP和反向代理服务器。也是由于其的特性，其应用非常广。</p><h3 id="反向代理"><a href="#反向代理" class="headerlink" title="反向代理"></a>反向代理</h3><p>在说反向代理之前说一下正向代理：某些情况下，代理服务器代理我们用户去访问服务器，需要用户手动的设置代理服务器的ip和端口号。正向代理的最大特点是代理服务器在客户端</p><p>反向代理是用来代理服务器的，代理服务器代理我们要访问的目标服务器。代理服务器接受请求，然后将请求转发给内部网络的服务器(集群化)，并将从服务器上得到的结果返回给客户端，此时代理服务器对外就表现为一个服务器。反向代理的最大特点是代理服务器在服务器端</p><p><img src="https://yerias.github.io/nginx_img/1.jpg" alt=""></p><h3 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h3><p>负载均衡是在高并发情况下需要使用。其原理就是将数据流量分摊到多个服务器执行，减轻每台服务器的压力，多台服务器(集群)共同完成工作任务，从而提高了数据的吞吐量。</p><p>Nginx可使用的负载均衡策略有：轮询（默认）、权重、ip_hash、url_hash(第三方)、fair(第三方)</p><p><img src="https://yerias.github.io/nginx_img/2.jpg" alt=""></p><p>Ip hash算法，对客户端请求的ip进行hash操作，然后根据hash结果将同一个客户端ip的请求分发给同一台服务器进行处理，可以解决session不共享的问题。 </p><p><img src="https://yerias.github.io/nginx_img/3.jpg" alt=""></p><h3 id="动静分离"><a href="#动静分离" class="headerlink" title="动静分离"></a>动静分离</h3><p>Nginx提供的动静分离是指把动态请求和静态请求分离开，合适的服务器处理相应的请求，使整个服务器系统的性能、效率更高。</p><p>Nginx可以根据配置对不同的请求做不同转发，这是动态分离的基础。静态请求对应的静态资源可以直接放在Nginx上做缓冲，更好的做法是放在相应的缓冲服务器上。动态请求由相应的后端服务器处理。</p><p>常见的静态文件有js、css、html，常见的动态文件有jsp、servlet</p><h2 id="Nginx的安装"><a href="#Nginx的安装" class="headerlink" title="Nginx的安装"></a>Nginx的安装</h2><h3 id="安装所需环境"><a href="#安装所需环境" class="headerlink" title="安装所需环境"></a>安装所需环境</h3><p><strong>一. gcc 安装</strong><br>安装 nginx 需要先将官网下载的源码进行编译，编译依赖 gcc 环境，如果没有 gcc 环境，则需要安装：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install gcc-c++</span><br></pre></td></tr></table></figure><p><strong>二. PCRE pcre-devel 安装</strong><br>PCRE(Perl Compatible Regular Expressions) 是一个Perl库，包括 perl 兼容的正则表达式库。nginx 的 http 模块使用 pcre 来解析正则表达式，所以需要在 linux 上安装 pcre 库，pcre-devel 是使用 pcre 开发的一个二次开发库。nginx也需要此库。命令：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install -y pcre pcre-devel</span><br></pre></td></tr></table></figure><p><strong>三. zlib 安装</strong><br>zlib 库提供了很多种压缩和解压缩的方式， nginx 使用 zlib 对 http 包的内容进行 gzip ，所以需要在 Centos 上安装 zlib 库。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install -y zlib zlib-devel</span><br></pre></td></tr></table></figure><p><strong>四. OpenSSL 安装</strong><br>OpenSSL 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及 SSL 协议，并提供丰富的应用程序供测试或其它目的使用。<br>nginx 不仅支持 http 协议，还支持 https（即在ssl协议上传输http），所以需要在 Centos 安装 OpenSSL 库。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install -y openssl openssl-devel</span><br></pre></td></tr></table></figure><p><strong>五. 综合操作下载命令</strong></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install gcc-c++ pcre pcre-devel zlib zlib-devel openssl openssl-devel -y</span><br></pre></td></tr></table></figure><h3 id="下载安装Nginx"><a href="#下载安装Nginx" class="headerlink" title="下载安装Nginx"></a>下载安装Nginx</h3><ol><li><p>官网下载<code>.tar.gz</code>安装包，地址：<a href="https://nginx.org/en/download.html" target="_blank" rel="noopener">https://nginx.org/en/download.html</a></p></li><li><p>使用<code>wget</code>命令下载（推荐）。确保系统已经安装了wget，如果没有安装，执行 yum install wget 安装。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget -c https://nginx.org/download/nginx-1.12.0.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>解压依然是直接命令：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf nginx-1.12.0.tar.gz -C ../app</span><br><span class="line">cd nginx-1.12.0</span><br></pre></td></tr></table></figure></li><li><p>默认配置(默认是安装在/usr/local/nginx，不推介指定安装目录)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./configure</span><br></pre></td></tr></table></figure></li><li><p>编译安装(root用户)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure></li><li><p>查找安装路径</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">whereis nginx</span><br></pre></td></tr></table></figure><p>结果：/usr/local/nginx</p></li></ol><h2 id="Nginx常用操作"><a href="#Nginx常用操作" class="headerlink" title="Nginx常用操作"></a>Nginx常用操作</h2><ol><li><p>启动、停止nginx</p><p>cd /usr/local/nginx/sbin/</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./nginx #启动</span><br><span class="line">./nginx -s stop#停止</span><br><span class="line">./nginx -s quit#退出</span><br><span class="line">./nginx -s reload#重新加载配置文件</span><br></pre></td></tr></table></figure><p>启动时报80端口被占用:nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)</p><p>解决办法：安装net-tool 包：<code>yum install net-tools</code></p><p><strong>命令详解：</strong></p><p><code>./nginx -s quit</code>:此方式停止步骤是待nginx进程处理任务完毕进行停止。<br><code>./nginx -s stop</code>:此方式相当于先查出nginx进程id再使用kill命令强制杀掉进程。</p></li><li><p>重启Nginx</p><p>对 nginx 进行重启相当于先停止再启动，即先执行停止命令再执行启动命令。如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./nginx -s quit</span><br><span class="line">./nginx</span><br></pre></td></tr></table></figure></li><li><p>重新加载配置文件</p><p>当 ngin x的配置文件 nginx.conf 修改后，要想让配置生效需要重启 nginx，使用<code>-s reload</code>不用先停止 ngin x再启动 nginx 即可将配置信息在 nginx 中生效，如下:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./nginx -s reload</span><br></pre></td></tr></table></figure><p>启动成功后，可打开浏览器访问，默认端口80</p></li><li><p>配置开机自启</p><p>即在<code>rc.local</code>增加启动代码就可以了。</p><p>vi /etc/rc.local</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/usr/local/nginx/sbin/nginx</span><br></pre></td></tr></table></figure><p>设置执行权限：<code>chmod 755 rc.local</code></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HUE安装&amp;集成</title>
      <link href="/2020/03/09/hue/1/"/>
      <url>/2020/03/09/hue/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li><p>hue简介</p></li><li><p>安装maven</p></li><li><p>安装ant</p></li><li><p>安装hue</p></li><li><p>hue集成hdfs</p></li><li><p>hue集成yarn</p></li><li><p>hue集成hive</p></li><li><p>hue集成mysql</p></li><li><p>hue集成zookeeper</p></li><li><p>hue集成hbase</p></li><li><p>hue集成oozie</p></li><li><p>Shell脚本</p></li></ol><h2 id="hue简介"><a href="#hue简介" class="headerlink" title="hue简介"></a>hue简介</h2><p>HUE=Hadoop User Experience</p><p>Hue是一个开源的Apache Hadoop UI系统，由Cloudera Desktop演化而来，最后Cloudera公司将其贡献给Apache基金会的Hadoop社区，它是基于Python Web框架Django实现的。</p><p>通过使用Hue，可以在浏览器端的Web控制台上与Hadoop集群进行交互，来分析处理数据，例如操作HDFS上的数据，运行MapReduce Job，执行Hive的SQL语句，浏览HBase数据库等等。（说人话就是支持提供各种Web图形化界面的）</p><h2 id="安装maven"><a href="#安装maven" class="headerlink" title="安装maven"></a>安装maven</h2><ol><li>上传解压apache-maven-3.6.3-bin.tar到~/app目录下</li><li>配置环境变量</li><li>输入<code>mvn -version</code>测试是否安装成功</li></ol><h2 id="安装ant"><a href="#安装ant" class="headerlink" title="安装ant"></a>安装ant</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install ant -y</span><br></pre></td></tr></table></figure><h2 id="安装hue"><a href="#安装hue" class="headerlink" title="安装hue"></a>安装hue</h2><ol><li><p>hue相关网站</p><p>hue官网：<a href="http://gethue.com/" target="_blank" rel="noopener">http://gethue.com/</a></p><p>配置文档：<a href="http://archive.cloudera.com/cdh5/cdh/5/hue-3.9.0-cdh5.16.2" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hue-3.9.0-cdh5.16.2</a></p><p>源码：<a href="https://github.com/cloudera/hue" target="_blank" rel="noopener">https://github.com/cloudera/hue</a></p><p>这里我们直接用下载hue：<a href="http://archive.cloudera.com/cdh5/cdh/5/hue-3.9.0-cdh5.16.2.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hue-3.9.0-cdh5.16.2.tar.gz</a></p></li><li><p>安装hue所需要的依赖包</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install asciidoc cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-plain gcc gcc-c++ krb5-devel libffi-devel libtidy libxml2-devel libxslt-devel make mysql mysql-devel openldap-devel python-devel sqlite-devel openssl-devel gmp-devel -y</span><br></pre></td></tr></table></figure></li><li><p>解压安装Hue的tar包</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd ~/software</span><br><span class="line">tar -zxvf hue-3.7.0-cdh5.3.6.tar -C ~/app</span><br></pre></td></tr></table></figure></li><li><p>编译</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd hue-3.7.0-cdh5.3.6</span><br><span class="line">make apps</span><br></pre></td></tr></table></figure></li><li><p>修改hue.ini配置文件</p><p>进入到desktop/conf目录下,找到hue.ini文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">secret_key=jFE93j;2[290-eiw.KEiwN2s3['d;/.q[eIW^y#e=+Iei*@Mn&lt;qW5o</span><br><span class="line">http_host=hadoop</span><br><span class="line">http_port=8888</span><br><span class="line">time_zone=Asia/Shanghai</span><br></pre></td></tr></table></figure></li><li><p>启动</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">build/env/bin/supervisor</span><br></pre></td></tr></table></figure></li></ol><h2 id="hue集成hdfs"><a href="#hue集成hdfs" class="headerlink" title="hue集成hdfs"></a>hue集成hdfs</h2><ol><li><p>配置hdfs.site.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>配置core-site.xml</p><p>设置代理用户</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hadoop.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hadoop.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果你的Hadoop配置了<code>高可用</code>，则必须通过httpfs来访问，需要添加如下属性，反则则不必须。（如果HUE服务与Hadoop服务不在同一节点，则必须配置）</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.httpfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.httpfs.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置httpfs-site.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>httpfs.proxyuser.hue.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>httpfs.proxyuser.hue.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>以上两个属性主要用于HUE服务与Hadoop服务不在同一台节点上所必须的配置。</p></li><li><p>配置hue.ini</p><p>找到<code>[[hdfs_clusters]]</code>标签</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS服务器地址</span><br><span class="line">fs_defaultfs=hdfs://hadoop:9000</span><br><span class="line"></span><br><span class="line">如果开启了高可用，需要配置如下</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># logical_name=mycluster</span></span></span><br><span class="line"></span><br><span class="line">向HDFS发送命令的请求地址</span><br><span class="line">webhdfs_url=http://hadoop:50070/webhdfs/v1</span><br><span class="line"></span><br><span class="line">HADOOP的一些配置</span><br><span class="line">hadoop_conf_dir=/home/hadoop/app/hadoop/etc/hadoop</span><br><span class="line">hadoop_hdfs_home=/home/hadoop/app/hadoop</span><br><span class="line">hadoop_bin=/home/hadoop/app/hadoop/bin</span><br></pre></td></tr></table></figure></li><li><p>如果配置了高可用则启动hue前需要先启动httpfs服务</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/home/hadoop/app/hadoop/sbin/httpfs.sh start</span><br></pre></td></tr></table></figure></li></ol><h2 id="hue集成yarn"><a href="#hue集成yarn" class="headerlink" title="hue集成yarn"></a>hue集成yarn</h2><p>如果没有在hadoop中配置历史节点，需要先配置历史节点</p><p>mapred-site.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>启动历史服务器</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver</span></span><br></pre></td></tr></table></figure><p>找到<code>[[yarn_clusters]]</code>标签，涉及修改配置如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn服务的配置</span><br><span class="line">resourcemanager_host=192.168.56.86</span><br><span class="line">resourcemanager_port=8032</span><br><span class="line"></span><br><span class="line">是否将作业提交到此群集，并监控作业执行情况</span><br><span class="line">submit_to=True</span><br><span class="line"><span class="meta">#</span><span class="bash">logical_name=cluster-yarn1(如果开高可用的话)</span></span><br><span class="line"></span><br><span class="line">配置yarn资源管理的访问入口</span><br><span class="line">resourcemanager_api_url=http://hadoop:8088</span><br><span class="line">proxy_api_url=http://hadoop:8088</span><br><span class="line"></span><br><span class="line">历史服务器管理的入口，查看作业的历史运行情况</span><br><span class="line">history_server_api_url=http://hadoop:19888</span><br></pre></td></tr></table></figure><p><code>注意</code>：hue界面的超级用户名字需要和提交到yarn的用户名字相同</p><h2 id="hue集成hive"><a href="#hue集成hive" class="headerlink" title="hue集成hive"></a>hue集成hive</h2><p>配置hive-site.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>TCP绑定的主机<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.long.polling.timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>5000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>HiveServer2在响应使用长轮询的异步调用之前等待的时间（毫秒）<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://hadoop:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>指向的是运行metastore服务的主机<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置hue.ini，找到[beeswax]属性标签，涉及修改如下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[beeswax]</span><br><span class="line">hive_server_host=hadoop</span><br><span class="line">hive_server_port=10000</span><br><span class="line">hive_conf_dir=/home/hadoop/app/hive/conf</span><br></pre></td></tr></table></figure><p>启动</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/hive --service metastore &amp;</span><br><span class="line">bin/hive --service hiveserver2 1&gt;/dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p><code>注意：</code>如果设置了uris，在今后使用Hive时，那么必须启动如上两个命令，否则Hive无法正常启动。</p><h2 id="hue集成mysql"><a href="#hue集成mysql" class="headerlink" title="hue集成mysql"></a>hue集成mysql</h2><p>配置hue.ini，找到[[[mysql]]]标签，涉及修改如下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[[[mysql]]]</span><br><span class="line">nice_name="mysql"</span><br><span class="line">engine=mysql</span><br><span class="line">host=hadoop</span><br><span class="line">port=3306</span><br><span class="line">user=root</span><br><span class="line">password=root</span><br></pre></td></tr></table></figure><h2 id="hue集成Zookeeper"><a href="#hue集成Zookeeper" class="headerlink" title="hue集成Zookeeper"></a>hue集成Zookeeper</h2><p>配置hue.ini</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zookeeper]</span><br><span class="line">[[clusters]]</span><br><span class="line">[[[default]]]</span><br><span class="line">host_ports=hadoop:2181</span><br></pre></td></tr></table></figure><p>启动zookeeper</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zkServer.sh start</span><br><span class="line">zkServer.sh status</span><br></pre></td></tr></table></figure><h2 id="hue集成hbase"><a href="#hue集成hbase" class="headerlink" title="hue集成hbase"></a>hue集成hbase</h2><h2 id="hue集成oozie"><a href="#hue集成oozie" class="headerlink" title="hue集成oozie"></a>hue集成oozie</h2><h2 id="Shell脚本"><a href="#Shell脚本" class="headerlink" title="Shell脚本"></a>Shell脚本</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line">echo "启动httpfs服务"</span><br><span class="line">/home/hadoop/app/hadoop/sbin/httpfs.sh start</span><br><span class="line"></span><br><span class="line">echo "启动历史服务"</span><br><span class="line">/home/hadoop/app/hadoop/sbin/mr-jobhistory-daemon.sh start historyserver</span><br><span class="line"></span><br><span class="line">echo "启动hive元数据"</span><br><span class="line"><span class="meta">$</span><span class="bash">HIVE_HOME/bin/hive --service metastore &amp;</span></span><br><span class="line"></span><br><span class="line">echo "启动hive服务端"</span><br><span class="line"><span class="meta">$</span><span class="bash">HIVE_HOME/bin/hive --service hiveserver2 1 &gt; /dev/null 2&gt;&amp;1 &amp;</span></span><br><span class="line"></span><br><span class="line">echo "启动hue"</span><br><span class="line"><span class="meta">$</span><span class="bash">HUE_HOME/build/env/bin/supervisor 1 &gt; /dev/null 2&gt;&amp;1 &amp;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hue </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FastJson的使用</title>
      <link href="/2020/03/07/java/9/"/>
      <url>/2020/03/07/java/9/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近在开发过程中使用了大量的<code>json</code>作为前后端数据交换的方式,由于之前没有对<code>json</code>做过系统的学习,所有在使用过程中查阅了大量的文档与资料,这里主要记录了我在开发后对<code>json</code>以及<code>FastJson</code>使用的总结</p><h2 id="Json介绍"><a href="#Json介绍" class="headerlink" title="Json介绍"></a>Json介绍</h2><p><code>JSON</code>(javaScript Object Notation)是一种轻量级的数据交换格式。主要采用键值对(<code>{&quot;name&quot;: &quot;json&quot;}</code>)的方式来保存和表示数据。<code>JSON</code>是<code>JS</code>对象的字符串表示法，它使用文本表示一个<code>JS</code>对象的信息，本质上是一个字符串。更多简介见<a href="http://www.json.org/json-zh.html" target="_blank" rel="noopener">介绍JSON</a>。</p><h2 id="FastJson-简介"><a href="#FastJson-简介" class="headerlink" title="FastJson  简介"></a>FastJson  简介</h2><p>在日志解析,前后端数据传输交互中,经常会遇到字符串(String)与<code>json</code>,<code>XML</code>等格式相互转换与解析，其中<code>json</code>以跨语言，跨前后端的优点在开发中被频繁使用，基本上可以说是标准的数据交换格式。<a href="https://github.com/alibaba/FastJson" target="_blank" rel="noopener">fastjson</a>是一个java语言编写的高性能且功能完善的JSON库，它采用一种“假定有序快速匹配”的算法，把<code>JSON Parse</code> 的性能提升到了极致。它的接口简单易用，已经被广泛使用在缓存序列化，协议交互，Web输出等各种应用场景中。</p><h2 id="FastJson-常用-API"><a href="#FastJson-常用-API" class="headerlink" title="FastJson  常用 API"></a>FastJson  常用 API</h2><p>导入Jar包</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;com.alibaba&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;fastjson&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;1.2.47&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>FastJson API 入口类是<code>com.alibaba.FastJson.JSON</code>,常用的序列化操作都可以在<code>JSON</code>类上的静态方法直接完成。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Object <span class="title">parse</span><span class="params">(String text)</span></span>; <span class="comment">// 把JSON文本parse为JSONObject或者JSONArray </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> JSONObject <span class="title">parseObject</span><span class="params">(String text)</span>； <span class="comment">// 把JSON文本parse成JSONObject    </span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> &lt;T&gt; T <span class="title">parseObject</span><span class="params">(String text, Class&lt;T&gt; clazz)</span></span>; <span class="comment">// 把JSON文本parse为JavaBean </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> JSONArray <span class="title">parseArray</span><span class="params">(String text)</span></span>; <span class="comment">// 把JSON文本parse成JSONArray </span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> &lt;T&gt; <span class="function">List&lt;T&gt; <span class="title">parseArray</span><span class="params">(String text, Class&lt;T&gt; clazz)</span></span>; <span class="comment">//把JSON文本parse成JavaBean集合 </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String <span class="title">toJSONString</span><span class="params">(Object object)</span></span>; <span class="comment">// 将JavaBean序列化为JSON文本 </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String <span class="title">toJSONString</span><span class="params">(Object object, <span class="keyword">boolean</span> prettyFormat)</span></span>; <span class="comment">// 将JavaBean序列化为带格式的JSON文本 </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Object <span class="title">toJSON</span><span class="params">(Object javaObject)</span></span>; <span class="comment">//将JavaBean转换为JSONObject或者JSONArray。</span></span><br></pre></td></tr></table></figure><h3 id="使用方法举例"><a href="#使用方法举例" class="headerlink" title="使用方法举例"></a>使用方法举例</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//将JSON文本转换为java对象</span></span><br><span class="line"><span class="keyword">import</span> com.alibaba.FastJson.JSON;</span><br><span class="line">Model model = JSON.parseObject(jsonStr, Model<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure><h3 id="有关类库的一些说明"><a href="#有关类库的一些说明" class="headerlink" title="有关类库的一些说明"></a>有关类库的一些说明</h3><ul><li>JSONArray : 相当于List</li><li>JSONObject: 相当于Map&lt;String,Object&gt;</li></ul><h2 id="FastJson-使用实例"><a href="#FastJson-使用实例" class="headerlink" title="FastJson  使用实例"></a>FastJson  使用实例</h2><h3 id="Java对象与Json字符串的互转"><a href="#Java对象与Json字符串的互转" class="headerlink" title="Java对象与Json字符串的互转"></a>Java对象与Json字符串的互转</h3><p><code>User</code>测试类</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.json.user;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-03-07 09:54</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String username;</span><br><span class="line">    <span class="keyword">private</span> String password;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">User</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">User</span><span class="params">(String username, String password)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.username = username;</span><br><span class="line">        <span class="keyword">this</span>.password = password;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getUsername</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> username;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUsername</span><span class="params">(String username)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.username = username;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPassword</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> password;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPassword</span><span class="params">(String password)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.password = password;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>UserGroup</code>测试类</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.json.user;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-03-07 09:55</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserGroup</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> List&lt;User&gt; users= <span class="keyword">new</span> ArrayList&lt;User&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">UserGroup</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">UserGroup</span><span class="params">(String name, List&lt;User&gt; users)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.users = users;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;User&gt; <span class="title">getUsers</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> users;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUsers</span><span class="params">(List&lt;User&gt; users)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.users = users;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>fastJson</code>测试类</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.json.user;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-03-07 09:58</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestFastJosn</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        objectToJson();</span><br><span class="line">        JsonToObject();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//java对象转 json字符串</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">objectToJson</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="comment">//简单java类转json</span></span><br><span class="line">        User user = <span class="keyword">new</span> User(<span class="string">"tunan"</span>,<span class="string">"123456"</span>);</span><br><span class="line">        String userJson = JSON.toJSONString(user);</span><br><span class="line">        System.out.println(<span class="string">"简单java类转字符串： "</span>+userJson);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//List&lt;Object&gt;转Json字符串</span></span><br><span class="line">        User user1 = <span class="keyword">new</span> User(<span class="string">"xiaoqi"</span>, <span class="string">"961228"</span>);</span><br><span class="line">        User user2 = <span class="keyword">new</span> User(<span class="string">"tunan"</span>, <span class="string">"123456"</span>);</span><br><span class="line">        List&lt;User&gt; users = <span class="keyword">new</span> ArrayList&lt;User&gt;();</span><br><span class="line">        users.add(user1);</span><br><span class="line">        users.add(user2);</span><br><span class="line">        String listJson = JSON.toJSONString(users);</span><br><span class="line">        System.out.println(<span class="string">"List&lt;Object&gt;转Json字符串： "</span>+listJson);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//复杂java转json字符串</span></span><br><span class="line">        UserGroup userGroup = <span class="keyword">new</span> UserGroup(<span class="string">"student"</span>, users);</span><br><span class="line">        String userGroupJson = JSON.toJSONString(userGroup);</span><br><span class="line">        System.out.println(<span class="string">"复杂java转json字符串： "</span>+userGroupJson);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Json字符串 转Java类</span></span><br><span class="line">    <span class="comment">//注：字符串中使用双引号需要转义 (" --&gt; \"),这里使用的是单引号</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">JsonToObject</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * json字符串转简单java对象</span></span><br><span class="line"><span class="comment">         * 字符串：&#123;"password":"123456","username":"dmego"&#125;</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        String userJson = <span class="string">"&#123;'password':'123456','username':'dmego'&#125;"</span>;</span><br><span class="line">        User user = JSON.parseObject(userJson, User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        System.out.println(<span class="string">"json字符串转简单java对象: "</span>+user.toString());</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * json字符串转List&lt;Object&gt;对象</span></span><br><span class="line"><span class="comment">         * 字符串：[&#123;"password":"123123","username":"zhangsan"&#125;,&#123;"password":"321321","username":"lisi"&#125;]</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        String listUserJson = <span class="string">"[&#123;'password':'123123','username':'zhangsan'&#125;,&#123;'password':'321321','username':'lisi'&#125;]"</span>;</span><br><span class="line">        List&lt;User&gt; userList = JSON.parseArray(listUserJson, User<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        System.out.println(<span class="string">"json字符串转List&lt;Object&gt;对象: "</span>+userList.toString());</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*json字符串转复杂java对象</span></span><br><span class="line"><span class="comment">         * 字符串：&#123;"name":"userGroup","users":[&#123;"password":"123123","username":"zhangsan"&#125;,&#123;"password":"321321","username":"lisi"&#125;]&#125;</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        String userGroupJson = <span class="string">"&#123;'name':'userGroup','users':[&#123;'password':'123123','username':'zhangsan'&#125;,&#123;'password':'321321','username':'lisi'&#125;]&#125;"</span>;</span><br><span class="line">        UserGroup userGroup = JSON.parseObject(userGroupJson, UserGroup<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        System.out.println(<span class="string">"json字符串转复杂java对象: "</span>+userGroup.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">简单java类转字符串： &#123;<span class="string">"password"</span>:<span class="string">"123456"</span>,<span class="string">"username"</span>:<span class="string">"tunan"</span>&#125;</span><br><span class="line">List&lt;Object&gt;转Json字符串： [&#123;<span class="string">"password"</span>:<span class="string">"961228"</span>,<span class="string">"username"</span>:<span class="string">"xiaoqi"</span>&#125;,&#123;<span class="string">"password"</span>:<span class="string">"123456"</span>,<span class="string">"username"</span>:<span class="string">"tunan"</span>&#125;]</span><br><span class="line">复杂java转json字符串： &#123;<span class="string">"name"</span>:<span class="string">"student"</span>,<span class="string">"users"</span>:[&#123;<span class="string">"password"</span>:<span class="string">"961228"</span>,<span class="string">"username"</span>:<span class="string">"xiaoqi"</span>&#125;,&#123;<span class="string">"password"</span>:<span class="string">"123456"</span>,<span class="string">"username"</span>:<span class="string">"tunan"</span>&#125;]&#125;</span><br><span class="line"></span><br><span class="line">json字符串转简单java对象: User&#123;username=<span class="string">'dmego'</span>, password=<span class="string">'123456'</span>&#125;</span><br><span class="line">json字符串转List&lt;Object&gt;对象: [User&#123;username=<span class="string">'zhangsan'</span>, password=<span class="string">'123123'</span>&#125;, User&#123;username=<span class="string">'lisi'</span>, password=<span class="string">'321321'</span>&#125;]</span><br><span class="line">json字符串转复杂java对象: UserGroup&#123;name=<span class="string">'userGroup'</span>, users=[User&#123;username=<span class="string">'zhangsan'</span>, password=<span class="string">'123123'</span>&#125;, User&#123;username=<span class="string">'lisi'</span>, password=<span class="string">'321321'</span>&#125;]&#125;</span><br></pre></td></tr></table></figure><p>通过结果看代码:</p><p>我们可以看到，在<code>JavaBean</code>转<code>Json</code>的时候，使用了<code>JSON.toJSONString()</code>方法</p><ol><li>该方法中传入的是对象，即生成对象的<code>Json</code>字符串</li><li>如果传入的是<code>List</code>，生成的就是对象的<code>Json</code>字符串数组</li><li>如果传入的是复杂类型既有对象，又有对象数组，那么生成的就是个<code>Json</code>对象，这个对象中包括对象的<code>Json</code>字符串和对象的<code>Json</code>字符串数组，<code>Json</code>的字符串数组中又包含对象的<code>Json</code>字符串。</li></ol><p>在<code>Json</code>转<code>JavaBean</code>的时候，会根据调用<code>JSON.parseObject()</code>和<code>JSON.parseArray()</code>的不同，输出不同的结果</p><ol><li>传入对象的<code>Json</code>字符串和需要生成的<code>Java</code>对象，需要调用<code>JSON.parseObject()</code>方法，输出的就是一个普通的对象</li><li>传入对象的<code>Json</code>字符串数组和要生成的<code>Java</code>对象，需要调用<code>JSON.parseArray()</code>方法，输出是一个<code>Java</code>对象数组</li><li>传入的即包含对象的<code>Json</code>字符串，又包含对象的<code>Json</code>字符串数组，那么需要调用一个<code>JSON.parseObject()</code>方法，该方法会输出一个复杂的Java对象，该对象既包括对象又包括对象数组。</li></ol><h2 id="FastJson解析复杂嵌套Json字符串"><a href="#FastJson解析复杂嵌套Json字符串" class="headerlink" title="FastJson解析复杂嵌套Json字符串"></a>FastJson解析复杂嵌套Json字符串</h2><p>这个实例是我在开发中用到的，先给出要解析的Json字符串</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">"id"</span>: <span class="string">"user_list"</span>,</span><br><span class="line">        <span class="string">"key"</span>: <span class="string">"id"</span>,</span><br><span class="line">        <span class="string">"tableName"</span>: <span class="string">"用户列表"</span>,</span><br><span class="line">        <span class="string">"className"</span>: <span class="string">"cn.dmego.domain.User"</span>,</span><br><span class="line">        <span class="string">"column"</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"key"</span>: <span class="string">"rowIndex"</span>,</span><br><span class="line">                <span class="string">"header"</span>: <span class="string">"序号"</span>,</span><br><span class="line">                <span class="string">"width"</span>: <span class="string">"50"</span>,</span><br><span class="line">                <span class="string">"allowSort"</span>: <span class="string">"false"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"key"</span>: <span class="string">"id"</span>,</span><br><span class="line">                <span class="string">"header"</span>: <span class="string">"id"</span>,</span><br><span class="line">                <span class="string">"hidden"</span>: <span class="string">"true"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"key"</span>: <span class="string">"name"</span>,</span><br><span class="line">                <span class="string">"header"</span>: <span class="string">"姓名"</span>,</span><br><span class="line">                <span class="string">"width"</span>: <span class="string">"100"</span>,</span><br><span class="line">                <span class="string">"allowSort"</span>: <span class="string">"true"</span></span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">"id"</span>: <span class="string">"role_list"</span>,</span><br><span class="line">        <span class="string">"key"</span>: <span class="string">"id"</span>,</span><br><span class="line">        <span class="string">"tableName"</span>: <span class="string">"角色列表"</span>,</span><br><span class="line">        <span class="string">"className"</span>: <span class="string">"cn.dmego.domain.Role"</span>,</span><br><span class="line">        <span class="string">"column"</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"key"</span>: <span class="string">"rowIndex"</span>,</span><br><span class="line">                <span class="string">"header"</span>: <span class="string">"序号"</span>,</span><br><span class="line">                <span class="string">"width"</span>: <span class="string">"50"</span>,</span><br><span class="line">                <span class="string">"allowSort"</span>: <span class="string">"false"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"key"</span>: <span class="string">"id"</span>,</span><br><span class="line">                <span class="string">"header"</span>: <span class="string">"id"</span>,</span><br><span class="line">                <span class="string">"hidden"</span>: <span class="string">"true"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"key"</span>: <span class="string">"name"</span>,</span><br><span class="line">                <span class="string">"header"</span>: <span class="string">"名称"</span>,</span><br><span class="line">                <span class="string">"width"</span>: <span class="string">"100"</span>,</span><br><span class="line">                <span class="string">"allowSort"</span>: <span class="string">"true"</span></span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>要想解析这种复杂的字符串，首先得先定义好与之相符的Java POJO 对象，经过观察，我们发现，这个是一个Json对象数组，每一个对象里包含了许多属性，其中还有一个属性的类型也是对象数组。所有，我们从里到外，先定义最里面的对象：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Column</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String key;</span><br><span class="line">    <span class="keyword">private</span> String header;</span><br><span class="line">    <span class="keyword">private</span> String width;</span><br><span class="line">    <span class="keyword">private</span> String allowSort;</span><br><span class="line">    <span class="keyword">private</span> String hidden;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Column&#123;"</span> +</span><br><span class="line">                <span class="string">"key='"</span> + key + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", header='"</span> + header + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", width='"</span> + width + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", allowSort='"</span> + allowSort + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", hidden='"</span> + hidden + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//这里省略部分getter与setter方法 </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>再定义外层的对象：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Query</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String id;</span><br><span class="line">    <span class="keyword">private</span> String key;</span><br><span class="line">    <span class="keyword">private</span> String tableName;</span><br><span class="line">    <span class="keyword">private</span> String className;</span><br><span class="line">    <span class="keyword">private</span> List&lt;Column&gt; column;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Query&#123;"</span> +</span><br><span class="line">                <span class="string">"id='"</span> + id + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", key='"</span> + key + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", tableName='"</span> + tableName + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", className='"</span> + className + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", column="</span> + column +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">     <span class="comment">//这里省略部分getter与setter方法 </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我的这个Json文件放置在类路径下，最后想将这个Json字符串转化为List对象，并且将column 对象数组转化为query对象里的List属性</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>: 将这个json字符串转化为List对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-03-07 10:47</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RoleTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        RoleTest roleTest = <span class="keyword">new</span> RoleTest();</span><br><span class="line">        List&lt;Query&gt; queries = roleTest.JsonToObject();</span><br><span class="line">        <span class="comment">//生成的是个List，需要循环输出</span></span><br><span class="line">        queries.forEach(System.out::println);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span>  List&lt;Query&gt; <span class="title">JsonToObject</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        ClassLoader loader = <span class="keyword">this</span>.getClass().getClassLoader();</span><br><span class="line">        InputStream in = loader.getResourceAsStream(<span class="string">"query.json"</span>);</span><br><span class="line">        <span class="comment">//这里是一次全部读出来了，大数据处理时需要按行读取</span></span><br><span class="line">        String jsonTxet = IOUtils.toString(in, <span class="string">"utf8"</span>);</span><br><span class="line">        List&lt;Query&gt; query = JSON.parseArray(jsonTxet, Query<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="keyword">return</span> query;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Query&#123;id=<span class="string">'user_list'</span>, key=<span class="string">'id'</span>, tableName=<span class="string">'用户列表'</span>, className=<span class="string">'cn.dmego.domain.User'</span>, column=[Column&#123;key=<span class="string">'rowIndex'</span>, header=<span class="string">'序号'</span>, width=<span class="string">'50'</span>, allowSort=<span class="string">'false'</span>, hidden=<span class="string">'null'</span>&#125;, Column&#123;key=<span class="string">'id'</span>, header=<span class="string">'id'</span>, width=<span class="string">'null'</span>, allowSort=<span class="string">'null'</span>, hidden=<span class="string">'true'</span>&#125;, Column&#123;key=<span class="string">'name'</span>, header=<span class="string">'姓名'</span>, width=<span class="string">'100'</span>, allowSort=<span class="string">'true'</span>, hidden=<span class="string">'null'</span>&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Query&#123;id=<span class="string">'role_list'</span>, key=<span class="string">'id'</span>, tableName=<span class="string">'角色列表'</span>, className=<span class="string">'cn.dmego.domain.Role'</span>, column=[Column&#123;key=<span class="string">'rowIndex'</span>, header=<span class="string">'序号'</span>, width=<span class="string">'50'</span>, allowSort=<span class="string">'false'</span>, hidden=<span class="string">'null'</span>&#125;, Column&#123;key=<span class="string">'id'</span>, header=<span class="string">'id'</span>, width=<span class="string">'null'</span>, allowSort=<span class="string">'null'</span>, hidden=<span class="string">'true'</span>&#125;, Column&#123;key=<span class="string">'name'</span>, header=<span class="string">'名称'</span>, width=<span class="string">'100'</span>, allowSort=<span class="string">'true'</span>, hidden=<span class="string">'null'</span>&#125;]&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala之高阶函数</title>
      <link href="/2020/03/03/scala/1/"/>
      <url>/2020/03/03/scala/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>sorted</li><li>sortBy</li><li>sortWith</li><li>flatten</li><li>map</li><li>flatMap</li><li>filter</li><li>groupBy</li><li>flod</li><li>reduce</li><li>wc</li></ol><h2 id="sorted"><a href="#sorted" class="headerlink" title="sorted"></a>sorted</h2><p>排序，默认升序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> array = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">    array.sorted</span><br></pre></td></tr></table></figure><h2 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h2><p>排序，指定排序规则</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> arrMap = <span class="type">Array</span>(('a', <span class="number">1</span>), ('b', <span class="number">4</span>), ('d', <span class="number">3</span>), ('c', <span class="number">2</span>))</span><br><span class="line">    arrMap.sortBy(_._1)</span><br><span class="line">    arrMap.sortBy(_._2)</span><br></pre></td></tr></table></figure><h2 id="sortWith"><a href="#sortWith" class="headerlink" title="sortWith"></a>sortWith</h2><p>排序，多个字段联合排序</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> arrMap2 = <span class="type">Array</span>((<span class="string">"a"</span>, <span class="number">2</span>), (<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">2</span>), (<span class="string">"d"</span>, <span class="number">5</span>), (<span class="string">"c"</span>, <span class="number">4</span>))</span><br><span class="line">    arrMap2.sortWith((t1,t2)=&gt;&#123;</span><br><span class="line">      <span class="keyword">if</span> (!t1._1.equalsIgnoreCase(t2._1))&#123;</span><br><span class="line">        t1._1.compareTo(t2._1)&lt;<span class="number">0</span></span><br><span class="line">      &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        t1._2.compareTo(t2._2)&lt;<span class="number">0</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure><h2 id="flatten"><a href="#flatten" class="headerlink" title="flatten"></a>flatten</h2><p>扁平化</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> flatten = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), <span class="type">Array</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>))</span><br><span class="line">    flatten.flatten</span><br></pre></td></tr></table></figure><h2 id="map"><a href="#map" class="headerlink" title="map"></a>map</h2><p>映射</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> map = <span class="type">Array</span>(<span class="string">"this is a demo,hello world"</span>)</span><br><span class="line">    map.map(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    map.map(_.split(<span class="string">" "</span>)).flatten.map((_,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><h2 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h2><p>flatMap ==&gt; flatten + map</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> flatMap = <span class="type">Array</span>(<span class="string">"this is a demo"</span>, <span class="string">"hello word"</span>)</span><br><span class="line">    flatMap.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    flatMap.flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    flatMap.flatMap(line =&gt; (<span class="keyword">for</span> (i&lt;- line.split(<span class="string">" "</span>)) <span class="keyword">yield</span> (i,<span class="number">1</span>)))</span><br></pre></td></tr></table></figure><h2 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h2><p>过滤</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> filter = <span class="type">Array</span>(<span class="string">"this is a demo"</span>, <span class="string">"hello word"</span>)</span><br><span class="line">    filter.filter(_.contains(<span class="string">"hello"</span>))</span><br><span class="line">    filter.filterNot(_.contains(<span class="string">"hello"</span>))</span><br></pre></td></tr></table></figure><h2 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h2><p>分组</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> groupBy = <span class="type">Array</span>(<span class="string">"good"</span>, <span class="string">"good"</span>, <span class="string">"study"</span>)</span><br><span class="line">    groupBy.groupBy(x=&gt;x)</span><br><span class="line">    groupBy.groupBy(x=&gt;x).map(t=&gt;(t._1,t._2.size))</span><br></pre></td></tr></table></figure><h2 id="flod"><a href="#flod" class="headerlink" title="flod"></a>flod</h2><p>聚合，需要指定默认值</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> flod = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">4</span>)</span><br><span class="line">    flod.fold(<span class="number">10</span>)((a,b)=&gt;&#123;</span><br><span class="line">      println(a+<span class="string">"  :  "</span>+b)</span><br><span class="line">      a-b</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure><h2 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h2><p>聚合</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> reduce = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">4</span>)</span><br><span class="line">    reduce.reduce(_+_)</span><br><span class="line">    reduce.reduce(_-_)</span><br><span class="line">    reduce.reduceLeft(_+_)</span><br><span class="line">    reduce.reduceLeft(_-_)</span><br><span class="line">    reduce.reduceRight(_+_)</span><br><span class="line">    reduce.reduceRight(_-_)</span><br></pre></td></tr></table></figure><h2 id="wc"><a href="#wc" class="headerlink" title="wc"></a>wc</h2><p>方案1</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> wc = <span class="type">Array</span>(<span class="string">"this is demo"</span>, <span class="string">"good good study"</span>, <span class="string">"day day up"</span>)</span><br><span class="line">    wc.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .groupBy(word=&gt;word)</span><br><span class="line">      .toList</span><br><span class="line">      .map(t=&gt;(t._1,t._2.size))</span><br><span class="line">      .sortBy(_._1)</span><br><span class="line">      .mkString(<span class="string">","</span>)</span><br></pre></td></tr></table></figure><p>方案2</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">wc.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">     .map((_,<span class="number">1</span>))</span><br><span class="line">     .groupBy(_._1)</span><br><span class="line">     .toList</span><br><span class="line">     .map(t=&gt;(t._1,t._2.size))</span><br><span class="line">     .sortBy(_._1)</span><br><span class="line">     .mkString(<span class="string">"\t"</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zookeeper的安装&amp;使用</title>
      <link href="/2020/03/02/zookeeper/1/"/>
      <url>/2020/03/02/zookeeper/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>什么是分布式</li><li>为什么选择Zookeeper</li><li>Zookeeper的设计目标</li><li>Zookeeper的数据模型</li><li>安装Zookeeper</li><li>单节点配置Zookeeper</li><li>多节点配置Zookeeper</li><li>Zookeeper的常用命令</li><li>Zookeeper的监听</li><li>Zookeeper四字命令</li></ol><h2 id="什么是分布式"><a href="#什么是分布式" class="headerlink" title="什么是分布式"></a>什么是分布式</h2><p>分布式系统</p><p>分布式系统是由一组通过网络进行通信、为了完成共同的任务而协调工作的计算机节点组成的系统。分布式系统的出现是为了用廉价的、普通的机器完成单个计算机无法完成的计算、存储任务。其目的是<strong>利用更多的机器，处理更多的数据</strong>。</p><p>分布式协调技术</p><p>分布式系统的出现带来了分布式协调的问题，也就是我们如何对分布式系统中的进程进行调度，假设我们在某一台机器上面挂载了一个资源，然后其他物理分布的进程都要竞争这个资源，但是我们又不希望他们同时访问，这时候就需要一个协调器，让他们有序的来访问在这个资源。这个协调器就是我们经常提到的那个<strong>锁</strong>，比如说”进程-1”在使用该资源的时候，会先去获得锁，”进程1”获得锁以后会对该资源保持<strong>独占</strong>，这样其他进程就无法访问该资源，”进程1”用完该资源以后就将锁释放掉，让其他进程来获得锁，那么通过这个锁机制，我们就能保证了分布式系统中多个进程能够有序的访问该临界资源。那么我们把这个分布式环境下的这个锁叫作<strong>分布式锁</strong>。分布式锁也就是我们<strong>分布式协调技术</strong>实现的核心内容。</p><h2 id="为什么选择zookeeper"><a href="#为什么选择zookeeper" class="headerlink" title="为什么选择zookeeper"></a>为什么选择zookeeper</h2><p>ZooKeeper是一种为分布式应用所设计的高可用、高性能且一致的开源协调服务，它提供了一项基本服务：<strong>分布式锁服务</strong>。由于ZooKeeper的开源特性，后来我们的开发者在分布式锁的基础上，摸索了出了其他的使用方法：<strong>配置维护、组服务、分布式消息队列</strong>、<strong>分布式通知/协调</strong>等。</p><p>ZooKeeper在实现这些服务时，首先它设计一种新的<strong>数据结构——Znode</strong>，然后在该数据结构的基础上定义了一些<strong>原语</strong>，也就是一些关于该数据结构的一些操作。有了这些数据结构和原语还不够，因为我们的ZooKeeper是工作在一个分布式的环境下，我们的服务是通过消息以网络的形式发送给我们的分布式应用程序，所以还需要一个<strong>通知机制</strong>——Watcher机制。那么总结一下，ZooKeeper所提供的服务主要是通过：数据结构+原语+watcher机制，三个部分来实现的。</p><p>ZooKeeper<strong>性能上的特点</strong>决定了它能够用在大型的、分布式的系统当中。从<strong>可靠性</strong>方面来说，它并不会因为一个节点的错误而崩溃。除此之外，它<strong>严格的序列访问控制</strong>意味着复杂的控制原语可以应用在客户端上。ZooKeeper在一致性、可用性、容错性的保证，也是ZooKeeper的成功之处，它获得的一切成功都与它采用的协议——Zab协议是密不可分的，这些内容将会在后面介绍。</p><h2 id="Zookeeper的设计目标"><a href="#Zookeeper的设计目标" class="headerlink" title="Zookeeper的设计目标"></a>Zookeeper的设计目标</h2><p>ZooKeeper 很简单。ZooKeeper允许分布式进程通过共享的层次命名空间相互协调，该命名空间的组织类似于标准文件系统。名称空间由数据寄存器组成——用ZooKeeper的说法称为znodes——这些寄存器类似于文件和目录。与典型的用于存储的文件系统不同，ZooKeeper数据保存在内存中，这意味着ZooKeeper可以实现高吞吐量和低延迟数。</p><p>ZooKeeper实现非常重视高性能、高可用性和严格有序的访问。ZooKeeper的性能方面意味着它可以用于大型分布式系统。可靠性方面使它不会成为单点故障。严格的排序意味着复杂的同步原语可以在客户端实现。</p><p>ZooKeeper 是复制。与它所协调的分布式进程一样，ZooKeeper本身也打算在一组称为集合的主机上进行复制。</p><p><img src="http://zookeeper.apache.org/doc/current/images/zkservice.jpg" alt="Zookeeper集群部署"></p><p>组成ZooKeeper服务的服务器必须相互了解。它们在内存中维护状态映像，以及持久存储中的事务日志和快照。只要大多数服务器可用，ZooKeeper服务就可用。</p><p>客户端连接到单个ZooKeeper服务器。客户端维护一个TCP连接，通过它发送请求、获取响应、获取监视事件和发送心跳。如果到服务器的TCP连接中断，客户机将连接到另一台服务器。</p><p>ZooKeeper是有序的。ZooKeeper用一个数字来标记每个更新，这个数字反映了所有ZooKeeper事务的顺序。后续操作可以使用该顺序来实现更高级别的抽象，比如同步原语。</p><p>ZooKeeper很快。在“以读取为主”的工作负载中，它的速度特别快。ZooKeeper应用程序运行在数千台机器上，当读操作比写操作更常见时，它的性能最好，比率约为10:1。</p><h2 id="Zookeeper的数据模型"><a href="#Zookeeper的数据模型" class="headerlink" title="Zookeeper的数据模型"></a>Zookeeper的数据模型</h2><p>ZooKeeper拥有一个层次的命名空间，这个和标准的文件系统非常相似，如下图所示。</p><p><img src="https://images0.cnblogs.com/blog/671563/201411/301534562152768.png" alt="ZooKeeper数据模型与文件系统目录树"></p><p>从图中我们可以看出ZooKeeper的数据模型，在结构上和标准文件系统的非常相似，都是采用这种树形层次结构，ZooKeeper树中的每个节点被称为—Znode。和文件系统的目录树一样，ZooKeeper树中的每个节点可以拥有子节点。但也有不同之处：</p><ol><li><p>引用方式</p><p>Zonde通过<strong>路径引用</strong>，如同Unix中的文件路径。路径必须是绝对的，因此他们必须由斜杠字符来<strong>开头</strong>。除此以外，他们必须是唯一的，也就是说每一个路径只有一个表示，因此这些路径不能改变。在ZooKeeper中，路径由Unicode字符串组成，并且有一些限制。字符串”/zookeeper”用以保存管理信息，比如关键配额信息。</p></li><li><p>Znode结构</p><p>ZooKeeper命名空间中的Znode，兼具文件和目录两种特点。既像文件一样维护着数据、元信息、ACL、时间戳等数据结构，又像目录一样可以作为路径标识的一部分。图中的每个节点称为一个Znode。 每个Znode由3部分组成:</p><ul><li><p>stat：此为状态信息, 描述该Znode的版本, 权限等信息</p></li><li><p>data：与该Znode关联的数据</p></li><li><p>children：该Znode下的子节点</p></li></ul><p>ZooKeeper虽然可以关联一些数据，但并没有被设计为常规的数据库或者大数据存储，相反的是，它用来<strong>管理调度数据</strong>，比如分布式应用中的配置文件信息、状态信息、汇集位置等等。这些数据的共同特性就是它们都是很小的数据，通常以KB为大小单位。ZooKeeper的服务器和客户端都被设计为严格检查并限制每个Znode的数据大小至多1M，但常规使用中应该远小于此值。</p></li><li><p>数据访问</p><p>ZooKeeper中的每个节点存储的数据要被<strong>原子性的操作</strong>。也就是说读操作将获取与节点相关的所有数据，写操作也将替换掉节点的所有数据。另外，每一个节点都拥有自己的ACL(访问控制列表)，这个列表规定了用户的权限，即限定了特定用户对目标节点可以执行的操作。</p></li><li><p>节点类型</p><p>ZooKeeper中的节点有两种，分别为<strong>临时节点</strong>和<strong>永久节点</strong>。节点的类型在创建时即被确定，并且不能改变。</p><p>临时节点: </p><ul><li>该节点的生命周期依赖于创建它们的会话。一旦会话(Session)结束，临时节点将被自动删除，当然可以也可以手动删除。虽然每个临时的Znode都会绑定到一个客户端会话，但他们对所有的客户端还是可见的。另外，ZooKeeper的临时节点不允许拥有子节点。</li></ul><p>永久节点：</p><ul><li>该节点的生命周期不依赖于会话，并且只有在客户端显示执行删除操作的时候，他们才能被删除。</li></ul></li><li><p>顺序节点</p><p>当创建Znode的时候，用户可以请求在ZooKeeper的路径结尾添加一个<strong>递增的计数</strong>。这个计数<strong>对于此节点的父节点来说</strong>是唯一的，它的格式为”%10d”(10位数字，没有数值的数位用0补充，例如”0000000001”)。当计数值大于232-1时，计数器将溢出。</p></li><li><p>观察</p><p>客户端可以在节点上设置watch，我们称之为<strong>监视器</strong>。当节点状态发生改变时(Znode的增、删、改)将会触发watch所对应的操作。当watch被触发时，ZooKeeper将会向客户端发送且仅发送一条通知，因为watch只能被触发一次，这样可以减少网络流量。</p></li></ol><h2 id="安装Zookeeper"><a href="#安装Zookeeper" class="headerlink" title="安装Zookeeper"></a>安装Zookeeper</h2><ol><li><p>下载源码包()</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">http:<span class="comment">//archive.cloudera.com/cdh5/cdh/5/zookeeper-3.4.5-cdh5.16.2.tar.gz</span></span><br></pre></td></tr></table></figure></li><li><p>编译</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">yum install ant<span class="comment">//zk的编译不是使用maven的，而是使用ant</span></span><br><span class="line">ant <span class="keyword">package</span></span><br></pre></td></tr></table></figure><p>需要注意两个坑:</p><ol><li>需要指定jdk为1.7</li><li>远端仓库连不上，需要改路径</li></ol></li><li><p>配置环境变量</p></li></ol><h2 id="单节点配置Zookeeper"><a href="#单节点配置Zookeeper" class="headerlink" title="单节点配置Zookeeper"></a>单节点配置Zookeeper</h2><ol><li>修改zoo.cfg文件(需要从zoo_sample.cfg复制一份)</li><li>修改dataDir路径，默认放在/tmp目录下，dataDir主要用于保存Zookeeper中的数据</li><li>启动服务端 <code>bin/zkServer.sh start</code></li><li>启动客户端 <code>bin/zkCli.sh</code></li></ol><h2 id="多节点配置Zookeeper"><a href="#多节点配置Zookeeper" class="headerlink" title="多节点配置Zookeeper"></a>多节点配置Zookeeper</h2><ol><li><p>修改zoo.cfg文件(需要从zoo_sample.cfg复制一份)</p></li><li><p>修改dataDir路径，默认放在/tmp目录下，dataDir主要用于保存Zookeeper中的数据</p></li><li><p>在zoo.cfg文件中配置集群的id、主机和端口号，<code>集群共享内容</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">server<span class="number">.2</span>=hadoop102:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line">server<span class="number">.3</span>=hadoop103:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line">server<span class="number">.4</span>=hadoop104:<span class="number">2888</span>:<span class="number">3888</span></span><br></pre></td></tr></table></figure><p><code>server.A=B:C:D</code><br>A：标识第几号服务器，有一个myid要与之对应<br>B：服务器的地址<br>C：follower与leader通信的端口<br>D：选举leader时的端口</p></li><li><p>在dataDir指定的路径下创建myid文件，添加与之对应的服务器编号，即编号A，<code>集群唯一内容</code>，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。</p></li><li><p>启动集群服务端 <code>bin/zkServer.sh start</code></p></li><li><p>启动集群客户端 ，工作中客户端连接集群命令: </p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">./zkCli.sh -server hadoop102:<span class="number">2181</span>,hadoop103:<span class="number">2181</span>,hadoop104:<span class="number">2181</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="Zookeeper的常用命令"><a href="#Zookeeper的常用命令" class="headerlink" title="Zookeeper的常用命令"></a>Zookeeper的常用命令</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">stat 状态</span><br><span class="line">set修改</span><br><span class="line">ls 查看</span><br><span class="line">ls2 ls+get</span><br><span class="line">delete 删除</span><br><span class="line">rmr 递归删除</span><br><span class="line">get 得到</span><br><span class="line">create 创建</span><br></pre></td></tr></table></figure><ol><li><p><code>stat path [watch]</code>：数据的状态</p><p><code>[zk: localhost:2181(CONNECTED) 1] stat /tunan</code> </p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">cZxid = <span class="number">0x21</span><span class="comment">//节点的id号</span></span><br><span class="line">ctime = Mon Mar <span class="number">02</span> <span class="number">15</span>:<span class="number">27</span>:<span class="number">36</span> CST <span class="number">2020</span><span class="comment">//节点的创建时间</span></span><br><span class="line">mZxid = <span class="number">0x32</span><span class="comment">//节点修改的id号</span></span><br><span class="line">mtime = Mon Mar <span class="number">02</span> <span class="number">15</span>:<span class="number">36</span>:<span class="number">45</span> CST <span class="number">2020</span><span class="comment">//节点修改的时间</span></span><br><span class="line">pZxid = <span class="number">0x38</span><span class="comment">//最后更新的子节点的id</span></span><br><span class="line">cversion = <span class="number">9</span><span class="comment">//节点所拥有的子节点的被修改的版本号</span></span><br><span class="line">dataVersion = <span class="number">1</span><span class="comment">//节点数据版本号</span></span><br><span class="line">aclVersion = <span class="number">0</span><span class="comment">//节点所拥有的ACL版本号</span></span><br><span class="line">ephemeralOwner = <span class="number">0x0</span><span class="comment">//特殊标记 0x0: 永久节点</span></span><br><span class="line">dataLength = <span class="number">3</span><span class="comment">//数据的长度</span></span><br><span class="line">numChildren = <span class="number">5</span><span class="comment">//子节点的个数</span></span><br></pre></td></tr></table></figure></li><li><p><code>set path data [version]</code>：修改数据</p><p><code>[zk: localhost:2181(CONNECTED) 5] set /tunan/aaa 123456</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">cZxid = <span class="number">0x31</span><span class="comment">//这是一个子节点的id跟上面的父节点的id不同</span></span><br><span class="line">ctime = Mon Mar <span class="number">02</span> <span class="number">15</span>:<span class="number">35</span>:<span class="number">31</span> CST <span class="number">2020</span><span class="comment">//子节点的创建时间</span></span><br><span class="line">mZxid = <span class="number">0x3a</span><span class="comment">//子节点的修改id</span></span><br><span class="line">mtime = Tue Mar <span class="number">03</span> <span class="number">11</span>:<span class="number">28</span>:<span class="number">06</span> CST <span class="number">2020</span><span class="comment">//子节点的修改时间</span></span><br><span class="line">pZxid = <span class="number">0x31</span><span class="comment">//最后更新的子节点的id，我们现在更新的是这个节点，所以对应cZxid</span></span><br><span class="line">cversion = <span class="number">0</span><span class="comment">//节点所拥有的子节点的被修改的版本号，明显和父节点不同</span></span><br><span class="line">dataVersion = <span class="number">1</span><span class="comment">//数据被修改过了，所以版本号变了</span></span><br><span class="line">aclVersion = <span class="number">0</span><span class="comment">//节点所拥有的ACL版本号</span></span><br><span class="line">ephemeralOwner = <span class="number">0x0</span><span class="comment">//特殊标记 0x0: 永久节点</span></span><br><span class="line">dataLength = <span class="number">6</span><span class="comment">//数据的长度</span></span><br><span class="line">numChildren = <span class="number">0</span><span class="comment">//子节点的个数</span></span><br></pre></td></tr></table></figure></li><li><p><code>ls path [watch]</code>：查看节点</p><p><code>[zk: localhost:2181(CONNECTED) 8] ls /tunan</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[aaa, ccc, bbb, eee, ddd]</span><br></pre></td></tr></table></figure></li><li><p><code>ls2 path [watch]</code>：查看并获取节点内容，等同于get</p><p><code>[zk: localhost:2181(CONNECTED) 9] ls2 /tunan</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[aaa, ccc, bbb, eee, ddd]<span class="comment">//节点的数据内容</span></span><br></pre></td></tr></table></figure></li><li><p><code>delete path [version]</code>：删除节点</p><p><code>[zk: localhost:2181(CONNECTED) 10] delete /tunan/aaa</code><br><code>[zk: localhost:2181(CONNECTED) 11] ls /tunan</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[ccc, bbb, eee, ddd]</span><br></pre></td></tr></table></figure></li><li><p><code>rmr path</code>：递归删除节点</p><p><code>[zk: localhost:2181(CONNECTED) 12] rmr /tunan/ccc</code><br><code>[zk: localhost:2181(CONNECTED) 13] ls /tunan</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">[bbb, eee, ddd]</span><br></pre></td></tr></table></figure></li><li><p><code>get path [watch]</code>：获取节点内容，等同于ls2</p><p><code>[zk: localhost:2181(CONNECTED) 14] get /tunan/bbb</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="number">222</span></span><br></pre></td></tr></table></figure></li><li><p><code>create [-s] [-e] path data acl</code>：创建节点</p><p>-s：顺序创建</p><p><code>[zk: localhost:2181(CONNECTED) 0] create -s /tunan/bbb 12345</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Created /tunan/bbb0000000007</span><br></pre></td></tr></table></figure><p><code>[zk: localhost:2181(CONNECTED) 1] create -s /tunan/bbb 12345</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Created /tunan/bbb0000000008</span><br></pre></td></tr></table></figure><p>-e：临时创建</p><p><code>[zk: localhost:2181(CONNECTED) 5] create -e /tunan/fff 12345</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Created /tunan/fff</span><br></pre></td></tr></table></figure></li></ol><h2 id="Zookeeper的监听"><a href="#Zookeeper的监听" class="headerlink" title="Zookeeper的监听"></a>Zookeeper的监听</h2><ol><li><p>watch概述</p><p>ZooKeeper可以为所有的<strong>读操作</strong>设置watch，这些读操作包括：exists()、getChildren()及getData()。watch事件是<strong>一次性的触发器</strong>，当watch的对象状态发生改变时，将会触发此对象上watch所对应的事件。watch事件将被<strong>异步</strong>地发送给客户端，并且ZooKeeper为watch机制提供了有序的<strong>一致性保证</strong>。理论上，客户端接收watch事件的时间要快于其看到watch对象状态变化的时间。</p></li><li><p>watch类型</p><p>ZooKeeper所管理的watch可以分为两类：</p><ul><li><p>数据watch(data watches)：<strong>getData</strong>和<strong>exists</strong>负责设置数据watch</p></li><li><p>孩子watch(child watches)：<strong>getChildren</strong>负责设置孩子watch</p></li></ul><p>我们可以通过操作<strong>返回的数据</strong>来设置不同的watch：</p><ul><li>getData和exists：</li><li>getChildren：返回孩子列表</li></ul><p>因此</p><ol><li><p>一个成功的<strong>setData操作</strong>将触发Znode的数据watch</p></li><li><p>一个成功的<strong>create操作</strong>将触发Znode的数据watch以及孩子watch</p></li><li><p>一个成功的<strong>delete操作</strong>将触发Znode的数据watch以及孩子watch</p></li></ol></li><li><p>watch注册与处触发</p><p>图 6.1 watch设置操作及相应的触发器如图下图所示：</p><p><img src="https://images0.cnblogs.com/blog/671563/201411/301534579188980.png" alt="img"></p><ol><li>exists操作上的watch，在被监视的Znode<strong>创建</strong>、<strong>删除</strong>或<strong>数据更新</strong>时被触发。</li><li>getData操作上的watch，在被监视的Znode<strong>删除</strong>或<strong>数据更新</strong>时被触发。在被创建时不能被触发，因为只有</li><li>getChildren操作上的watch，在被监视的Znode的子节点<strong>创建</strong>或<strong>删除</strong>，或是这个Znode自身被<strong>删除</strong>时被触发。可以通过查看watch事件类型来区分是Znode，还是他的子节点被删除：NodeDelete表示Znode被删除，NodeDeletedChanged表示子节点被删除。</li></ol><p>Watch由客户端所连接的ZooKeeper服务器在本地维护，因此watch可以非常容易地设置、管理和分派。当客户端连接到一个新的服务器时，任何的会话事件都将可能触发watch。另外，当从服务器断开连接的时候，watch将不会被接收。但是，当一个客户端重新建立连接的时候，任何先前注册过的watch都会被重新注册。</p></li><li><p>需要注意的几点</p><p>Zookeeper的watch实际上要处理两类事件：</p><ul><li><p>连接状态事件(type=None, path=null)</p><p>这类事件不需要注册，也不需要我们连续触发，我们只要处理就行了。</p></li><li><p>节点事件</p><p>节点的建立，删除，数据的修改。它是one time trigger，我们需要不停的注册触发，还可能发生事件丢失的情况。</p></li></ul><p>上面2类事件都在Watch中处理，也就是重载的<strong>process(Event event)</strong></p><p>节点事件的触发，通过函数exists，getData或getChildren来处理这类函数，有双重作用：</p><ol><li>注册触发事件</li><li>函数本身的功能</li></ol><p>函数的本身的功能又可以用异步的回调函数来实现,重载processResult()过程中处理函数本身的的功能。</p></li><li><p>命令行操作</p><p>监听创建节点(<code>stat命令</code>)</p><p><code>stat /xiaoqi watch</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 13] create /xiaoqi 111</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeCreated path:/xiaoqi</span><br><span class="line">Created /xiaoqi</span><br></pre></td></tr></table></figure><p>监听修改节点(<code>stat命令</code>)</p><p><code>stat /xiaoqi watch</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 16] set /xiaoqi 1234</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeDataChanged path:/xiaoqi</span><br></pre></td></tr></table></figure><p>监听删除节点(<code>stat命令</code>)</p><p><code>stat /xiaoqi watch</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 20] delete /xiaoqi</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeDeleted path:/xiaoqi</span><br></pre></td></tr></table></figure><p>监听父节点删除子节点(<code>ls命令</code>)</p><p><code>ls /xiaoqi watch</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 29] create /xiaoqi/aaa 111</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/xiaoqi</span><br></pre></td></tr></table></figure><p>监听父节点创建子节点(<code>ls命令</code>)</p><p><code>ls /xiaoqi watch</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 31] delete /xiaoqi/aaa</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/xiaoqi</span><br></pre></td></tr></table></figure></li></ol><h2 id="Zookeeper四字命令"><a href="#Zookeeper四字命令" class="headerlink" title="Zookeeper四字命令"></a>Zookeeper四字命令</h2><p><a href="">官网</a></p><p>无须启动zk客户端，直接在命令行输入<code>echo {}| nc localhost 2181</code> 即可</p><p><code>stat:</code> 列出服务器和连接的客户机的简要信息。</p><p>echo stat | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Zookeeper version: <span class="number">3.4</span><span class="number">.5</span>-cdh5<span class="number">.16</span><span class="number">.2</span>--<span class="number">1</span>, built on <span class="number">06</span>/<span class="number">03</span>/<span class="number">2019</span> <span class="number">10</span>:<span class="number">40</span> GMT</span><br><span class="line">Clients:<span class="comment">//客户端</span></span><br><span class="line"> /<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">58056</span>[<span class="number">0</span>](queued=<span class="number">0</span>,recved=<span class="number">1</span>,sent=<span class="number">0</span>)</span><br><span class="line"> /<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">57020</span>[<span class="number">1</span>](queued=<span class="number">0</span>,recved=<span class="number">288</span>,sent=<span class="number">296</span>)</span><br><span class="line"></span><br><span class="line">Latency min/avg/max: <span class="number">0</span>/<span class="number">0</span>/<span class="number">20</span></span><br><span class="line">Received: <span class="number">454</span><span class="comment">//接收</span></span><br><span class="line">Sent: <span class="number">461</span><span class="comment">//发送</span></span><br><span class="line">Connections: <span class="number">2</span><span class="comment">//连接数</span></span><br><span class="line">Outstanding: <span class="number">0</span></span><br><span class="line">Zxid: <span class="number">0x53</span></span><br><span class="line">Mode: standalone<span class="comment">//模式</span></span><br><span class="line">Node count: <span class="number">17</span><span class="comment">//节点数</span></span><br></pre></td></tr></table></figure><p><code>ruok:</code> 测试服务器是否在非错误状态下运行。</p><p>echo ruok | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">imok</span><br></pre></td></tr></table></figure><p><code>dump:</code> 列出未完成的会话和临时节点。这只对leader有效。</p><p>echo dump | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SessionTracker dump:</span><br><span class="line"><span class="function">Session <span class="title">Sets</span> <span class="params">(<span class="number">3</span>)</span>:</span></span><br><span class="line"><span class="function">0 expire at Tue Mar 03 12:29:46 CST 2020:</span></span><br><span class="line"><span class="function">0 expire at Tue Mar 03 12:29:56 CST 2020:</span></span><br><span class="line"><span class="function">1 expire at Tue Mar 03 12:30:06 CST 2020:</span></span><br><span class="line"><span class="function">0x1709e65d9b00001</span></span><br><span class="line"><span class="function">ephemeral nodes dump:</span></span><br><span class="line"><span class="function">Sessions with <span class="title">Ephemerals</span> <span class="params">(<span class="number">1</span>)</span>:</span></span><br><span class="line"><span class="function">0x1709e65d9b00001:</span></span><br><span class="line"><span class="function">/tunan/fff<span class="comment">//临时节点</span></span></span><br></pre></td></tr></table></figure><p><code>conf:</code> 打印关于服务配置的详细信息。</p><p>echo conf | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">clientPort=<span class="number">2181</span><span class="comment">//端口</span></span><br><span class="line">dataDir=/home/hadoop/tmp/zookeeper/version-<span class="number">2</span><span class="comment">//数据目录</span></span><br><span class="line">dataLogDir=/home/hadoop/tmp/zookeeper/version-<span class="number">2</span><span class="comment">//日志目录</span></span><br><span class="line">tickTime=<span class="number">2000</span><span class="comment">//心跳时间，单位毫秒</span></span><br><span class="line">maxClientCnxns=<span class="number">60</span><span class="comment">//最大连接数</span></span><br><span class="line">minSessionTimeout=<span class="number">4000</span><span class="comment">//最小的超时时间</span></span><br><span class="line">maxSessionTimeout=<span class="number">40000</span><span class="comment">//最大的超时时间</span></span><br><span class="line">serverId=<span class="number">0</span><span class="comment">//服务器的id</span></span><br></pre></td></tr></table></figure><p><code>envi:</code> 打印出服务环境的细节</p><p>echo envi | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Environment:</span><br><span class="line">zookeeper.version=<span class="number">3.4</span><span class="number">.5</span>-cdh5<span class="number">.16</span><span class="number">.2</span>--<span class="number">1</span>, built on <span class="number">06</span>/<span class="number">03</span>/<span class="number">2019</span> <span class="number">10</span>:<span class="number">40</span> GMT</span><br><span class="line">host.name=aliyun</span><br><span class="line">java.version=<span class="number">1.8</span><span class="number">.0_144</span></span><br><span class="line">java.vendor=Oracle Corporation</span><br><span class="line">java.home=/usr/java/jdk1<span class="number">.8</span><span class="number">.0_144</span>/jre</span><br><span class="line">java<span class="class">.<span class="keyword">class</span>.<span class="title">path</span></span>=/home/hadoop/app/zookeeper/bin/../build/classes:/home/hadoop/app/zookeeper/bin/../build/lib<span class="comment">/*.jar:/home/hadoop/app/zookeeper/bin/../share/zookeeper/zookeeper-3.4.5-cdh5.16.2.jar:/home/hadoop/app/zookeeper/bin/../share/zookeeper/slf4j-log4j12-1.7.5.jar:/home/hadoop/app/zookeeper/bin/../share/zookeeper/slf4j-api-1.7.5.jar:/home/hadoop/app/zookeeper/bin/../share/zookeeper/netty-3.10.5.Final.jar:/home/hadoop/app/zookeeper/bin/../share/zookeeper/log4j-1.2.16.jar:/home/hadoop/app/zookeeper/bin/../share/zookeeper/jline-2.11.jar:/home/hadoop/app/zookeeper/bin/../src/java/lib/*.jar:/home/hadoop/app/zookeeper/bin/../conf:</span></span><br><span class="line"><span class="comment">java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</span></span><br><span class="line"><span class="comment">java.io.tmpdir=/tmp</span></span><br><span class="line"><span class="comment">java.compiler=&lt;NA&gt;</span></span><br><span class="line"><span class="comment">os.name=Linux</span></span><br><span class="line"><span class="comment">os.arch=amd64</span></span><br><span class="line"><span class="comment">os.version=3.10.0-514.26.2.el7.x86_64</span></span><br><span class="line"><span class="comment">user.name=hadoop</span></span><br><span class="line"><span class="comment">user.home=/home/hadoop</span></span><br><span class="line"><span class="comment">user.dir=/home/hadoop/app/zookeeper-3.4.5-cdh5.16.2/bin</span></span><br></pre></td></tr></table></figure><p><code>mntr:</code> 输出可用于监视集群健康状况的变量列表。</p><p>echo mntr | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">zk_version<span class="number">3.4</span><span class="number">.5</span>-cdh5<span class="number">.16</span><span class="number">.2</span>--<span class="number">1</span>, built on <span class="number">06</span>/<span class="number">03</span>/<span class="number">2019</span> <span class="number">10</span>:<span class="number">40</span> GMT</span><br><span class="line">zk_avg_latency<span class="number">0</span></span><br><span class="line">zk_max_latency<span class="number">20</span></span><br><span class="line">zk_min_latency<span class="number">0</span></span><br><span class="line">zk_packets_received<span class="number">494</span></span><br><span class="line">zk_packets_sent<span class="number">501</span></span><br><span class="line">zk_num_alive_connections<span class="number">2</span></span><br><span class="line">zk_outstanding_requests<span class="number">0</span></span><br><span class="line">zk_server_statestandalone</span><br><span class="line">zk_znode_count<span class="number">17</span></span><br><span class="line">zk_watch_count<span class="number">1</span></span><br><span class="line">zk_ephemerals_count<span class="number">1</span></span><br><span class="line">zk_approximate_data_size<span class="number">253</span></span><br><span class="line">zk_open_file_descriptor_count<span class="number">27</span></span><br><span class="line">zk_max_file_descriptor_count<span class="number">65535</span></span><br><span class="line">zk_fsync_threshold_exceed_count<span class="number">0</span></span><br></pre></td></tr></table></figure><p><code>wchs:</code> 按会话列出关于服务器监视的详细信息。</p><p>echo wchs | nc localhost 2181</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span> connections watching <span class="number">1</span> paths</span><br><span class="line">Total watches:<span class="number">1</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Zookeeper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Zookeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>项目复盘</title>
      <link href="/2020/02/21/offlinedw/8.%E9%A1%B9%E7%9B%AE%E5%A4%8D%E7%9B%98/"/>
      <url>/2020/02/21/offlinedw/8.%E9%A1%B9%E7%9B%AE%E5%A4%8D%E7%9B%98/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>任务分配问题</li><li>hive问题</li><li>etl代码问题</li><li>任务调度问题</li></ol><h2 id="任务分配问题"><a href="#任务分配问题" class="headerlink" title="任务分配问题"></a>任务分配问题</h2><ol><li>立项开始就需要梳理开发流程，精确到每一个开发环节。</li><li>开发定义好接口尽量并行开发。</li><li>建立规范、代码版本同步。</li><li>定义ods层表名,dws层表名。</li><li>有人跟踪项目进度有没有阻碍，进度落后需要帮忙</li></ol><h2 id="hive问题"><a href="#hive问题" class="headerlink" title="hive问题"></a>hive问题</h2><ol><li>建表需要加注释，规范表名。</li><li>重建外部表需要删除数据。</li></ol><h2 id="etl代码问题"><a href="#etl代码问题" class="headerlink" title="etl代码问题"></a>etl代码问题</h2><ol><li>ip解析应该放在setup上。</li><li>searcher 提到全局静态变量</li></ol><h2 id="任务调度"><a href="#任务调度" class="headerlink" title="任务调度"></a>任务调度</h2><ol><li>每个统计任务直接跟sqoop同步任务</li><li>数据倾斜group by 优化 skewindata  = true</li></ol>]]></content>
      
      
      <categories>
          
          <category> OfflineDW </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OfflineDW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban的安装&amp;使用&amp;坑</title>
      <link href="/2020/02/20/azkaban/1/"/>
      <url>/2020/02/20/azkaban/1/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>Azkaban的安装</li><li>Azkaban的使用</li><li>Git的安装</li></ol><h2 id="Git的安装"><a href="#Git的安装" class="headerlink" title="Git的安装"></a>Git的安装</h2><p>在安装Azkaban之前要安装Git</p><ol><li><p>获取github最新的Git安装包下载链接，进入Linux服务器，执行下载，命令为： </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https://github.com/git/git/archive/v2.17.0.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>压缩包解压，命令为：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -zxvf v2.17.0.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>安装编译源码所需依赖，命令为：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel gcc perl-ExtUtils-MakeMaker</span><br></pre></td></tr></table></figure><p>耐心等待安装，出现提示输入y即可；</p></li><li><p>安装依赖时，yum自动安装了Git，需要卸载旧版本Git，命令为：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum remove git -y</span><br></pre></td></tr></table></figure></li><li><p>进入解压后的文件夹，命令 cd git-2.17.0 ，然后执行编译，命令为:  </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">make prefix=/usr/local/git all</span><br></pre></td></tr></table></figure></li><li><p>安装Git至/usr/local/git路径，命令为:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">make prefix=/usr/local/git install</span><br></pre></td></tr></table></figure></li><li><p>打开环境变量配置文件，命令 vim /etc/profile ，在底部加上Git相关配置信息：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export GIT_HOME=/usr/local/git/bin</span><br><span class="line">export PATH=$GIT_HOME/bin/:$PATH</span><br></pre></td></tr></table></figure></li><li><p>输入命令 <code>git --version</code> ，查看安装的git版本，校验通过，安装成功。</p></li></ol><h2 id="Azkaban的安装"><a href="#Azkaban的安装" class="headerlink" title="Azkaban的安装"></a>Azkaban的安装</h2><ol><li><p>下载<a href="https://github.com/azkaban/azkaban/releases" target="_blank" rel="noopener">Azkaban</a></p></li><li><p>解压</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xzvf 3.81.0.tar.gz -C ../app/</span><br></pre></td></tr></table></figure></li><li><p>安装<a href="https://azkaban.readthedocs.io/en/latest/getStarted.html" target="_blank" rel="noopener">编译文档</a></p><p>在开始编译之前要下载名为<a href="https://services.gradle.org/distributions/gradle-4.6-all.zip" target="_blank" rel="noopener">gradle-4.6-all.zip</a>的包，如果直接让系统下载超级慢，下载好了放在同目录下，并且在<code>/azkaban/gradle/wrapper/gradle-wrapper.properties</code> 中注释下载路径，并指定已经下载好的路径</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">distributionUrl=gradle-4.6-all.zip</span><br><span class="line"><span class="meta">#</span><span class="bash">distributionUrl=https\://services.gradle.org/distributions/gradle-4.6-all.zip</span></span><br></pre></td></tr></table></figure><p>还需要安装gcc环境:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install -y gcc-c++*</span><br></pre></td></tr></table></figure><p>修改maven仓库为阿里云镜像</p><p><code>vim azkaban-3.81.0/build.gradle 54行左右</code> </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">maven &#123;</span><br><span class="line">//这里是阿里云</span><br><span class="line">url &apos;http://maven.aliyun.com/nexus/content/repositories/central/&apos;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>   正式编译</p>   <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./gradlew build installDist -x test</span><br></pre></td></tr></table></figure><p>   编译后生成三个文件</p>   <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">azkaban-exec-server</span><br><span class="line">azkaban-solo-server</span><br><span class="line">azkaban-web-server</span><br></pre></td></tr></table></figure><ol start="4"><li><p>执行<code>azkaban-solo-server</code></p><ol><li><p>解压<code>azkaban-solo-server</code>到<code>app</code>目录下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xzvf azkaban-solo-server/build/distributionsa/zkaban-solo-server-0.1.0-SNAPSHOT.tar.gz ~/app</span><br></pre></td></tr></table></figure></li><li><p>启动: <code>bin/start-solo.sh</code>，生成<code>AzkabanSingleServer</code>进程</p></li><li><p>修改配置(时区、用户)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">时区: default.timezone.id=Asia/Shanghai</span><br><span class="line">用户: &lt;user password="tunan" roles="admin" username="tunan"/&gt;</span><br></pre></td></tr></table></figure></li><li><p>web端登录，端口8081</p></li></ol></li><li><p>azkaban需要至少3G内存，如果内存不足3G，需要设置不检查内存</p><p>azkaban-web-server-2.7.0/plugins/jobtypes/commonprivate.properties</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">memCheck.enabled=false</span><br></pre></td></tr></table></figure></li></ol><h2 id="Azkaban的使用"><a href="#Azkaban的使用" class="headerlink" title="Azkaban的使用"></a>Azkaban的使用</h2><h3 id="单节点部署"><a href="#单节点部署" class="headerlink" title="单节点部署"></a>单节点部署</h3><p><a href="https://azkaban.readthedocs.io/en/latest/createFlows.html" target="_blank" rel="noopener">官方文档</a></p><ol><li><p>创建flow20.projec文件，写入信息: </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">azkaban-flow-version: 2.0</span><br></pre></td></tr></table></figure></li><li><p>创建basic.flow文件，写入信息:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: jobA</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: echo "This is an echoed text."</span><br></pre></td></tr></table></figure></li><li><p>压缩Archive.zip文件，在web页面中提交</p><p><code>坑</code>: 提交的用户和hadoop上的用户不同</p></li><li><p>多依赖案例</p><p>flow20.projec文件相同，basic.flow文件内容不同，文件名可不同</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: jobC</span><br><span class="line">    type: noop</span><br><span class="line">    # jobC depends on jobA and jobB</span><br><span class="line">    dependsOn:</span><br><span class="line">      - jobA</span><br><span class="line">      - jobB</span><br><span class="line"></span><br><span class="line">  - name: jobA</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: echo "This is an echoed text."</span><br><span class="line"></span><br><span class="line">  - name: jobB</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: pwd</span><br></pre></td></tr></table></figure></li><li><p>wc案例</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: tunan-wordcount</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: hadoop jar /home/hadoop/app/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar wordcount /azkaban/data/wc.txt /out</span><br></pre></td></tr></table></figure><ol><li>直接点击程序修改输出文件参数</li><li>使用Flow Parameters修改输出文件参数</li></ol></li><li><p>hive案例</p><p>除了固定的flow20.project文件，需要修改hive.flow文件和添加stat.sh文件</p><p>hive.flow</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: exe hive</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: sh stat.sh</span><br></pre></td></tr></table></figure><p>stat.sh</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line">hive -e "select * from offline_dw.dws_country_traffic"</span><br></pre></td></tr></table></figure><p>上传相同的文件，新版本会覆盖旧的版本</p></li><li><p>调度执行</p><p>在执行页面的左下角点击Schedule即可，并可在Scheduling中查看调度信息，然后在History中查看历史执行信息</p></li></ol><h3 id="多节点部署"><a href="#多节点部署" class="headerlink" title="多节点部署"></a>多节点部署</h3><ol><li><p>在mysql中创建数据库</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE DATABASE azkaban;</span><br></pre></td></tr></table></figure></li><li><p>创建用户</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE USER 'tunan'@'%' IDENTIFIED BY 'tunan';</span><br></pre></td></tr></table></figure></li><li><p>赋权给用户</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span>,<span class="keyword">INSERT</span>,<span class="keyword">UPDATE</span>,<span class="keyword">DELETE</span> <span class="keyword">ON</span> azkaban.* <span class="keyword">to</span> <span class="string">'tunan'</span>@<span class="string">'%'</span> <span class="keyword">WITH</span> <span class="keyword">GRANT</span> <span class="keyword">OPTION</span>;</span><br></pre></td></tr></table></figure></li><li><p>在mysql中生成需要的表</p><p>这里的source可能会遇到权限问题，解决的办法很多，如移动其他位置</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">use</span> azkaban;</span><br><span class="line">source /home/hadoop/app/azkaban/azkaban-db/build/<span class="keyword">install</span>/azkaban-db/<span class="keyword">create</span>-<span class="keyword">all</span>-<span class="keyword">sql</span><span class="number">-0.1</span><span class="number">.0</span>-SNAPSHOT.sql;</span><br></pre></td></tr></table></figure></li><li><p>执行SQL获取动态端口号</p><p><code>mysql&gt; select * from executors ;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">----+--------+-------+--------+</span></span><br><span class="line">| id | host   | port  | active |</span><br><span class="line">+<span class="comment">----+--------+-------+--------+</span></span><br><span class="line">|  1 | aliyun | 46418 |      0 |</span><br><span class="line">+<span class="comment">----+--------+-------+--------+</span></span><br></pre></td></tr></table></figure></li><li><p>激活执行器</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -G "http://aliyun:46418/executor?action=activate"</span><br></pre></td></tr></table></figure></li><li><p>把<code>azkaban-exec-server</code>和<code>azkaban-web-server</code>解压，并启动</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xzvf azkaban-exec-server/build/distributionsa/azkaban-exec-server-0.1.0-SNAPSHOT.tar.gz ~/app</span><br><span class="line">tar -xzvf azkaban-web-server/build/distributionsa/azkaban-web-server-0.1.0-SNAPSHOT.tar.gz ~/app</span><br></pre></td></tr></table></figure></li><li><p>登录web使用，使用方式和单节点一样</p></li></ol><h3 id="二次开发"><a href="#二次开发" class="headerlink" title="二次开发"></a>二次开发</h3><p>azkaban的二次开发需要调用az已经做好的接口</p><p><a href="https://azkaban.readthedocs.io/en/latest/ajaxApi.html#request-parameters-1" target="_blank" rel="noopener">官方文档</a></p><p>命令行获取session.id</p><p><code>curl -k -X POST --data &quot;action=login&amp;username=azkaban&amp;password=azkaban&quot; http://aliyun:8081</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "session.id" : "f1725f59-9d20-4fe2-b89e-3715ef85****",</span><br><span class="line">  "status" : "success"</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="常用功能"><a href="#常用功能" class="headerlink" title="常用功能"></a>常用功能</h2><ol><li>配置代理用户</li><li>配置hadoop.home</li><li>配置Spark作业提交</li></ol>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>结果数据的展示</title>
      <link href="/2020/02/20/offlinedw/7.%E7%BB%93%E6%9E%9C%E6%95%B0%E6%8D%AE%E7%9A%84%E5%B1%95%E7%A4%BA/"/>
      <url>/2020/02/20/offlinedw/7.%E7%BB%93%E6%9E%9C%E6%95%B0%E6%8D%AE%E7%9A%84%E5%B1%95%E7%A4%BA/</url>
      
        <content type="html"><![CDATA[<p>由于时间忙不过来，这篇暂时不更新，仅作为维护项目的完整性存在。。。</p>]]></content>
      
      
      <categories>
          
          <category> OfflineDW </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OfflineDW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>业务数据的抽取</title>
      <link href="/2020/02/19/offlinedw/6.%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%8A%BD%E5%8F%96/"/>
      <url>/2020/02/19/offlinedw/6.%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%8A%BD%E5%8F%96/</url>
      
        <content type="html"><![CDATA[<p>由于时间忙不过来，这篇暂时不更新，仅作为项目的完整性存在。。。</p>]]></content>
      
      
      <categories>
          
          <category> OfflineDW </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OfflineDW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库的分层</title>
      <link href="/2020/02/18/offlinedw/5.%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E5%88%86%E5%B1%82/"/>
      <url>/2020/02/18/offlinedw/5.%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E5%88%86%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<p>系统架构图:</p><p><img src="https://yerias.github.io/offlinedw/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E9%A1%B9%E7%9B%AE%E5%9B%BE_3.jpg" alt="数据仓库架构项目图"></p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>数据仓库为什么要分层</li><li>数据仓库如何分层</li><li>使用脚本将数据执行分层</li><li>数据分析案例</li><li>使用crontab调度脚本(临时)</li></ol><h2 id="数据仓库为什么要分层"><a href="#数据仓库为什么要分层" class="headerlink" title="数据仓库为什么要分层"></a>数据仓库为什么要分层</h2><p>分层的主要原因是在管理数据的时候，能对数据有一个更加清晰的掌控，详细来讲，主要有下面几个原因：</p><ol><li><code>清晰数据结构</code>，每一个数据分层都有它的作用域，这样我们在使用表的时候能更方便地定位和理解。</li><li><code>数据血缘追踪</code>，简单来说，我们最终给业务呈现的是一个能直接使用业务表，但是它的来源有很多，如果有一张来源表出问题了，我们希望能够快速准确地定位到问题，并清楚它的危害范围。</li><li><code>减少重复开发</code>，规范数据分层，开发一些通用的中间层数据，能够减少极大的重复计算。</li><li><code>把复杂问题简单化</code>，将一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。</li><li><code>屏蔽原始数据的异常</code></li><li><code>屏蔽业务的影响</code>，不必改一次业务就需要重新接入数据</li></ol><h2 id="数据仓库如何分层"><a href="#数据仓库如何分层" class="headerlink" title="数据仓库如何分层"></a>数据仓库如何分层</h2><p>数据仓库的分层标准: 不为分层而分层</p><h3 id="标准的分层"><a href="#标准的分层" class="headerlink" title="标准的分层"></a>标准的分层</h3><ul><li>ODS：<code>数据原始层</code>: 直接加载原始数据，不做任何处理(不做ETL)</li><li>DWD：<code>数据明细层</code>，对ODS层进行清洗(ETL)</li><li>DWS：<code>数据服务层</code>，基于DWD做统计分析</li><li>ADS：<code>数据应用层</code>，为各种统计报表提供数据</li></ul><h3 id="我们的分层"><a href="#我们的分层" class="headerlink" title="我们的分层"></a>我们的分层</h3><ul><li>ODS(operate data store)：<code>数据原始层</code>，最接近数据源中数据的一层，数据源中的数据，经过抽取、洗净、传输，也就说传说中的ETL之后，装入本层</li><li>DWS(data warehouse server)：<code>数据明细层</code>，从ODS层中获得的数据按照主题建立各种数据模型。在这里，我们需要了解四个概念：维（dimension）、事实（Fact）、指标（Index）和粒度（ Granularity）。</li><li>ADS：<code>数据应用层</code>，该层主要是提供数据产品和数据分析使用的数据。 比如我们经常说的报表数据，或者说那种大宽表，一般就放在这里。</li></ul><h2 id="使用脚本将数据执行分层"><a href="#使用脚本将数据执行分层" class="headerlink" title="使用脚本将数据执行分层"></a>使用脚本将数据执行分层</h2><p>现在我们知道了数据需要在Hive中分成ODS层、DWS层和ADS层，每一层的建表标准是<code>ods_</code>、<code>dws_</code>、<code>ads_</code></p><ol><li><p>建库</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> offline_dw;</span><br></pre></td></tr></table></figure></li><li><p>建ODS层外部分区表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> ods_access(</span><br><span class="line"><span class="string">`year`</span><span class="keyword">String</span>,</span><br><span class="line"><span class="string">`month`</span><span class="keyword">String</span>,</span><br><span class="line"><span class="string">`day`</span><span class="keyword">String</span>,</span><br><span class="line">country<span class="keyword">String</span>,</span><br><span class="line">province<span class="keyword">String</span>,</span><br><span class="line">city<span class="keyword">String</span>,</span><br><span class="line">area<span class="keyword">String</span>,</span><br><span class="line">proxyIp<span class="keyword">String</span>,</span><br><span class="line">responseTime<span class="built_in">BigInt</span>,</span><br><span class="line">referer<span class="keyword">String</span>,</span><br><span class="line"><span class="string">`method`</span><span class="keyword">String</span>,</span><br><span class="line"><span class="keyword">http</span><span class="keyword">String</span>,</span><br><span class="line"><span class="string">`domain`</span><span class="keyword">String</span>,</span><br><span class="line"><span class="string">`path`</span><span class="keyword">String</span>,</span><br><span class="line">httpCode<span class="keyword">String</span>,</span><br><span class="line">requestSize<span class="built_in">BigInt</span>,</span><br><span class="line">responseSize<span class="built_in">bigInt</span>,</span><br><span class="line"><span class="keyword">cache</span><span class="keyword">String</span>,</span><br><span class="line">userId <span class="built_in">BigInt</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span>(d <span class="keyword">String</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span></span><br><span class="line">location <span class="string">"/item/offline-dw/ods/access/"</span>;</span><br></pre></td></tr></table></figure></li><li><p>写脚本(etl.sh)</p><ol><li>使用MR通过ETL把源数据写入ODS临时目录</li><li>把ODS临时目录中<code>part*</code>开头的数据移动到ODS分区目录下，并指定分区</li><li>接着Hive执行命令新建分区</li></ol><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line"></span><br><span class="line">if [ $# -eq 1 ];then</span><br><span class="line">        time=$1</span><br><span class="line">else</span><br><span class="line">        time=`date --date "1 days ago" +%Y%m%d`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">MR做ETL 注意输入和输出(带时间)</span></span><br><span class="line">hadoop jar /home/hadoop/lib/hadoop-client-1.0.0.jar com.tunan.item.ETLDriver -libjars $LIBJARS /item/offline-dw/raw/access/$time /item/offline-dw/ods_tmp/access/$time</span><br><span class="line"><span class="meta">#</span><span class="bash">删除分区目录(d=时间)</span></span><br><span class="line">hdfs dfs -rm -r -f  /item/offline-dw/ods/access/d=$time</span><br><span class="line"><span class="meta">#</span><span class="bash">创建分区目录(d=时间)</span></span><br><span class="line">hdfs dfs -mkdir -p /item/offline-dw/ods/access/d=$time</span><br><span class="line"><span class="meta">#</span><span class="bash">ods_tmp移动数据到ods((时间目录下的part* d=时间))  </span></span><br><span class="line">hdfs dfs -mv /item/offline-dw/ods_tmp/access/$time/part* /item/offline-dw/ods/access/d=$time</span><br><span class="line"><span class="meta">#</span><span class="bash">删除ods_tmp(带时间)</span></span><br><span class="line">hdfs dfs -rm -r -f /item/offline-dw/ods_tmp/access/$time</span><br><span class="line"><span class="meta">#</span><span class="bash">hive命令行刷新分区 (判断表是否存在，partition(d=<span class="string">"<span class="variable">$time</span>"</span>))</span></span><br><span class="line">hive -e "alter table offline_dw.ods_access add if not exists partition(d=$time)"</span><br></pre></td></tr></table></figure></li></ol><h2 id="数据分析案例-DWS-ADS层"><a href="#数据分析案例-DWS-ADS层" class="headerlink" title="数据分析案例(DWS/ADS层)"></a>数据分析案例(DWS/ADS层)</h2><p>这层的数据可以作为DWS层，我们要什么数据就建什么表(分区)</p><ul><li><p>统计国家流量</p><p>思路: 建分区表，字段分别是国家和流量，最后分组查询，插入表中</p><ol><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> dws_country_traffic(</span><br><span class="line">country <span class="keyword">String</span>,</span><br><span class="line">traffic <span class="built_in">BigInt</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span>(d <span class="keyword">String</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询数据插入表中</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> offline_dw.dws_country_traffic <span class="keyword">partition</span>(d=<span class="string">'$time'</span>) </span><br><span class="line"><span class="keyword">select</span> country,<span class="keyword">sum</span>(responseSize) <span class="keyword">from</span> offline_dw.ods_access <span class="keyword">where</span> d = <span class="string">'$time'</span><span class="keyword">group</span> <span class="keyword">by</span> country;</span><br></pre></td></tr></table></figure></li></ol></li><li><p>统计域名流量</p><p>思路: 建分区表，字段分别是域名和流量，最后分组查询，插入表中</p><ol><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> dws_domain_traffic(</span><br><span class="line"><span class="keyword">domain</span> <span class="keyword">String</span>,</span><br><span class="line">traffic <span class="built_in">BigInt</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span>(d <span class="keyword">String</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询数据插入表中</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> offline_dw.dws_domain_traffic <span class="keyword">partition</span>(d=<span class="string">'$time'</span>) </span><br><span class="line"><span class="keyword">select</span> <span class="keyword">domain</span>,<span class="keyword">sum</span>(responseSize) <span class="keyword">from</span> offline_dw.ods_access <span class="keyword">where</span> d=<span class="string">'$time'</span><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">domain</span>;</span><br></pre></td></tr></table></figure></li></ol></li><li><p>写成脚本方便调度</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line"></span><br><span class="line">if [ $# -eq 1 ];then</span><br><span class="line">        time=$1</span><br><span class="line">else</span><br><span class="line">        time=`date --date '1 days ago' +%Y%m%d`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive -e "insert overwrite table offline_dw.dws_country_traffic partition(d='$time') </span><br><span class="line">select country,sum(responseSize) from offline_dw.ods_access where d='$time' group by country;"</span><br><span class="line"></span><br><span class="line">hive -e "insert overwrite table offline_dw.dws_domain_traffic partition(d='$time') </span><br><span class="line">select domain,sum(responseSize)  from offline_dw.ods_access where d='$time' group by domain;"</span><br></pre></td></tr></table></figure></li></ul><h2 id="使用crontab调度脚本-临时"><a href="#使用crontab调度脚本-临时" class="headerlink" title="使用crontab调度脚本(临时)"></a>使用crontab调度脚本(临时)</h2><p>每天凌晨一点执行etl.sh，每天凌晨两点执行stats.sh</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">crontab -e</span><br><span class="line"></span><br><span class="line">0 1 * * * /home/hadoop/offline_dw/script/etl.sh</span><br><span class="line">0 2 * * * /home/hadoop/offline_dw/script/stats.sh</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> OfflineDW </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OfflineDW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>项目数据的ETL</title>
      <link href="/2020/02/17/offlinedw/4.%E9%A1%B9%E7%9B%AE%E6%95%B0%E6%8D%AE%E7%9A%84ETL/"/>
      <url>/2020/02/17/offlinedw/4.%E9%A1%B9%E7%9B%AE%E6%95%B0%E6%8D%AE%E7%9A%84ETL/</url>
      
        <content type="html"><![CDATA[<p>系统架构图:</p><p><img src="https://yerias.github.io/offlinedw/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E9%A1%B9%E7%9B%AE%E5%9B%BE_2.jpg" alt="数据仓库架构项目图"></p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li><p>为什么要进行ETL</p></li><li><p>什么是ETL</p></li><li><p>ETL该怎么做</p></li><li><p>ETL在服务器上运行需要解决的问题</p></li></ol><h2 id="为什么要进行ETL"><a href="#为什么要进行ETL" class="headerlink" title="为什么要进行ETL"></a>为什么要进行ETL</h2><p>在上一步我们使用Flume采集数据到HDFS，从系统架构图来看现在要进行数据的ETL操作，ETL进程对数据进行规范化、验证、清洗，并最终装载进入数据仓库</p><h2 id="什么是ETL"><a href="#什么是ETL" class="headerlink" title="什么是ETL"></a>什么是ETL</h2><p>ETL 即 Extract Transform Load的首字母 ==&gt; 抽取、转换、加载</p><h2 id="ETL该怎么做"><a href="#ETL该怎么做" class="headerlink" title="ETL该怎么做"></a>ETL该怎么做</h2><p>数据采集到HDFS上指定的目录下，通过MR写入数据，进行ETL操作，并写出到指定的目录下，ETL操作包括定义数据字段的序列化类，把时间解析出年月日，把URL解析为http、domain和path、对异常值进行处理(try/catch)，使用计数器。</p><p>需要注意:</p><ol><li><p>时间解析参考代码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//时间</span></span><br><span class="line">String time = split[<span class="number">0</span>];</span><br><span class="line">SimpleDateFormat format = <span class="keyword">new</span> SimpleDateFormat(<span class="string">"[dd/MM/yyyy:HH:mm:ss +0800]"</span>);</span><br><span class="line">Date date = format.parse(split[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">Calendar calendar = Calendar.getInstance();</span><br><span class="line">calendar.setTime(date);</span><br><span class="line"></span><br><span class="line"><span class="comment">//year</span></span><br><span class="line">String year =String.valueOf(calendar.get(Calendar.YEAR));</span><br><span class="line">access.setYear(year);</span><br><span class="line"></span><br><span class="line"><span class="comment">//month</span></span><br><span class="line"><span class="keyword">int</span>  month = calendar.get(Calendar.MONTH)+<span class="number">1</span>;</span><br><span class="line">access.setMont(month &lt; <span class="number">10</span> ? <span class="string">"0"</span>+month:String.valueOf(month))</span><br><span class="line"></span><br><span class="line"><span class="comment">//day</span></span><br><span class="line"><span class="keyword">int</span> day = calendar.get(Calendar.DAY_OF_MONTH);</span><br><span class="line">access.setDay(day &lt; <span class="number">10</span> ? <span class="string">"0"</span>+day:String.valueOf(day));</span><br></pre></td></tr></table></figure></li><li><p>异常值是舍去还是保留，这跟try/catch如何操作有关系，参考代码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//要数据,设默认值为0</span></span><br><span class="line"><span class="keyword">long</span> responseSize = <span class="number">0L</span>;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    responseSize = Long.parseLong(split[<span class="number">9</span>].trim());</span><br><span class="line">    access.setResponseSize(responseSize);</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">    access.setResponseSize(responseSize);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 不要数据，产生异常直接返回</span></span><br><span class="line">Long responseSize = Long.parseLong(split[<span class="number">9</span>].trim());</span><br><span class="line">access.setResponseSize(responseSize);</span><br></pre></td></tr></table></figure></li><li><p>计数器mapper中的参考代码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">context.getCounter(<span class="string">"ETL"</span>,<span class="string">"SUCCEED"</span>).increment(<span class="number">1</span>);</span><br></pre></td></tr></table></figure></li><li><p>计数器Driver中的参考代码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//通过迭代器获取mapper中的计数器</span></span><br><span class="line">CounterGroup group = job.getCounters().getGroup(<span class="string">"ETL"</span>);</span><br><span class="line">Iterator&lt;Counter&gt; iterator = group.iterator();</span><br><span class="line"><span class="keyword">while</span>(iterator.hasNext())&#123;</span><br><span class="line">Counter counter = iterator.next();</span><br><span class="line">     System.out.println(counter.getName() + <span class="string">"==&gt;"</span> + counter.getValue());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意: 这里可以通过jdbc将计数器结果根据批次写到mysql数据库中</p></li></ol><h2 id="ETL在服务器上运行需要解决的问题"><a href="#ETL在服务器上运行需要解决的问题" class="headerlink" title="ETL在服务器上运行需要解决的问题"></a>ETL在服务器上运行需要解决的问题</h2><p>在本地测试好代码后，上传Jar包到服务器上，跑HDFS上的数据</p><p>首先创建三个文件夹lib、data、script放ETL相关的文件，运行脚本的shell文件就在script目录下</p><p>由于我们把ETL打的瘦包，所以很多数据需要的依赖Jar包得不到，还有ip解析库的数据库也需要上传到本地文件下</p><p>思路是:</p><ol><li><p>把ip解析库放到项目的resources目录下</p></li><li><p>把需要的依赖上传到lib目录下</p></li><li><p>在<code>~/.bashrc</code>文件下导入LIBJARS路径用来指向lib目录下的依赖</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">export LIBJARS=/home/hadoop/lib/LIBJARS</span><br><span class="line">export LIBJARS=$LIBJARS/commons-lang3-<span class="number">3.4</span>.jar,$LIBJARS/qqwry.dat,$LIBJARS/mysql-connector-java-<span class="number">5.1</span><span class="number">.27</span>-bin.jar</span><br></pre></td></tr></table></figure></li><li><p>执行jar包命令写进脚本，执行脚本即可</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">time=<span class="number">20200217</span></span><br><span class="line">hadoop jar hadoop-client-<span class="number">1.0</span><span class="number">.0</span>.jar  com.tunan.ip.ipParseDriver  -libjars $LIBJARS  /item/offline-dw/raw/access/$time /item/offline-dw/tmp/access/$time/</span><br></pre></td></tr></table></figure><p><code>-libjars</code>用来指定外部依赖，<code>$LIBJARS</code>指向<code>~/.bashrc</code>文件中的路径</p><p><code>/item/offline-dw/raw/access/$time</code>是源数据，这个数据一般保存7天后即可删除</p><p><code>/item/offline-dw/tmp/access/$time</code>/是ETL后的数据</p><p><code>$LIBJARS/qqwry.dat</code>是ip解析库的路径</p></li><li><p>还可以把ip解析库在服务器上的路径写死在代码中，就不用手动指定ip解析库的路径了</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> OfflineDW </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OfflineDW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>项目数据的采集</title>
      <link href="/2020/02/16/offlinedw/3.%E9%A1%B9%E7%9B%AE%E6%95%B0%E6%8D%AE%E7%9A%84%E9%87%87%E9%9B%86/"/>
      <url>/2020/02/16/offlinedw/3.%E9%A1%B9%E7%9B%AE%E6%95%B0%E6%8D%AE%E7%9A%84%E9%87%87%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<p>系统架构图:</p><p><img src="https://yerias.github.io/offlinedw/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E9%A1%B9%E7%9B%AE%E5%9B%BE_1.jpg" alt="数据仓库架构项目图"></p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>Flume是用来做什么的</p><p>为什么要使用Flume</p><p>Flume具体怎么用</p><p>java客户端上传消息到Flume写到HDFS</p><p>解决Flume采集数据时生成大量小文件的问题</p><h2 id="为什么要使用Flume"><a href="#为什么要使用Flume" class="headerlink" title="为什么要使用Flume"></a>为什么要使用Flume</h2><p>在开源框架的选择中，因为Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. 所以我们选择了Flume作为数据的采集工具</p><h2 id="Flume是用来做什么的"><a href="#Flume是用来做什么的" class="headerlink" title="Flume是用来做什么的"></a>Flume是用来做什么的</h2><p>从系统架构图上来看，用户只要产生行为，那么日志就会在Nginx服务器中保存，所以我们现在要做的就是把数据从Nginx服务器中使用Flume采集到HDFS上</p><h2 id="Flume具体怎么用"><a href="#Flume具体怎么用" class="headerlink" title="Flume具体怎么用"></a>Flume具体怎么用</h2><h3 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h3><p>Flume就是一个针对日志数据进行采集和汇总的一个框架</p><p>Flume的进程叫做Agent，每个Agent中有Srouce、Channel、Sink</p><p>Flume从使用层面来讲就是写配置文件，其实就是配置我们的Agent，只要学会从官网查配置就行了</p><p><a href="http://flume.apache.org" target="_blank" rel="noopener">Flume官网</a></p><h3 id="Agent"><a href="#Agent" class="headerlink" title="Agent"></a>Agent</h3><p>Source中的常用方式有 avro、exec、spooling、taildir、kafka</p><p>Channel中的常用方式有 memory、kafka、file</p><p>Sink中的常用方式有 hdfs、logger、avro、kafka</p><h3 id="示例配置"><a href="#示例配置" class="headerlink" title="示例配置"></a>示例配置</h3><p>它描述了一个单节点Flume部署。该配置允许用户生成事件，然后将它们记录到控制台。以后写配置只需要在这个配置文件的基础上做出选择性的修改即可</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>有了这个配置文件，我们可以按如下方式启动Flume:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/example.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>输入端:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">telnet localhost 44444</span><br></pre></td></tr></table></figure><h3 id="TAILDIR"><a href="#TAILDIR" class="headerlink" title="TAILDIR"></a>TAILDIR</h3><p>taildir是Source端可以选择的一个类型，它可以同时支持目录和文件，并且支持offset，Flume挂了重启后可以接着上次消费的地方继续消费，下面提供一个配置taildir的文件</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = taildir</span><br><span class="line">a1.sources.r1.positionFile = /home/hadoop/taildir_position.json#这个目录的上一级目录不能存在</span><br><span class="line">a1.sources.r1.filegroups = f1 f2</span><br><span class="line">a1.sources.r1.filegroups.f1 = /home/hadoop/data/flume/example</span><br><span class="line">a1.sources.r1.headers.f1.headerKey1 = value1</span><br><span class="line">a1.sources.r1.filegroups.f2 = /home/hadoop/data/flume/.*log.*</span><br><span class="line">a1.sources.r1.headers.f2.headerKey1 = value2</span><br><span class="line">a1.sources.r1.headers.f2.headerKey2 = value2-<span class="number">2</span></span><br><span class="line">a1.sources.r1.fileHeader = <span class="keyword">true</span></span><br><span class="line">a1.sources.ri.maxBatchCount = <span class="number">1000</span></span><br></pre></td></tr></table></figure><p>其中<code>taildir_position.json</code>文件是用来记录文件读取offset的位置，方便下次继续从offset位置读取</p><p>注意<code>taildir_position.json</code>文件不能存在上级目录，不然会报错</p><h2 id="java客户端上传消息到Flume写到HDFS"><a href="#java客户端上传消息到Flume写到HDFS" class="headerlink" title="java客户端上传消息到Flume写到HDFS"></a>java客户端上传消息到Flume写到HDFS</h2><p>参考博客: <a href="https://blog.csdn.net/lbship/article/details/85336555" target="_blank" rel="noopener">https://blog.csdn.net/lbship/article/details/85336555</a></p><ol><li><p>在java客户端编写代码生成logger数据</p><p>添加依赖</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">   &lt;groupId&gt;org.apache.flume.flume-ng-clients&lt;/groupId&gt;</span><br><span class="line">   &lt;artifactId&gt;flume-ng-log4jappender&lt;/artifactId&gt;</span><br><span class="line">   &lt;version&gt;1.6.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.slf4j&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.7.21&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flume.flume-ng-clients&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flume-ng-log4jappender&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.6.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>生产日志</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> Logger logger = Logger.getLogger(LoggerGenerator<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(i&lt;<span class="number">99</span>)&#123;</span><br><span class="line">        Thread.sleep(<span class="number">500</span>);</span><br><span class="line">        logger.info(<span class="string">"now is : "</span>+i);</span><br><span class="line">        i++;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>在log4j.properties文件中添加代码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">log4j.rootCategory=info,console,flume  <span class="comment">//rootCategory需要指定flume</span></span><br><span class="line">    </span><br><span class="line">log4j.appender.flume=org.apache.flume.clients.log4jappender.Log4jAppender</span><br><span class="line">log4j.appender.flume.Hostname=<span class="number">121.196</span><span class="number">.220</span><span class="number">.143</span></span><br><span class="line">log4j.appender.flume.Port=<span class="number">41414</span></span><br><span class="line">log4j.appender.flume.UnsafeMode=<span class="keyword">true</span></span><br></pre></td></tr></table></figure></li><li><p>在Hostname服务端接收log4j.properties文件中指定的端口即可，注意Source类型需要使用avro(序列化)</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">a1.sources.r1.port = <span class="number">41414</span></span><br></pre></td></tr></table></figure></li><li><p>写出到HDFS，这里需要注意解决时间戳和乱码问题</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = /flume/%y-%m-%d/%H%M/%S#时间需要根据实际情况修改</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = events-</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp=true#解决时间戳问题</span><br><span class="line">a1.sinks.k1.hdfs.fileType=DataStream #解决乱码</span><br></pre></td></tr></table></figure><p>使用<code>useLocalTimeStamp=true</code>参数解决时间戳异常，使用<code>fileType=DataStream</code>解决文件内容乱码问题</p></li></ol><h2 id="解决Flume采集数据时生成大量小文件的问题"><a href="#解决Flume采集数据时生成大量小文件的问题" class="headerlink" title="解决Flume采集数据时生成大量小文件的问题"></a>解决Flume采集数据时生成大量小文件的问题</h2><p>在使用Flume采集数据时，由于默认参数的影响会生产大量小文件，我们先看默认参数</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hdfs.rollInterval<span class="number">30</span>滚动当前文件之前要等待的秒数</span><br><span class="line">hdfs.rollSize<span class="number">1024</span>触发滚动当前文件的大小，单位bytes(B)</span><br><span class="line">hdfs.rollCount<span class="number">10</span>触发滚动当前文件的events数量</span><br></pre></td></tr></table></figure><p>我们看到默认生成文件有三个条件，每30秒、每1M、每10个events，这样的配置会生成大量的小文件，所以我们要对这三个文件进行修改</p><p>最终生成的文件必须综合时间、文件大小、event数量来决定，时间太长或者文件太大都不利于最终生成的文件。该时间还需要配合<code>hdfs.path</code>参数指定的生成文件时间。</p><p>注意<code>sink.type</code>如果是<code>memory</code>模式，注意文件的大小，防止内存不足，太大可以设置<code>sink.type = file</code></p>]]></content>
      
      
      <categories>
          
          <category> OfflineDW </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OfflineDW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>项目开发的流程</title>
      <link href="/2020/02/15/offlinedw/2.%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E6%B5%81%E7%A8%8B/"/>
      <url>/2020/02/15/offlinedw/2.%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>为什么是离线数据仓库</li><li>采集什么日志</li><li>技术实现流程</li></ol><h2 id="为什么是离线数据仓库"><a href="#为什么是离线数据仓库" class="headerlink" title="为什么是离线数据仓库"></a>为什么是离线数据仓库</h2><h3 id="什么是数据仓库"><a href="#什么是数据仓库" class="headerlink" title="什么是数据仓库"></a>什么是数据仓库</h3><p>将多个数据源的数据经过ETL之后，按照一定的主题继承，提供 <code>决策支持</code> 和 <code>联机分析应用</code> 的结构化数据环境</p><h3 id="为什么要建数据仓库"><a href="#为什么要建数据仓库" class="headerlink" title="为什么要建数据仓库"></a>为什么要建数据仓库</h3><p>摆脱多种不同数据源、异构数据库、不同数据格式等等带来的问题</p><h2 id="采集用户行为日志"><a href="#采集用户行为日志" class="headerlink" title="采集用户行为日志"></a>采集用户行为日志</h2><p>既然要建数据仓库，那么第一步需要考虑的是我们的数据从哪里来？来的什么数据？这些数据是做什么的？</p><h3 id="数据是从哪里来的？"><a href="#数据是从哪里来的？" class="headerlink" title="数据是从哪里来的？"></a>数据是从哪里来的？</h3><p><code>数据</code>通过<code>采集团队</code>从<code>ngnix</code>服务器(请求携带的数据经过nginx服务器)、<code>埋点日志</code>(用户行为过程及结果的记录)和<code>SDK</code>(软件开发工具包)通过<code>flume</code>采集产生的数据到HDFS上</p><h3 id="来的是什么数据？"><a href="#来的是什么数据？" class="headerlink" title="来的是什么数据？"></a>来的是什么数据？</h3><p>采集到的是用户的行为数据，包括日志和业务数据，只要是用户访问、搜索、点击、收藏都会产生数据通过flume采集到了HDFS上</p><h3 id="数据可以用来做什么的？"><a href="#数据可以用来做什么的？" class="headerlink" title="数据可以用来做什么的？"></a>数据可以用来做什么的？</h3><p>数据运用在数据仓库中会进行一系列的ETL、调度、建模，最终用来可视化分析。</p><h2 id="技术实现流程"><a href="#技术实现流程" class="headerlink" title="技术实现流程"></a>技术实现流程</h2><h3 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h3><p>从输入==&gt;到输出</p><p>可以使用的工具:</p><ol><li>SDK</li><li>Flume(推介)</li><li>Sqoop</li><li>DataX</li></ol><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>数据预处理也就是对数据进行ETL/清洗操作</p><ol><li>格式处理：时间、IP、URL</li><li>数据拆分：1 col ==&gt; n cols  (URL、UA)</li><li>数据补充：1 col ==&gt; n cols</li></ol><h3 id="数据入库"><a href="#数据入库" class="headerlink" title="数据入库"></a>数据入库</h3><p>数据清洗后的数据导入到你HIVE中的库/表中(大宽表/N多列)</p><ol><li>分析的维度：day or hour</li><li>表：分区外部表</li></ol><h3 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h3><p>根据不同业务的执行SQL并且通过SQOOP存到RDBMS/NoSQL的表中</p><h3 id="数据展示"><a href="#数据展示" class="headerlink" title="数据展示"></a>数据展示</h3><ol><li>WEB ==&gt; RDBMS</li><li>Echarts</li><li>d3</li><li>HUE</li><li>Zeppelin</li></ol><h3 id="项目架构"><a href="#项目架构" class="headerlink" title="项目架构"></a>项目架构</h3><p><img src="https://yerias.github.io/offlinedw/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E9%A1%B9%E7%9B%AE%E5%9B%BE.jpg" alt="数据仓库架构项目图"></p><h3 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h3><ol><li><p>评估你这个业务线需要多少资源，数据量的计算: 一条数据的大小==&gt;*用户数*每个人一天产生的数据量==&gt;*副本数*天数(365)==&gt;*每年30%的上升空间</p></li><li><p>每个节点磁盘多少？内存多少？物理核多少？通过计算得出需要的节点数</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">core：<span class="number">32</span>/<span class="number">64</span> 决定了应用程序的快慢</span><br><span class="line">memory：<span class="number">256</span>/<span class="number">512</span> 决定了应用程序的生死</span><br><span class="line">disk：<span class="number">10</span>T*数量</span><br></pre></td></tr></table></figure></li></ol><h2 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h2><p>对数据进行ETL操作时，N个业务：至少是3*N个SQL(3层)，并且尽可能把一些麻烦的/繁琐的/join等SQL操作提前进行</p>]]></content>
      
      
      <categories>
          
          <category> OfflineDW </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OfflineDW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>项目开发的准备</title>
      <link href="/2020/02/14/offlinedw/1.%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E5%87%86%E5%A4%87/"/>
      <url>/2020/02/14/offlinedw/1.%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E5%87%86%E5%A4%87/</url>
      
        <content type="html"><![CDATA[<hr><p>涉及到具体的文档，这里只描述流程</p><hr><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li><p>项目调研</p></li><li><p>需求分析</p></li><li><p>方案设计</p></li></ol><h2 id="项目调研"><a href="#项目调研" class="headerlink" title="项目调研"></a>项目调研</h2><p>是什么行业? </p><p>关于什么业务？</p><p>调研人员(资深的产品经理/业务分析人员)</p><h2 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h2><p>要做什么?</p><p>做成啥样?</p><ol><li>需求(表层的需求、隐藏的需求、售前团队的需求)</li><li>产出(需求规格说明书、进度规划：甘特图)</li><li>人员(项目经理、产品、leader)</li></ol><h2 id="方案设计"><a href="#方案设计" class="headerlink" title="方案设计"></a>方案设计</h2><h3 id="概要设计"><a href="#概要设计" class="headerlink" title="概要设计"></a>概要设计</h3><ol><li>技术架构(框架的调研、”糙”点的测试报告)</li><li>模块</li><li>功能点</li><li>和甲方确认过程</li></ol><h3 id="详细设计"><a href="#详细设计" class="headerlink" title="详细设计"></a>详细设计</h3><p>详细设计是最复杂的、篇幅最大的，针对具体功能的实现</p><p>基本要求(类、方法(入参、出参)、UML、图表)</p><p>系统要求/非功能需求</p><ol><li>扩展性</li><li>容错性</li><li>是否支持定制化：Azkaban</li><li>监控/告警(发生后)/预警(发生前)</li></ol><h3 id="功能开发"><a href="#功能开发" class="headerlink" title="功能开发"></a>功能开发</h3><p>码农对着文档把方法实现了而已，CICD(持续集成(CI)、持续交付(CD))</p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><ol><li>测试覆盖率 95%</li><li>场景测试(案例)</li><li>功能测试</li><li>联调测试(一个完整流程)</li><li>性能/压力测试</li><li>用户测试</li></ol><h3 id="部署上线"><a href="#部署上线" class="headerlink" title="部署上线"></a>部署上线</h3><ol><li>试运行(“双活”系统做DIFF)</li><li>正式上线</li></ol><h3 id="后期维护"><a href="#后期维护" class="headerlink" title="后期维护"></a>后期维护</h3><p>后期维护的费用并不低，包括开发新功能，修复老bug</p>]]></content>
      
      
      <categories>
          
          <category> OfflineDW </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
            <tag> OfflineDW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GIT的常用操作&amp;GITHUB的常用操作&amp;在IDEA中使用GIT操作GITHUB</title>
      <link href="/2019/02/01/git/1/"/>
      <url>/2019/02/01/git/1/</url>
      
        <content type="html"><![CDATA[<h2 id="GIT实战操作"><a href="#GIT实战操作" class="headerlink" title="GIT实战操作"></a>GIT实战操作</h2><ol><li><p>创建版本库</p><p>在项目文件夹内，执行: git init</p></li><li><p>提交文件</p><p>新建文件后，通过git status 进行查看文件状态(可选)</p><p>将文件添加到暂存区  git add 文件名</p><p>或者也可以git commit –m “注释内容”, 直接带注释提交</p></li><li><p>查看文件提交记录</p><p>git log –pretty=oneline 文件名   进行查看历史记录</p></li><li><p>回退历史</p><p>git reset –hard HEAD~n 回退n次操作</p></li><li><p>版本穿越</p><p>进行查看历史记录的版本号，执行 git reflog 文件名</p><p>执行 git reset –hard 版本号</p></li><li><p>还原文件</p><p>git checkout – 文件名 </p></li><li><p>删除某个文件</p><p>先删除文件 git rm 文件名</p><p>再git add 再提交</p></li><li><p>创建分支</p><p>git branch &lt;分支名&gt;</p><p>git branch –v 查看分支</p></li><li><p>切换分支</p><p>git checkout –b &lt;分支名&gt;</p></li><li><p>合并分支</p><p>先切换到主干  git checkout master</p><p>git merge &lt;分支名&gt;</p></li><li><p>合并时冲突</p><p>程序合并时发生冲突系统会提示CONFLICT关键字，命令行后缀会进入MERGING状态，表示此时是解决冲突的状态。</p><p>然后修改冲突文件的内容，再次git add <file> 和git commit 提交后，后缀MERGING消失，说明冲突解决完成。</p></li></ol><h2 id="GITHUB实战操作"><a href="#GITHUB实战操作" class="headerlink" title="GITHUB实战操作"></a>GITHUB实战操作</h2><ol><li><p>搭建代码库</p><ul><li><p>git init</p></li><li><p>git config </p><ul><li>git config –global(全局) user.email “<a href="mailto:you@example.com" target="_blank" rel="noopener">you@example.com</a>”</li><li>git config –global(全局) user.name “Your Name”</li></ul></li></ul></li><li><p>提交代码到本地仓库</p><ul><li><p>git add 文件名</p></li><li><p>git commit –m “注释内容”</p></li></ul></li><li><p>GitHub准备工作：</p><ul><li><p>注册GitHub账号</p></li><li><p>在GitHub搭建项目</p></li></ul></li><li><p>推送代码到远端</p><ul><li><p>git remote add origin <url>(仓库地址)</p></li><li><p>git push origin master</p></li></ul></li><li><p>其他用户克隆</p><p>git clone <url></p></li><li><p>其他用户提交代码到本地仓库</p><ul><li><p>git add 文件名</p></li><li><p>git commit –m “注释内容”</p></li></ul></li><li><p>其他用户推送到远端仓库</p><ul><li>git push origin master</li></ul></li><li><p>其他用户拉取代码</p><ul><li>git pull origin master</li></ul></li><li><p>增加远程地址</p><ul><li>git remote add &lt;远端代号(origin)&gt;  &lt;远端地址&gt;</li></ul></li><li><p>推送到远程库</p><ul><li>git push  &lt;远端代号&gt;  &lt;本地分支名称&gt;</li></ul></li><li><p>合作开发权限</p><p><img src="https://yerias.github.io/git/%E6%B7%BB%E5%8A%A0%E5%90%88%E4%BD%9C%E4%BC%99%E4%BC%B41.jpg" alt="添加合作伙伴1"></p><p><img src="https://yerias.github.io/git/%E6%B7%BB%E5%8A%A0%E5%90%88%E4%BD%9C%E4%BC%99%E4%BC%B42.jpg" alt="添加合作伙伴2"></p></li><li><p>协作冲突</p><p>在上传或同步代码时，由于你和他人都改了同一文件的同一位置的代码，版本管理软件无法判断究竟以谁为准，就会报告冲突,需要程序员手工解决。</p><ul><li><p>修改合并</p></li><li><p>git add 文件名</p></li><li><p>git commit –m “注释内容”</p></li><li><p>git push origin master</p></li></ul></li></ol><h2 id="在IDEA中使用GIT"><a href="#在IDEA中使用GIT" class="headerlink" title="在IDEA中使用GIT"></a>在IDEA中使用GIT</h2><ol><li><p>配置</p><p>setting配置GIT</p><p><img src="https://yerias.github.io/git/%E9%85%8D%E7%BD%AEgit%E6%89%A7%E8%A1%8C%E7%A8%8B%E5%BA%8F.jpg" alt="配置git执行程序"></p></li><li><p>创建仓库</p><p>VCS配置账户密码创建仓库</p><p><img src="https://yerias.github.io/git/%E5%88%9B%E5%BB%BAgithub%E4%BB%93%E5%BA%93.jpg" alt="创建github仓库"></p></li><li><p>提交代码</p><p><img src="https://yerias.github.io/git/%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%811.jpg" alt="提交代码1"></p><p><img src="https://yerias.github.io/git/%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%812.jpg" alt="提交代码2"></p><p><img src="https://yerias.github.io/git/%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%813.jpg" alt="提交代码3"></p></li><li><p>同步代码</p><p><img src="https://yerias.github.io/git/%E5%90%8C%E6%AD%A5%E4%BB%A3%E7%A0%81.jpg" alt="同步代码"></p></li><li><p>克隆项目</p><p><img src="https://yerias.github.io/git/%E5%85%8B%E9%9A%86%E4%BB%A3%E7%A0%811.jpg" alt="克隆代码1"></p><p><img src="https://yerias.github.io/git/%E5%85%8B%E9%9A%86%E4%BB%A3%E7%A0%812.jpg" alt="克隆代码2"></p></li><li><p>解决版本冲突</p><p>代码添加到公共区间再次提交</p><p><img src="https://yerias.github.io/git/%E7%89%88%E6%9C%AC%E5%86%B2%E7%AA%81.jpg" alt="版本冲突"></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IO流&amp;比较器&amp;内部类&amp;Random</title>
      <link href="/2019/01/08/java/8/"/>
      <url>/2019/01/08/java/8/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>IO流</li><li>比较器</li><li>内部类</li><li>Random</li></ol><h2 id="IO流"><a href="#IO流" class="headerlink" title="IO流"></a>IO流</h2><p>Java中的流根据传输方向分为输入输出流，根据操作数据的不同又可以分为字节流和字符流</p><h3 id="字节流"><a href="#字节流" class="headerlink" title="字节流"></a>字节流</h3><p>所有的字节流都继承自InputStream接口和OutputStream接口</p><p>用于文件传输的是FileInputStream类和FileOutputStream类，传输的是字节，使用FileInputStream读取文件时，可以使用byte字节数组建立一个字节数组缓冲区，读取数据时read方法中传入一个字节数组，每次读取一个字节数组的数据，即可实现缓冲区读取数据</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">byte</span>[] buff = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</span><br><span class="line">in.read(buff)  <span class="comment">//数据读进buff中</span></span><br><span class="line">out.write(buff) <span class="comment">//write中传入buff写出数据</span></span><br></pre></td></tr></table></figure><p>在字节流的IO包中提供了两个带缓冲的字节流，分别是BufferedInputStream和BufferedOutputStream，他们的构造方法中分别接受InputStream和OutputStream，类型的参数作为对象，这两</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">new</span> BufferedInputStream(<span class="keyword">new</span> InputStream(filePath));</span><br></pre></td></tr></table></figure><h3 id="字符流"><a href="#字符流" class="headerlink" title="字符流"></a>字符流</h3><h2 id="比较器"><a href="#比较器" class="headerlink" title="比较器"></a>比较器</h2><h2 id="内部类"><a href="#内部类" class="headerlink" title="内部类"></a>内部类</h2><h2 id="Random"><a href="#Random" class="headerlink" title="Random"></a>Random</h2>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA.UTIL包下的HASHSET和迭代器快速失败的的源码解析以及多线程下故障现象、导致原因、以及解决办法和优化建议</title>
      <link href="/2019/01/05/java/5/"/>
      <url>/2019/01/05/java/5/</url>
      
        <content type="html"><![CDATA[<hr><p>该篇博客不适合小白，只做针对性的api源码解析，以及适合我自身的案例研究</p><hr>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA.UTIL包下的HASHMAP和迭代器快速失败的的源码解析以及多线程下故障现象、导致原因、以及解决办法和优化建议</title>
      <link href="/2019/01/04/java/4/"/>
      <url>/2019/01/04/java/4/</url>
      
        <content type="html"><![CDATA[<hr><p>该篇博客不适合小白，只做针对性的api源码解析，以及适合我自身的案例研究</p><hr>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA.UTIL包下的LINKEDLIST和迭代器快速失败的源码解析以及多线程下故障现象、导致原因、以及解决办法和优化建议</title>
      <link href="/2019/01/03/java/3/"/>
      <url>/2019/01/03/java/3/</url>
      
        <content type="html"><![CDATA[<hr><p>该篇博客不适合小白，只做针对性的api源码解析，以及适合我自身的案例研究</p><hr><ol><li><p>使用多个线程往<code>LinkedList</code>中添加元素</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">50</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">        list.add(UUID.randomUUID().toString().substring(<span class="number">0</span>,<span class="number">8</span>));</span><br><span class="line">        System.out.println(list);</span><br><span class="line">    &#125;,String.valueOf(i)).start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>故障现象</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Exception in thread <span class="string">"3"</span> java.util.ConcurrentModificationException</span><br></pre></td></tr></table></figure><p>我们知道<code>LinkedList</code>是线程不安全的，当多线程操作时，线程操作迭代器的同时其他线程改变了元素的值就会产生<code>ConcurrentModificationException</code>异常，<code>ConcurrentModificationException</code>是在操作<code>Iterator</code>时抛出的异常</p></li><li><p>故障原因</p><p>从前一篇的<code>ArrayList</code>中的解析中得出，我们最终都会经过迭代器中的代码检查<code>modCount</code> 和 <code>expectedModCount</code>的值相不相等</p><p>所以我们这里就不追踪栈异常信息，而直接看看源码中的操作是如何让<code>modCount</code> 和<code>expectedModCount</code>不相等的</p><ol><li><p>首先找出问题的关键代码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">      <span class="keyword">private</span> Node&lt;E&gt; lastReturned;<span class="comment">//最近一次返回的节点，也是当前持有的节点</span></span><br><span class="line">      <span class="keyword">private</span> Node&lt;E&gt; next;<span class="comment">//对下一个元素的引用</span></span><br><span class="line">      <span class="keyword">private</span> <span class="keyword">int</span> nextIndex;<span class="comment">//下一个节点的索引</span></span><br><span class="line">      <span class="keyword">private</span> <span class="keyword">int</span> expectedModCount = modCount;</span><br><span class="line">      </span><br><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">          checkForComodification();<span class="comment">//每次添加元素前，都检查一次modCount和expectedModCount是否相等，如果不相等就直接返回ConcurrentModificationException异常，产生快速失败,多线程环境下这里通不过</span></span><br><span class="line">          <span class="keyword">if</span> (!hasNext())</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> NoSuchElementException();</span><br><span class="line">      </span><br><span class="line">          lastReturned = next;<span class="comment">//当前节点--&gt;下一个节点</span></span><br><span class="line">          next = next.next;<span class="comment">//下下个节点的指正往前移一个节点</span></span><br><span class="line">          nextIndex++;<span class="comment">//index++</span></span><br><span class="line">          <span class="keyword">return</span> lastReturned.item; <span class="comment">//返回移动后的节点的数据</span></span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">checkForComodification</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (modCount != expectedModCount)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>在解析如何修改<code>modCount</code> 值之前我们应该弄明白，<code>LinkedList</code>中的<code>Node</code>节点是如何实现的</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="comment">//私有节点类Node，在1.8版本之前叫做Entry</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span>&lt;<span class="title">E</span>&gt; </span>&#123;</span><br><span class="line">       E item;<span class="comment">//存储的元素</span></span><br><span class="line">       Node&lt;E&gt; next;<span class="comment">//后继结点</span></span><br><span class="line">       Node&lt;E&gt; prev;<span class="comment">//前驱结点</span></span><br><span class="line">      </span><br><span class="line">       <span class="comment">// 前驱结点、存储的元素和后继结点作为参数的构造方法</span></span><br><span class="line">       Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123;</span><br><span class="line">           <span class="keyword">this</span>.item = element;</span><br><span class="line">           <span class="keyword">this</span>.next = next;</span><br><span class="line">           <span class="keyword">this</span>.prev = prev;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></li><li><p>源码中是如何修改<code>modCount</code> 的值的(不包括作为队列和双端队列的方法)(开始怼源码)</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">transient</span> <span class="keyword">int</span> size = <span class="number">0</span>;  <span class="comment">//元素数量</span></span><br><span class="line"><span class="keyword">transient</span> Node&lt;E&gt; first;  <span class="comment">//首节点引用，为null</span></span><br><span class="line"><span class="keyword">transient</span> Node&lt;E&gt; last;  <span class="comment">//尾结点引用，为null</span></span><br><span class="line">      </span><br><span class="line"><span class="comment">//追加一个元素到列表的末尾</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">add</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//把元素存放到链表的末尾</span></span><br><span class="line">       linkLast(e);</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//尾部添加元素</span></span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">linkLast</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//获取当前尾节点的引用</span></span><br><span class="line">       <span class="keyword">final</span> Node&lt;E&gt; l = last;</span><br><span class="line">       <span class="comment">//构建一个新节点，prev的值为l(尾节点)、节点数据为e、next的值为null</span></span><br><span class="line">       <span class="keyword">final</span> Node&lt;E&gt; newNode = <span class="keyword">new</span> Node&lt;&gt;(l, e, <span class="keyword">null</span>);</span><br><span class="line">       <span class="comment">//新节点作为尾结点</span></span><br><span class="line">       last = newNode;</span><br><span class="line">       <span class="comment">//如果原尾节点为null</span></span><br><span class="line">       <span class="keyword">if</span> (l == <span class="keyword">null</span>)</span><br><span class="line">           <span class="comment">//即原链表为null，链表的首节点也是newNode</span></span><br><span class="line">           first = newNode;</span><br><span class="line">       <span class="keyword">else</span></span><br><span class="line">           <span class="comment">//否则，原节点的next设置为newNode</span></span><br><span class="line">           l.next = newNode;</span><br><span class="line">       <span class="comment">//数量+1</span></span><br><span class="line">       size++;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//将指定元素插入到列表中的指定位置。</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> index, E element)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查index是否在有效范围内</span></span><br><span class="line">       checkPositionIndex(index);</span><br><span class="line"><span class="comment">//如果index==size，说明添加的位置是末尾</span></span><br><span class="line">       <span class="keyword">if</span> (index == size)</span><br><span class="line">           <span class="comment">//在末尾添加元素</span></span><br><span class="line">           linkLast(element);</span><br><span class="line">       <span class="keyword">else</span></span><br><span class="line">           <span class="comment">//把element元素插入到index指定的节点位置</span></span><br><span class="line">           linkBefore(element, node(index));</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//检查index是否在有效范围内</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">checkPositionIndex</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (!isPositionIndex(index))</span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> IndexOutOfBoundsException(outOfBoundsMsg(index));</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">isPositionIndex</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">return</span> index &gt;= <span class="number">0</span> &amp;&amp; index &lt;= size;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//返回指定元素索引处的(非空)节点。(注意这里有技巧)</span></span><br><span class="line">   <span class="function">Node&lt;E&gt; <span class="title">node</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//如果index小于size的1/2长度</span></span><br><span class="line">       <span class="keyword">if</span> (index &lt; (size &gt;&gt; <span class="number">1</span>)) &#123;</span><br><span class="line">           <span class="comment">//获取头结点的引用</span></span><br><span class="line">           Node&lt;E&gt; x = first;</span><br><span class="line">           <span class="comment">//从前到后遍历index个节点，最后返回</span></span><br><span class="line">           <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; index; i++)</span><br><span class="line">               x = x.next;</span><br><span class="line">           <span class="keyword">return</span> x;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="comment">//否则获取最后一个节点的引用</span></span><br><span class="line">           Node&lt;E&gt; x = last;</span><br><span class="line">           <span class="comment">//从后到前遍历index个节点，最后返回</span></span><br><span class="line">           <span class="keyword">for</span> (<span class="keyword">int</span> i = size - <span class="number">1</span>; i &gt; index; i--)</span><br><span class="line">               x = x.prev;</span><br><span class="line">           <span class="keyword">return</span> x;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//在非空节点succ之前插入元素e。</span></span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">linkBefore</span><span class="params">(E e, Node&lt;E&gt; succ)</span> </span>&#123;</span><br><span class="line"><span class="comment">//获取succ节点(index)的前一个节点的引用</span></span><br><span class="line">       <span class="keyword">final</span> Node&lt;E&gt; pred = succ.prev;</span><br><span class="line">       <span class="comment">//创建一个新的节点，维护的新节点的前一个节点是pred、元素本身、和元素的下一个节点succ</span></span><br><span class="line">       <span class="keyword">final</span> Node&lt;E&gt; newNode = <span class="keyword">new</span> Node&lt;&gt;(pred, e, succ);</span><br><span class="line">       <span class="comment">//succ节点的上一个节点指向newNode</span></span><br><span class="line">       succ.prev = newNode;</span><br><span class="line">       <span class="keyword">if</span> (pred == <span class="keyword">null</span>)</span><br><span class="line">           <span class="comment">//如果pred节点为null，则说明没有值，则newNode就是第一个节点</span></span><br><span class="line">           first = newNode;</span><br><span class="line">       <span class="keyword">else</span></span><br><span class="line">           <span class="comment">//否则pred的下一个节点指向newNode</span></span><br><span class="line">           pred.next = newNode;</span><br><span class="line">       <span class="comment">//数量+1</span></span><br><span class="line">       size++;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//将指定集合中的所有元素追加到这个列表</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">addAll</span><span class="params">(Collection&lt;? extends E&gt; c)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//调用了插入集合元素的方法，指定了index=size</span></span><br><span class="line">       <span class="keyword">return</span> addAll(size, c);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//将指定集合中的所有元素插入其中列表</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">addAll</span><span class="params">(<span class="keyword">int</span> index, Collection&lt;? extends E&gt; c)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查index是否有效</span></span><br><span class="line">       checkPositionIndex(index);</span><br><span class="line"><span class="comment">//先把集合转换成数组，这也就限定了集合必须是List下的实现类</span></span><br><span class="line">       Object[] a = c.toArray();</span><br><span class="line">       <span class="comment">//获取集合的长度</span></span><br><span class="line">       <span class="keyword">int</span> numNew = a.length;</span><br><span class="line">       <span class="comment">//如果集合的长度为0，即为空</span></span><br><span class="line">       <span class="keyword">if</span> (numNew == <span class="number">0</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">       </span><br><span class="line"><span class="comment">//定义两个节点，succ指向当前需要插入节点的位置，pred指向其前一个节点</span></span><br><span class="line">    Node&lt;E&gt; pred, succ;</span><br><span class="line">       <span class="comment">//如果在尾部插入元素</span></span><br><span class="line">       <span class="keyword">if</span> (index == size) &#123;</span><br><span class="line">           <span class="comment">//当前节点为空</span></span><br><span class="line">           succ = <span class="keyword">null</span>;</span><br><span class="line">           <span class="comment">//当前节点的上一个节点就是最后一个节点</span></span><br><span class="line">           pred = last;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="comment">//否则获取index位置的节点指向succ</span></span><br><span class="line">           succ = node(index);</span><br><span class="line">           <span class="comment">//index位置节点的前一个引用指向pred</span></span><br><span class="line">           pred = succ.prev;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="comment">//遍历集合，每一个节点都做重复创建，添加引用</span></span><br><span class="line">       <span class="keyword">for</span> (Object o : a) &#123;</span><br><span class="line">           <span class="meta">@SuppressWarnings</span>(<span class="string">"unchecked"</span>) E e = (E) o;</span><br><span class="line">           <span class="comment">//创建节点，该节点的pred指向pred节点(最初的index节点的前一个节点)，元素本身e，下一个节点指向null。</span></span><br><span class="line">           Node&lt;E&gt; newNode = <span class="keyword">new</span> Node&lt;&gt;(pred, e, <span class="keyword">null</span>);</span><br><span class="line">           <span class="comment">//判断是否为链表头</span></span><br><span class="line">           <span class="keyword">if</span> (pred == <span class="keyword">null</span>)</span><br><span class="line">               first = newNode;</span><br><span class="line">           <span class="keyword">else</span></span><br><span class="line">               <span class="comment">//pred节点的next也指向新节点</span></span><br><span class="line">               pred.next = newNode;</span><br><span class="line">           <span class="comment">//新节点又继续作为pred节点存在</span></span><br><span class="line">           pred = newNode;</span><br><span class="line">       &#125;</span><br><span class="line"><span class="comment">//集合遍历完成后，如果当前节点为空节点，即在链表的末尾添加元素，就把pred指向尾结点，succ节点不存在</span></span><br><span class="line">       <span class="keyword">if</span> (succ == <span class="keyword">null</span>) &#123;</span><br><span class="line">           last = pred;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="comment">//如果是在链表中间插入的集合，当前节点succ是作位pred的next元素存在</span></span><br><span class="line">           pred.next = succ;</span><br><span class="line">           <span class="comment">//同时pred指向succ的上一个节点引用，相互引用</span></span><br><span class="line">           succ.prev = pred;</span><br><span class="line">       &#125;</span><br><span class="line"><span class="comment">//size+numNew个数</span></span><br><span class="line">       size += numNew;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除列表中指定位置的元素。</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> E <span class="title">remove</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查index是否有效</span></span><br><span class="line">       checkElementIndex(index);</span><br><span class="line">       <span class="comment">//删除index节点的元素</span></span><br><span class="line">       <span class="keyword">return</span> unlink(node(index));</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//删除节点</span></span><br><span class="line">   <span class="function">E <span class="title">unlink</span><span class="params">(Node&lt;E&gt; x)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//获取节点的元素</span></span><br><span class="line">       <span class="keyword">final</span> E element = x.item;</span><br><span class="line">       <span class="comment">//获取元素的下一个节点的引用</span></span><br><span class="line">       <span class="keyword">final</span> Node&lt;E&gt; next = x.next;</span><br><span class="line">       <span class="comment">//获取元素的上一个节点的引用</span></span><br><span class="line">       <span class="keyword">final</span> Node&lt;E&gt; prev = x.prev;</span><br><span class="line">      </span><br><span class="line">       <span class="comment">//如果上一个节点等于null，则说明前面没有节点，把next节点的下一个节点引用为第一个节点</span></span><br><span class="line">       <span class="keyword">if</span> (prev == <span class="keyword">null</span>) &#123;</span><br><span class="line">           first = next;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="comment">//否则把next节点引用为prev节点的next指向的节点</span></span><br><span class="line">           prev.next = next;</span><br><span class="line">           <span class="comment">//x节点的上一个节点置空</span></span><br><span class="line">           x.prev = <span class="keyword">null</span>;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="comment">//如果next节点等于null，为空值，就直接把prev赋值为last节点</span></span><br><span class="line">       <span class="keyword">if</span> (next == <span class="keyword">null</span>) &#123;</span><br><span class="line">           last = prev;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="comment">//否则把next节点的上一个节点指向prev</span></span><br><span class="line">           next.prev = prev;</span><br><span class="line">           <span class="comment">//x节点的下一个节点置空</span></span><br><span class="line">           x.next = <span class="keyword">null</span>;</span><br><span class="line">       &#125;</span><br><span class="line"><span class="comment">//最后把x的数据置空</span></span><br><span class="line">       x.item = <span class="keyword">null</span>;</span><br><span class="line">       size--;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">       <span class="comment">//返回已经删除的元素值</span></span><br><span class="line">       <span class="keyword">return</span> element;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//从列表中删除指定元素的第一个匹配项</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">remove</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//如果对象o等于null(这里说明linkedList允许存放null值)</span></span><br><span class="line">       <span class="keyword">if</span> (o == <span class="keyword">null</span>) &#123;</span><br><span class="line">           <span class="comment">//从头到尾遍历找出第一个符合数据为null的节点</span></span><br><span class="line">           <span class="keyword">for</span> (Node&lt;E&gt; x = first; x != <span class="keyword">null</span>; x = x.next) &#123;</span><br><span class="line">               <span class="keyword">if</span> (x.item == <span class="keyword">null</span>) &#123;</span><br><span class="line">                   <span class="comment">//删除节点</span></span><br><span class="line">                   unlink(x);</span><br><span class="line">                   <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="comment">//否则 从头到尾遍历找出第一个符合数据为null的节点</span></span><br><span class="line">           <span class="keyword">for</span> (Node&lt;E&gt; x = first; x != <span class="keyword">null</span>; x = x.next) &#123;</span><br><span class="line">               <span class="comment">//找出符合qeuals条件的对象</span></span><br><span class="line">               <span class="keyword">if</span> (o.equals(x.item)) &#123;</span><br><span class="line">                   <span class="comment">//删除</span></span><br><span class="line">                   unlink(x);</span><br><span class="line">                   <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//从列表中删除所有元素。</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">//注意使用的是for循环遍历全部节点</span></span><br><span class="line">           <span class="keyword">for</span> (Node&lt;E&gt; x = first; x != <span class="keyword">null</span>; ) &#123;</span><br><span class="line">               <span class="comment">//每个节点的所有信息全部置空</span></span><br><span class="line">               Node&lt;E&gt; next = x.next;</span><br><span class="line">               x.item = <span class="keyword">null</span>;</span><br><span class="line">               x.next = <span class="keyword">null</span>;</span><br><span class="line">               x.prev = <span class="keyword">null</span>;</span><br><span class="line">               <span class="comment">//循环赋值</span></span><br><span class="line">               x = next;</span><br><span class="line">           &#125;</span><br><span class="line">      <span class="comment">//首节点和尾结点赋值为null</span></span><br><span class="line">           first = last = <span class="keyword">null</span>;</span><br><span class="line">           size = <span class="number">0</span>;</span><br><span class="line">           modCount++;</span><br><span class="line">       &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//获取指定位置节点元素</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> E <span class="title">get</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查index是否有效</span></span><br><span class="line">       checkElementIndex(index);</span><br><span class="line">       <span class="comment">//返回index节点的数据</span></span><br><span class="line">       <span class="keyword">return</span> node(index).item;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//将列表中指定位置的元素替换为指定元素。</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> E <span class="title">set</span><span class="params">(<span class="keyword">int</span> index, E element)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查index是否有效</span></span><br><span class="line">       checkElementIndex(index);</span><br><span class="line">       <span class="comment">//获取index的节点</span></span><br><span class="line">       Node&lt;E&gt; x = node(index);</span><br><span class="line">       <span class="comment">//获取节点的item</span></span><br><span class="line">       E oldVal = x.item;</span><br><span class="line"><span class="comment">//新的元素替换旧的元素</span></span><br><span class="line">       x.item = element;</span><br><span class="line">       <span class="comment">//返回旧元素</span></span><br><span class="line">       <span class="keyword">return</span> oldVal;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//将双向链表转换成数组</span></span><br><span class="line">   <span class="keyword">public</span> Object[] toArray() &#123;</span><br><span class="line">       <span class="comment">//创建一个Size大小的数组</span></span><br><span class="line">       Object[] result = <span class="keyword">new</span> Object[size];</span><br><span class="line">       <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">       <span class="comment">//循环遍历所有节点</span></span><br><span class="line">       <span class="keyword">for</span> (Node&lt;E&gt; x = first; x != <span class="keyword">null</span>; x = x.next)</span><br><span class="line">           <span class="comment">//将节点数据存入数组</span></span><br><span class="line">           result[i++] = x.item;</span><br><span class="line">       <span class="comment">//返回数组</span></span><br><span class="line">       <span class="keyword">return</span> result;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>和<code>ArrayList</code>一样，无论<code>add()</code>、<code>remove()</code>，还是<code>clear()</code>，只要涉及到修改集合中的元素个数时，都会改变<code>modCount</code>(全局)的值。由此回到<code>next()</code>方法中我们发现当 ‘A’ 线程正在做迭代器遍历操作时，<code>modCount</code>赋值给了<code>expectedModCount</code>，每次调用<code>next()</code>方法都会做一次<code>modCount != expectedModCount</code>的校验，此时线程 ’B‘ 进来了调用了add方法修改了<code>modCount</code>的值，此时<code>modCount</code>变成了N+1，判断为false，抛出<code>ConcurrentModificationException</code>异常，产生fail-fast事件 。</p></li></ol></li><li><p>fail-fast`事件</p><p>当多个线程对同一个集合进行操作的时候，某线程访问集合的过程中，该集合的内容被其他线程所改变(即其它线程通过<code>add</code>、<code>remove</code>、<code>clear</code>等方法，改变了<code>modCount</code>的值)；这时，就会抛出<code>ConcurrentModificationException</code>异常。与此对应的安全失败，在后面再解析。</p></li><li><p>解决方法</p><p>使用<code>Collections.synchronizedList()</code>方法包装，但是效率低下，这种方法实际上只是将原来非线程安全的<code>LinkedList</code>中的方法加上一个<code>synchronized</code>同步代码块 (哭了。。。)</p></li><li><p>优化建议</p><p>在多线程下如果有按数据索引访问元素的情形，采用<code>Collections.synchronizedList(new LinkedList&lt;&gt;())</code>方法</p></li><li><p>LinkedList的常用API</p><table><thead><tr><th>方法</th><th>描述</th></tr></thead><tbody><tr><td><code>add(E e)</code></td><td>将指定的元素添加到列表的末尾。</td></tr><tr><td><code>addFirst(E e)</code></td><td>在此列表的开始处插入指定的元素。</td></tr><tr><td><code>addLast(E e)</code></td><td>将指定的元素列表的结束。</td></tr><tr><td><code>get(int index)</code></td><td>返回此列表中指定位置的元素。</td></tr><tr><td><code>remove(int index)</code></td><td>移除此列表中指定位置的元素。</td></tr><tr><td><code>size()</code></td><td>返回此列表中元素的数目。</td></tr><tr><td><code>toArray()</code></td><td>返回一个数组，包含在这个列表中的所有元素在适当的顺序（从第一个到最后一个元素）。</td></tr><tr><td><code>clear()</code></td><td>从这个列表中移除所有的元素。</td></tr><tr><td><code>indexOf(Object o)</code></td><td>返回此列表中指定元素的第一个出现的索引</td></tr><tr><td><code>peek()</code></td><td>检索，但不删除，此列表的头（第一个元素）。</td></tr><tr><td><code>poll()</code></td><td>检索并删除此列表的头（第一个元素）。</td></tr><tr><td><code>pop()</code></td><td>从这个列表所表示的堆栈中弹出一个元素。</td></tr><tr><td><code>push(E e)</code></td><td>将一个元素推到由该列表所表示的堆栈上。</td></tr><tr><td><code>offer(E e)</code></td><td>将指定的元素添加到列表的尾部（最后一个元素）。</td></tr></tbody></table></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA.UTIL包下的ARRAYLIST和迭代器快速失败的源码解析以及多线程下故障现象、导致原因、以及解决办法和优化建议</title>
      <link href="/2019/01/02/java/2/"/>
      <url>/2019/01/02/java/2/</url>
      
        <content type="html"><![CDATA[<hr><p>该篇博客不适合小白，只做针对性的api源码解析，以及适合我自身的案例研究</p><hr><ol><li><p>使用多个线程往<code>ArrayList</code>中添加元素</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建资源集合</span></span><br><span class="line">List&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建30个线程添加元素</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">30</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">new</span> Thread(()-&gt;&#123;</span><br><span class="line">        list.add(UUID.randomUUID().toString().substring(<span class="number">0</span>,<span class="number">8</span>));</span><br><span class="line">        System.out.println(list);</span><br><span class="line">    &#125;).start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>故障现象</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Exception in thread <span class="string">"2"</span> java.util.ConcurrentModificationException</span><br></pre></td></tr></table></figure><p>我们知道<code>ArrayList</code>是线程不安全的，当多线程操作时，线程操作迭代器的同时其他线程改变了元素的值就会产生<code>ConcurrentModificationException</code>异常，<code>ConcurrentModificationException</code>是在操作<code>Iterator</code>时抛出的异常</p></li><li><p>故障原因</p><p>java中的<code>java.util</code>包下的类全部都是快速失败的，那么为什么在多线程操作ArrayList的时候会出现<code>ConcurrentModificationException</code>异常呢？</p><p>在我们的案例中，每个线程添加一次元素我们就打印一次集合中的元素，通过源码追踪，得出下面的内容</p><ol><li><p>打印内容，经过第二行代码，String调用了valueOf(x)方法</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">println</span><span class="params">(Object x)</span> </span>&#123;</span><br><span class="line">    String s = String.valueOf(x);</span><br><span class="line">    <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">        print(s);</span><br><span class="line">        newLine();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>ValueOf(Object x)方法是把Object类型的对象x转换成String类型，判断不为null，进入obj也就是我们的集合</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">valueOf</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (obj == <span class="keyword">null</span>) ? <span class="string">"null"</span> : obj.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>集合中的toString方法定义在了AbstractCollection类中，我们的错误出现在了 <code>E e = it.next();</code>获取元素这里，继续追踪错误。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Iterator&lt;E&gt; it = iterator();<span class="comment">//创建迭代器对象</span></span><br><span class="line">    <span class="keyword">if</span> (! it.hasNext())<span class="comment">//判断集合是否为空</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"[]"</span>;</span><br><span class="line">      </span><br><span class="line">    StringBuilder sb = <span class="keyword">new</span> StringBuilder();<span class="comment">//创建StringBuilder动态构造字符串</span></span><br><span class="line">    sb.append(<span class="string">'['</span>);</span><br><span class="line">    <span class="keyword">for</span> (;;) &#123;<span class="comment">//死循环</span></span><br><span class="line">        E e = it.next();<span class="comment">//获取元素 E ==&gt; String 是传进来的泛型</span></span><br><span class="line">        sb.append(e == <span class="keyword">this</span> ? <span class="string">"(this Collection)"</span> : e);<span class="comment">//添加元素</span></span><br><span class="line">        <span class="keyword">if</span> (! it.hasNext())<span class="comment">//循环换了就返回集合的字符串类型</span></span><br><span class="line">            <span class="keyword">return</span> sb.append(<span class="string">']'</span>).toString();</span><br><span class="line">        sb.append(<span class="string">','</span>).append(<span class="string">' '</span>); <span class="comment">//这里是没有return之前会进来，添加分隔符</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>下面是迭代器中的next()方法</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="comment">// 用来记录List修改的次数：每修改一次(添加/删除等操作)，将modCount+1</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">transient</span> <span class="keyword">int</span> modCount = <span class="number">0</span>;</span><br><span class="line">      </span><br><span class="line"><span class="keyword">int</span> cursor;       <span class="comment">// 下一个要返回的元素的索引</span></span><br><span class="line">      <span class="keyword">int</span> lastRet = -<span class="number">1</span>; <span class="comment">// 最后一个返回元素的索引;-1:没有</span></span><br><span class="line">      <span class="keyword">int</span> expectedModCount = modCount;<span class="comment">//这是非常关键的一步，把modCount赋值给expectedModCount，用来在迭代器遍历时next()和remove()方法中做校验</span></span><br><span class="line">      </span><br><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">          checkForComodification();<span class="comment">//每次添加元素前，都检查一次modCount和expectedModCount是否相等，如果不相等就直接返回ConcurrentModificationException异常，产生快速失败,多线程环境下这里通不过</span></span><br><span class="line">          <span class="keyword">int</span> i = cursor;</span><br><span class="line">          <span class="keyword">if</span> (i &gt;= size)</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> NoSuchElementException();</span><br><span class="line">          Object[] elementData = ArrayList.<span class="keyword">this</span>.elementData;</span><br><span class="line">          <span class="keyword">if</span> (i &gt;= elementData.length)</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</span><br><span class="line">          cursor = i + <span class="number">1</span>;</span><br><span class="line">          <span class="keyword">return</span> (E) elementData[lastRet = i];</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">checkForComodification</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (modCount != expectedModCount)<span class="comment">//多线程环境下，modCount和expectedModCount不相等</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> ConcurrentModificationException();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>那么在多线程环境下是如何让<code>modCount</code> != <code>expectedModCount</code>的呢？</p><p>我们先看源码中是如何修改<code>modCount</code> 的值的(开始怼源码)</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="comment">// list中容量变化时，对应的同步函数</span></span><br><span class="line"><span class="keyword">transient</span> Object[] elementData; <span class="comment">// 保存了添加到ArrayList中的元素</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;<span class="comment">//默认是空元素</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_CAPACITY = <span class="number">10</span>;<span class="comment">//初始容量</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">ensureCapacity</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">int</span> minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA)</span><br><span class="line">           <span class="comment">//元素列表不等于默认列表</span></span><br><span class="line">           ? <span class="number">0</span></span><br><span class="line">           <span class="comment">//元素列表等于默认列表，返回默认初始值10</span></span><br><span class="line">           : DEFAULT_CAPACITY;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">if</span> (minCapacity &gt; minExpand) &#123;<span class="comment">//传入的容量大于最小扩展容量(minExpand)，则调用ensureExplicitCapacity()方法传入minCapacity</span></span><br><span class="line">           ensureExplicitCapacity(minCapacity);</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//判断元素列表是否为初始的元素列表，返回默认容量10和传入的容量中较大的值</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">calculateCapacity</span><span class="params">(Object[] elementData, <span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123;</span><br><span class="line">           <span class="keyword">return</span> Math.max(DEFAULT_CAPACITY, minCapacity);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> minCapacity;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//JDK1.8 中，add、addAll方法会先调用者方法判断是否需要扩容</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">ensureCapacityInternal</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//把元素列表和扩展的容量传入到calculateCapacity方法，做一个判断</span></span><br><span class="line">       ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//list中容量变化时，modCount+1</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">ensureExplicitCapacity</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">      </span><br><span class="line">       <span class="comment">// 如果minCapacity大于elementData的长度，则进行扩容</span></span><br><span class="line">       <span class="keyword">if</span> (minCapacity - elementData.length &gt; <span class="number">0</span>)</span><br><span class="line">           <span class="comment">//调用grow扩容</span></span><br><span class="line">           grow(minCapacity);</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MAX_ARRAY_SIZE = Integer.MAX_VALUE - <span class="number">8</span>;</span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">grow</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//旧容量</span></span><br><span class="line">       <span class="keyword">int</span> oldCapacity = elementData.length;</span><br><span class="line">       <span class="comment">//新容量=旧容量+旧容量的1/2 ==&gt;扩容1.5倍</span></span><br><span class="line">       <span class="keyword">int</span> newCapacity = oldCapacity + (oldCapacity &gt;&gt; <span class="number">1</span>);</span><br><span class="line">       <span class="comment">//如果计算出来的新容量比传进来的容量小，则以传入的容量为准</span></span><br><span class="line">       <span class="keyword">if</span> (newCapacity - minCapacity &lt; <span class="number">0</span>)</span><br><span class="line">           newCapacity = minCapacity;</span><br><span class="line">       <span class="comment">//如果新容量大于MAX_ARRAY_SIZE(Integer.MAX_VALUE - 8)，则把新容量交给hugeCapacity方法</span></span><br><span class="line">       <span class="keyword">if</span> (newCapacity - MAX_ARRAY_SIZE &gt; <span class="number">0</span>)</span><br><span class="line">           newCapacity = hugeCapacity(minCapacity);<span class="comment">//hugeCapacity实际上是做了一次内存溢出的判断，因为MAX_ARRAY_SIZE的容量已经接近溢出的边缘</span></span><br><span class="line">       <span class="comment">//Arrays.copyOf()方法放入elementData的元素，并把容量扩容为newCapacity</span></span><br><span class="line">       elementData = Arrays.copyOf(elementData, newCapacity);</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment">//主要检查内存是否溢出</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">hugeCapacity</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">       <span class="comment">// 内存溢出</span></span><br><span class="line">       <span class="keyword">if</span> (minCapacity &lt; <span class="number">0</span>) </span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> OutOfMemoryError();</span><br><span class="line">       <span class="keyword">return</span> (minCapacity &gt; MAX_ARRAY_SIZE) ?</span><br><span class="line">           Integer.MAX_VALUE :</span><br><span class="line">           MAX_ARRAY_SIZE;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line">   <span class="comment">// 添加元素到队列最后</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">add</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//判断是否需要扩容和modCount+1,以及判断内存溢出</span></span><br><span class="line">       ensureCapacityInternal(size + <span class="number">1</span>);  </span><br><span class="line">       <span class="comment">//将元素添加到数组末尾</span></span><br><span class="line">       elementData[size++] = e;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">   <span class="comment">// 添加元素到指定的位置</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> index, E element)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查index的范围</span></span><br><span class="line">       rangeCheckForAdd(index);</span><br><span class="line"><span class="comment">//判断是否需要扩容和modCount+1,以及判断内存溢出</span></span><br><span class="line">       ensureCapacityInternal(size + <span class="number">1</span>);  </span><br><span class="line">       <span class="comment">//把原数组的index位置移动到目标数组的index+1的位置,长度=数组长度-插入位置，这样就把index位置空出来了，涉及到内存操作，贼慢</span></span><br><span class="line">       System.arraycopy(elementData, index, elementData, index + <span class="number">1</span>,</span><br><span class="line">                        size - index);</span><br><span class="line">       <span class="comment">//把元素插入到数组中的index位置</span></span><br><span class="line">       elementData[index] = element;</span><br><span class="line">       size++;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//指定位置的范围检查，适用于add和addAll.</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">rangeCheckForAdd</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (index &gt; size || index &lt; <span class="number">0</span>)</span><br><span class="line">           <span class="comment">//throw new 下标越界异常</span></span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> IndexOutOfBoundsException(outOfBoundsMsg(index));</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line">   <span class="comment">// 添加集合</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">addAll</span><span class="params">(Collection&lt;? extends E&gt; c)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//把集合转换成一个Object类型的数组</span></span><br><span class="line">       Object[] a = c.toArray();</span><br><span class="line">       <span class="comment">//获集合的长度，用作扩容和复制</span></span><br><span class="line">       <span class="keyword">int</span> numNew = a.length;</span><br><span class="line">       <span class="comment">//判断是否需要扩容和modCount+1,以及判断内存溢出</span></span><br><span class="line">       ensureCapacityInternal(size + numNew);</span><br><span class="line">       <span class="comment">//把集合从0位置开始移动到目标数组的size位置,长度就等于集合的长度</span></span><br><span class="line">       System.arraycopy(a, <span class="number">0</span>, elementData, size, numNew);</span><br><span class="line">       <span class="comment">//计算size</span></span><br><span class="line">       size += numNew;</span><br><span class="line">       <span class="keyword">return</span> numNew != <span class="number">0</span>;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//类似于addAll，插入的位置变了而已</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">addAll</span><span class="params">(<span class="keyword">int</span> index, Collection&lt;? extends E&gt; c)</span> </span>&#123;&#125;</span><br><span class="line">  </span><br><span class="line">      </span><br><span class="line">   <span class="comment">// 删除指定位置的元素 </span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> E <span class="title">remove</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//检查范围</span></span><br><span class="line">       rangeCheck(index);</span><br><span class="line"><span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">       <span class="comment">//找出index位置的元素</span></span><br><span class="line">       E oldValue = elementData(index);</span><br><span class="line"><span class="comment">//计算数组从index到size的长度，-1是为了减去index的位置</span></span><br><span class="line">       <span class="keyword">int</span> numMoved = size - index - <span class="number">1</span>;</span><br><span class="line">       <span class="comment">//如果计算后的长度大于0，则使用System.arraycopy复制index后的元素向前移动一位，内存操作，贼慢</span></span><br><span class="line">       <span class="keyword">if</span> (numMoved &gt; <span class="number">0</span>)</span><br><span class="line">           System.arraycopy(elementData, index+<span class="number">1</span>, elementData, index,numMoved);</span><br><span class="line">        <span class="comment">//size--,并赋值为null</span></span><br><span class="line">       elementData[--size] = <span class="keyword">null</span>; <span class="comment">// clear to let GC do its work</span></span><br><span class="line"><span class="comment">//返回删除元素后的数组</span></span><br><span class="line">       <span class="keyword">return</span> oldValue;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//检查索引的范围</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">rangeCheck</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (index &gt;= size)</span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> IndexOutOfBoundsException(outOfBoundsMsg(index));</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">   <span class="comment">// 快速删除指定位置的元素,和remove(int index)类似，省去了范围校验</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">fastRemove</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">       <span class="keyword">int</span> numMoved = size - index - <span class="number">1</span>;</span><br><span class="line">       <span class="keyword">if</span> (numMoved &gt; <span class="number">0</span>)</span><br><span class="line">           System.arraycopy(elementData, index+<span class="number">1</span>, elementData, index,numMoved);</span><br><span class="line">       <span class="comment">//原来这里也会有GC回收</span></span><br><span class="line">       elementData[--size] = <span class="keyword">null</span>;</span><br><span class="line">   &#125;</span><br><span class="line">      </span><br><span class="line">   <span class="comment">// 清空集合</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       <span class="comment">//修改modCount</span></span><br><span class="line">       modCount++;</span><br><span class="line">      </span><br><span class="line">       <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++)</span><br><span class="line">           <span class="comment">//这里也会有GC回收</span></span><br><span class="line">           elementData[i] = <span class="keyword">null</span>;</span><br><span class="line">      </span><br><span class="line">       size = <span class="number">0</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   ...</span><br></pre></td></tr></table></figure><p>一遍源码读下来，发现无论是<code>add()</code>、<code>remove()</code>，还是<code>clear()</code>，只要涉及到修改集合中的元素个数时，都会改变<code>modCount</code>(全局)的值。</p><p>由此回到<code>next()</code>方法中我们发现当 ‘A’ 线程正在做迭代器遍历操作时，<code>modCount</code>赋值给了<code>expectedModCount</code>，每次调用<code>next()</code>方法都会做一次<code>modCount != expectedModCount</code>的校验，此时线程 ’B‘ 进来了调用了add方法修改了<code>modCount</code>的值，此时<code>modCount</code>变成了N+1，判断为false，抛出<code>ConcurrentModificationException</code>异常，产生fail-fast事件 。</p></li></ol></li><li><p><code>fail-fast</code>事件</p><p>当多个线程对同一个集合进行操作的时候，某线程访问集合的过程中，该集合的内容被其他线程所改变(即其它线程通过<code>add</code>、<code>remove</code>、<code>clear</code>等方法，改变了<code>modCount</code>的值)；这时，就会抛出<code>ConcurrentModificationException</code>异常。与此对应的安全失败，在后面再解析。</p></li><li><p>解决方法</p><p>经过源码解析<code>ConcurrentModificationException</code>异常是因为多个线程同时调用add()方法导致的，解决的办法有一下三种:</p><ol><li><p>使用<code>Vector()</code>替代<code>ArrayList()</code>，但是这种方法效率低下，因为<code>Vector()</code>的几乎所有方法都加上了<code>synchronized</code>修饰符，<code>synchronized</code>保证了在同一时刻最多只有一个线程访问该段代码，虽然jdk1.5引入了自旋锁、锁粗化、轻量级锁和偏向锁，但还是太重，效率很低。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;String&gt; list = <span class="keyword">new</span> Vector&lt;String&gt;());</span><br></pre></td></tr></table></figure></li><li><p>使用<code>Collections.synchronizedList()</code>包装<code>ArrayList</code>，但是这种方法实际上只是将原来非线程安全的<code>ArrayList</code>中的方法加上一个<code>synchronized</code>同步代码块 (哭了。。。)</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;String&gt; list = Collections.synchronizedList(<span class="keyword">new</span> ArrayList&lt;String&gt;());</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> Object mutex;     <span class="comment">// 对象锁</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">get</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line"><span class="keyword">synchronized</span> (mutex) &#123;<span class="comment">//同步代码块</span></span><br><span class="line">        <span class="keyword">return</span> list.get(index);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">set</span><span class="params">(<span class="keyword">int</span> index, E element)</span> </span>&#123;</span><br><span class="line"><span class="keyword">synchronized</span> (mutex) &#123;<span class="comment">//同步代码块</span></span><br><span class="line">        <span class="keyword">return</span> list.set(index, element);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure></li><li><p>第三种也是推介的一种方法就是使用java.util.concurrent包下的CopyOnWriteArrayList解决，俗称写时复制机制，是读写分离的一种实现，这种方法在后面将详细源码解析。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;String&gt; list = <span class="keyword">new</span> CopyOnWriteArrayList&lt;String&gt;();</span><br></pre></td></tr></table></figure></li></ol></li><li><p>优化建议</p><p>在多线程下如果有按数据索引访问元素的情形，采用<code>CopyOnWriteArrayList()</code>方法</p></li><li><p>ArrayList的常用API</p><table><thead><tr><th>方法</th><th>描述</th></tr></thead><tbody><tr><td>add(E e)</td><td>将指定的元素列表的结束。</td></tr><tr><td>addAll(Collection c)</td><td>追加指定集合的所有元素到这个列表的末尾，按他们的指定集合的迭代器返回。</td></tr><tr><td>clear()</td><td>从这个列表中移除所有的元素。</td></tr><tr><td>contains(Object o)</td><td>返回 <code>true</code>如果这个列表包含指定元素。</td></tr><tr><td>get(int index)</td><td>返回此列表中指定位置的元素。</td></tr><tr><td>iterator()</td><td>在这个列表中的元素上返回一个正确的顺序。</td></tr><tr><td>remove(int index)</td><td>移除此列表中指定位置的元素。</td></tr><tr><td>set(int index, E element)</td><td>用指定元素替换此列表中指定位置的元素。</td></tr><tr><td>size()</td><td>返回此列表中元素的数目</td></tr><tr><td>toArray()</td><td>返回一个数组，包含在这个列表中的所有元素在适当的顺序</td></tr></tbody></table></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lock可重入锁与函数式接口Runnable接口的lambda编程方式</title>
      <link href="/2019/01/01/java/1/"/>
      <url>/2019/01/01/java/1/</url>
      
        <content type="html"><![CDATA[<h2 id="多线程企业级Demo"><a href="#多线程企业级Demo" class="headerlink" title="多线程企业级Demo"></a>多线程企业级Demo</h2><ol><li><p>前提</p><p>所有的多线程开开发遵循一个规则:</p><p><code>在高内聚低耦合的前提下，线程--&gt;操作--&gt;资源类</code></p><p>在这个条件下我们写一个卖票的Demo，三个售票员卖出30张票</p></li><li><p>资源类</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//资源类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Ticket</span> </span>&#123;</span><br><span class="line">    <span class="comment">//创建一个可重入的lock锁</span></span><br><span class="line">    Lock lock = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line">    <span class="comment">//总30张票</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> number = <span class="number">30</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sale</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        lock.lock();<span class="comment">//上锁</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (number &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                System.out.println(Thread.currentThread().getName() + <span class="string">"\t卖出第"</span> + (number--) + <span class="string">"\t张票，还剩下"</span> + number);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            lock.unlock();<span class="comment">//解锁</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>线程</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">40</span>; i++) &#123;</span><br><span class="line">            <span class="comment">//待操作的代码</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;,<span class="string">"a"</span>).start();</span><br></pre></td></tr></table></figure><p><code>注意:</code> 我们查看Runnable()接口的源代码发现，这是一个函数式接口，下面提供源码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@FunctionalInterface</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所以我们可以使用lambda表达式简化操作</p></li><li><p>操作</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//new资源类</span></span><br><span class="line">Ticket ticket = <span class="keyword">new</span> Ticket();</span><br><span class="line"><span class="keyword">new</span> Thread(() -&gt; &#123; <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">40</span>; i++) ticket.sale();&#125;,<span class="string">"A卖票员"</span>).start();</span><br><span class="line"><span class="keyword">new</span> Thread(() -&gt; &#123; <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">40</span>; i++) ticket.sale();&#125;,<span class="string">"B卖票员"</span>).start();</span><br><span class="line"><span class="keyword">new</span> Thread(() -&gt; &#123; <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">40</span>; i++) ticket.sale();&#125;,<span class="string">"C卖票员"</span>).start();</span><br></pre></td></tr></table></figure></li><li><p>结果：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">A卖票员卖出第<span class="number">30</span>张票，还剩下<span class="number">29</span></span><br><span class="line">A卖票员卖出第<span class="number">29</span>张票，还剩下<span class="number">28</span></span><br><span class="line">A卖票员卖出第<span class="number">28</span>张票，还剩下<span class="number">27</span></span><br><span class="line">...</span><br><span class="line">A卖票员卖出第<span class="number">1</span>张票，还剩下<span class="number">0</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="Lambda表达式与函数式接口"><a href="#Lambda表达式与函数式接口" class="headerlink" title="Lambda表达式与函数式接口"></a>Lambda表达式与函数式接口</h2><p>在jdk1.8中，引入了函数式接口，函数式接口中只能声明一个抽象方法，lambda表达式可以直接使用这个接口</p><ol><li><p>定义函数式接口</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@FunctionalInterface</span><span class="comment">//显式指定 当接口中只有一条抽象方法时，默认是函数式接口</span></span><br><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">Foo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>使用lambda表达式实现</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="comment">//指定接口的实现</span></span><br><span class="line">    Foo foo = () -&gt; System.out.println(<span class="string">"hello FunctionInterface.."</span>);</span><br><span class="line">    foo.sayHello();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>还需要知道的是在jdk1.8中，引入了default方法修饰接口，并且可以在接口中声明static方法，但是必须实现方法，这里和我们印象中的java接口有点不同</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//@FunctionalInterface  //声明函数式接口 只有一条抽象方法时可以省略</span></span><br><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">Foo</span> </span>&#123;</span><br><span class="line">    <span class="comment">//声明抽象方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//声明默认方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">default</span> <span class="keyword">int</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span>  x+y;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//声明静态方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">dec</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> x-y;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义类，没有实现接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FuncitonInterfaceDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//Foo对象接收实现的接口</span></span><br><span class="line">        Foo foo = () -&gt; System.out.println(<span class="string">"hello FunctionInterface.."</span>);</span><br><span class="line">        foo.sayHello();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//Foo对象的引用调用add方法</span></span><br><span class="line">        System.out.println(foo.add(<span class="number">10</span>, <span class="number">5</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//接口名直接调用static修饰的dec方法</span></span><br><span class="line">        System.out.println(Foo.dec(<span class="number">10</span>, <span class="number">5</span>));</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume的Channel选择器&amp;Flume的Sink选择器&amp;Channel的两种类型</title>
      <link href="/2018/12/03/flume/3/"/>
      <url>/2018/12/03/flume/3/</url>
      
        <content type="html"><![CDATA[<h2 id="Flume的Channel选择器"><a href="#Flume的Channel选择器" class="headerlink" title="Flume的Channel选择器"></a>Flume的Channel选择器</h2><p>Flume的Channel选择器有<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#replicating-channel-selector-default" target="_blank" rel="noopener">Replicating Channel Selector (default)</a>和<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#multiplexing-channel-selector" target="_blank" rel="noopener">Multiplexing Channel Selector</a>，作用分别是复制和多路分发，默认用的复制</p><p>下面我们用两个案例分别实现Replicating Channel Selector和Multiplexing Channel Selector</p><h3 id="Replicating-Channel-Selector"><a href="#Replicating-Channel-Selector" class="headerlink" title="Replicating Channel Selector"></a>Replicating Channel Selector</h3><ol><li><p>需要实现的功能</p><p><img src="https://yerias.github.io/flume_img/ReplicatingChannel.png" alt="ReplicatingChannel"></p></li><li><p>代码实现</p></li></ol><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2#两个sink</span><br><span class="line">a1.channels = c1 c2 #两个channel</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat#nc输入</span><br><span class="line">a1.sources.r1.bind = 0.0.0.0</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k2.type = logger#第一个sink</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = hdfs#第二个sink</span><br><span class="line">a1.sinks.k1.hdfs.path = /flume/%y-%m-%d/%H/%M</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = replicating-</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">a1.sinks.k1.hdfs.fileType=DataStream</span><br><span class="line"></span><br><span class="line">a1.sources.r1.selector.type = replicating#选择器</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory#第一个memory</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type = memory#第二个memory</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1 c2//source连上两个channel</span><br><span class="line">a1.sinks.k1.channel = c1//channel1连上sink1</span><br><span class="line">a1.sinks.k2.channel = c2//channel2连上sink2</span><br></pre></td></tr></table></figure><ol start="3"><li>执行命令</li></ol><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/nc-replicating-logger_and_hdfs.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><ol start="4"><li><p>发送消息</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">telnet localhost 44444</span><br></pre></td></tr></table></figure><p>结果：成功</p></li></ol><h3 id="Multiplexing-Channel-Selector"><a href="#Multiplexing-Channel-Selector" class="headerlink" title="Multiplexing Channel Selector"></a>Multiplexing Channel Selector</h3><ol><li><p>需要实现的功能</p><p><img src="https://yerias.github.io/flume_img/multiplexingChannel.jpg" alt="multiplexingChannel"></p><p>数据从Source到Channel中间会经过一个<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#static-interceptor" target="_blank" rel="noopener">拦截器</a>，拦截器中的Key和Value参数被添加到了所有的Event上，在经过<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#multiplexing-channel-selector" target="_blank" rel="noopener">选择器</a>的时候，会根据拦截器中的Event所带的Value值的不同发送到不同的Sink</p></li><li><p>代码实现</p><p>nc-memory-arvo1.conf</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">a1.sources.r1.port = <span class="number">44441</span></span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun</span><br><span class="line">a1.sinks.k1.port = <span class="number">55555</span></span><br><span class="line"></span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = <span class="keyword">static</span></span><br><span class="line">a1.sources.r1.interceptors.i1.key = state</span><br><span class="line">a1.sources.r1.interceptors.i1.value = UA</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>nc-memory-arvo2.conf</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">a1.sources.r1.port = <span class="number">44442</span></span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun</span><br><span class="line">a1.sinks.k1.port = <span class="number">55555</span></span><br><span class="line"></span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = <span class="keyword">static</span></span><br><span class="line">a1.sources.r1.interceptors.i1.key = state</span><br><span class="line">a1.sources.r1.interceptors.i1.value = UB</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>vim nc-memory-arvo3.conf</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">a1.sources.r1.port = <span class="number">44443</span></span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun</span><br><span class="line">a1.sinks.k1.port = <span class="number">55555</span></span><br><span class="line"></span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = <span class="keyword">static</span></span><br><span class="line">a1.sources.r1.interceptors.i1.key = state</span><br><span class="line">a1.sources.r1.interceptors.i1.value = UC</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>vim avro-memory-multi.conf</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2 k3</span><br><span class="line">a1.channels = c1 c2 c3</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = <span class="number">55555</span></span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = logger</span><br><span class="line"></span><br><span class="line">a1.sinks.k3.type = hdfs</span><br><span class="line">a1.sinks.k3.hdfs.path = /flume/%y-%m-%d/%H/%M</span><br><span class="line">a1.sinks.k3.hdfs.filePrefix = multiplexing-</span><br><span class="line">a1.sinks.k3.hdfs.useLocalTimeStamp = <span class="keyword">true</span></span><br><span class="line">a1.sinks.k3.hdfs.fileType=DataStream</span><br><span class="line">a1.sinks.k3.hdfs.writeFormat=Text</span><br><span class="line"></span><br><span class="line">a1.sources.r1.selector.type = multiplexing</span><br><span class="line">a1.sources.r1.selector.header = state</span><br><span class="line">a1.sources.r1.selector.mapping.UA = c1</span><br><span class="line">a1.sources.r1.selector.mapping.UB = c2</span><br><span class="line">a1.sources.r1.selector.<span class="keyword">default</span> = c3</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c3.type = memory</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1 c2 c3</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br><span class="line">a1.sinks.k3.channel = c3</span><br></pre></td></tr></table></figure></li><li><p>执行命令</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/nc-memory-arvo1.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/nc-memory-arvo2.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/nc-memory-arvo3.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/avro-memory-multi.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li><li><p>发送消息</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">telnet aliyun <span class="number">44441</span></span><br><span class="line">telnet aliyun <span class="number">44442</span></span><br><span class="line">telnet aliyun <span class="number">44443</span></span><br></pre></td></tr></table></figure><p>结果：成功</p></li></ol><h2 id="Flume的Sink选择器"><a href="#Flume的Sink选择器" class="headerlink" title="Flume的Sink选择器"></a>Flume的Sink选择器</h2><p>Flume的Sink选择器常用的有两种：<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#failover-sink-processor" target="_blank" rel="noopener">Failover Sink Processor</a>和<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#load-balancing-sink-processor" target="_blank" rel="noopener">Load balancing Sink Processor</a></p><h3 id="Failover-Sink-Processor"><a href="#Failover-Sink-Processor" class="headerlink" title="Failover Sink Processor"></a>Failover Sink Processor</h3><p>Failover Sink Processor可以在Agent中的Sink端做一个类似于灾备的Sink组，官方文档中介绍Failover Sink Processor维护一个按优先级排序的Sink列表，确保只要有一个可用的Sink，就会处理Event。</p><p>Failover Sink Processor的工作方式是将多个Sink组成Sinks组，他们有一个优先级的顺序关系，优先级大的先被激活。如果Sink在发送Event时失败，则下一个具有最高优先级的Sink将会用于发送Event。例如，优先级为100的接收器在优先级为80的接收器之前被激活。如果没有指定优先级，则根据在配置中指定Sink的顺序确定优先级。</p><table><thead><tr><th align="left">Property Name</th><th align="left">Default</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left"><strong>sinks</strong></td><td align="left">–</td><td align="left">Space-separated list of sinks that are participating in the group</td></tr><tr><td align="left"><strong>processor.type</strong></td><td align="left"><code>default</code></td><td align="left">The component type name, needs to be <code>failover</code></td></tr><tr><td align="left"><strong>processor.priority.</strong></td><td align="left">–</td><td align="left">Priority value. <sinkName> must be one of the sink instances associated with the current sink group A higher priority value Sink gets activated earlier. A larger absolute value indicates higher priority</td></tr><tr><td align="left">processor.maxpenalty</td><td align="left">30000</td><td align="left">The maximum backoff period for the failed Sink (in millis)</td></tr></tbody></table><ol><li><p>需要实现的功能</p><p>​    ![Failover Sink Processor](<a href="https://yerias.github.io/flume_img/Failover">https://yerias.github.io/flume_img/Failover</a> Sink Processor.jpg)</p><p>Agent1发送Event，如果Sink组中的任意一个Sink接收Event失败，其他的Sink激活继续接收</p></li><li><p>代码实现</p><p>nc-memory-avro.conf</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinkgroups.g1.processor.type = failover</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k1 = 5</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k2 = 10</span><br><span class="line">a1.sinkgroups.g1.processor.maxpenalty = 10000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun</span><br><span class="line">a1.sinks.k1.port = 55551</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = aliyun</span><br><span class="line">a1.sinks.k2.port = 55552</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure><p>avro1-memory-logger.conf </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = 55551</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>avro2-memory-logger.conf </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = 55552</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>执行命令</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/failover/nc-memory-avro.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/failover/avro1-memory-logger.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/failover/avro2-memory-logger.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li><li><p>发送消息</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">telnet aliyun 44444</span><br></pre></td></tr></table></figure><p>结果：成功</p></li></ol><h3 id="Load-balancing-Sink-Processor"><a href="#Load-balancing-Sink-Processor" class="headerlink" title="Load balancing Sink Processor"></a>Load balancing Sink Processor</h3><p>Load balancing Sink Processor提供了在多个Sink 上实现负载均衡的能力。它维护一个活动Sink的索引列表，发送的Event必须分布在这个列表上。实现支持通过round_robin或random分配负载。默认为round_robin类型，但是可以通过配置覆盖。自定义选择机制是通过继承AbstractSinkSelector的自定义类来支持的。</p><p>调用时，选择器使用其配置的选择机制选择下一个Sink 调用它。对于round_robin和random，如果选择的Sink 不能传递Event，处理器将通过其配置的选择机制选择下一个可用的Sink 。这种方法不会将失败的Sink加入黑名单，而是继续乐观地尝试每个可用的Sink。如果所有的Sink调用都导致失败，则整个程序运行失败</p><p>如果启用了backoff，Sink处理器将把失败的Sink列入黑名单，超过给定的时间后删除他们。当超时结束时，如果Sink仍然没有响应，超时将以指数方式增加，以避免在没有响应的Sink上陷入长时间的等待。禁用此功能后，在循环中，所有失败的Sink负载将按行传递到下一个Sink，因此不是均匀的。</p><table><thead><tr><th align="left">Property Name</th><th align="left">Default</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left"><strong>processor.sinks</strong></td><td align="left">–</td><td align="left">Space-separated list of sinks that are participating in the group</td></tr><tr><td align="left"><strong>processor.type</strong></td><td align="left"><code>default</code></td><td align="left">The component type name, needs to be <code>load_balance</code></td></tr><tr><td align="left">processor.backoff</td><td align="left">false</td><td align="left">Should failed sinks be backed off exponentially.</td></tr><tr><td align="left">processor.selector</td><td align="left"><code>round_robin</code></td><td align="left">Selection mechanism. Must be either <code>round_robin</code>, <code>random</code> or FQCN of custom class that inherits from <code>AbstractSinkSelector</code></td></tr><tr><td align="left">processor.selector.maxTimeOut</td><td align="left">30000</td><td align="left">Used by backoff selectors to limit exponential backoff (in milliseconds)</td></tr></tbody></table><ol><li><p>需要实现的功能</p><p>![Load balancing Sink Processor](<a href="https://yerias.github.io/flume_img/flume_img/Load">https://yerias.github.io/flume_img/flume_img/Load</a> balancing Sink Processor.jpg)</p><p>Agent1发送Event，Sink组中的每个Sink根据配置的选择器机制选择发送到哪一个Sink</p></li><li><p>代码实现</p><p>nc-memory-avro.conf</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">a1.sinkgroups.g1.processor.backoff = true</span><br><span class="line">a1.sinkgroups.g1.processor.selector = random</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun</span><br><span class="line">a1.sinks.k1.port = 55551</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = aliyun</span><br><span class="line">a1.sinks.k2.port = 55552</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure><p>avro1-memory-logger.conf </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = 55551</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>avro2-memory-logger.conf</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = aliyun</span><br><span class="line">a1.sources.r1.port = 55552</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>执行命令</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/load_balancing/nc-memory-avro.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/load_balancing/avro1-memory-logger.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/load_balancing/avro2-memory-logger.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li><li><p>发送消息</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">telnet aliyun 44444</span><br></pre></td></tr></table></figure><p>结果：成功</p></li></ol><h2 id="Channel的两种类型"><a href="#Channel的两种类型" class="headerlink" title="Channel的两种类型"></a>Channel的两种类型</h2><p>Channel有File和Memory两种类型，Memory的特点是使用内存，速度快，但是安全性没有保障；File的特点是数据都会写进文件，速度慢，但是安全性高。</p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>案例&amp;Flume单源单出口&amp;Flume单源多出口&amp;Flume多源单出口</title>
      <link href="/2018/12/02/flume/2/"/>
      <url>/2018/12/02/flume/2/</url>
      
        <content type="html"><![CDATA[<h2 id="安装地址"><a href="#安装地址" class="headerlink" title="安装地址"></a>安装地址</h2><ol><li><p>Flume官网地址</p><p><a href="http://flume.apache.org/" target="_blank" rel="noopener">http://flume.apache.org/</a></p></li><li><p>文档查看地址</p><p><a href="http://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="noopener">http://flume.apache.org/FlumeUserGuide.html</a></p></li><li><p>下载地址</p><p><a href="http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.16.2-src.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.16.2-src.tar.gz</a></p></li></ol><h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><ol><li><p>将apache-flume-1.7.0-bin.tar.gz上传到linux的/opt/software目录下</p></li><li><p>解压apache-flume-1.7.0-bin.tar.gz到/opt/module/目录下</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun software]$ tar -zxf apache-flume-1.7.0-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li><li><p>修改apache-flume-1.7.0-bin的名称为flume</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun module]$ mv apache-flume-1.7.0-bin flume</span><br></pre></td></tr></table></figure></li><li><p>将flume/conf下的flume-env.sh.template文件修改为flume-env.sh，并配置flume-env.sh文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun conf]$ mv flume-env.sh.template flume-env.sh</span><br><span class="line">[aliyun@aliyun conf]$ vi flume-env.sh</span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure></li></ol><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><ol><li><p>安装netcat工具</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun software]$ sudo yum install -y nc</span><br></pre></td></tr></table></figure></li><li><p>判断44444端口是否被占用</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume-telnet]$ sudo netstat -tunlp | grep 44444</span><br></pre></td></tr></table></figure></li><li><p>创建Flume Agent配置文件flume-netcat-logger.conf</p><p>在flume目录下创建job文件夹并进入job文件夹。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ mkdir job</span><br><span class="line">[aliyun@aliyun flume]$ cd job/</span><br></pre></td></tr></table></figure></li><li><p>在job文件夹下创建Flume Agent配置文件flume-netcat-logger.conf。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ vim flume-netcat-logger.conf</span><br></pre></td></tr></table></figure></li><li><p>添加内容如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent    #a1表示agent的名称</span><br><span class="line">a1.sources = r1#r1表示a1的输入源</span><br><span class="line">a1.sinks = k1#k1表示a1的输出目的地</span><br><span class="line">a1.channels = c1#c1表示a1的缓冲区</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat#表示a1的输入源类型为netcat端口类型</span><br><span class="line">a1.sources.r1.bind = localhost#表示a1的监听的主机</span><br><span class="line">a1.sources.r1.port = 44444#表示a1的监听的端口号</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger#表示a1的输出目的地是控制台logger类型</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory#表示a1的channel类型是memory内存型</span><br><span class="line">a1.channels.c1.capacity = 1000#表示a1的channel总容量1000个event</span><br><span class="line">a1.channels.c1.transactionCapacity = 10#表示a1的channel传输时收集到了100条event以后再去提交事务</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1#将r1和c1连接起来</span><br><span class="line">a1.sinks.k1.channel = c1#将k1和c1连接起来</span><br></pre></td></tr></table></figure></li><li><p>先开启flume监听端口</p><p>第一种写法：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>第二种写法：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ bin/flume-ng agent -c conf/ -n a1 –f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>参数说明：</p><p>​        <code>--conf conf/：</code>表示配置文件存储在conf/目录</p><p>​        <code>--name a1：</code>表示给agent起名为a1</p><pre><code>`--conf-file job/flume-netcat.conf ：`flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。</code></pre><p>​        <code>--Dflume.root.logger==INFO,console ：</code>-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error。</p></li><li><p>使用netcat工具向本机的44444端口发送内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun ~]$ nc localhost 44444</span><br><span class="line">hello </span><br><span class="line">aliyun</span><br></pre></td></tr></table></figure></li><li><p>在Flume监听页面观察接收数据情况</p></li></ol><h2 id="实时读取本地文件到HDFS案例"><a href="#实时读取本地文件到HDFS案例" class="headerlink" title="实时读取本地文件到HDFS案例"></a>实时读取本地文件到HDFS案例</h2><p>案例需求：实时监控Hive日志，并上传到HDFS中</p><p><img src="https://yerias.github.io/flume_img/%E5%8D%95%E6%BA%90%E5%8D%95%E5%87%BA%E5%8F%A31.jpg" alt="单源单出口1"></p><ol><li><p>给Flume的lib目录下添加aliyun相关的jar包</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">commons-configuration-1.6.jar</span><br><span class="line">aliyun-auth-2.7.2.jar</span><br><span class="line">aliyun-common-2.7.2.jar</span><br><span class="line">aliyun-hdfs-2.7.2.jar</span><br><span class="line">commons-io-2.4.jar</span><br><span class="line">htrace-core-3.1.0-incubating.jar</span><br></pre></td></tr></table></figure></li><li><p>创建flume-file-hdfs.conf文件</p><p>创建文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ touch flume-file-hdfs.conf</span><br></pre></td></tr></table></figure><p>注：要想读取Linux系统中的文件，就得按照Linux命令的规则执行命令。由于Hive日志在Linux系统中所以读取文件的类型选择：exec即execute执行的意思。表示执行Linux命令来读取文件。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ vim flume-file-hdfs.conf</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources = r2#定义source</span><br><span class="line">a2.sinks = k2#定义sink</span><br><span class="line">a2.channels = c2#定义channels</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a2.sources.r2.type = exec#定义source类型为exec可执行命令的</span><br><span class="line">a2.sources.r2.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line">a2.sources.r2.shell = /bin/bash -c#执行shell脚本的绝对路径</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k2.type = hdfs</span><br><span class="line">a2.sinks.k2.hdfs.path = hdfs://aliyun:9000/flume/%Y%m%d/%H</span><br><span class="line">a2.sinks.k2.hdfs.filePrefix = logs-#上传文件的前缀</span><br><span class="line">a2.sinks.k2.hdfs.round = true#是否按照时间滚动文件夹</span><br><span class="line">a2.sinks.k2.hdfs.roundValue = 1#多少时间单位创建一个新的文件夹</span><br><span class="line">a2.sinks.k2.hdfs.roundUnit = hour#重新定义时间单位</span><br><span class="line">a2.sinks.k2.hdfs.useLocalTimeStamp = true#是否使用本地时间戳</span><br><span class="line">a2.sinks.k2.hdfs.batchSize = 1000#积攒多少个Event才flush到HDFS一次</span><br><span class="line">a2.sinks.k2.hdfs.fileType = DataStream#设置文件类型，可支持压缩</span><br><span class="line">a2.sinks.k2.hdfs.rollInterval = 60#多久生成一个新的文件</span><br><span class="line">a2.sinks.k2.hdfs.rollSize = 134217700#设置每个文件的滚动大小</span><br><span class="line">a2.sinks.k2.hdfs.rollCount = 0#文件的滚动与Event数量无关</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a2.channels.c2.type = memory</span><br><span class="line">a2.channels.c2.capacity = 1000</span><br><span class="line">a2.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r2.channels = c2</span><br><span class="line">a2.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：</p><p>对于所有与时间相关的转义序列，Event Header中必须存在以 “timestamp”的key（除非hdfs.useLocalTimeStamp设置为true，此方法会使用TimestampInterceptor自动添加timestamp）。</p></li><li><p>执行监控配置</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/flume-file-hdfs.conf</span><br></pre></td></tr></table></figure></li><li><p>开启aliyun和Hive并操作Hive产生日志</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun aliyun-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[aliyun@aliyun103 aliyun-2.7.2]$ sbin/start-yarn.sh</span><br><span class="line"></span><br><span class="line">[aliyun@aliyun hive]$ bin/hive</span><br><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure></li><li><p>在HDFS上查看文件。</p></li></ol><h2 id="实时读取目录文件到HDFS案例"><a href="#实时读取目录文件到HDFS案例" class="headerlink" title="实时读取目录文件到HDFS案例"></a>实时读取目录文件到HDFS案例</h2><p>案例需求：使用Flume监听整个目录的文件</p><p><img src="https://yerias.github.io/flume_img/%E5%8D%95%E6%BA%90%E5%8D%95%E5%87%BA%E5%8F%A32.jpg" alt="单源单出口2"></p><ol><li><p>创建配置文件flume-dir-hdfs.conf</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ touch flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure></li><li><p>打开文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ vim flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure></li><li><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a3.sources = r3#定义sources</span><br><span class="line">a3.sinks = k3#定义sink</span><br><span class="line">a3.channels = c3#定义channel</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a3.sources.r3.type = spooldir#定义souce类型为目录</span><br><span class="line">a3.sources.r3.spoolDir = /opt/module/flume/upload#定义监控目录</span><br><span class="line">a3.sources.r3.fileSuffix = .COMPLETED#定义文件上传完的后缀</span><br><span class="line">a3.sources.r3.fileHeader = true#是否有文件头</span><br><span class="line">a3.sources.r3.ignorePattern = ([^ ]*\.tmp)#忽略所有以.tmp结尾的文件，不上传</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a3.sinks.k3.type = hdfs#sink类型为hdfs</span><br><span class="line">a3.sinks.k3.hdfs.pat=hdfs://aliyun:9000/flume/upload/%Y%m%d/%H  #文件上传到hdfs的路径</span><br><span class="line"></span><br><span class="line">a3.sinks.k3.hdfs.filePrefix = upload-#上传文件到hdfs的前缀</span><br><span class="line">a3.sinks.k3.hdfs.round = true#是否按照时间滚动文件夹</span><br><span class="line">a3.sinks.k3.hdfs.roundValue = 1#多少时间单位创建一个新的文件夹</span><br><span class="line">a3.sinks.k3.hdfs.roundUnit = hour#重新定义时间单位</span><br><span class="line">a3.sinks.k3.hdfs.useLocalTimeStamp = true#是否使用本地时间戳</span><br><span class="line">a3.sinks.k3.hdfs.batchSize = 100#积攒多少个Event才flush到HDFS一次</span><br><span class="line">a3.sinks.k3.hdfs.fileType = DataStream#设置文件类型，可支持压缩</span><br><span class="line">a3.sinks.k3.hdfs.rollInterval = 60#多久生成一个新的文件</span><br><span class="line">a3.sinks.k3.hdfs.rollSize = 134217700#设置每个文件的滚动大小大概是128M</span><br><span class="line">a3.sinks.k3.hdfs.rollCount = 0#文件的滚动与Event数量无关</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a3.channels.c3.type = memory</span><br><span class="line">a3.channels.c3.capacity = 1000</span><br><span class="line">a3.channels.c3.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a3.sources.r3.channels = c3</span><br><span class="line">a3.sinks.k3.channel = c3</span><br></pre></td></tr></table></figure></li><li><p>启动监控文件夹命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure><p>说明： 在使用Spooling Directory Source时</p><ol><li>不要在监控目录中创建并持续修改文件</li><li>上传完成的文件会以.COMPLETED结尾</li><li>被监控文件夹每500毫秒扫描一次文件变动</li></ol></li><li><p>向upload文件夹中添加文件</p><p>在/opt/module/flume目录下创建upload文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ mkdir upload</span><br></pre></td></tr></table></figure><p>向upload文件夹中添加文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun upload]$ touch aliyun.txt</span><br><span class="line">[aliyun@aliyun upload]$ touch aliyun.tmp</span><br><span class="line">[aliyun@aliyun upload]$ touch aliyun.log</span><br></pre></td></tr></table></figure></li><li><p>查看HDFS上的数据</p></li><li><p>等待1s，再次查询upload文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun upload]$ ll</span><br><span class="line">总用量 0</span><br><span class="line">-rw-rw-r--. 1 aliyun aliyun 0 5月  20 22:31 aliyun.log.COMPLETED</span><br><span class="line">-rw-rw-r--. 1 aliyun aliyun 0 5月  20 22:31 aliyun.tmp</span><br><span class="line">-rw-rw-r--. 1 aliyun aliyun 0 5月  20 22:31 aliyun.txt.COMPLETED</span><br></pre></td></tr></table></figure></li></ol><h2 id="单数据源多出口案例-选择器"><a href="#单数据源多出口案例-选择器" class="headerlink" title="单数据源多出口案例(选择器)"></a>单数据源多出口案例(选择器)</h2><p>案例需求：使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。同时Flume-1将变动内容传递给Flume-3，Flume-3负责输出到Local FileSystem。</p><p><img src="https://yerias.github.io/flume_img/%E5%8D%95%E6%BA%90%E5%A4%9A%E5%87%BA%E5%8F%A31.jpg" alt="单源多出口1"></p><ol><li><p>准备工作</p><p>在/opt/module/flume/job目录下创建group1文件夹</p><p><code>[hadoop@aliyun102 job]$ cd group1/</code></p><p>在/opt/module/datas/目录下创建flume3文件夹</p><p><code>[hadoop@aliyun102 datas]$ mkdir flume3</code></p></li><li><p>创建flume-file-flume.conf</p><p>配置1个接收日志文件的source和两个channel、两个sink，分别输送给flume-flume-hdfs和flume-flume-dir。</p><p>创建配置文件并打开</p><p><code>[hadoop@aliyun102 group1]$ touch flume-file-flume.conf</code></p><p><code>[hadoop@aliyun102 group1]$ vim flume-file-flume.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"></span><br><span class="line"># 将数据流复制给所有channel</span><br><span class="line">a1.sources.r1.selector.type = replicating</span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"> </span><br><span class="line"># Describe the sink</span><br><span class="line"># sink端的avro是一个数据发送者</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun102 </span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = aliyun102</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure><p>注：Avro是由aliyun创始人Doug Cutting创建的一种语言无关的数据序列化和RPC框架。</p><p>注：RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。</p></li><li><p>创建flume-flume-hdfs.conf</p><p>配置上级Flume输出的Source，输出是到HDFS的Sink。</p><p>创建配置文件并打开</p><p><code>[hadoop@aliyun102 group1]$ touch flume-flume-hdfs.conf</code></p><p><code>[hadoop@aliyun102 group1]$ vim flume-flume-hdfs.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line"># source端的avro是一个数据接收服务</span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = aliyun102</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k1.type = hdfs</span><br><span class="line">a2.sinks.k1.hdfs.path = hdfs://aliyun102:9000/flume2/%Y%m%d/%H</span><br><span class="line">#上传文件的前缀</span><br><span class="line">a2.sinks.k1.hdfs.filePrefix = flume2-</span><br><span class="line">#是否按照时间滚动文件夹</span><br><span class="line">a2.sinks.k1.hdfs.round = true</span><br><span class="line">#多少时间单位创建一个新的文件夹</span><br><span class="line">a2.sinks.k1.hdfs.roundValue = 1</span><br><span class="line">#重新定义时间单位</span><br><span class="line">a2.sinks.k1.hdfs.roundUnit = hour</span><br><span class="line">#是否使用本地时间戳</span><br><span class="line">a2.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">#积攒多少个Event才flush到HDFS一次</span><br><span class="line">a2.sinks.k1.hdfs.batchSize = 100</span><br><span class="line">#设置文件类型，可支持压缩</span><br><span class="line">a2.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">#多久生成一个新的文件</span><br><span class="line">a2.sinks.k1.hdfs.rollInterval = 600</span><br><span class="line">#设置每个文件的滚动大小大概是128M</span><br><span class="line">a2.sinks.k1.hdfs.rollSize = 134217700</span><br><span class="line">#文件的滚动与Event数量无关</span><br><span class="line">a2.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>创建flume-flume-dir.conf</p><p>配置上级Flume输出的Source，输出是到本地目录的Sink。</p><p>创建配置文件并打开</p><p><code>[hadoop@aliyun102 group1]$ touch flume-flume-dir.conf</code></p><p><code>[hadoop@aliyun102 group1]$ vim flume-flume-dir.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = aliyun102</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a3.sinks.k1.type = file_roll</span><br><span class="line">a3.sinks.k1.sink.directory = /opt/module/data/flume3</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure><p>提示：输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录。</p></li><li><p>执行配置文件</p><p>分别开启对应配置文件：flume-flume-dir，flume-flume-hdfs，flume-file-flume。</p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group1/flume-flume-dir.conf</code></p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group1/flume-flume-hdfs.conf</code></p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group1/flume-file-flume.conf</code></p></li><li><p>启动aliyun和Hive</p><p><code>[hadoop@aliyun102 aliyun-2.7.2]$ sbin/start-dfs.sh</code></p><p><code>[hadoop@aliyun103 aliyun-2.7.2]$ sbin/start-yarn.sh</code></p><p><code>[hadoop@aliyun102 hive]$ bin/hive</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure></li><li><p>检查HDFS上数据</p></li><li><p>检查/opt/module/datas/flume3目录中数据</p><p><code>[hadoop@aliyun102 flume3]$ ll</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">总用量 8</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 5942 5月 22 00:09 1526918887550-3</span><br></pre></td></tr></table></figure></li></ol><h2 id="单数据源多出口案例-Sink组"><a href="#单数据源多出口案例-Sink组" class="headerlink" title="单数据源多出口案例(Sink组)"></a>单数据源多出口案例(Sink组)</h2><p>案例需求：使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。同时Flume-1将变动内容传递给Flume-3，Flume-3也负责存储到HDFS </p><p><img src="https://yerias.github.io/flume_img/%E5%8D%95%E6%BA%90%E5%A4%9A%E5%87%BA%E5%8F%A32.jpg" alt="单源多出口2"></p><ol><li><p>准备工作</p><p>在/opt/module/flume/job目录下创建group2文件夹</p><p><code>[hadoop@aliyun102 job]$ cd group2/</code></p></li><li><p>创建flume-netcat-flume.conf</p><p>配置1个接收日志文件的source和1个channel、两个sink，分别输送给flume-flume-console1和flume-flume-console2。</p><p>创建配置文件并打开</p><p><code>[hadoop@aliyun102 group2]$ touch flume-netcat-flume.conf</code></p><p><code>[hadoop@aliyun102 group2]$ vim flume-netcat-flume.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">a1.sinkgroups.g1.processor.backoff = true</span><br><span class="line">a1.sinkgroups.g1.processor.selector = round_robin</span><br><span class="line">a1.sinkgroups.g1.processor.selector.maxTimeOut=10000</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun102</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = aliyun102</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure><p>注：Avro是由aliyun创始人Doug Cutting创建的一种语言无关的数据序列化和RPC框架。</p><p>注：RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。</p></li><li><p>创建flume-flume-console1.conf</p><p>配置上级Flume输出的Source，输出是到本地控制台。</p><p>创建配置文件并打开</p><p><code>[hadoop@aliyun102 group2]$ touch flume-flume-console1.conf</code></p><p><code>[hadoop@aliyun102 group2]$ vim flume-flume-console1.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = aliyun102</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>创建flume-flume-console2.conf</p><p>配置上级Flume输出的Source，输出是到本地控制台。</p><p>创建配置文件并打开</p><p><code>[hadoop@aliyun102 group2]$ touch flume-flume-console2.conf</code></p><p><code>[hadoop@aliyun102 group2]$ vim flume-flume-console2.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = aliyun102</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure></li><li><p>执行配置文件</p><p>分别开启对应配置文件：flume-flume-console2，flume-flume-console1，flume-netcat-flume。</p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group2/flume-flume-console2.conf -Dflume.root.logger=INFO,console</code></p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group2/flume-flume-console1.conf -Dflume.root.logger=INFO,console</code></p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group2/flume-netcat-flume.conf</code></p></li><li><p>使用netcat工具向本机的44444端口发送内容</p><p><code>$ nc localhost 44444</code></p></li><li><p>查看Flume2及Flume3的控制台打印</p></li></ol><h2 id="多数据源汇总案例"><a href="#多数据源汇总案例" class="headerlink" title="多数据源汇总案例"></a>多数据源汇总案例</h2><p>案例需求：</p><p>aliyun103上的Flume-1监控文件/opt/module/group.log，</p><p>aliyun102上的Flume-2监控某一个端口的数据流，</p><p>Flume-1与Flume-2将数据发送给aliyun104上的Flume-3，Flume-3将最终数据打印到控制台。</p><p><img src="https://yerias.github.io/flume_img/%E5%A4%9A%E6%BA%90%E5%8D%95%E5%87%BA%E5%8F%A3.jpg" alt="多源单出口"></p><ol><li><p>准备工作</p><p>分发Flume</p><p><code>[hadoop@aliyun102 module]$ xsync flume</code></p><p>在aliyun102、aliyun103以及aliyun104的/opt/module/flume/job目录下创建一个group3文件夹。</p><p><code>[hadoop@aliyun102 job]$ mkdir group3</code></p><p><code>[hadoop@aliyun103 job]$ mkdir group3</code></p><p><code>[hadoop@aliyun104 job]$ mkdir group3</code></p></li><li><p>创建flume1-logger-flume.conf</p><p>配置Source用于监控hive.log文件，配置Sink输出数据到下一级Flume。</p><p>在aliyun103上创建配置文件并打开</p><p><code>[hadoop@aliyun103 group3]$ touch flume1-logger-flume.conf</code></p><p><code>[hadoop@aliyun103 group3]$ vim flume1-logger-flume.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/group.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun104</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>创建flume2-netcat-flume.conf</p><p>配置Source监控端口44444数据流，配置Sink数据到下一级Flume：</p><p>在aliyun102上创建配置文件并打开</p><p><code>[hadoop@aliyun102 group3]$ touch flume2-netcat-flume.conf</code></p><p><code>[hadoop@aliyun102 group3]$ vim flume2-netcat-flume.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a2.sources.r1.type = netcat</span><br><span class="line">a2.sources.r1.bind = aliyun102</span><br><span class="line">a2.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k1.type = avro</span><br><span class="line">a2.sinks.k1.hostname = aliyun104</span><br><span class="line">a2.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>创建flume3-flume-logger.conf</p><p>配置source用于接收flume1与flume2发送过来的数据流，最终合并后sink到控制台。</p><p>在aliyun104上创建配置文件并打开</p><p><code>[hadoop@aliyun104 group3]$ touch flume3-flume-logger.conf</code></p><p><code>[hadoop@aliyun104 group3]$ vim flume3-flume-logger.conf</code></p><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = aliyun104</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line"># Describe the sink</span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>执行配置文件</p><p>分别开启对应配置文件：flume3-flume-logger.conf，flume2-netcat-flume.conf，flume1-logger-flume.conf。</p><p><code>[hadoop@aliyun104 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group3/flume3-flume-logger.conf -Dflume.root.logger=INFO,console</code></p><p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group3/flume2-netcat-flume.conf</code></p><p><code>[hadoop@aliyun103 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group3/flume1-logger-flume.conf</code></p></li><li><p>在aliyun103上向/opt/module目录下的group.log追加内容</p><p><code>[hadoop@aliyun103 module]$ echo &#39;hello&#39; &gt; group.log</code></p></li><li><p>在aliyun102上向44444端口发送数据</p><p><code>[hadoop@aliyun102 flume]$ telnet aliyun102 44444</code></p></li><li><p>检查aliyun104上数据</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume架构摸排</title>
      <link href="/2018/12/01/flume/1/"/>
      <url>/2018/12/01/flume/1/</url>
      
        <content type="html"><![CDATA[<h2 id="Flume定义"><a href="#Flume定义" class="headerlink" title="Flume定义"></a>Flume定义</h2><p>Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单。</p><h2 id="Flume的优点"><a href="#Flume的优点" class="headerlink" title="Flume的优点"></a>Flume的优点</h2><ol><li><p>可以和任意存储进程集成。</p></li><li><p>输入的的数据速率大于写入目的存储的速率，flume会进行缓冲，减小hdfs的压力。</p></li><li><p>flume中的事务基于channel，使用了两个事务模型（sender + receiver），确保消息被可靠发送。</p></li></ol><p>Flume使用两个独立的事务分别负责从soucrce到channel，以及从channel到sink的事件传递。一旦事务中所有的数据全部成功提交到channel，那么source才认为该数据读取完成。同理，只有成功被sink写出去的数据，才会从channel中移除。</p><h2 id="Flume组成架构"><a href="#Flume组成架构" class="headerlink" title="Flume组成架构"></a>Flume组成架构</h2><p><img src="https://yerias.github.io/flume_img/flume.jpg" alt="flume架构图"></p><ul><li><p>Agent</p><p>Agent是一个JVM进程，它以事件的形式将数据从源头送至目的。</p><p>Agent主要有3个部分组成，Source、Channel、Sink。</p></li><li><p>Source</p><p>Source是负责接收数据到Flume Agent的组件。</p><p>Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。</p></li><li><p>Channel</p><p>Channel是位于Source和Sink之间的缓冲区。因此，Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。</p><p>Flume自带两种Channel：Memory Channel和File Channel。</p><ol><li><p>Memory Channel是内存中的队列。Memory Channel在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么Memory Channel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。</p></li><li><p>File Channel将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。</p></li></ol></li><li><p>Sink</p><p>Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。</p><p>Sink是完全事务性的。在从Channel批量删除数据之前，每个Sink用Channel启动一个事务。批量事件一旦成功写出到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。</p><p>Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。</p></li><li><p>Event</p><p>传输单元，Flume数据传输的基本单元，以事件的形式将数据从源头送至目的地。 Event由可选的header和载有数据的一个byte array 构成。Header是容纳了key-value字符串对的HashMap。</p></li></ul><h2 id="Flume拓扑结构"><a href="#Flume拓扑结构" class="headerlink" title="Flume拓扑结构"></a>Flume拓扑结构</h2><h3 id="Flume-Agent连接"><a href="#Flume-Agent连接" class="headerlink" title="Flume Agent连接"></a>Flume Agent连接</h3><p><img src="https://yerias.github.io/flume_img/flume2.jpg" alt="flume2"></p><p>这种模式是将多个flume给顺序连接起来了，从最初的source开始到最终sink传送的目的存储系统。此模式不建议桥接过多的flume数量， flume数量过多不仅会影响传输速率，而且一旦传输过程中某个节点flume宕机，会影响整个传输系统。</p><h3 id="单source，多channel、sink"><a href="#单source，多channel、sink" class="headerlink" title="单source，多channel、sink"></a>单source，多channel、sink</h3><p><img src="https://yerias.github.io/flume_img/flume3.jpg" alt="flume3"></p><p>Flume支持将事件流向一个或者多个目的地。这种模式将数据源复制到多个channel中，每个channel都有相同的数据，sink可以选择传送的不同的目的地。</p><h3 id="Flume负载均衡"><a href="#Flume负载均衡" class="headerlink" title="Flume负载均衡"></a>Flume负载均衡</h3><p><img src="https://yerias.github.io/flume_img/flume4.jpg" alt="flume4"></p><p>Flume支持使用将多个sink逻辑上分到一个sink组，flume将数据发送到不同的sink，主要解决负载均衡和故障转移问题。</p><h3 id="Flume-Agent聚合"><a href="#Flume-Agent聚合" class="headerlink" title="Flume Agent聚合"></a>Flume Agent聚合</h3><p><img src="https://yerias.github.io/flume_img/flume5.jpg" alt="flume5"></p><p>这种模式是我们最常见的，也非常实用，日常web应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用flume的这种组合方式能很好的解决这一问题，每台服务器部署一个flume采集日志，传送到一个集中收集日志的flume，再由此flume上传到hdfs、hive、hbase、jms等，进行日志分析。</p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SQOOP安装&amp;RDBMS导入HDFS&amp;RDBMS导入HIVE&amp;HDFS导入RDBMS&amp;HIVE导入RDBMS&amp;SQOOP的ETL案例&amp;在SHELL中操作MYSQL</title>
      <link href="/2018/12/01/sqoop/1/"/>
      <url>/2018/12/01/sqoop/1/</url>
      
        <content type="html"><![CDATA[<p>首先抛出两个场景</p><ol><li>数据数据在RDBMS中，你想使用Hive进行处理，怎么做</li><li>使用Hive统计分析好了，数据还在Hive中，如何导到RDBMS中</li></ol><h2 id="Sqoop安装"><a href="#Sqoop安装" class="headerlink" title="Sqoop安装"></a>Sqoop安装</h2><ol><li><p>下载并解压</p><p>下载地址：<a href="http://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.16.2.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.16.2.tar.gz</a></p><p>上传安装包sqoop-1.4.6-cdh5.16.2.tar.gz到主机中</p><p>解压sqoop安装包到指定目录，如：$ tar -zxf sqoop-1.4.6-cdh5.16.2.tar.gz -C /opt/module/</p></li><li><p>修改配置文件</p><p>重命名配置文件</p><p><code>$ mv sqoop-env-template.sh sqoop-env.sh</code></p><p>修改配置文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HADOOP_COMMON_HOME=/opt/module/hadoop</span><br><span class="line">export HADOOP_MAPRED_HOME=/opt/module/hadoop</span><br><span class="line">export HIVE_HOME=/opt/module/hive</span><br><span class="line">export ZOOKEEPER_HOME=/opt/module/zookeeper</span><br><span class="line">export ZOOCFGDIR=/opt/module/zookeeper</span><br><span class="line">export HBASE_HOME=/opt/module/hbase</span><br></pre></td></tr></table></figure></li><li><p>拷贝JDBC驱动</p><p><code>$ cp mysql-connector-java-5.1.27-bin.jar /opt/module/sqoop/lib/</code></p></li><li><p>验证Sqoop</p><p><code>$ bin/sqoop help</code></p><p>出现一些Warning警告（警告信息已省略），并伴随着帮助命令的输出：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Available commands:</span><br><span class="line">  codegen            Generate code to interact with database records</span><br><span class="line">  create-hive-table  Import a table definition into Hive</span><br><span class="line">  eval               Evaluate a SQL statement and display the results</span><br><span class="line">  export             Export an HDFS directory to a database table</span><br><span class="line">  help               List available commands</span><br><span class="line">  import             Import a table from a database to HDFS</span><br><span class="line">  import-all-tables  Import tables from a database to HDFS</span><br><span class="line">  import-mainframe   Import datasets from a mainframe server to HDFS</span><br><span class="line">  job                Work with saved jobs</span><br><span class="line">  list-databases     List available databases on a server</span><br><span class="line">  list-tables        List available tables in a database</span><br><span class="line">  merge              Merge results of incremental imports</span><br><span class="line">  metastore          Run a standalone Sqoop metastore</span><br><span class="line">  version            Display version information</span><br></pre></td></tr></table></figure></li><li><p>测试Sqoop是否能够成功连接数据库</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop \</span><br><span class="line">list-databases \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/ \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root</span></span><br></pre></td></tr></table></figure><p>数据库用到的参数:</p><ul><li>“\“： 代表换行</li><li>“connect”：连接数据库</li><li>username：用户</li><li>password：密码</li></ul><p>显示所有的数据库列表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">information_schema</span><br><span class="line">leyou1</span><br><span class="line">metastore</span><br><span class="line">mypython</span><br><span class="line">mysql</span><br><span class="line">performance_schema</span><br><span class="line">sys</span><br><span class="line">travel</span><br><span class="line">tunan</span><br></pre></td></tr></table></figure></li></ol><h2 id="RDBMS到HDFS"><a href="#RDBMS到HDFS" class="headerlink" title="RDBMS到HDFS"></a>RDBMS到HDFS</h2><h3 id="MySQL准备表和数据"><a href="#MySQL准备表和数据" class="headerlink" title="MySQL准备表和数据"></a>MySQL准备表和数据</h3><ol><li><p>确定mysql服务开启正常</p></li><li><p>在mysql中创建库表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> company;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> company.staff(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">4</span>) primary <span class="keyword">key</span> <span class="keyword">not</span> <span class="literal">null</span> auto_increment, </span><br><span class="line">    <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">255</span>), </span><br><span class="line">    sex <span class="built_in">varchar</span>(<span class="number">255</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure></li><li><p>插入数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> company.staff(<span class="keyword">name</span>, sex) <span class="keyword">values</span>(<span class="string">'Thomas'</span>, <span class="string">'Male'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> company.staff(<span class="keyword">name</span>, sex) <span class="keyword">values</span>(<span class="string">'Catalina'</span>, <span class="string">'FeMale'</span>);</span><br></pre></td></tr></table></figure></li><li><p>查看数据</p><p><code>select * from staff;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">----+----------+--------+</span></span><br><span class="line">| id | name     | sex    |</span><br><span class="line">+<span class="comment">----+----------+--------+</span></span><br><span class="line">|  1 | Thomas   | Male   |</span><br><span class="line">|  2 | Catalina | FeMale |</span><br><span class="line">+<span class="comment">----+----------+--------+</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="将MySQL数据导入到HDFS"><a href="#将MySQL数据导入到HDFS" class="headerlink" title="将MySQL数据导入到HDFS"></a>将MySQL数据导入到HDFS</h3><h4 id="全部导入"><a href="#全部导入" class="headerlink" title="全部导入"></a>全部导入</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://aliyun:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table staff \</span><br><span class="line">--target-dir /user/company \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &quot;\t&quot;</span><br></pre></td></tr></table></figure><p>全部导入用到的参数:</p><ul><li>table：指定被导入的表名</li><li>target-dir：指定导入路径</li><li>delete-target-dir：如果目标目录存在就删除它</li><li>num-mappers：mapper的个数</li><li>fields-terminated-by：指定字段分隔符</li></ul><h4 id="查询导入-query"><a href="#查询导入-query" class="headerlink" title="查询导入(query)"></a>查询导入(query)</h4><p>query参数就可以让用户随意写sql语句来查询了。query和table参数是互斥的。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://aliyun:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--target-dir /user/company \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &quot;\t&quot; \</span><br><span class="line">--query &apos;select name,sex from staff where id &lt;=1 and $CONDITIONS;&apos;</span><br></pre></td></tr></table></figure><ul><li>query：指定查询SQL where条件要有$CONDITIONS</li></ul><p>注意:  <code>must contain &#39;$CONDITIONS&#39; in WHERE clause.</code></p><p>如果query后使用的是双引号，则$CONDITIONS前必须加转移符，防止shell识别为自己的变量。</p><h4 id="导入指定列-columns"><a href="#导入指定列-columns" class="headerlink" title="导入指定列(columns)"></a>导入指定列(columns)</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--target-dir /user/company \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--num-mappers 1 \</span></span><br><span class="line"><span class="comment">--fields-terminated-by "\t" \</span></span><br><span class="line"><span class="comment">--columns id,sex \</span></span><br><span class="line"><span class="comment">--table staff</span></span><br></pre></td></tr></table></figure><ul><li>columns：指定导入的列</li></ul><h4 id="筛选查询导入数据-where"><a href="#筛选查询导入数据-where" class="headerlink" title="筛选查询导入数据(where)"></a>筛选查询导入数据(where)</h4><p>where参数可以进行一些简单的筛选</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--target-dir /user/company \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--num-mappers 1 \</span></span><br><span class="line"><span class="comment">--fields-terminated-by "\t" \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--where "id=1"</span></span><br></pre></td></tr></table></figure><ul><li>where：指定查询过滤条件</li></ul><h4 id="增量导入"><a href="#增量导入" class="headerlink" title="增量导入"></a>增量导入</h4><p>增量导入的一个场景就是昨天导入了一批数据，今天又增加了部分数据，现在要把这部分数据也导入到hdfs中。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://localhost:3306/company \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--target-dir /user/company/ \</span></span><br><span class="line"><span class="comment">--num-mappers 1 \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--null-string "" \</span></span><br><span class="line"><span class="comment">--null-non-string "0" \</span></span><br><span class="line"><span class="comment">--check-column "id" \</span></span><br><span class="line"><span class="comment">--incremental append \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line"><span class="comment">--last-value 0</span></span><br></pre></td></tr></table></figure><ul><li>null-string：字符串为null怎么处理</li><li>null-non-string：其他类型为null怎么处理</li><li>check-column：根据哪一行做增量导入</li><li>last-value：开始增量导入的上个位置</li></ul><h2 id="RDBMS导入到Hive"><a href="#RDBMS导入到Hive" class="headerlink" title="RDBMS导入到Hive"></a>RDBMS导入到Hive</h2><ol><li><p>在导入hive之前先在hive创建一样的表结构</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> staff(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">4</span>) , </span><br><span class="line">    <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">255</span>), </span><br><span class="line">    sex <span class="built_in">varchar</span>(<span class="number">255</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure></li><li><p>使用sqoop导入数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--hive-overwrite \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--null-string "" \</span></span><br><span class="line"><span class="comment">--null-non-string "0" \</span></span><br><span class="line"><span class="comment">--hive-import \</span></span><br><span class="line"><span class="comment">--hive-database default \</span></span><br><span class="line"><span class="comment">--hive-table staff \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line"><span class="comment">--num-mappers 1</span></span><br></pre></td></tr></table></figure><ul><li><p>hive-import：数据从关系数据库中导入到hive表中</p></li><li><p>hive-overwrite：覆盖掉在hive表中已经存在的数据</p></li><li><p>hive-table：后面接hive表,默认使用MySQL的表名</p></li><li><p>如果导入的是分区表，需要指定分区的key和value</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--hive-partition-key key \</span></span><br><span class="line"><span class="comment">--hive-partition-value value \</span></span><br></pre></td></tr></table></figure></li></ul><p>该过程分为两步，第一步将数据导入到HDFS，第二步将导入到HDFS的数据迁移到Hive仓库，第一步默认的临时目录是/user/hadoop/表名</p></li><li><p>查看hive表中的数据:</p><p><code>select * from staff;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">id|name    |sex   |</span><br><span class="line"><span class="comment">--|--------|------|</span></span><br><span class="line"> 1|Thomas  |Male  |</span><br><span class="line"> 2|Catalina|FeMale|</span><br></pre></td></tr></table></figure></li></ol><h2 id="HDFS导出到RDBMS"><a href="#HDFS导出到RDBMS" class="headerlink" title="HDFS导出到RDBMS"></a>HDFS导出到RDBMS</h2><ol><li><p>首先保证MySQL创建了一张和Hive一样表结构的表用来接收数据</p><p>注意表结构和分隔符都要一样</p></li><li><p>写Sqoop代码(批量导入)</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line">-Dsqoop.export.records.per.statement=10 \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--export-dir /user/company/ \</span></span><br><span class="line"><span class="comment">--null-string "" \</span></span><br><span class="line"><span class="comment">--null-non-string "0" \</span></span><br><span class="line"><span class="comment">--columns "id,name" \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure><ul><li><code>Dsqoop.export.records.per.statement</code>：批量更新，每隔10条提交一次 </li><li>export-dir：导出的hdfs目录</li><li>table：导入的表名</li><li>columns：指定导入的列</li></ul><p><code>注意:</code> MySQL中如果表不存在，不会自动创建</p></li></ol><h2 id="Hive导出到RDBMS"><a href="#Hive导出到RDBMS" class="headerlink" title="Hive导出到RDBMS"></a>Hive导出到RDBMS</h2><ol><li><p>首先保证MySQL创建了一张和Hive一样表结构的表用来接收数据</p><p>注意表结构和分隔符都要一样</p></li><li><p>写Sqoop代码</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--num-mappers 1 \</span></span><br><span class="line"><span class="comment">--export-dir /user/hive/warehouse/staff \</span></span><br><span class="line"><span class="comment">--input-fields-terminated-by "\t"</span></span><br></pre></td></tr></table></figure><ul><li><p>export-dir：指定被导出的目录</p></li><li><p>input-fields-terminated-by：导入的分隔符格式，和导入的fields-terminated-by有区别</p></li></ul><p><code>注意:</code> Mysql中如果表不存在，不会自动创建</p></li><li><p>查看MySQL数据库中的数据</p><p><code>select * from staff;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">----+----------+--------+</span></span><br><span class="line">| id | name     | sex    |</span><br><span class="line">+<span class="comment">----+----------+--------+</span></span><br><span class="line">|  1 | Thomas   | Male   |</span><br><span class="line">|  2 | Catalina | FeMale |</span><br><span class="line">|  3 | tunan    | Male   |</span><br><span class="line">+<span class="comment">----+----------+--------+</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="Sqoop的综合操作"><a href="#Sqoop的综合操作" class="headerlink" title="Sqoop的综合操作"></a>Sqoop的综合操作</h2><p>需求：emp和dept表是在MySQL，把MySQL数据抽取到Hive进行统计分析，然后把统计的结果回写到MySQL中</p><ol><li><p>在Hive中创建与MySQL中emp和dept表相对应的表emp_hive，dept_hive</p><p><code>emp_hive表</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> emp_hive(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">job <span class="keyword">string</span>,</span><br><span class="line">mgr <span class="built_in">int</span>,</span><br><span class="line">hiredate <span class="keyword">string</span>,</span><br><span class="line">sal <span class="keyword">double</span>,</span><br><span class="line">comm <span class="keyword">double</span>,</span><br><span class="line">deptno <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p><code>dept_hive表</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> dept_hive(</span><br><span class="line">deptno <span class="built_in">int</span>,</span><br><span class="line">dname <span class="keyword">string</span>,</span><br><span class="line">loc <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure></li><li><p>创建在Hive中的中间结果表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> mid_hive(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">deptno <span class="built_in">int</span>,</span><br><span class="line">dname <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure></li><li><p>将MySQL中的emp表中的数据传到emp_hive中</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/tunan \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--table emp \</span></span><br><span class="line"><span class="comment">--hive-overwrite \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--null-string "-" \</span></span><br><span class="line"><span class="comment">--null-non-string "0" \</span></span><br><span class="line"><span class="comment">--hive-import \</span></span><br><span class="line"><span class="comment">--hive-database default \</span></span><br><span class="line"><span class="comment">--hive-table emp_hive \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure></li><li><p>将MySQL中的dept表中的数据传到dept_hive</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/tunan \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--table dept \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--null-string "-" \</span></span><br><span class="line"><span class="comment">--null-non-string '0' \</span></span><br><span class="line"><span class="comment">--hive-import \</span></span><br><span class="line"><span class="comment">--hive-database default \</span></span><br><span class="line"><span class="comment">--hive-table dept_hive \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure></li><li><p>将Hive中的表进行业务处理</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> </span><br><span class="line">mid_hive</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">e.empno,e.ename,d.deptno,d.dname</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">emp_hive  e</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">dept_hive  d</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">e.deptno=d.deptno</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">*</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">mid_hive;</span><br></pre></td></tr></table></figure></li><li><p>查看中间表的数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> mid_hive;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |deptno|dname     |</span><br><span class="line"><span class="comment">-----|------|------|----------|</span></span><br><span class="line"> 7369|SMITH |    20|RESEARCH  |</span><br><span class="line"> 7499|ALLEN |    30|SALES     |</span><br><span class="line"> 7521|WARD  |    30|SALES     |</span><br><span class="line"> 7566|JONES |    20|RESEARCH  |</span><br><span class="line"> 7654|MARTIN|    30|SALES     |</span><br><span class="line"> 7698|BLAKE |    30|SALES     |</span><br><span class="line"> 7782|CLARK |    10|ACCOUNTING|</span><br><span class="line"> 7788|SCOTT |    20|RESEARCH  |</span><br><span class="line"> 7839|KING  |    10|ACCOUNTING|</span><br><span class="line"> 7844|TURNER|    30|SALES     |</span><br><span class="line"> 7876|ADAMS |    20|RESEARCH  |</span><br><span class="line"> 7900|JAMES |    30|SALES     |</span><br><span class="line"> 7902|FORD  |    20|RESEARCH  |</span><br><span class="line"> 7934|MILLER|    10|ACCOUNTING|</span><br></pre></td></tr></table></figure></li><li><p>在MySQL中创建返回数据的表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="string">`mid`</span>(</span><br><span class="line">empno <span class="built_in">int</span>(<span class="number">11</span>),</span><br><span class="line">ename <span class="built_in">varchar</span>(<span class="number">20</span>),</span><br><span class="line">deptno <span class="built_in">int</span>(<span class="number">11</span>),</span><br><span class="line">dname <span class="built_in">varchar</span>(<span class="number">20</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p>将处理好的数据用Sqoop发回MySQL</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/tunan \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--table mid \</span></span><br><span class="line">-m 1 \</span><br><span class="line"><span class="comment">--export-dir /user/hive/warehouse/mid_hive \</span></span><br><span class="line"><span class="comment">--input-fields-terminated-by '\t'</span></span><br></pre></td></tr></table></figure></li><li><p>查看MySQL中已经发回的数据</p><p><code>select * from mid;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">-------+--------+--------+------------+</span></span><br><span class="line">| empno | ename  | deptno | dname      |</span><br><span class="line">+<span class="comment">-------+--------+--------+------------+</span></span><br><span class="line">|  7369 | SMITH  |     20 | RESEARCH   |</span><br><span class="line">|  7499 | ALLEN  |     30 | SALES      |</span><br><span class="line">|  7521 | WARD   |     30 | SALES      |</span><br><span class="line">|  7566 | JONES  |     20 | RESEARCH   |</span><br><span class="line">|  7654 | MARTIN |     30 | SALES      |</span><br><span class="line">|  7698 | BLAKE  |     30 | SALES      |</span><br><span class="line">|  7782 | CLARK  |     10 | ACCOUNTING |</span><br><span class="line">|  7788 | SCOTT  |     20 | RESEARCH   |</span><br><span class="line">|  7839 | KING   |     10 | ACCOUNTING |</span><br><span class="line">|  7844 | TURNER |     30 | SALES      |</span><br><span class="line">|  7876 | ADAMS  |     20 | RESEARCH   |</span><br><span class="line">|  7900 | JAMES  |     30 | SALES      |</span><br><span class="line">|  7902 | FORD   |     20 | RESEARCH   |</span><br><span class="line">|  7934 | MILLER |     10 | ACCOUNTING |</span><br><span class="line">+<span class="comment">-------+--------+--------+------------+</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="shell操作数据库"><a href="#shell操作数据库" class="headerlink" title="shell操作数据库"></a>shell操作数据库</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql -uroot -pruozedata &lt;&lt;EOF</span><br><span class="line"><span class="keyword">use</span> sqoop;</span><br><span class="line"><span class="keyword">truncate</span> etl_result;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HIVE调优(2)</title>
      <link href="/2018/11/13/hive/13/"/>
      <url>/2018/11/13/hive/13/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>Hadoop 框架计算特性</li><li>优化常用手段</li><li>排序选择</li><li>怎样做笛卡尔积</li><li>怎样写 in/exists 语句</li><li>设置合理的 maptask 数量</li><li>小文件合并</li><li>设置合理的 reduceTask 的数量</li><li>合理利用分桶：Bucketing 和 Sampling</li><li>合理利用分区：Partition </li><li>Join 优化</li><li>Group By 优化</li><li>合理利用文件存储格式 </li><li>本地模式执行 MapReduce</li><li>并行化处理</li><li>设置压缩存储</li></ol><h2 id="Hadoop-框架计算特性"><a href="#Hadoop-框架计算特性" class="headerlink" title="Hadoop 框架计算特性"></a>Hadoop 框架计算特性</h2><ol><li><p>数据量大不是问题，数据倾斜是个问题</p></li><li><p>jobs 数比较多的作业运行效率相对比较低，比如即使有几百行的表，如果多次关联多次 汇总，产生十几个 jobs，耗时很长。原因是 map reduce 作业初始化的时间是比较长的</p></li><li><p>sum,count,max,min 等 UDAF，不怕数据倾斜问题，hadoop 在 map 端的汇总合并优化，使 数据倾斜不成问题</p></li><li><p>count(distinct userid)，在数据量大的情况下，效率较低，如果是多 count(distinct userid,month)效率更低，因为 count(distinct)是按 group by 字段分组，按 distinct 字段排序， 一般这种分布方式是很</p></li></ol><p>倾斜的，比如 PV 数据，淘宝一天 30 亿的 pv，如果按性别分组，分 配 2 个 reduce，每个 reduce 期望处理 15 亿数据，但现实必定是男少女多</p><h2 id="优化常用手段"><a href="#优化常用手段" class="headerlink" title="优化常用手段"></a>优化常用手段</h2><ol><li><p>好的模型设计事半功倍</p></li><li><p>解决数据倾斜问题</p></li><li><p>减少 job 数</p></li><li><p>设置合理的 MapReduce 的 task 数，能有效提升性能。(比如，10w+级别的计算，用 160个 reduce，那是相当的浪费，1 个足够)</p></li><li><p>了解数据分布，自己动手解决数据倾斜问题是个不错的选择。这是通用的算法优化，但 算法优化有时不能适应特定业务背景，开发人员了解业务，了解数据，可以通过业务逻辑精 确有效的解决数据倾斜问题</p></li><li><p>数据量较大的情况下，慎用 count(distinct)，group by 容易产生倾斜问题</p></li><li><p>对小文件进行合并，是行之有效的提高调度效率的方法，假如所有的作业设置合理的文 件数，对云梯的整体调度效率也会产生积极的正向影响</p></li><li><p>优化时把握整体，单个作业最优不如整体最优</p></li></ol><h2 id="排序选择"><a href="#排序选择" class="headerlink" title="排序选择"></a>排序选择</h2><ul><li><p><strong>cluster by</strong>：对同一字段分桶并排序，不能和 sort by 连用</p></li><li><p><strong>distribute by + sort by</strong>：分桶，保证同一字段值只存在一个结果文件当中，结合 sort by 保证 每个 reduceTask 结果有序</p></li><li><p><strong>sort by</strong>：单机排序，单个 reduce 结果有序</p></li><li><p><strong>order by</strong>：全局排序，缺陷是只能使用一个 reduce</p></li></ul><p><strong>一定要区分这四种排序的使用方式和适用场景</strong></p><h2 id="怎样做笛卡尔积"><a href="#怎样做笛卡尔积" class="headerlink" title="怎样做笛卡尔积"></a>怎样做笛卡尔积</h2><p>当 Hive 设定为严格模式（hive.mapred.mode=strict）时，不允许在 HQL 语句中出现笛卡尔积， 这实际说明了 Hive 对笛卡尔积支持较弱。因为找不到 Join key，Hive 只能使用 1 个 reducer 来完成笛卡尔积。</p><p>当然也可以使用 limit 的办法来减少某个表参与 join 的数据量，但对于需要笛卡尔积语义的 需求来说，经常是一个大表和一个小表的 Join 操作，结果仍然很大（以至于无法用单机处 理），这时 MapJoin才是最好的解决办法。MapJoin，顾名思义，会在 Map 端完成 Join 操作。 这需要将 Join 操作的一个或多个表完全读入内存。</p><p>PS：MapJoin 在子查询中可能出现未知 BUG。在大表和小表做笛卡尔积时，规避笛卡尔积的 方法是，给 Join 添加一个 Join key，<strong>原理很简单：将小表扩充一列 join key，并将小表的条目复制数倍，join</strong> <strong>key 各不相同；将大表扩充一列 join key 为随机数。</strong></p><p><strong>精髓就在于复制几倍，最后就有几个 reduce 来做，而且大表的数据是前面小表扩张 key 值 范围里面随机出来的，所以复制了几倍 n，就相当于这个随机范围就有多大 n，那么相应的， 大表的数据就被随机的分为了 n 份。并且最后处理所用的 reduce 数量也是 n，而且也不会 出现数据倾斜。</strong></p><h2 id="怎样写-in-exists-语句"><a href="#怎样写-in-exists-语句" class="headerlink" title="怎样写 in/exists 语句"></a>怎样写 in/exists 语句</h2><p>虽然经过测验，hive1.2.1 也支持 in/exists 操作，但还是推荐使用 hive 的一个高效替代方案：<strong>left semi join</strong></p><p>比如说：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">where</span> a.id <span class="keyword">in</span> (<span class="keyword">select</span> b.id <span class="keyword">from</span> b);</span><br><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">where</span> <span class="keyword">exists</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> b <span class="keyword">where</span> a.id = b.id);</span><br></pre></td></tr></table></figure><p>应该转换成：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">left</span> <span class="keyword">semi</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.id = b.id;</span><br></pre></td></tr></table></figure><h2 id="设置合理的-maptask-数量"><a href="#设置合理的-maptask-数量" class="headerlink" title="设置合理的 maptask 数量"></a>设置合理的 maptask 数量</h2><ol><li><p>Map 数过大</p><p>Map 阶段输出文件太小，产生大量小文件</p><p>初始化和创建 Map 的开销很大</p></li><li><p>Map 数太小</p><p>文件处理或查询并发度小，Job 执行时间过长</p><p>大量作业时，容易堵塞集群 </p></li></ol><p>在 MapReduce 的编程案例中，我们得知，一个MR Job的 MapTask 数量是由输入分片 InputSplit 决定的。而输入分片是由 FileInputFormat.getSplit()决定的。一个输入分片对应一个 MapTask， 而输入分片是由三个参数决定的：</p><p><img src="https://yerias.github.io/hive_img/%E8%AE%BE%E7%BD%AEmap%E4%BB%BB%E5%8A%A1%E6%95%B0%E9%87%8F.png" alt="设置map任务数量"></p><p>输入分片大小的计算是这么计算出来的：</p><p><strong>long splitSize = Math.max(minSize, Math.min(maxSize, blockSize))</strong></p><p>默认情况下，输入分片大小和 HDFS 集群默认数据块大小一致，也就是默认一个数据块，启 用一个 MapTask 进行处理，这样做的好处是避免了服务器节点之间的数据传输，提高 job 处 理效率</p><p>两种经典的控制 MapTask 的个数方案：减少 MapTask 数或者增加 MapTask 数</p><ol><li>减少 MapTask 数是通过合并小文件来实现，这一点主要是针对数据源</li><li>增加 MapTask 数可以通过控制上一个 job 的 reduceTask 个数</li></ol><p>因为 Hive 语句最终要转换为一系列的 MapReduce Job 的，而每一个 MapReduce Job 是由一 系列的 MapTask 和 ReduceTask 组成的，默认情况下， MapReduce 中一个 MapTask 或者一个 ReduceTask 就会启动一个 JVM 进程，一个 Task 执行完毕后， JVM 进程就退出。这样如果任 务花费时间很短，又要多次启动 JVM 的情况下，JVM 的启动时间会变成一个比较大的消耗， 这个时候，就可以通过重用 JVM 来解决：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set mapred.job.reuse.jvm.num.tasks=5</span><br></pre></td></tr></table></figure><h2 id="小文件合并"><a href="#小文件合并" class="headerlink" title="小文件合并"></a>小文件合并</h2><p>文件数目过多，会给 HDFS 带来压力，并且会影响处理效率，可以通过合并 Map 和 Reduce 的 结果文件来消除这样的影响：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.merge.mapfiles = true ##在 map only 的任务结束时合并小文件</span><br><span class="line">set hive.merge.mapredfiles = false ## true 时在 MapReduce 的任务结束时合并小文件</span><br><span class="line">set hive.merge.size.per.task = 256*1000*1000 ##合并文件的大小</span><br><span class="line">set mapred.max.split.size=256000000; ##每个 Map 最大分割大小</span><br><span class="line">set mapred.min.split.size.per.node=1; ##一个节点上 split 的最少值</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; ##执行 Map 前进行小文件合并</span><br></pre></td></tr></table></figure><h2 id="设置合理的-reduceTask-的数量"><a href="#设置合理的-reduceTask-的数量" class="headerlink" title="设置合理的 reduceTask 的数量"></a>设置合理的 reduceTask 的数量</h2><p>Hadoop MapReduce 程序中，reducer 个数的设定极大影响执行效率，这使得 Hive 怎样决定 reducer 个数成为一个关键问题。遗憾的是 Hive 的估计机制很弱，不指定 reducer 个数的情 况下，Hive 会猜测确定一个 reducer 个数，基于以下两个设定：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive.exec.reducers.bytes.per.reducer（默认为 256000000）</span><br><span class="line">hive.exec.reducers.max（默认为 1009）</span><br><span class="line">mapreduce.job.reduces=-1（设置一个常量 reducetask 数量）</span><br></pre></td></tr></table></figure><p>计算 reducer 数的公式很简单： N=min(参数 2，总输入数据量/参数 1) 通常情况下，有必要手动指定 reducer 个数。考虑到 map 阶段的输出数据量通常会比输入有 大幅减少，因此即使不设定 reducer 个数，重设参数 2 还是必要的。</p><p><strong>依据 Hadoop 的经验，可以将参数 2 设定为 0.95*(集群中 datanode 个数)。</strong> </p><h2 id="合并-MapReduce-操作"><a href="#合并-MapReduce-操作" class="headerlink" title="合并 MapReduce 操作"></a>合并 MapReduce 操作</h2><p>Multi-group by 是 Hive 的一个非常好的特性，它使得 Hive 中利用中间结果变得非常方便。 例如：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">FROM (<span class="keyword">SELECT</span> a.status, b.school, b.gender <span class="keyword">FROM</span> status_updates a <span class="keyword">JOIN</span> <span class="keyword">profiles</span> b <span class="keyword">ON</span> (a.userid =</span><br><span class="line">b.userid <span class="keyword">and</span> a.ds=<span class="string">'2009-03-20'</span> ) ) subq1</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> gender_summary <span class="keyword">PARTITION</span>(ds=<span class="string">'2009-03-20'</span>)</span><br><span class="line"><span class="keyword">SELECT</span> subq1.gender, <span class="keyword">COUNT</span>(<span class="number">1</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> subq1.gender</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> school_summary <span class="keyword">PARTITION</span>(ds=<span class="string">'2009-03-20'</span>)</span><br><span class="line"><span class="keyword">SELECT</span> subq1.school, <span class="keyword">COUNT</span>(<span class="number">1</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> subq1.school</span><br></pre></td></tr></table></figure><p>上述查询语句使用了 multi-group by 特性连续 group by 了 2 次数据，使用不同的 group by key。 这一特性可以减少一次 MapReduce 操作</p><h2 id="合理利用分桶：Bucketing-和-Sampling"><a href="#合理利用分桶：Bucketing-和-Sampling" class="headerlink" title="合理利用分桶：Bucketing 和 Sampling"></a>合理利用分桶：Bucketing 和 Sampling</h2><p>Bucket 是指将数据以指定列的值为 key 进行 hash，hash 到指定数目的桶中。这样就可以支 持高效采样了。如下例就是以 userid 这一列为 bucket 的依据，共设置 32 个 buckets</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> page_view(viewTime <span class="built_in">INT</span>, userid <span class="built_in">BIGINT</span>,</span><br><span class="line"> page_url <span class="keyword">STRING</span>, referrer_url <span class="keyword">STRING</span>,</span><br><span class="line"> ip <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'IP Address of the User'</span>)</span><br><span class="line"> <span class="keyword">COMMENT</span> <span class="string">'This is the page view table'</span></span><br><span class="line"> PARTITIONED <span class="keyword">BY</span>(dt <span class="keyword">STRING</span>, country <span class="keyword">STRING</span>)</span><br><span class="line"> CLUSTERED <span class="keyword">BY</span>(userid) SORTED <span class="keyword">BY</span>(viewTime) <span class="keyword">INTO</span> <span class="number">32</span> BUCKETS</span><br><span class="line"> <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line"> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'1'</span></span><br><span class="line"> COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'2'</span></span><br><span class="line"> <span class="keyword">MAP</span> <span class="keyword">KEYS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'3'</span></span><br><span class="line"> <span class="keyword">STORED</span> <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure><p>通常情况下，Sampling 在全体数据上进行采样，这样效率自然就低，它要去访问所有数据。 而如果一个表已经对某一列制作了 bucket，就可以采样所有桶中指定序号的某个桶，这就 减少了访问量。</p><p>如下例所示就是采样了 page_view 中 32 个桶中的第三个桶的全部数据：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> page_view <span class="keyword">TABLESAMPLE</span>(<span class="keyword">BUCKET</span> <span class="number">3</span> <span class="keyword">OUT</span> <span class="keyword">OF</span> <span class="number">32</span>);</span><br></pre></td></tr></table></figure><p>如下例所示就是采样了 page_view 中 32 个桶中的第三个桶的一半数据：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> page_view <span class="keyword">TABLESAMPLE</span>(<span class="keyword">BUCKET</span> <span class="number">3</span> <span class="keyword">OUT</span> <span class="keyword">OF</span> <span class="number">64</span>);</span><br></pre></td></tr></table></figure><h2 id="合理利用分区：Partition"><a href="#合理利用分区：Partition" class="headerlink" title="合理利用分区：Partition"></a>合理利用分区：Partition</h2><p> Partition 就是分区。分区通过在创建表时启用 partitioned by 实现，用来 partition 的维度并不 是实际数据的某一列，具体分区的标志是由插入内容时给定的。当要查询某一分区的内容时 可以采用 where 语句，形似 where tablename.partition_column = a 来实现。</p><p>创建含分区的表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> page_view(viewTime <span class="built_in">INT</span>, userid <span class="built_in">BIGINT</span>,</span><br><span class="line"> page_url <span class="keyword">STRING</span>, referrer_url <span class="keyword">STRING</span>,</span><br><span class="line"> ip <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'IP Address of the User'</span>)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span>(<span class="built_in">date</span> <span class="keyword">STRING</span>, country <span class="keyword">STRING</span>)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'1'</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> TEXTFILE;</span><br></pre></td></tr></table></figure><p>载入内容，并指定分区标志</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/pv_2008-06-08_us.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> page_view</span><br><span class="line"><span class="keyword">partition</span>(<span class="built_in">date</span>=<span class="string">'2008-06-08'</span>, country=<span class="string">'US'</span>);</span><br></pre></td></tr></table></figure><p>查询指定标志的分区内容</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> page_views.* <span class="keyword">FROM</span> page_views</span><br><span class="line"> <span class="keyword">WHERE</span> page_views.date &gt;= <span class="string">'2008-03-01'</span> <span class="keyword">AND</span> page_views.date &lt;= <span class="string">'2008-03-31'</span> <span class="keyword">AND</span></span><br><span class="line">page_views.referrer_url <span class="keyword">like</span> <span class="string">'%xyz.com'</span>;</span><br></pre></td></tr></table></figure><h2 id="Join-优化"><a href="#Join-优化" class="headerlink" title="Join 优化"></a>Join 优化</h2><p>总体原则：</p><ol><li><p>优先过滤后再进行 Join 操作，最大限度的减少参与 join 的数据量</p></li><li><p>小表 join 大表，最好启动 mapjoin</p></li><li><p>Join on 的条件相同的话，最好放入同一个 job，并且 join 表的排列顺序从小到大 </p></li></ol><p><strong>在使用写有 Join 操作的查询语句时有一条原则：应该将条目少的表/子查询放在 Join 操作 符的左边。</strong>原因是在 Join 操作的 Reduce 阶段，位于 Join 操作符左边的表的内容会被加 载进内存，将条目少的表放在左边，可以有效减少发生 OOM 错误的几率。对于一条语句 中有多个 Join 的情况，如果 Join 的条件相同，比如查询</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> pv_users</span><br><span class="line"><span class="keyword">SELECT</span> pv.pageid, u.age <span class="keyword">FROM</span> page_view p</span><br><span class="line"><span class="keyword">JOIN</span> <span class="keyword">user</span> u <span class="keyword">ON</span> (pv.userid = u.userid)</span><br><span class="line"><span class="keyword">JOIN</span> newuser x <span class="keyword">ON</span> (u.userid = x.userid);</span><br></pre></td></tr></table></figure><p>如果 Join 的 key 相同，不管有多少个表，都会则会合并为一个 Map-Reduce 任务，而不 是”n”个，在做 OUTER JOIN 的时候也是一样</p><p>如果 join 的条件不相同，比如：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> pv_users</span><br><span class="line"> <span class="keyword">SELECT</span> pv.pageid, u.age <span class="keyword">FROM</span> page_view p</span><br><span class="line"> <span class="keyword">JOIN</span> <span class="keyword">user</span> u <span class="keyword">ON</span> (pv.userid = u.userid)</span><br><span class="line"> <span class="keyword">JOIN</span> newuser x <span class="keyword">on</span> (u.age = x.age);</span><br></pre></td></tr></table></figure><p>Map-Reduce 的任务数目和 Join 操作的数目是对应的，上述查询和以下查询是等价的</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--先 page_view 表和 user 表做链接</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tmptable</span><br><span class="line"> <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> page_view p <span class="keyword">JOIN</span> <span class="keyword">user</span> u <span class="keyword">ON</span> (pv.userid = u.userid);</span><br><span class="line"><span class="comment">-- 然后结果表 temptable 和 newuser 表做链接</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> pv_users</span><br><span class="line"> <span class="keyword">SELECT</span> x.pageid, x.age <span class="keyword">FROM</span> tmptable x <span class="keyword">JOIN</span> newuser y <span class="keyword">ON</span> (x.age = y.age);</span><br></pre></td></tr></table></figure><p>在编写 Join 查询语句时，如果确定是由于 join 出现的数据倾斜，那么请做如下设置：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.skewjoin.key=100000; // 这个是 join 的键对应的记录条数超过这个值则会进行 分拆，值根据具体数据量设置</span><br><span class="line">set hive.optimize.skewjoin=true; // 如果是 join 过程出现倾斜应该设置为 true</span><br></pre></td></tr></table></figure><h2 id="Group-By-优化"><a href="#Group-By-优化" class="headerlink" title="Group By 优化"></a>Group By 优化</h2><h3 id="1-Map-端部分聚合"><a href="#1-Map-端部分聚合" class="headerlink" title="1. Map 端部分聚合"></a>1. Map 端部分聚合</h3><p>并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进 行部分聚合，最后在Reduce 端得出最终结果。</p><p>MapReduce 的 combiner 组件参数包括：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.map.aggr = true 是否在 Map 端进行聚合，默认为 True</span><br><span class="line">set hive.groupby.mapaggr.checkinterval = 100000 在 Map 端进行聚合操作的条目数目</span><br></pre></td></tr></table></figure><h3 id="2-使用-Group-By-有数据倾斜的时候进行负载均衡"><a href="#2-使用-Group-By-有数据倾斜的时候进行负载均衡" class="headerlink" title="2. 使用 Group By 有数据倾斜的时候进行负载均衡"></a>2. 使用 Group By 有数据倾斜的时候进行负载均衡</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.groupby.skewindata = true</span><br></pre></td></tr></table></figure><p>当 sql 语句使用 groupby 时数据出现倾斜时，如果该变量设置为 true，那么 Hive 会自动进行 负载均衡。<strong>策略就是把 MR 任务拆分成两个：第一个先做预汇总，第二个再做最终汇总</strong></p><p>在 MR 的第一个阶段中，Map 的输出结果集合会缓存到 maptaks 中，每个 Reduce 做部分聚 合操作，并输出结果，这样处理的结果是相同 Group By Key 有可能被分发到不同的 Reduce 中， 从而达到负载均衡的目的；第二个阶段 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成 最终的聚合操作。</p><h2 id="合理利用文件存储格式"><a href="#合理利用文件存储格式" class="headerlink" title="合理利用文件存储格式"></a>合理利用文件存储格式</h2><p>创建表时，尽量使用 orc、parquet 这些列式存储格式，因为列式存储的表，每一列的数据在 物理上是存储在一起的，Hive 查询时会只遍历需要列数据，大大减少处理的数据量。</p><h2 id="本地模式执行-MapReduce"><a href="#本地模式执行-MapReduce" class="headerlink" title="本地模式执行 MapReduce"></a>本地模式执行 MapReduce</h2><p>Hive 在集群上查询时，默认是在集群上 N 台机器上运行， 需要多个机器进行协调运行，这 个方式很好地解决了大数据量的查询问题。但是当 Hive 查询处理的数据量比较小时，其实 没有必要启动分布式模式去执行，因为以分布式方式执行就涉及到跨网络传输、多节点协调 等，并且消耗资源。这个时间可以只使用本地模式来执行 mapreduce job，只在一台机器上 执行，速度会很快。启动本地模式涉及到三个参数：</p><p><img src="https://yerias.github.io/hive_img/%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F.png" alt="本地模式"></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.exec.mode.local.auto=true 是打开 hive 自动判断是否启动本地模式的开关，但是只 是打开这个参数并不能保证启动本地模式，要当 map 任务数不超过</span><br><span class="line">hive.exec.mode.local.auto.input.files.max 的个数并且 map 输入文件大小不超过</span><br><span class="line">hive.exec.mode.local.auto.inputbytes.max 所指定的大小时，才能启动本地模式。</span><br></pre></td></tr></table></figure><h2 id="并行化处理"><a href="#并行化处理" class="headerlink" title="并行化处理"></a>并行化处理</h2><p>一个 hive sql 语句可能会转为多个 mapreduce Job，每一个 job 就是一个 stage，这些 job 顺序 执行，这个在 cli 的运行日志中也可以看到。但是有时候这些任务之间并不是是相互依赖的， 如果集群资源允许的话，可以让多个并不相互依赖 stage 并发执行，这样就节约了时间，提 高了执行速度，但是如果集群资源匮乏时，启用并行化反倒是会导致各个 job 相互抢占资源 而导致整体执行性能的下降。启用并行化：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.parallel=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number=<span class="number">8</span>; //同一个 sql 允许并行任务的最大线程数</span><br></pre></td></tr></table></figure><h2 id="设置压缩存储"><a href="#设置压缩存储" class="headerlink" title="设置压缩存储"></a>设置压缩存储</h2><h3 id="1-压缩的原因"><a href="#1-压缩的原因" class="headerlink" title="1. 压缩的原因"></a>1. 压缩的原因</h3><p>Hive 最终是转为 MapReduce 程序来执行的，而 MapReduce 的性能瓶颈在于网络 IO 和 磁盘 IO，要解决性能瓶颈，最主要的是减少数据量，对数据进行压缩是个好的方式。压缩 虽然是减少了数据量，但是压缩过程要消耗 CPU 的，但是在 Hadoop 中， 往往性能瓶颈不 在于 CPU，CPU 压力并不大，所以压缩充分利用了比较空闲的 CPU</p><h3 id="2-常用压缩方法对比"><a href="#2-常用压缩方法对比" class="headerlink" title="2. 常用压缩方法对比"></a>2. 常用压缩方法对比</h3><p><img src="https://yerias.github.io/hive_img/%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%E5%AF%B9%E6%AF%94.png" alt="压缩格式对比"></p><p>各个压缩方式所对应的 Class 类：</p><p><img src="https://yerias.github.io/hive_img/%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%E5%AF%B9%E5%BA%94%E7%9A%84%E7%B1%BB.png" alt="压缩格式对应的类"></p><h3 id="3-压缩方式的选择"><a href="#3-压缩方式的选择" class="headerlink" title="3. 压缩方式的选择"></a>3. 压缩方式的选择</h3><ol><li><p>压缩比率</p><p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E6%AF%94.png" alt="压缩比"></p></li><li><p>压缩解压缩速度</p><p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E6%97%B6%E9%97%B4.png" alt="压缩时间"></p></li><li><p>是否支持 Split</p><p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9.png" alt="压缩"></p></li></ol><h3 id="4-压缩使用"><a href="#4-压缩使用" class="headerlink" title="4. 压缩使用"></a>4. 压缩使用</h3><p>Job 输出文件按照 block 以 GZip 的方式进行压缩：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set mapreduce.output.fileoutputformat.compress=true // 默认值是 false</span><br><span class="line">set mapreduce.output.fileoutputformat.compress.type=BLOCK // 默认值是 Record</span><br><span class="line">set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec // 默认值是 org.apache.hadoop.io.compress.DefaultCodec</span><br></pre></td></tr></table></figure><p>Map 输出结果也以 Gzip 进行压缩：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set mapred.map.output.compress=true</span><br><span class="line">set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.GzipCodec // 默认值是 org.apache.hadoop.io.compress.DefaultCodec</span><br></pre></td></tr></table></figure><p>对 Hive 输出结果和中间都进行压缩：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.exec.compress.output=true // 默认值是 false，不压缩</span><br><span class="line">set hive.exec.compress.intermediate=true // 默认值是 false，为 true 时 MR 设置的压缩才启用</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windows加载Hive源码，并且查询结果输出在控制台</title>
      <link href="/2018/11/13/hive/14/"/>
      <url>/2018/11/13/hive/14/</url>
      
        <content type="html"><![CDATA[<h2 id="编译源码"><a href="#编译源码" class="headerlink" title="编译源码"></a>编译源码</h2><ol><li><p>下载Hive源码:<a href="http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.16.2-src.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.16.2-src.tar.gz</a></p></li><li><p>编译Hive源码(切记不要idea里面执行命令)：mvn clean package -DskipTests=true -Phadoop-2</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mvn clean package -DskipTests=true -Phadoop-2</span><br><span class="line"></span><br><span class="line">[INFO] Reactor Summary:</span><br><span class="line">[INFO]</span><br><span class="line">[INFO] Hive 1.1.0-cdh5.16.2 ............................... SUCCESS [  3.119 s]</span><br><span class="line">[INFO] Hive Classifications ............................... SUCCESS [  2.406 s]</span><br><span class="line">[INFO] Hive Shims Common .................................. SUCCESS [  3.327 s]</span><br><span class="line">[INFO] Hive Shims 0.23 .................................... SUCCESS [  3.494 s]</span><br><span class="line">[INFO] Hive Shims Scheduler ............................... SUCCESS [  2.423 s]</span><br><span class="line">[INFO] Hive Shims ......................................... SUCCESS [  1.463 s]</span><br><span class="line">[INFO] Hive Common ........................................ SUCCESS [  8.382 s]</span><br><span class="line">[INFO] Hive Serde ......................................... SUCCESS [  8.001 s]</span><br><span class="line">[INFO] Hive Metastore ..................................... SUCCESS [ 28.285 s]</span><br><span class="line">[INFO] Hive Ant Utilities ................................. SUCCESS [  1.668 s]</span><br><span class="line">[INFO] Spark Remote Client ................................ SUCCESS [  4.915 s]</span><br><span class="line">[INFO] Hive Query Language ................................ SUCCESS [01:36 min]</span><br><span class="line">[INFO] Hive Service ....................................... SUCCESS [ 22.921 s]</span><br><span class="line">[INFO] Hive Accumulo Handler .............................. SUCCESS [  5.496 s]</span><br><span class="line">[INFO] Hive JDBC .......................................... SUCCESS [  5.797 s]</span><br><span class="line">[INFO] Hive Beeline ....................................... SUCCESS [  3.957 s]</span><br><span class="line">[INFO] Hive CLI ........................................... SUCCESS [  4.060 s]</span><br><span class="line">[INFO] Hive Contrib ....................................... SUCCESS [  4.321 s]</span><br><span class="line">[INFO] Hive HBase Handler ................................. SUCCESS [  5.518 s]</span><br><span class="line">[INFO] Hive HCatalog ...................................... SUCCESS [  1.399 s]</span><br><span class="line">[INFO] Hive HCatalog Core ................................. SUCCESS [  5.933 s]</span><br><span class="line">[INFO] Hive HCatalog Pig Adapter .......................... SUCCESS [  4.632 s]</span><br><span class="line">[INFO] Hive HCatalog Server Extensions .................... SUCCESS [  4.477 s]</span><br><span class="line">[INFO] Hive HCatalog Webhcat Java Client .................. SUCCESS [  4.903 s]</span><br><span class="line">[INFO] Hive HCatalog Webhcat .............................. SUCCESS [  7.452 s]</span><br><span class="line">[INFO] Hive HCatalog Streaming ............................ SUCCESS [  4.306 s]</span><br><span class="line">[INFO] Hive HWI ........................................... SUCCESS [  3.461 s]</span><br><span class="line">[INFO] Hive ODBC .......................................... SUCCESS [  3.061 s]</span><br><span class="line">[INFO] Hive Shims Aggregator .............................. SUCCESS [  0.840 s]</span><br><span class="line">[INFO] Hive TestUtils ..................................... SUCCESS [  1.077 s]</span><br><span class="line">[INFO] Hive Packaging 1.1.0-cdh5.16.2 ..................... SUCCESS [  4.194 s]</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 04:22 min</span><br><span class="line">[INFO] Finished at: 2020-04-12T18:50:46+08:00</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure></li><li><p>IDEA导入源码</p></li></ol><h2 id="本地调试Hive"><a href="#本地调试Hive" class="headerlink" title="本地调试Hive"></a>本地调试Hive</h2><ol><li><p>集群服务器启动metastore:hive –service metastore -p 9083 &amp;</p></li><li><p>分别把集群的配置文件加载到hive-cli的resources下</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">core-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://ruozedata001:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">hdfs-site.xml</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.client.use.datanode.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">mapred-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">yarn-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">hive-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.cli.print.current.db&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.cli.print.header&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.metastore.uris&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;thrift://ruozedata001:9083&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li><li><p>在CliDriver类中加入vm参数:<code>-Djline.WindowsTerminal.directConsole=false</code>,Hive是默认Jline输入到控制太只支持Linux和mac,所以需要关闭</p><p>![hive client](<a href="https://yerias.github.io/hive_img/hive">https://yerias.github.io/hive_img/hive</a> client.png)</p></li><li><p>运行CliDriver</p></li><li><p>控制台输入</p><p>![hive client](<a href="https://yerias.github.io/hive_img/hive">https://yerias.github.io/hive_img/hive</a> client2.png)</p></li></ol><p><em>20200415更新：</em> idea2020.1版本支持本地调试hive，无需任何设置</p><hr><p>转载自：<a href="https://blog.csdn.net/jim8973/article/details/105503221" target="_blank" rel="noopener">https://blog.csdn.net/jim8973/article/details/105503221</a></p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HIVE调优之存储格式</title>
      <link href="/2018/11/12/hive/12/"/>
      <url>/2018/11/12/hive/12/</url>
      
        <content type="html"><![CDATA[<p>目录</p><ol><li>行式数据库和列式数据库的对比</li><li>存储格式的比较</li><li>存储格式的应用</li></ol><h2 id="行式数据库和列式数据库的对比"><a href="#行式数据库和列式数据库的对比" class="headerlink" title="行式数据库和列式数据库的对比"></a>行式数据库和列式数据库的对比</h2><ol><li><p>存储比较</p><p>行式数据库存储在hdfs上式按行进行存储的，一个block存储一或多行数据。而列式数据库在hdfs上则是按照列进行存储，一个block可能有一列或多列数据。</p></li><li><p>压缩比较</p><p>对于行式数据库，必然按行压缩，当一行中有多个字段，各个字段对应的数据类型可能不一致，压缩性能压缩比就比较差。</p><p>对于列式数据库，必然按列压缩，每一列对应的是相同数据类型的数据，故列式数据库的压缩性能要强于行式数据库。</p></li><li><p>查询比较</p><p>假设执行的查询操作是：<code>select id,name from table_emp;</code></p><p>对于行式数据库，它要遍历一整张表将每一行中的id,name字段拼接再展现出来，这样需要查询的数据量就比较大，效率低。</p><p>对于列式数据库，它只需找到对应的id,name字段的列展现出来即可，需要查询的数据量小，效率高。</p><p>假设执行的查询操作是：<code>select * from table_emp;</code></p><p>对于这种查询整个表全部信息的操作，由于列式数据库需要将分散的行进行重新组合，行式数据库效率就高于列式数据库。</p><p><strong>但是，在大数据领域，进行全表查询的场景少之又少，进而我们使用较多的还是列式数据库及列式储存。</strong></p></li></ol><h2 id="存储格式的比较"><a href="#存储格式的比较" class="headerlink" title="存储格式的比较"></a>存储格式的比较</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">: SEQUENCEFILE</span><br><span class="line">| TEXTFILE  -- (Default, depending on hive.default.fileformat configuration)</span><br><span class="line">| RCFILE   -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">| ORC     -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line">| PARQUET   -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line">| AVRO    -- (Note: Available in Hive 0.14.0 and later)</span><br><span class="line">| JSONFILE  -- (Note: Available in Hive 4.0.0 and later)</span><br><span class="line">| INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname</span><br></pre></td></tr></table></figure><ul><li><p><strong>SEQUENCEFILE:</strong> Hadoop API提供的一种二进制文件支持，其具有使用方便、可分割、可压缩的特点。 </p></li><li><p><strong>TEXTFILE:</strong> textfile为默认格式，存储方式为行存储。数据不做压缩，磁盘开销大，数据解析开销大。 </p></li><li><p><strong>RCFILE（Record Columnar File）:</strong> 一种行列存储相结合的存储方式。 </p></li><li><p><strong>ORC:</strong> 数据按照行分块，每个块按照列存储，其中每个块都存储有一个索引。hive给出的新格式，属于RCFILE的升级版,性能有大幅度提升,而且数据可以压缩存储,压缩快 快速列存取。 </p><p><strong>ORC这种文件格式可以作用于表或者表的分区，可以通过以下几种方式进行指定：</strong></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">CREATE TABLE ... STORED AS ORC(常用)</span><br><span class="line">ALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORC</span><br><span class="line">SET hive.default.fileformat=Orc</span><br></pre></td></tr></table></figure></li><li><p><strong>PARQUET:</strong> Parquet也是一种行式存储，同时具有很好的压缩性能；同时可以减少大量的表扫描和反序列化的时间。</p></li></ul><p>如果要使用其他格式作为hive表数据的存储格式，则必须先存在一张相同数据的存储格式为TEXTFILE的表table_t0,然后在建表时使用“<code>insert into table table_stored_file_ORC select from table_t0;</code>”创建。或者使用”<code>create table as select from table_t0;</code>”创建。</p><p>相同数据，分别以TextFile、SequenceFile、RcFile、ORC、Parquet存储的比较。</p><ol><li><p>源文件大小</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop data]$ hdfs dfs -du -s -h /data/page_views.dat</span><br><span class="line">18.1 M  18.1 M  /data/page_views.dat</span><br></pre></td></tr></table></figure></li><li><p>TextFile</p><ul><li><p>建表&amp;加载数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_text( </span><br><span class="line">track_time <span class="keyword">string</span>, </span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>, </span><br><span class="line">session_id <span class="keyword">string</span>, </span><br><span class="line">referer <span class="keyword">string</span>, </span><br><span class="line">ip <span class="keyword">string</span>, </span><br><span class="line">end_user_id <span class="keyword">string</span>, </span><br><span class="line">city_id <span class="keyword">string</span> </span><br><span class="line">)<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> TEXTFilE;</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载导入</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">'/data/page_views.dat'</span> <span class="keyword">into</span> <span class="keyword">table</span> page_views_text;</span><br></pre></td></tr></table></figure></li><li><p>表大小</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">18.1 M  18.1 M  /user/hive/warehouse/store_format.db/page_views_text</span><br></pre></td></tr></table></figure></li><li><p>查询时读取数据量大小(单位字节，下面都用这个SQL测试)</p><p><code>select count(1) from page_views_text where session_id=&quot;B58W48U4WKZCJ5D1T3Z9ZY88RU7QA7B1&quot;;</code> </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 19024045</span><br></pre></td></tr></table></figure></li></ul></li><li><p>SequenceFile</p><ul><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_seq( </span><br><span class="line">track_time <span class="keyword">string</span>, </span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>, </span><br><span class="line">session_id <span class="keyword">string</span>, </span><br><span class="line">referer <span class="keyword">string</span>, </span><br><span class="line">ip <span class="keyword">string</span>, </span><br><span class="line">end_user_id <span class="keyword">string</span>, </span><br><span class="line">city_id <span class="keyword">string</span> </span><br><span class="line">)<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> SequenceFile;</span><br><span class="line"></span><br><span class="line"><span class="comment">#查询导入</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> page_views_Seq  <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text;</span><br></pre></td></tr></table></figure></li><li><p>表大小</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">19.6 M  19.6 M  /user/hive/warehouse/store_format.db/page_views_seq</span><br></pre></td></tr></table></figure></li><li><p>查询时读取数据量大小(字节)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 61513817</span><br></pre></td></tr></table></figure></li></ul></li><li><p>RcFile</p><ul><li><p>建表(CTAS)</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_rcfile</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> RcFile</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text</span><br></pre></td></tr></table></figure></li><li><p>表大小</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">17.9 M  17.9 M  /user/hive/warehouse/store_format.db/page_views_rcfile</span><br></pre></td></tr></table></figure></li><li><p>查询时读取数据量大小(字节)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 3726738</span><br></pre></td></tr></table></figure></li></ul></li><li><p>ORC</p><ul><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_orc</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> ORC</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text</span><br></pre></td></tr></table></figure></li><li><p>表大小</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 1258828</span><br></pre></td></tr></table></figure></li><li><p>查询时读取数据量大小(字节)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2.8 M  2.8 M  /user/hive/warehouse/store_format.db/page_views_orc</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Parquet</p><ul><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_par</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> Parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text</span><br></pre></td></tr></table></figure></li><li><p>表大小</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">13.1 M  13.1 M  /user/hive/warehouse/store_format.db/page_views_par</span><br></pre></td></tr></table></figure></li><li><p>查询时读取数据量大小(字节)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 2688348</span><br></pre></td></tr></table></figure></li></ul></li></ol><p>不同的存储格式不仅表现在存储空间上的不同，对于数据的查询，效率也不一样。因为对于不同的存储格式，执行相同的查询操作，他们访问的数据量大小是不一样的。</p><p>不同格式表存储大小的比较</p><p><img src="https://yerias.github.io/hive_img/%E8%A1%A8%E5%A4%A7%E5%B0%8F%E6%AF%94%E8%BE%83.jpg" alt="表大小比较"></p><p>不同格式表读取数据量比较</p><p><img src="https://yerias.github.io/hive_img/%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%87%8F%E6%AF%94%E8%BE%83.jpg" alt="读取数据量比较"></p><h2 id="存储格式的应用"><a href="#存储格式的应用" class="headerlink" title="存储格式的应用"></a>存储格式的应用</h2><p>原文件还是上面的那个</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop data]$ hdfs dfs -du -s -h /data/page_views.dat</span><br><span class="line">18.1 M  18.1 M  /data/page_views.dat</span><br></pre></td></tr></table></figure><ol><li><p>ORC+Zlip结合</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_orc_zlib</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> ORC </span><br><span class="line">TBLPROPERTIES(<span class="string">"orc.compress"</span>=<span class="string">"ZLIB"</span>)</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text;</span><br></pre></td></tr></table></figure><p>用ORC+Zlip之后的文件为2.8M</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2.8 M  2.8 M  /user/hive/warehouse/store_format.db/page_views_orc_zlib</span><br></pre></td></tr></table></figure></li><li><p>Parquet+gzip结合</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> parquet.compression=gzip;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_parquet_gzip</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> PARQUET </span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text;</span><br></pre></td></tr></table></figure><p>用Parquet+gzip之后的文件为3.9M</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">3.9 M  3.9 M  /user/hive/warehouse/store_format.db/page_views_parquet_gzip</span><br></pre></td></tr></table></figure></li><li><p>Parquet+Lzo结合</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.exec.compress.output=<span class="literal">true</span>;  </span><br><span class="line"><span class="keyword">SET</span> mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.lzopCodec;</span><br><span class="line"><span class="keyword">SET</span> mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec; </span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_parquet_lzo <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> PARQUET</span><br><span class="line">TBLPROPERTIES(<span class="string">"parquet.compression"</span>=<span class="string">"lzo"</span>)</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text;</span><br></pre></td></tr></table></figure><p>用Parquet+Lzo(未建立索引)之后的文件为6.2M</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">6.2 M  6.2 M  /user/hive/warehouse/store_format.db/page_views_parquet_lzo</span><br></pre></td></tr></table></figure><p>建立索引(表好像没啥用)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar $HADOOP_HOME/share/hadoop/common/hadoop-lzo.jar com.hadoop.compression.lzo.LzoIndexer /user/hive/warehouse/store_format.db/page_views_parquet_lzo</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HIVE Skewed Table&amp;List Bucketing</title>
      <link href="/2018/11/11/hive/11/"/>
      <url>/2018/11/11/hive/11/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>HIVE Skewed Table</li><li>Skewed Join Optimization(最优化)</li><li>Basic Partitioning</li><li>List Bucketing</li><li>Skewed Table vs List Bucketing Table</li><li>List Bucketing Validation</li></ol><h2 id="HIVE-Skewed-Table"><a href="#HIVE-Skewed-Table" class="headerlink" title="HIVE Skewed Table"></a>HIVE Skewed Table</h2><p>Skewed Table可用于提高一个或多个列具有偏斜值的表的性能。通过指定经常出现的值（严重偏斜），Hive会自动将它们拆分成单独的文件（或在列表存储的情况下为目录），并在查询过程中考虑到这一事实，以便它可以跳过或包括整个文件（或在列表存储的情况下为目录）。<strong>可以在表创建过程中在每个表级别上指定。</strong></p><p>若是指定了STORED AS DIRECTORIES，也就是使用列表桶（ListBucketing），hive会对倾斜的值建立子目录，查询会更加得到优化。</p><p>下面的示例显示具有三个偏斜值的一列，还可以选择使用STORED AS DIRECTORIES子句来指定列表存储。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> list_bucket_single (<span class="keyword">key</span> <span class="keyword">STRING</span>, <span class="keyword">value</span> <span class="keyword">STRING</span>)</span><br><span class="line">SKEWED <span class="keyword">BY</span> (<span class="keyword">key</span>) <span class="keyword">ON</span> (<span class="number">1</span>,<span class="number">5</span>,<span class="number">6</span>) [<span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES];</span><br></pre></td></tr></table></figure><p>这是一个带有两个倾斜列的表的示例。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> list_bucket_multiple (col1 <span class="keyword">STRING</span>, col2 <span class="built_in">int</span>, col3 <span class="keyword">STRING</span>)</span><br><span class="line">SKEWED <span class="keyword">BY</span> (col1, col2) <span class="keyword">ON</span> ((<span class="string">'s1'</span>,<span class="number">1</span>), (<span class="string">'s3'</span>,<span class="number">3</span>), (<span class="string">'s13'</span>,<span class="number">13</span>), (<span class="string">'s78'</span>,<span class="number">78</span>)) [<span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES];</span><br></pre></td></tr></table></figure><p>可以使用alter table语句来对已创建的表修改倾斜信息。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name SKEWED <span class="keyword">BY</span> (col_name1, col_name2, ...) <span class="keyword">ON</span> ([(col_name1_value, col_name2_value, ...) [, (col_name1_value, col_name2_value), ...][<span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES];</span><br></pre></td></tr></table></figure><p><code>STORED AS DIRECTORIES</code>选项确定倾斜表是否使用列表存储功能，该功能为倾斜值创建子目录。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">NOT</span> SKEWED;</span><br></pre></td></tr></table></figure><p><code>NOT SKEWED</code>选项使表不倾斜，并关闭列表存储功能（因为列表存储表始终是倾斜的）。这会影响在ALTER语句之后创建的分区，但对在ALTER语句之前创建的分区没有影响。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">NOT</span> <span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES;</span><br></pre></td></tr></table></figure><p><code>NOT STORED</code>会关闭列表存储功能，但是表仍然歪斜。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">SET</span> SKEWED LOCATION (col_name1=<span class="string">"location1"</span> [, col_name2=<span class="string">"location2"</span>, ...] );</span><br></pre></td></tr></table></figure><p>修改list bucketing倾斜值的存储位置映射。</p><h2 id="Skewed-Join-Optimization-最优化"><a href="#Skewed-Join-Optimization-最优化" class="headerlink" title="Skewed Join Optimization(最优化)"></a>Skewed Join Optimization(最优化)</h2><p>两个大数据表的连接由一组MapReduce作业完成，它们首先根据连接键对表进行排序，然后将它们连接起来，mapper将相同键的行发送到同一个reduce。<br>假设表A id字段有值1，2，3，4，并且表B也含有id列，含有值1，2，3。我们使用如下语句来进行连接。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> A.id <span class="keyword">from</span> A <span class="keyword">join</span> B <span class="keyword">on</span> A.id = B.id</span><br></pre></td></tr></table></figure><p>将会有一组mappers读这两个表并基于连接键id发送到reducers，假设id=1的行分发到Reducer R1，id=2的分发到R2等等，这些reducer对A和B进行交叉连接，R4从A得到id=4的所有行，但是不会产生任何结果。</p><p>现在我们假定A在id=1上倾斜，这样R2和R3将会很快完成但是R1会执行很长时间，因此成为job的瓶颈。若是用户知道这些倾斜信息，这种瓶颈可以使用如下方法人工避免：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> A.id <span class="keyword">from</span> A <span class="keyword">join</span> B <span class="keyword">on</span> A.id = B.id <span class="keyword">where</span> A.id &lt;&gt; <span class="number">1</span>;</span><br><span class="line"><span class="keyword">select</span> A.id <span class="keyword">from</span> A <span class="keyword">join</span> B <span class="keyword">on</span> A.id = B.id <span class="keyword">where</span> A.id = <span class="number">1</span> <span class="keyword">and</span> B.id = <span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>第一个查询没有倾斜数据将会很快的完成，如果我们假定表B中只有少量的B.id=1的行，能够直接加载到内存中，通过将B的数据存储到内存中的哈希表中，join将会高效的完成，因此可以再mappper端进行连接，而不用reduce，效率会高很多，最后合并结果。</p><p>优点：</p><ul><li>如果少量的倾斜键占了很大一部分数据，它们将不会成为瓶颈。</li></ul><p>缺点：</p><ul><li><p>表A和表B需要分别读和处理两次；</p></li><li><p>结果需要合并；</p></li><li><p>需要人工的处理倾斜数据。</p></li></ul><p>hive为了避免上面的操作，在处理数据是对倾斜值进行特殊处理，首先读表B并且存储B.id=1的数据到内存的哈希表中，运行一组mappers来读取表A并做以下操作：</p><ol><li>若id为1，使用B的id=1的哈希表来计算结果</li><li>对于其他值，发送到reducer端来join，这个reduce也会从B的mapper中得到对应需要连接的数据。</li></ol><p>使用这种方法，最终我们只读取B两次，并且A中的倾斜数据在mapper中进行连接，不会被发送到reducer，其他的键值通过map/reduce。</p><p>假设B的行很少，而键在A中倾斜。因此可以将这些行加载到内存中。<strong>若是使用ListBucketing对倾斜值单独存储，会有更好的性能。在读倾斜的数据到内存中时可以指定到倾斜目录下的数据。</strong></p><h2 id="Basic-Partitioning"><a href="#Basic-Partitioning" class="headerlink" title="Basic Partitioning"></a>Basic Partitioning</h2><p>有如下问题：存在许多表是这种格式</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> T(a, b, c, ....., x) partitioned <span class="keyword">by</span> (ds);</span><br></pre></td></tr></table></figure><p>但是以下查询需要更加高效(这不扯蛋吗，有分区的查询条件不是分区)：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> x = <span class="number">10</span>;</span><br></pre></td></tr></table></figure><p>字段x中含有倾斜数据，一般情况下x的值中大约有1000个值有重度倾斜，其他值基数很小，当然，每天倾斜的X的值可能改变，上述要求可以通过以下方式解决：</p><p>为值“ x”创建一个分区。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> T(a,b,c, .......) partitioned <span class="keyword">by</span> (ds, x)</span><br></pre></td></tr></table></figure><p>优点</p><ul><li>现有的Hive就足够了。</li></ul><p>缺点</p><ul><li>HDFS可伸缩性：HDFS中的文件数量增加。</li><li>HDFS可伸缩性：HDFS中的中间文件数量增加。例如，如果有1000个映射器和1000个分区，并且每个映射器每个键至少获得1行，我们最终将创建100万个中间文件。</li><li>Metastore的可伸缩性：Metastore会随着分区数量的增长而扩展。</li></ul><h2 id="List-Bucketing"><a href="#List-Bucketing" class="headerlink" title="List Bucketing"></a>List Bucketing</h2><p>上边方法提到将含有倾斜值得列作为分区存储，但是可能产生大量的目录，为什么不把列值不倾斜的放在一起呢，将每个倾斜的值单独存放一个目录，于是有了List Bucketing。</p><p>这个映射在表或分区级别的Metastore中维护。倾斜键的列表存储在表级别中，这个列表可以由客户端周期的提供，并且在新的分区载入时可以被更新。</p><p>如下例子，一个表含有一个x字段倾斜值的列表：6，20，30，40。当一个新的分区载入时，它会创建5个目录（4个目录对应x的4个倾斜值，另外一个目录是其余值）。这个表的数据被分成了5个部分：6，20，30，40，others。这跟上一节介绍的分桶表类似，桶的个数决定了文件的个数。倾斜键的整个列表存储在每个表或者分区中。</p><p>当使用一下查询时，hive编译器会仅仅使用x=30对应的目录去运行map-reduce。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> ds = <span class="string">'2012-04-15'</span> <span class="keyword">and</span> x = <span class="number">30</span>;</span><br></pre></td></tr></table></figure><p>若是查询是x=50，则会使用x=others对应的目录去运行map-reduce作业。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> ds = <span class="string">'2012-04-15'</span> <span class="keyword">and</span> x = <span class="number">50</span>;</span><br></pre></td></tr></table></figure><p>这种方法在一下条件下是很有效的：</p><ol><li><strong>每个分区的倾斜键占总数据的一大部分</strong>。在上边的例子中，如果倾斜的键（6，20，30，40）只占一小部分数据（比如20%）,那么在查询x=50时依然需要扫描80%的数据。</li><li><strong>每个分区的倾斜键数量非常少</strong>，因为这个倾斜值列表存在在元数据库中，在元数据库中为每个分区存储100w个倾斜键是没有意义的。</li></ol><p>这种方法也可被扩展到含有多个列产生的倾斜键，例如我们想优化一下查询</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> x = <span class="number">10</span> <span class="keyword">and</span> y = <span class="string">'b'</span>;</span><br></pre></td></tr></table></figure><p>扩展以上的方法，对于（x，y）每个倾斜值,也按照上边方式单独存储，因此元数据库会有以下映射： </p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">(10, 'a') <span class="comment">--&gt; 1, (10, 'b') --&gt; 2, (20, 'c') --&gt; 3, (others) --&gt; 4.</span></span><br></pre></td></tr></table></figure><p>因此可直接找到2对应的目录，减少要处理的数据。</p><p>同时以下查询也会有一定程度的优化：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> x = <span class="number">10</span>; </span><br><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> y = <span class="string">'b'</span>;</span><br></pre></td></tr></table></figure><p>以上两个语句在执行的过程中会裁剪掉一部分数据，例如，对x=10的查询hive编译器可以裁剪掉 ( 20 , c ) 对应的文件，对于 y = ‘b’，( 10 , ‘a’ )  和 ( 20 , ‘c’ ) 对应的文件会被裁剪掉，一定程度能够减少扫描的数据量。</p><p>这种方法不适用于以下场景：</p><ul><li>倾斜键数目非常多，元数据规模问题</li><li>许多情况下，倾斜键由多个列组成，但是在查询中，没有使用到倾斜键中的那些列。</li></ul><h2 id="Skewed-Table-vs-List-Bucketing-Table"><a href="#Skewed-Table-vs-List-Bucketing-Table" class="headerlink" title="Skewed Table vs List Bucketing Table"></a>Skewed Table vs List Bucketing Table</h2><ul><li>Skewed Table是一个表它含有倾斜的信息。</li><li>List Bucketing Table是Skewed Table，此外，它告诉hive使用列表桶的特点：为倾斜值创建子目录。</li></ul><p>以下说明两者的存储区别：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t1 (x <span class="keyword">string</span>) skewed <span class="keyword">by</span> (x) <span class="keyword">on</span> (‘a’, ‘b’) partitioned <span class="keyword">by</span> dt location ‘/<span class="keyword">user</span>/hive/warehouse/t1’;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t2 (x <span class="keyword">string</span>) skewed <span class="keyword">by</span> (x) <span class="keyword">on</span> (‘a’, ‘b’) <span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES partitioned <span class="keyword">by</span> dt location ‘/<span class="keyword">user</span>/hive/warehouse/t2’ ;</span><br></pre></td></tr></table></figure><p>两者存储的形式如下所示：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">/user/hive/warehouse/t1/dt=something/data.txt</span><br><span class="line">/user/hive/warehouse/t2/dt=something/x=a/data.txt</span><br><span class="line">/user/hive/warehouse/t2/dt=something/x=b/data.txt</span><br><span class="line">/user/hive/warehouse/t2/dt=something/default/data.txt</span><br></pre></td></tr></table></figure><h2 id="List-Bucketing-Validation"><a href="#List-Bucketing-Validation" class="headerlink" title="List Bucketing Validation"></a>List Bucketing Validation</h2><p>由于列表桶的子目录特点，它不能够与一些特征共存。</p><p>DDL</p><p>列表桶与以下共存会抛出编译错误：</p><ul><li>normal bucketing (clustered by, tablesample, etc.)</li><li>external table</li><li>“ load data …”</li><li>CTAS (Create Table As Select) queries</li></ul><p>DML</p><p>与一下DML操作共存也会跳出错误：</p><ul><li>“ insert into ”</li><li>normal bucketing (clustered by, tablesample, etc.)</li><li>external table</li><li>non-RCfile due to merge</li><li>non-partitioned table</li></ul><hr><p>参考文献：</p><p>1.<a href="https://cwiki.apache.org/confluence/display/Hive/ListBucketing" target="_blank" rel="noopener">HIVE ListBucketing</a><br>2.<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-SkewedTables" target="_blank" rel="noopener">HIVE LanguageManualDDL-SkewedTables</a><br>3.<a href="https://cwiki.apache.org/confluence/display/Hive/Skewed+Join+Optimization" target="_blank" rel="noopener">HIVE Skewed Join Optimization</a></p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大表Join大表&amp;大表Join小表&amp;group By解决数据倾斜</title>
      <link href="/2018/11/10/hive/10/"/>
      <url>/2018/11/10/hive/10/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>大表Join大表</li><li>大表Join小表</li><li>group By解决</li></ol><h2 id="大表Join大表"><a href="#大表Join大表" class="headerlink" title="大表Join大表"></a>大表Join大表</h2><h3 id="思路一：SMBJoin"><a href="#思路一：SMBJoin" class="headerlink" title="思路一：SMBJoin"></a>思路一：SMBJoin</h3><p>smb是sort  merge bucket操作，首先进行排序，继而合并，然后放到所对应的bucket中去，bucket是hive中和分区表类似的技术，就是按照key进行hash，相同的hash值都放到相同的buck中去。在进行两个表联合的时候。我们首先进行分桶，在join会大幅度的对性能进行优化。也就是说，在进行联合的时候，是bukect中的一小部分和bukect中的一小部分进行联合，table联合都是等值连接，相同的key都放到了同一个bucket中去了，那么在联合的时候就会大幅度的减小无关项的扫描。</p><ol><li><p>设置参数</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join.noconditionaltask=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li><li><p>两个表的bucket数量相等</p></li><li><p>Bucket列、Join列、Sort列、Skewed列为相同的字段</p></li><li><p>必须是应用在bucket mapjoin 的场景中</p></li><li><p>注意点</p><p>hive并不检查两个join的表是否已经做好bucket且sorted，需要用户自己去保证join的表，否则可能数据不正确。有两个办法</p><ul><li><p>hive.enforce.sorting 设置为true</p></li><li><p>手动生成符合条件的数据，通过在sql中用distributed c1 sort by c1 或者 cluster by c1，表创建时必须是CLUSTERED且SORTED，如下</p></li><li><p>创建Skewed Table提高有一个或多个列有倾斜值的表的性能，例如：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_smb_2(<span class="keyword">mid</span> <span class="keyword">string</span>,age_id <span class="keyword">string</span>)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span>(<span class="keyword">mid</span>) SORTED <span class="keyword">BY</span>(<span class="keyword">mid</span>) <span class="keyword">INTO</span> <span class="number">500</span> BUCKETS</span><br><span class="line">SKEWED <span class="keyword">BY</span>(<span class="keyword">mid</span>) <span class="keyword">on</span> (<span class="number">001</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>案例</p><ol><li><p>设置先关参数</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join.noconditionaltask=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li><li><p>创建桶表</p><p>user表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_info_bucket(userid <span class="keyword">string</span>,uname <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span>(userid) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line">SKEWED <span class="keyword">BY</span>(userid) <span class="keyword">on</span> (<span class="number">001</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line">STORED AS DIRECTORIES</span><br></pre></td></tr></table></figure><p>domain表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> domain_info_bucket(userid <span class="keyword">string</span>,domainid <span class="keyword">string</span>,<span class="keyword">domain</span> <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span> (userid) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line">SKEWED <span class="keyword">BY</span>(userid) <span class="keyword">on</span> (<span class="number">001</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line">STORED AS DIRECTORIES</span><br></pre></td></tr></table></figure></li><li><p>分别倒入数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> user_info_bucket <span class="keyword">select</span> userid ,uname <span class="keyword">from</span> <span class="keyword">user</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> domain_info_bucket <span class="keyword">select</span> userid ,domainid,<span class="keyword">domain</span> <span class="keyword">from</span> doamin</span><br></pre></td></tr></table></figure></li><li><p>查询</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> user_info_bucket u  <span class="keyword">join</span> domain_info_bucket d <span class="keyword">on</span>(u.userid==d.userid)</span><br></pre></td></tr></table></figure></li></ol></li></ol><h3 id="思路二：一分为二"><a href="#思路二：一分为二" class="headerlink" title="思路二：一分为二"></a>思路二：一分为二</h3><p>选择临时表的方式，将数据一分为二，把倾斜的key，和不倾斜的key分开处理，不倾斜的正常join，倾斜的根据情况选择mapjoin或加盐处理，最后结果union all结果</p><p>user表为用户基本表，domain为用户访问域名的宽表</p><p><strong>注意：</strong>我们其实隐含使用到了mapjoin，hive中的参数为<code>set hive.auto.convert.join=true;</code>，自动开启，默认25M，不能超过1G。</p><ol start="0"><li><p>创建中间表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tmp_table(userid <span class="keyword">string</span>,uname <span class="keyword">string</span>)</span><br><span class="line">SKEWED <span class="keyword">BY</span>(userid) <span class="keyword">on</span> (<span class="number">001</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line">STORED AS DIRECTORIES</span><br></pre></td></tr></table></figure></li><li><p><code>count(*)</code>出符合倾斜条件的数据存入中间表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> tmp_table</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">d.userid,u.uname</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span> userid</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span> </span><br><span class="line">userid,</span><br><span class="line"><span class="keyword">count</span>(userid) u_cunt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line"><span class="keyword">domain</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">userid) t</span><br><span class="line"><span class="keyword">where</span> u_cunt&gt;<span class="number">100</span>) d</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span></span><br><span class="line">(<span class="keyword">select</span> </span><br><span class="line">userid,uname</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line"><span class="keyword">user</span>) u</span><br><span class="line"><span class="keyword">on</span> d.userid = u.userid;</span><br></pre></td></tr></table></figure></li><li><p>一分为二，分别查询中出结果，再union all</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">u1.userid,u1.uname,d1.domain</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span> </span><br><span class="line">userid,uname</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line"><span class="keyword">user</span>) u1</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">(<span class="keyword">select</span> </span><br><span class="line">d.userid,d.domain</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line"><span class="keyword">domain</span> d</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span></span><br><span class="line">tmp_table t</span><br><span class="line"><span class="keyword">on</span> </span><br><span class="line">d.userid = t.userid</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">t.userid <span class="keyword">is</span> <span class="literal">null</span>) d1</span><br><span class="line"><span class="keyword">on</span> u1.userid = d1.userid</span><br><span class="line"></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">u2.userid,u2.uname,d2.domain</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(<span class="keyword">select</span> </span><br><span class="line">userid,uname</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line"><span class="keyword">user</span>) u2</span><br><span class="line"><span class="keyword">join</span> </span><br><span class="line">(<span class="keyword">select</span> </span><br><span class="line">d.userid,d.domain</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line"><span class="keyword">domain</span> d</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span></span><br><span class="line">tmp_table t</span><br><span class="line"><span class="keyword">on</span> </span><br><span class="line">d.userid = t.userid</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">t.userid <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>) d2</span><br><span class="line"><span class="keyword">on</span> u2.userid = d2.userid</span><br></pre></td></tr></table></figure></li></ol><h2 id="大表Join小表"><a href="#大表Join小表" class="headerlink" title="大表Join小表"></a>大表Join小表</h2><h3 id="思路：MapJoin"><a href="#思路：MapJoin" class="headerlink" title="思路：MapJoin"></a>思路：MapJoin</h3><p>大表Join小表很好解决，把小表放进内存，大表再去匹配即可。</p><p>思路：</p><ol><li><p>开启MapJoin</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li><li><p>调整MapJoin小表大小，默认25M(调整为可以容忍的大小)</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize</span><br></pre></td></tr></table></figure></li><li><p>如果是MR，小表放进Map，大表进入Mapper匹配Map(使用对象存储结果)</p></li></ol><h2 id="group-By解决"><a href="#group-By解决" class="headerlink" title="group By解决"></a>group By解决</h2><h3 id="思路：加盐去盐"><a href="#思路：加盐去盐" class="headerlink" title="思路：加盐去盐"></a>思路：加盐去盐</h3><ol><li><p>开启数据倾斜时的负载均衡</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata=<span class="literal">true</span></span><br></pre></td></tr></table></figure></li><li><p>对groupby的key加盐去盐</p><p>开启上面这个参数，hive会自动拆解成两个MR，加盐去盐，最终输出结果</p><p>MR需要写两个MR，一个对key加盐，一个对key去盐</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HIVE调优(1)</title>
      <link href="/2018/11/09/hive/9/"/>
      <url>/2018/11/09/hive/9/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>Fetch</li><li>本地模式</li><li>JVM重用</li><li>map数量</li><li>reduce数量</li><li>推测执行</li></ol><h2 id="Fetch"><a href="#Fetch" class="headerlink" title="Fetch"></a>Fetch</h2><p>通过修改<code>hive.fetch.task.conversion</code>参数可以让一些select查询可以转换为单个获取任务，不需要执行MapReduce任务，从而最小化延迟。</p><p>目前的版本中支持none、minimal和more</p><ul><li><code>none</code>: 是禁用这一特性</li><li><code>minimal</code>: 允许使用<code>SELECT *</code>、<code>FILTER on partition columns (WHERE and HAVING clauses)</code>、<code>LIMIT only</code></li><li><code>more</code>: 最大程度的允许使用 <code>SELECT</code>, <code>FILTER</code>, <code>LIMIT only (including TABLESAMPLE, virtual columns)、where</code></li></ul><p>当前版本默认使用more</p><h2 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h2><p><code>hive.exec.mode.local.auto</code> 参数决定hive是否允许使用本地化，默认<code>hive.exec.mode.local.auto=false</code> 没有启用本地化</p><p><code>hive.exec.mode.local.auto.inputbytes.max</code> 参数规定了使用本地化处理的最大的文件字节数，默认是128M</p><p><code>hive.exec.mode.local.auto.input.files.max</code> 参数规定了使用本地化处理的最大文件数，默认是4个</p><h2 id="JVM重用"><a href="#JVM重用" class="headerlink" title="JVM重用"></a>JVM重用</h2><p>在目前使用的版本中，<code>mapreduce.job.jvm.numtasks</code> 参数可以控制Java虚拟机的回收，由于<code>mapTask</code>或者<code>reduceTask</code>都是进程，需要启用JVM，作业运行结束了关闭JVM，使用这个参数控制JVM运行完作业不关机，继续执行作业。默认是1个。</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.jvm.numtasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>How many tasks to run per jvm. If set to -1, there is no limit.  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="map数量"><a href="#map数量" class="headerlink" title="map数量"></a>map数量</h2><p><code>mapTask</code>数量由输入的文件大小、文件数和输入的文件产生多少个block决定。</p><p>那么我们如何考虑map的数量呢?</p><p>理论上讲<code>mapTask</code>越多Map作业的并行度越高，但是耗费的时间和资源也越多，map、reduce作业都是进程级别。</p><p>场景：128M的数据只有一两个列的数据记录数有几千万个，必须拆开用NTILE(n)==&gt; 改变map任务的数量</p><h2 id="reduce数量"><a href="#reduce数量" class="headerlink" title="reduce数量"></a>reduce数量</h2><p><code>mapred.reduce.tasks=-1</code> 参数决定每个作业的默认数量。通常设置为接近可用主机数量的素数。通过将此属性设置为-1,Hive将自动计算出还原器的数量。默认是-1，即自动计算</p><p><code>hive.exec.reducers.bytes.per.reducer=256M</code> 参数决定了reduce最大的字节数，在Hive 0.14.0及以后的版本中，默认为256 MB，也就是说，如果输入大小为1 GB，那么将使用4个reduce。</p><p><code>hive.exec.reducers.max=1099</code> 参数决定最大可以使用的reduce数量，如果<code>mapred.reduce.tasks</code> 参数为-1，即自动计算reduce数量，那么Hive将使用这个参数作为最大的reduce数量，自动确定reduce的数量。</p><p>计算reducer数的公式很简单N=min( <code>hive.exec.reducers.max</code> ，总输入数据量/ <code>hive.exec.reducers.bytes.per.reducer</code> )</p><p>reduce的数量决定最终文件输出的数量<br>思路：reduce数量越多，小文件越多，reduce数量越少，文件大耗费的时间多，最终在reduce文件的大小和需要消耗的时间取个折中。 如果没有reduce，那么map的数据个数决定了输出文件个数 。</p><p>Spark3.0 自动适配</p><h2 id="推测执行"><a href="#推测执行" class="headerlink" title="推测执行"></a>推测执行</h2><ol><li><p>作业完成时间取决于最慢的任务完成时间</p><p>一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。</p><p>思考：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？</p></li><li><p>推测执行机制</p><p>发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果。</p></li><li><p>执行推测任务的前提条件</p></li></ol><ul><li><p>每个Task只能有一个备份任务</p></li><li><p>当前 Job 已完成的 Task 必须不小于0.05（5%）</p></li><li><p>开启推测执行参数设置。mapred-site.xml文件中默认是打开的。</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some map tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some reduce tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul><ol start="4"><li>不能启用推测执行机制情况</li></ol><ul><li><p>任务间存在严重的数据倾斜，数据倾斜跑不过去的，开启多少个推测执行都跑不过去；</p></li><li><p>特殊任务，比如任务向数据库中写数据。</p></li></ul><ol start="5"><li><p>生产建议</p><p>一般生产生禁用此功能，除非特殊场景直接命令开启</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windowing functions&amp;The OVER clause&amp;Analytics functions</title>
      <link href="/2018/11/08/hive/8/"/>
      <url>/2018/11/08/hive/8/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>The OVER clause</li><li>Analytics functions</li><li>Windowing functions</li></ol><h2 id="The-OVER-clause"><a href="#The-OVER-clause" class="headerlink" title="The OVER clause"></a>The OVER clause</h2><p>聚合函数是将多行数据按照规则聚合为一行，比如count()、sum()、min()、max()、avg()</p><p>窗口函数是在做聚合的基础上，要返回的数据不仅仅是一行</p><p>窗口函数是在窗口的基础上做统计分析，对其所作用的窗口中的每一条记录输出一条结果</p><p>窗口函数借助于over() 函数开窗</p><p>窗口函数的标准聚合函数同样包括count()、sum()、min()、max()、avg()</p><p>窗口可以在一个窗口子句中单独定义。窗口规范支持以下格式:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">(ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING</span><br></pre></td></tr></table></figure><p>窗口有以上三种定义方式，分别是<code>从某行之后到某行</code>、<code>某行之后到某行之前</code>、<code>某行到某行之前</code></p><p><code>PRECEDING</code>: 往前<br><code>FOLLOWING</code>: 往后<br><code>CURRENT ROW</code>: 当前行<br><code>UNBOUNDED</code>: 起点，<code>UNBOUNDED PRECEDING</code> 表示从前面的起点， <code>UNBOUNDED FOLLOWING</code>: 表示到后面的终点</p><ol><li><p>准备数据</p><p>window01.txt </p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ruozedata,2019-04-10,1</span><br><span class="line">ruozedata,2019-04-11,5</span><br><span class="line">ruozedata,2019-04-12,7</span><br><span class="line">ruozedata,2019-04-13,3</span><br><span class="line">ruozedata,2019-04-14,2</span><br><span class="line">ruozedata,2019-04-15,4</span><br><span class="line">ruozedata,2019-04-16,4</span><br></pre></td></tr></table></figure></li><li><p>创建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span>  <span class="keyword">database</span> window_over;</span><br><span class="line"><span class="keyword">use</span> window_over</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> window01(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">String</span>,</span><br><span class="line"><span class="built_in">date</span> <span class="keyword">String</span>,</span><br><span class="line">grade <span class="built_in">Int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure></li><li><p>导入数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/window/window01.txt'</span> <span class="keyword">into</span>  <span class="keyword">table</span>  window01;</span><br></pre></td></tr></table></figure></li><li><p>查询</p><p>问题：窗口到底怎么开? </p><p>核心：从什么地方开始到什么地方结束</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line"><span class="keyword">name</span>,</span><br><span class="line"><span class="built_in">date</span>,</span><br><span class="line">grade,</span><br><span class="line"><span class="keyword">sum</span>(grade) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> grade <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> g1,//第一行到当前行的grade求<span class="keyword">sum</span>()</span><br><span class="line"><span class="keyword">sum</span>(grade) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> grade <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">3</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> g2,//第三行到当前行的grade求<span class="keyword">sum</span>()</span><br><span class="line"><span class="keyword">sum</span>(grade) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> grade <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">3</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="number">1</span> <span class="keyword">following</span>) <span class="keyword">as</span> g3,//第三行到后一行的grade求<span class="keyword">sum</span>()</span><br><span class="line"><span class="keyword">sum</span>(grade) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> grade <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="keyword">unbounded</span> <span class="keyword">FOLLOWING</span>) <span class="keyword">as</span> g4//当前行到最后一行的grade求<span class="keyword">sum</span>()</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">window01;</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name     |date      |grade|g1|g2|g3|g4|</span><br><span class="line"><span class="comment">---------|----------|-----|--|--|--|--|</span></span><br><span class="line">ruozedata|2019-04-10|    1| 1| 1| 3|26|</span><br><span class="line">ruozedata|2019-04-14|    2| 3| 3| 6|25|</span><br><span class="line">ruozedata|2019-04-13|    3| 6| 6|10|23|</span><br><span class="line">ruozedata|2019-04-16|    4|10|10|14|20|</span><br><span class="line">ruozedata|2019-04-15|    4|14|13|18|16|</span><br><span class="line">ruozedata|2019-04-11|    5|19|16|23|12|</span><br><span class="line">ruozedata|2019-04-12|    7|26|20|20| 7|</span><br></pre></td></tr></table></figure></li></ol><h2 id="Analytics-functions"><a href="#Analytics-functions" class="headerlink" title="Analytics functions"></a>Analytics functions</h2><p>分析函数有RANK、ROW_NUMBER、DENSE_RANK、CUME_DIST、PERCENT_RANK、NTILE</p><p>这些函数可以分为三部分，第一部分是排序相关的RANK、ROW_NUMBER、DENSE_RANK，第二部分是占比相关的CUME_DIST、PERCENT_RANK，第三部分是把表切成指定分区的NTILE</p><h3 id="排序相关"><a href="#排序相关" class="headerlink" title="排序相关"></a>排序相关</h3><ol><li><p>准备数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">gifshow.com,2019-04-10,1</span><br><span class="line">gifshow.com,2019-04-11,5</span><br><span class="line">gifshow.com,2019-04-12,7</span><br><span class="line">gifshow.com,2019-04-13,3</span><br><span class="line">gifshow.com,2019-04-14,2</span><br><span class="line">gifshow.com,2019-04-15,4</span><br><span class="line">gifshow.com,2019-04-16,4</span><br><span class="line">yy.com,2019-04-10,2</span><br><span class="line">yy.com,2019-04-11,3</span><br><span class="line">yy.com,2019-04-12,5</span><br><span class="line">yy.com,2019-04-13,6</span><br><span class="line">yy.com,2019-04-14,3</span><br><span class="line">yy.com,2019-04-15,9</span><br><span class="line">yy.com,2019-04-16,7</span><br></pre></td></tr></table></figure></li><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> traffic(</span><br><span class="line"><span class="keyword">domain</span> <span class="keyword">String</span>,</span><br><span class="line"><span class="built_in">date</span> <span class="keyword">String</span>,</span><br><span class="line">grade <span class="built_in">Int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span>  <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure></li><li><p>导数</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/window/rank.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> traffic;</span><br></pre></td></tr></table></figure></li><li><p>查询</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line"><span class="keyword">domain</span>,</span><br><span class="line"><span class="built_in">date</span>,</span><br><span class="line">grade,</span><br><span class="line"><span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> r1,</span><br><span class="line">ROW_NUMBER() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> r2,</span><br><span class="line"><span class="keyword">DENSE_RANK</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> r3</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">traffic;</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">domain     |date      |grade|r1|r2|r3|</span><br><span class="line"><span class="comment">-----------|----------|-----|--|--|--|</span></span><br><span class="line">gifshow.com|2019-04-10|    1| 1| 1| 1|</span><br><span class="line">gifshow.com|2019-04-14|    2| 2| 2| 2|</span><br><span class="line">gifshow.com|2019-04-13|    3| 3| 3| 3|</span><br><span class="line">gifshow.com|2019-04-16|    4| 4| 4| 4|</span><br><span class="line">gifshow.com|2019-04-15|    4| 4| 5| 4|</span><br><span class="line">gifshow.com|2019-04-11|    5| 6| 6| 5|</span><br><span class="line">gifshow.com|2019-04-12|    7| 7| 7| 6|</span><br><span class="line">yy.com     |2019-04-10|    2| 1| 1| 1|</span><br><span class="line">yy.com     |2019-04-11|    3| 2| 2| 2|</span><br><span class="line">yy.com     |2019-04-14|    3| 2| 3| 2|</span><br><span class="line">yy.com     |2019-04-12|    5| 4| 4| 3|</span><br><span class="line">yy.com     |2019-04-13|    6| 5| 5| 4|</span><br><span class="line">yy.com     |2019-04-16|    7| 6| 6| 5|</span><br><span class="line">yy.com     |2019-04-15|    9| 7| 7| 6|</span><br></pre></td></tr></table></figure></li><li><p>总结</p><p><code>RANK()</code>：分组内生成编号，排名相同相同的名词留空位<br><code>DENSE_RANK()</code>： 分组内生成编号，排名相同相同的名词不留空位<br><code>ROW_NUMBER()</code>： 从1开始，按照排序，生成分组内记录的序号(推介使用)</p></li></ol><h3 id="占比相关"><a href="#占比相关" class="headerlink" title="占比相关"></a>占比相关</h3><p><code>CUME_DIST()</code>: 小于等于当前行值(OVER中order by指定的字段排序)的行数/分组内的总行数<br><code>PERCENT_RANK()</code>: 分组内当前行的rank -1 / 分组内总行数 -1</p><ol><li><p>准备数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">dept01,ruoze,10000</span><br><span class="line">dept01,jepson,20000</span><br><span class="line">dept01,xingxing,30000</span><br><span class="line">dept02,zhangsan,40000</span><br><span class="line">dept02,lisi,50000</span><br></pre></td></tr></table></figure></li><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> window02(</span><br><span class="line">dept <span class="keyword">String</span>,</span><br><span class="line"><span class="keyword">user</span> <span class="keyword">String</span>,</span><br><span class="line">sal <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure></li><li><p>导数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/window/window02.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> window02;</span><br></pre></td></tr></table></figure></li><li><p>查询</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">dept,</span><br><span class="line"><span class="keyword">user</span>,</span><br><span class="line">sal,</span><br><span class="line"><span class="keyword">CUME_DIST</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> sal) cume,</span><br><span class="line"><span class="keyword">PERCENT_RANK</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> sal) <span class="keyword">percent</span></span><br><span class="line"><span class="keyword">from</span> window02;</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">dept  |user    |sal  |cume1|cume2             |percent|</span><br><span class="line"><span class="comment">------|--------|-----|-----|------------------|-------|</span></span><br><span class="line">dept01|ruoze   |10000|  0.2|0.3333333333333333|      0|</span><br><span class="line">dept01|jepson  |20000|  0.4|0.6666666666666666|    0.5|</span><br><span class="line">dept01|xingxing|30000|  0.6|                 1|      1|</span><br><span class="line">dept02|zhangsan|40000|  0.8|               0.5|      0|</span><br><span class="line">dept02|lisi    |50000|    1|                 1|      1|</span><br></pre></td></tr></table></figure></li></ol><h3 id="分区相关"><a href="#分区相关" class="headerlink" title="分区相关"></a>分区相关</h3><p><code>NTILE(num)</code>：将数据按照输入的数成<code>num</code>片，并且记录分片号</p><p>场景：128M的数据只有一两个列的数据记录数有几千万个，必须拆开用<code>NTILE(n)</code>==&gt; 改变map任务的数量</p><ol><li><p>查询</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line"><span class="keyword">domain</span>,</span><br><span class="line"><span class="built_in">date</span>,</span><br><span class="line">grade,</span><br><span class="line">ntile(<span class="number">2</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> n1,</span><br><span class="line">ntile(<span class="number">3</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> n2,</span><br><span class="line">ntile(<span class="number">4</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> n3</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">traffic;</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">domain     |date      |grade|n1|n2|n3|</span><br><span class="line"><span class="comment">-----------|----------|-----|--|--|--|</span></span><br><span class="line">gifshow.com|2019-04-10|    1| 1| 1| 1|</span><br><span class="line">gifshow.com|2019-04-14|    2| 1| 1| 1|</span><br><span class="line">gifshow.com|2019-04-13|    3| 1| 1| 2|</span><br><span class="line">gifshow.com|2019-04-16|    4| 1| 2| 2|</span><br><span class="line">gifshow.com|2019-04-15|    4| 2| 2| 3|</span><br><span class="line">gifshow.com|2019-04-11|    5| 2| 3| 3|</span><br><span class="line">gifshow.com|2019-04-12|    7| 2| 3| 4|</span><br><span class="line">yy.com     |2019-04-10|    2| 1| 1| 1|</span><br><span class="line">yy.com     |2019-04-11|    3| 1| 1| 1|</span><br><span class="line">yy.com     |2019-04-14|    3| 1| 1| 2|</span><br><span class="line">yy.com     |2019-04-12|    5| 1| 2| 2|</span><br><span class="line">yy.com     |2019-04-13|    6| 2| 2| 3|</span><br><span class="line">yy.com     |2019-04-16|    7| 2| 3| 3|</span><br><span class="line">yy.com     |2019-04-15|    9| 2| 3| 4|</span><br></pre></td></tr></table></figure></li></ol><h2 id="Windowing-functions"><a href="#Windowing-functions" class="headerlink" title="Windowing functions"></a>Windowing functions</h2><p><code>LAG(col,n,default)</code>: 窗口内往上取N行的值，如果有default就取default，没有就用null</p><p><code>LEAD(col,n,default)</code>: 窗口内往下取N行的值，如果有default就取default，没有就用null</p><ol><li><p>准备数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie1,2015-04-10 10:00:02,url2</span><br><span class="line">cookie1,2015-04-10 10:00:00,url1</span><br><span class="line">cookie1,2015-04-10 10:03:04,1url3</span><br><span class="line">cookie1,2015-04-10 10:50:05,url6</span><br><span class="line">cookie1,2015-04-10 11:00:00,url7</span><br><span class="line">cookie1,2015-04-10 10:10:00,url4</span><br><span class="line">cookie1,2015-04-10 10:50:01,url5</span><br><span class="line">cookie2,2015-04-10 10:00:02,url22</span><br><span class="line">cookie2,2015-04-10 10:00:00,url11</span><br><span class="line">cookie2,2015-04-10 10:03:04,1url33</span><br><span class="line">cookie2,2015-04-10 10:50:05,url66</span><br><span class="line">cookie2,2015-04-10 11:00:00,url77</span><br><span class="line">cookie2,2015-04-10 10:10:00,url44</span><br><span class="line">cookie2,2015-04-10 10:50:01,url55</span><br></pre></td></tr></table></figure></li><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> window03(</span><br><span class="line">cookid <span class="keyword">String</span>,</span><br><span class="line"><span class="built_in">time</span> <span class="keyword">String</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">String</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure></li><li><p>导数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/window/window03.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> window03;</span><br></pre></td></tr></table></figure></li><li><p>查询</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">cookid,</span><br><span class="line"><span class="built_in">time</span>,</span><br><span class="line"><span class="keyword">url</span>,</span><br><span class="line">lag(<span class="built_in">time</span>,<span class="number">1</span>,<span class="string">'1970-00-00 00:00:00'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> cookid <span class="keyword">order</span> <span class="keyword">by</span> <span class="built_in">time</span>),</span><br><span class="line"><span class="keyword">lead</span>(<span class="built_in">time</span>,<span class="number">2</span>,<span class="string">"null"</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> cookid <span class="keyword">order</span> <span class="keyword">by</span> <span class="built_in">time</span>)</span><br><span class="line"><span class="keyword">from</span> window03;</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookid |time               |url   |lag                |lead               |</span><br><span class="line"><span class="comment">-------|-------------------|------|-------------------|-------------------|</span></span><br><span class="line">cookie1|2015-04-10 10:00:00|url1  |1970-00-00 00:00:00|2015-04-10 10:03:04|</span><br><span class="line">cookie1|2015-04-10 10:00:02|url2  |2015-04-10 10:00:00|2015-04-10 10:10:00|</span><br><span class="line">cookie1|2015-04-10 10:03:04|1url3 |2015-04-10 10:00:02|2015-04-10 10:50:01|</span><br><span class="line">cookie1|2015-04-10 10:10:00|url4  |2015-04-10 10:03:04|2015-04-10 10:50:05|</span><br><span class="line">cookie1|2015-04-10 10:50:01|url5  |2015-04-10 10:10:00|2015-04-10 11:00:00|</span><br><span class="line">cookie1|2015-04-10 10:50:05|url6  |2015-04-10 10:50:01|null               |</span><br><span class="line">cookie1|2015-04-10 11:00:00|url7  |2015-04-10 10:50:05|null               |</span><br><span class="line">cookie2|2015-04-10 10:00:00|url11 |1970-00-00 00:00:00|2015-04-10 10:03:04|</span><br><span class="line">cookie2|2015-04-10 10:00:02|url22 |2015-04-10 10:00:00|2015-04-10 10:10:00|</span><br><span class="line">cookie2|2015-04-10 10:03:04|1url33|2015-04-10 10:00:02|2015-04-10 10:50:01|</span><br><span class="line">cookie2|2015-04-10 10:10:00|url44 |2015-04-10 10:03:04|2015-04-10 10:50:05|</span><br><span class="line">cookie2|2015-04-10 10:50:01|url55 |2015-04-10 10:10:00|2015-04-10 11:00:00|</span><br><span class="line">cookie2|2015-04-10 10:50:05|url66 |2015-04-10 10:50:01|null               |</span><br><span class="line">cookie2|2015-04-10 11:00:00|url77 |2015-04-10 10:50:05|null               |</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>创建伪表&amp;自定义UDF函数&amp;MR解决数据倾斜的问题&amp;行转列案例&amp;列转行案例&amp;使用hive实现wc&amp;修改hadoop的URI带来的hive数据库路径问题&amp;多文件多目录做wc或建表带来的问题</title>
      <link href="/2018/11/07/hive/7/"/>
      <url>/2018/11/07/hive/7/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>创建伪表</li><li>自定义UDF函数</li><li>MR解决数据倾斜的问题(引入)</li><li>行转列案例</li><li>列转行案例</li><li>使用hive实现wc</li><li>修改hadoop的URI带来的hive数据库路径问题</li><li>多文件多目录做wc或建表带来的问题</li></ol><h2 id="创建伪表"><a href="#创建伪表" class="headerlink" title="创建伪表"></a>创建伪表</h2><ol><li><p>创建表dual</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dual(a <span class="keyword">string</span>);</span><br></pre></td></tr></table></figure></li><li><p>创建数据并导入到表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">touch dual.txt</span><br><span class="line">echo 'X' &gt;dual.txt</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/dual.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> dual;</span><br></pre></td></tr></table></figure></li></ol><h2 id="自定义UDF函数"><a href="#自定义UDF函数" class="headerlink" title="自定义UDF函数"></a>自定义UDF函数</h2><p>hive官网关于用户如何自定义UDF: <a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins1" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/HivePlugins1</a>. </p><p>hive查看函数: show functions;</p><p>hive查看jar包: list jars;</p><ol><li><p>首先，需要创建一个扩展UDF的新类，其中有一个或多个名为evaluate的方法</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">// 继承UDF</span><br><span class="line">public class UDFPrintf extends UDF &#123;</span><br><span class="line">    </span><br><span class="line">//方法重载</span><br><span class="line">    public void evaluate()&#123;</span><br><span class="line">        System.out.println("你不要老婆吧");</span><br><span class="line">    &#125;</span><br><span class="line">    public void evaluate(String name)&#123;</span><br><span class="line">        System.out.println(name + ",你要老婆不要?");</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>对方法添加描述，先看系统的</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc function extended upper;</span><br><span class="line">upper(str) - Returns str <span class="keyword">with</span> <span class="keyword">all</span> <span class="keyword">characters</span> <span class="keyword">changed</span> <span class="keyword">to</span> uppercase</span><br><span class="line">Synonyms: <span class="keyword">ucase</span></span><br><span class="line">Example:</span><br><span class="line">  &gt; <span class="keyword">SELECT</span> <span class="keyword">upper</span>(<span class="string">'Facebook'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;</span><br><span class="line">  'FACEBOOK'</span><br><span class="line">=================================================================</span><br><span class="line">@Description(</span><br><span class="line">    name = "upper,ucase",</span><br><span class="line">    value = "_FUNC_(str) - Returns str <span class="keyword">with</span> <span class="keyword">all</span> <span class="keyword">characters</span> <span class="keyword">changed</span> <span class="keyword">to</span> uppercase<span class="string">",</span></span><br><span class="line"><span class="string">    extended = "</span>Example:\n  &gt; <span class="keyword">SELECT</span> _FUNC_(<span class="string">'Facebook'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;\n    </span><br><span class="line">    'FACEBOOK'"</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>我们仿照上面的给自定义的方法写个描述(Description)</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">@Description(</span><br><span class="line">    name = "my_printf,ucase",</span><br><span class="line">    value = "_FUNC_(str) - 返回一个名字加上字符串",</span><br><span class="line">    extended = "Example:\n  &gt; SELECT _FUNC_('老王') FROM src LIMIT 1;\n  </span><br><span class="line">    '老王,你要老婆不要?'"</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p>打包上传到Linux，启动hive，jar包添加到class path，创建<strong>临时UDF函数</strong>，只能在当前session中有效</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">add jar /home/hadoop/lib/hive-client-1.0.0.jar;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">temporary</span> <span class="keyword">function</span> my_printf <span class="keyword">as</span> <span class="string">'org.hadoop.hive.UDF.UDFPrintf'</span>;</span><br></pre></td></tr></table></figure></li><li><p>伪表使用自定义函数</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> my_printf() <span class="keyword">from</span> dual;</span><br><span class="line">hello,小七</span><br><span class="line"><span class="keyword">select</span> my_printf(<span class="string">"老王"</span>) <span class="keyword">from</span> dual;</span><br><span class="line">hello,老王</span><br></pre></td></tr></table></figure></li><li><p>上面的方法只能创建临时函数，我们接下来将会创建永久函数，需要把jar把上传到hdfs</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hdfs dfs -mkdir /jar</span><br><span class="line">hdfs dfs -put /home/hadoop/lib/hive-client-1.0.0.jar /jar</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">FUNCTION</span> my_printf <span class="keyword">AS</span> <span class="string">'org.hadoop.hive.UDF.UDFPrintf'</span> <span class="keyword">USING</span> JAR <span class="string">'hdfs:///jar/hive-client-1.0.0.jar'</span>;</span><br></pre></td></tr></table></figure></li><li><p>然后可以在多个窗口执行创建的永久函数</p></li></ol><h2 id="MR解决数据倾斜的思想"><a href="#MR解决数据倾斜的思想" class="headerlink" title="MR解决数据倾斜的思想"></a>MR解决数据倾斜的思想</h2><p>核心思想==&gt;先加盐(随机数)，再去盐(随机数)</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">public class skew &#123;</span><br><span class="line">    private List&lt;String&gt; newList = new ArrayList&lt;&gt;();</span><br><span class="line">    private Random r = new Random();</span><br><span class="line">    public void incRandom(List list)&#123;</span><br><span class="line">        newList.clear();</span><br><span class="line">        list.forEach( word -&gt;&#123;</span><br><span class="line">            int i = r.nextInt(10);</span><br><span class="line">            newList.add(i+"_"+word);</span><br><span class="line">        &#125;);</span><br><span class="line">        newList.forEach(System.out::println);</span><br><span class="line">    &#125;</span><br><span class="line">    public void decRandom()&#123;</span><br><span class="line">        newList.forEach(word-&gt;&#123;</span><br><span class="line">            System.out.println(word.substring(word.lastIndexOf("_")+1));</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        skew skew = new skew();</span><br><span class="line">        List&lt;String&gt; arr = new ArrayList&lt;&gt;();</span><br><span class="line">        arr.add("老王");</span><br><span class="line">        arr.add("狗子");</span><br><span class="line">        arr.add("张三");</span><br><span class="line">        skew.incRandom(arr);</span><br><span class="line">        System.out.println("<span class="comment">-------------------");</span></span><br><span class="line">        skew.decRandom();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">7_老王</span><br><span class="line">2_狗子</span><br><span class="line">4_张三</span><br><span class="line"><span class="comment">-------------------</span></span><br><span class="line">老王</span><br><span class="line">狗子</span><br><span class="line">张三</span><br></pre></td></tr></table></figure><h2 id="行转列案例"><a href="#行转列案例" class="headerlink" title="行转列案例"></a>行转列案例</h2><p>实现部门号的所有学生格式为</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">(dept01,Alaowang|wangwu)</span><br></pre></td></tr></table></figure><ol><li><p>数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">laowangdept01A</span><br><span class="line">zhangsandept02A</span><br><span class="line">lisidept01B</span><br><span class="line">wangwudept01A</span><br><span class="line">zhaoliudept02A</span><br></pre></td></tr></table></figure></li><li><p>创建表结构</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> row2col(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">dept <span class="keyword">string</span>,</span><br><span class="line">grade <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure></li><li><p>导入数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/row2col.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> row2col;</span><br></pre></td></tr></table></figure></li><li><p>实现(dept01,A    laowang|wangwu)</p><ol><li><p>第一步组合dept和grade成dept_grade</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">concat_ws</span>(<span class="string">','</span>,dept,grade) dept_grade,<span class="keyword">name</span> <span class="keyword">from</span> row2col;</span><br></pre></td></tr></table></figure></li><li><p>第二步按dept_grade分组求collect_set()将返回的数组使用concat_ws()函数转成字符串</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">t.dept_grade,<span class="keyword">concat_ws</span>(<span class="string">'|'</span>,collect_set(t.name)) <span class="keyword">names</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span> <span class="keyword">concat_ws</span>(<span class="string">','</span>,dept,grade) dept_grade,<span class="keyword">name</span> <span class="keyword">from</span> row2col) t</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">t.dept_grade;</span><br></pre></td></tr></table></figure><p>collect_set()返回一个数组，concat_ws()返回一个指定字符切分数组的字符串</p></li><li><p>结果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">dept_grade|names           |</span><br><span class="line"><span class="comment">----------|----------------|</span></span><br><span class="line">dept01,A  |laowang|wangwu  |</span><br><span class="line">dept01,B  |lisi            |</span><br><span class="line">dept02,A  |zhangsan|zhaoliu|</span><br></pre></td></tr></table></figure></li></ol></li></ol><h2 id="列转行案例"><a href="#列转行案例" class="headerlink" title="列转行案例"></a>列转行案例</h2><p>实现所有课程的格式为</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">1       zhangsan        化学</span><br><span class="line">1       zhangsan        物理</span><br><span class="line">...</span><br></pre></td></tr></table></figure><ol><li><p>数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">1,zhangsan,化学:物理:数学:语文</span><br><span class="line">2,lisi,化学:数学:生物:生理:卫生</span><br><span class="line">3,wangwu,化学:语文:英语:体育:生物</span><br></pre></td></tr></table></figure></li><li><p>建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> col2row(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">subjects <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">":"</span>;</span><br></pre></td></tr></table></figure></li><li><p>导数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/col2row.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> col2row;</span><br></pre></td></tr></table></figure></li><li><p>实现</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line"><span class="keyword">id</span>,<span class="keyword">name</span>,subject </span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">col2row</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(subjects) t <span class="keyword">as</span> subject;</span><br></pre></td></tr></table></figure></li><li><p>结果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">id|name    |subject|</span><br><span class="line"><span class="comment">--|--------|-------|</span></span><br><span class="line"> 1|zhangsan|化学     |</span><br><span class="line"> 1|zhangsan|物理     |</span><br><span class="line"> 1|zhangsan|数学     |</span><br><span class="line"> 1|zhangsan|语文     |</span><br><span class="line"> 2|lisi    |化学     |</span><br><span class="line"> 2|lisi    |数学     |</span><br><span class="line"> 2|lisi    |生物     |</span><br><span class="line"> 2|lisi    |生理     |</span><br><span class="line"> 2|lisi    |卫生     |</span><br><span class="line"> 3|wangwu  |化学     |</span><br><span class="line"> 3|wangwu  |语文     |</span><br><span class="line"> 3|wangwu  |英语     |</span><br><span class="line"> 3|wangwu  |体育     |</span><br><span class="line"> 3|wangwu  |生物     |</span><br></pre></td></tr></table></figure></li></ol><h2 id="使用hive实现wc"><a href="#使用hive实现wc" class="headerlink" title="使用hive实现wc"></a>使用hive实现wc</h2><ol><li><p>数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">化学:物理:数学:语文:英语</span><br><span class="line">化学:数学:生物:生理:卫生</span><br><span class="line">化学:语文:英语:体育:生物</span><br><span class="line">数学:语文:英语:体育:生物</span><br></pre></td></tr></table></figure></li><li><p>切分返回数组</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">split</span>(line,<span class="string">":"</span>) <span class="keyword">as</span> subjects <span class="keyword">from</span> wc</span><br></pre></td></tr></table></figure></li><li><p>炸开数组返回单词</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">words</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span> <span class="keyword">split</span>(line,<span class="string">":"</span>) subjects <span class="keyword">from</span> wc) <span class="keyword">as</span> t</span><br><span class="line"> <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(t.subjects) t1 <span class="keyword">as</span> words;</span><br></pre></td></tr></table></figure></li><li><p>每个单词分组求count()</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">t2.words,<span class="keyword">count</span>(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span></span><br><span class="line">words</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span> <span class="keyword">split</span>(line,<span class="string">":"</span>) subjects <span class="keyword">from</span> wc) <span class="keyword">as</span> t</span><br><span class="line"> <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(t.subjects) t1 <span class="keyword">as</span> words) t2</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> t2.words;</span><br></pre></td></tr></table></figure></li></ol><h2 id="修改hadoop的URI带来的hive数据库路径问题"><a href="#修改hadoop的URI带来的hive数据库路径问题" class="headerlink" title="修改hadoop的URI带来的hive数据库路径问题"></a>修改hadoop的URI带来的hive数据库路径问题</h2><p>hive的数据保存在hadoop中，而hive的源数据保存在mysql中</p><p>这里就有一个问题，如果修改了hadoop的fs.defaultFS这个参数</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://aliyun:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>那么hive的元数据没有修改就会找mysql中保存的路径</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">*************************** 1. row ***************************</span><br><span class="line">          DB_ID: 1</span><br><span class="line">           DESC: Default Hive database</span><br><span class="line">DB_LOCATION_URI: hdfs://aliyun:9000/user/hive/warehouse</span><br><span class="line">           NAME: default</span><br><span class="line">     OWNER_NAME: public</span><br><span class="line">     OWNER_TYPE: ROLE</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>这时候需要使用到一个命令metatool来将mysql中的元数据修改为新的仓库路径</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">-updateLocation &lt;new-loc&gt; &lt;old-loc&gt;</span><br></pre></td></tr></table></figure><h2 id="多文件多目录做wc或建表带来的问题"><a href="#多文件多目录做wc或建表带来的问题" class="headerlink" title="多文件多目录做wc或建表带来的问题"></a>多文件多目录做wc或建表带来的问题</h2><p>我们查看一下hdfs中的目录结构</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ hdfs dfs -ls /mdir<span class="comment">/*</span></span><br><span class="line"><span class="comment">-rwxr-xr-x   1 hadoop supergroup         13 2020-02-05 00:59 /mdir/1.txt</span></span><br><span class="line"><span class="comment">-rwxr-xr-x   1 hadoop supergroup         14 2020-02-05 00:59 /mdir/2.txt</span></span><br><span class="line"><span class="comment">-rwxr-xr-x   1 hadoop supergroup         11 2020-02-05 00:59 /mdir/mdir2/3.txt</span></span><br></pre></td></tr></table></figure><p>存在嵌套目录，在做wc或者建表时，将会报错</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> dir (<span class="keyword">name</span> <span class="keyword">string</span>) location <span class="string">'/mdir'</span>;</span><br><span class="line">Failed <span class="keyword">with</span> <span class="keyword">exception</span> java.io.IOException:java.io.IOException: <span class="keyword">Not</span> a <span class="keyword">file</span>: hdfs://aliyun:<span class="number">9000</span>/mdir<span class="comment">/*</span></span><br></pre></td></tr></table></figure><p>通过设置参数<code>mapreduce.input.fileinputformat.input.dir.recursive</code>为<code>true</code>来解决这个问题，该参数默认为<code>false</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dir;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>可以直接在mapped-site.xml文件中直接配配置参数</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.input.fileinputformat.input.dir.recursive&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Order By&amp;Sort By&amp;Distribute By&amp;Cluster By</title>
      <link href="/2018/11/06/hive/6/"/>
      <url>/2018/11/06/hive/6/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>全局排序（Order By）</li><li>Reduce内部排序（Sort By）</li><li>分区排序（Distribute By）</li><li>Cluster By</li></ol><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><ol><li><p>准备测试数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">7369SMITHCLERK79021980-12-17800.0020</span><br><span class="line">7499ALLENSALESMAN76981981-2-201600.00300.0030</span><br><span class="line">7521WARDSALESMAN76981981-2-221250.00500.0030</span><br><span class="line">7566JONESMANAGER78391981-4-22975.0020</span><br><span class="line">7654MARTINSALESMAN76981981-9-281250.001400.0030</span><br><span class="line">7698BLAKEMANAGER78391981-5-12850.0030</span><br><span class="line">7782CLARKMANAGER78391981-6-92450.0010</span><br><span class="line">7788SCOTTANALYST75661987-4-193000.0020</span><br><span class="line">7839KINGPRESIDENT1981-11-175000.0010</span><br><span class="line">7844TURNERSALESMAN76981981-9-81500.000.0030</span><br><span class="line">7876ADAMSCLERK77881987-5-231100.0020</span><br><span class="line">7900JAMESCLERK76981981-12-3950.0030</span><br><span class="line">7902FORDANALYST75661981-12-33000.0020</span><br><span class="line">7934MILLERCLERK77821982-1-231300.0010</span><br></pre></td></tr></table></figure></li><li><p>创建emp</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> emp(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">job <span class="keyword">string</span>,</span><br><span class="line">mgr <span class="built_in">int</span>,</span><br><span class="line">hiredate <span class="keyword">string</span>, </span><br><span class="line">sal <span class="keyword">double</span>, </span><br><span class="line">comm <span class="keyword">double</span>,</span><br><span class="line">deptno <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure></li><li><p>加载数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/emp.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> emp;</span><br></pre></td></tr></table></figure></li><li><p>验证数据</p><p><code>select * from emp;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br></pre></td></tr></table></figure></li></ol><h2 id="全局排序（Order-By）-慎用"><a href="#全局排序（Order-By）-慎用" class="headerlink" title="全局排序（Order By）[慎用]"></a>全局排序（Order By）[慎用]</h2><p>语句格式:     <code>order by col1,col2 asc/desc</code></p><p>使用order by语句排序的是全局排序，只能有一个reduce作用来完成，与此对应的是sort by由多个reduce来完成</p><p>案例实操</p><ol><li><p>查询员工信息按工资升序排列</p><p><code>select * from emp order by sal;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br></pre></td></tr></table></figure></li><li><p>查询员工信息按工资降序排列(增加一个替换null值的功能)</p><p><code>select *,nvl(comm,&#39;-1&#39;) from emp order by sal desc;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|_c1   |</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|------|</span></span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|-1    |</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|-1    |</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|-1    |</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|-1    |</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|-1    |</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|-1    |</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|300.0 |</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|0.0   |</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|-1    |</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|1400.0|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|500.0 |</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|-1    |</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|-1    |</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|-1    |</span><br></pre></td></tr></table></figure></li><li><p>按照员工薪水的2倍排序</p><p><code>select *,sal+nvl(comm,0) as salandcomm from emp order by salandcomm;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|salandcomm|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|----------|</span></span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|       800|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|       950|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|      1100|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|      1300|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|      1500|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|      1750|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|      1900|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|      2450|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|      2650|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|      2850|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|      2975|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|      3000|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|      3000|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|      5000|</span><br></pre></td></tr></table></figure></li><li><p>按照部门和工资升序排序</p><p><code>select * from emp order by deptno,sal+nvl(comm,0);</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br></pre></td></tr></table></figure></li></ol><h2 id="Reduce内部排序（Sort-By）"><a href="#Reduce内部排序（Sort-By）" class="headerlink" title="Reduce内部排序（Sort By）"></a>Reduce内部排序（Sort By）</h2><p>对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用<strong>sort by</strong>。</p><p>Sort by为每个reducer产生一个排序文件。每个Reducer内部进行排序，对全局结果集来说不是排序。</p><ol><li><p>设置reduce个数，不然一个一个reduce没有效果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br></pre></td></tr></table></figure></li><li><p>根据部门编号降序查看员工信息</p><p><code>select * from emp order by deptno desc;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br></pre></td></tr></table></figure></li><li><p>将查询结果导入到文件中（按照部门编号降序排序）[指定了格式]</p><p><code>insert overwrite local directory &#39;/home/hadoop/emp&#39; row format delimited fields terminated by &quot;\t&quot; select * from emp order by deptno desc;</code></p><p>打开/home/hadoop/emp下的文件</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun emp.txt]$ cat 000000_0 </span><br><span class="line">7521WARDSALESMAN76981981-2-221250.0500.030</span><br><span class="line">7499ALLENSALESMAN76981981-2-201600.0300.030</span><br><span class="line">7900JAMESCLERK76981981-12-3950.0\N30</span><br><span class="line">7844TURNERSALESMAN76981981-9-81500.00.030</span><br><span class="line">7698BLAKEMANAGER78391981-5-12850.0\N30</span><br><span class="line">7654MARTINSALESMAN76981981-9-281250.01400.030</span><br><span class="line">7876ADAMSCLERK77881987-5-231100.0\N20</span><br><span class="line">7902FORDANALYST75661981-12-33000.0\N20</span><br><span class="line">7788SCOTTANALYST75661987-4-193000.0\N20</span><br><span class="line">7369SMITHCLERK79021980-12-17800.0\N20</span><br><span class="line">7566JONESMANAGER78391981-4-22975.0\N20</span><br><span class="line">7934MILLERCLERK77821982-1-231300.0\N10</span><br><span class="line">7839KINGPRESIDENT\N1981-11-175000.0\N10</span><br><span class="line">7782CLARKMANAGER78391981-6-92450.0\N10</span><br></pre></td></tr></table></figure></li></ol><h2 id="发送分区排序（Distribute-By）"><a href="#发送分区排序（Distribute-By）" class="headerlink" title="发送分区排序（Distribute By）"></a>发送分区排序（Distribute By）</h2><p>在有些情况下，我们需要控制某个特定行应该到哪个reducer，通常是为了进行后续的聚集操作。<strong>distribute by</strong> 子句可以做这件事。<strong>distribute by</strong>类似MR中<strong>partition</strong>（自定义分区），进行分区，结合<strong>sort by</strong>使用。 </p><p>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><ol><li><p>先按照部门编号分区，再按照员工编号降序排序。</p><p><code>select * from emp distribute by deptno sort by empno desc;</code> </p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br></pre></td></tr></table></figure><p><code>注意:</code> </p><ol><li>distribute by的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。</li><li>Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。</li></ol></li></ol><h2 id="Cluster-By"><a href="#Cluster-By" class="headerlink" title="Cluster By"></a>Cluster By</h2><p>当<code>distribute by</code>和<code>sorts by</code>字段相同时，可以使用<code>cluster by</code>方式。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</p><p>当分区字段和排序字段都是部门编号的时候我们可以这么做</p><p><code>select * from emp cluster by deptno;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br></pre></td></tr></table></figure><p><code>注意:</code> 注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HS2&amp;Hive的复杂数据结构&amp;行列互转&amp;常用函数&amp;静动态分区表&amp;桶表</title>
      <link href="/2018/11/05/hive/5/"/>
      <url>/2018/11/05/hive/5/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>HS2</li><li>复杂数据结构</li><li>行列互转</li><li>常用函数</li><li>静动态分区表</li><li>桶表</li></ol><h2 id="SH2"><a href="#SH2" class="headerlink" title="SH2"></a>SH2</h2><p>HS2是HiveServer2的简称</p><ul><li><p>HS2: Server端，默认端口10000</p><p>修改端口的方式是通过设置hive.server2.thrift.port的值</p></li><li><p>beeline: Client端</p><p>连接方式: <code>./beeline -u jdbc:hive2://hadoop001:10000/matestore -n hadoop</code></p></li></ul><h2 id="复杂数据结构"><a href="#复杂数据结构" class="headerlink" title="复杂数据结构"></a>复杂数据结构</h2><p>复杂数据类型有三种，分别是: array、map、structs</p><h3 id="array"><a href="#array" class="headerlink" title="array"></a>array</h3><p><code>array</code>的建表格式:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hive_array(<span class="keyword">name</span> <span class="keyword">string</span>, work_locations <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line">COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span>;</span><br></pre></td></tr></table></figure><p>注意: <code>COLLECTION ITEMS TERMINATED BY &#39;,&#39;</code>是指定array数组中的元素分隔符</p><h4 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h4><ol><li><p>第一步准备数据</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pk      beijing,shanghai,tianjin,hangzhou</span><br><span class="line">jepson  changchu,chengdu,wuhan,beijing</span><br></pre></td></tr></table></figure></li><li><p>第二步建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> array_(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">address <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span> </span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure></li><li><p>第三步装载数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/hive_array.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> array_;</span><br></pre></td></tr></table></figure></li><li><p>第四步查询数据是否加载成功</p><p><code>select * from array_;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name  |address                                    |</span><br><span class="line"><span class="comment">------|-------------------------------------------|</span></span><br><span class="line">pk    |["beijing","shanghai","tianjin","hangzhou"]|</span><br><span class="line">jepson|["changchu","chengdu","wuhan","beijing"]   |</span><br></pre></td></tr></table></figure></li></ol><h4 id="对array数组的”取”"><a href="#对array数组的”取”" class="headerlink" title="对array数组的”取”"></a>对array数组的”取”</h4><ol><li><p>根据数组的下标取出指定元素，下标从0开始</p><p><code>select name,address[1] as address from array_;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name  |address |</span><br><span class="line"><span class="comment">------|--------|</span></span><br><span class="line">pk    |shanghai|</span><br><span class="line">jepson|chengdu |</span><br></pre></td></tr></table></figure></li><li><p>获取数组中元素的个数</p><p><code>select name,size(address) as size from array_;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name  |size|</span><br><span class="line"><span class="comment">------|----|</span></span><br><span class="line">pk    |   4|</span><br><span class="line">jepson|   4|</span><br></pre></td></tr></table></figure></li><li><p>获取数组中包含某元素的记录</p><p><code>select name,address from array_ where  array_contains(address,&quot;shanghai&quot;);</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name|address                                    |</span><br><span class="line"><span class="comment">----|-------------------------------------------|</span></span><br><span class="line">pk  |["beijing","shanghai","tianjin","hangzhou"]|</span><br></pre></td></tr></table></figure></li></ol><h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><p><code>map</code>的建表格式:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hive_map(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>, members <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;, age <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line">COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'#'</span></span><br><span class="line"><span class="keyword">MAP</span> <span class="keyword">KEYS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">':'</span>;</span><br></pre></td></tr></table></figure><p>注意: <code>COLLECTION ITEMS TERMINATED BY &#39;#&#39;</code>是指定map键值对中的元素分隔符，<code>MAP KEYS TERMINATED BY &#39;:&#39;</code>是指定key和value的分隔符</p><h4 id="准备数据-1"><a href="#准备数据-1" class="headerlink" title="准备数据"></a>准备数据</h4><ol><li><p>第一步准备数据</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1,zhangsan,father:xiaoming#mother:xiaohuang#brother:xiaoxu,28</span><br><span class="line">2,lisi,father:mayun#mother:huangyi#brother:guanyu,22</span><br><span class="line">3,wangwu,father:wangjianlin#mother:ruhua#sister:jingtian,29</span><br><span class="line">4,mayun,father:mayongzhen#mother:angelababy,26</span><br></pre></td></tr></table></figure></li><li><p>第二步建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> map_(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">family <span class="keyword">map</span>&lt;<span class="keyword">String</span>,<span class="keyword">String</span>&gt;,</span><br><span class="line">age <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"#"</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">":"</span>;</span><br></pre></td></tr></table></figure></li><li><p>第三步加载数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/hive_map.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> map_;</span><br></pre></td></tr></table></figure></li><li><p>第四步查询数据是否加载成功</p><p><code>select * from map_;</code></p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">id|name    |family                                                       |age|</span><br><span class="line"><span class="comment">--|--------|-------------------------------------------------------------|---|</span></span><br><span class="line"> 1|zhangsan|&#123;"father":"xiaoming","mother":"xiaohuang","brother":"xiaoxu"&#125;| 28|</span><br><span class="line"> 2|lisi    |&#123;"father":"mayun","mother":"huangyi","brother":"guanyu"&#125;     | 22|</span><br><span class="line"> 3|wangwu  |&#123;"father":"wangjianlin","mother":"ruhua","sister":"jingtian"&#125;| 29|</span><br><span class="line"> 4|mayun   |&#123;"father":"mayongzhen","mother":"angelababy"&#125;                | 26|</span><br></pre></td></tr></table></figure></li></ol><h4 id="对map键值对的”取”"><a href="#对map键值对的”取”" class="headerlink" title="对map键值对的”取”"></a>对map键值对的”取”</h4><ol><li><p>根据key取value</p><p><code>select name,family[&#39;father&#39;] as father,age from map_;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name    |father     |age|</span><br><span class="line"><span class="comment">--------|-----------|---|</span></span><br><span class="line">zhangsan|xiaoming   | 28|</span><br><span class="line">lisi    |mayun      | 22|</span><br><span class="line">wangwu  |wangjianlin| 29|</span><br><span class="line">mayun   |mayongzhen | 26|</span><br></pre></td></tr></table></figure></li><li><p>取出所有key集合</p><p><code>select name,map_keys(family) as map_keys,age from map_;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name    |map_keys                     |age|</span><br><span class="line"><span class="comment">--------|-----------------------------|---|</span></span><br><span class="line">zhangsan|["father","mother","brother"]| 28|</span><br><span class="line">lisi    |["father","mother","brother"]| 22|</span><br><span class="line">wangwu  |["father","mother","sister"] | 29|</span><br><span class="line">mayun   |["father","mother"]          | 26|</span><br></pre></td></tr></table></figure></li><li><p>取出所有的value集合</p><p><code>select name,map_values(family) as map_values,age from map_;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name    |map_values                        |age|</span><br><span class="line"><span class="comment">--------|----------------------------------|---|</span></span><br><span class="line">zhangsan|["xiaoming","xiaohuang","xiaoxu"] | 28|</span><br><span class="line">lisi    |["mayun","huangyi","guanyu"]      | 22|</span><br><span class="line">wangwu  |["wangjianlin","ruhua","jingtian"]| 29|</span><br><span class="line">mayun   |["mayongzhen","angelababy"]       | 26|</span><br></pre></td></tr></table></figure></li><li><p>求key的数量(对key集合求size)</p><p><code>select name,size(map_keys(family)) as key_size,age from map_;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name    |key_size|age|</span><br><span class="line"><span class="comment">--------|--------|---|</span></span><br><span class="line">zhangsan|       3| 28|</span><br><span class="line">lisi    |       3| 22|</span><br><span class="line">wangwu  |       3| 29|</span><br><span class="line">mayun   |       2| 26|</span><br></pre></td></tr></table></figure></li><li><p>求key是否包含某个元素(对key的集合求contains)</p><p><code>select name,family,age from map_ where array_contains((map_keys(family)),&quot;brother&quot;);</code></p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">name    |family                                                       |age|</span><br><span class="line"><span class="comment">--------|-------------------------------------------------------------|---|</span></span><br><span class="line">zhangsan|&#123;"father":"xiaoming","mother":"xiaohuang","brother":"xiaoxu"&#125;| 28|</span><br><span class="line">lisi    |&#123;"father":"mayun","mother":"huangyi","brother":"guanyu"&#125;     | 22|</span><br></pre></td></tr></table></figure></li></ol><h3 id="structs"><a href="#structs" class="headerlink" title="structs"></a>structs</h3><p><code>structs</code>的建表格式:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hive_struct(</span><br><span class="line">ip <span class="keyword">string</span>, info <span class="keyword">struct</span>&lt;<span class="keyword">name</span>:<span class="keyword">string</span>,age:<span class="built_in">int</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'#'</span></span><br><span class="line">COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">':'</span>;</span><br></pre></td></tr></table></figure><p>注意: <code>COLLECTION ITEMS TERMINATED BY &#39;:&#39;</code>是指定每个元素之间的分隔符</p><h4 id="准备数据-2"><a href="#准备数据-2" class="headerlink" title="准备数据"></a>准备数据</h4><ol><li><p>第一步准备数据</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">192.168.1.1#zhangsan:40</span><br><span class="line">192.168.1.2#lisi:50</span><br><span class="line">192.168.1.3#wangwu:60</span><br><span class="line">192.168.1.4#zhaoliu:70</span><br></pre></td></tr></table></figure></li><li><p>第二步建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> structs_(</span><br><span class="line">address <span class="keyword">string</span>,</span><br><span class="line"><span class="string">`user`</span> <span class="keyword">struct</span>&lt;<span class="keyword">name</span>:<span class="keyword">string</span>,age:<span class="built_in">int</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"#"</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">":"</span>;</span><br></pre></td></tr></table></figure></li><li><p>第三步加载数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/hive_struct.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> structs_;</span><br></pre></td></tr></table></figure></li><li><p>第四步查询数据是否装载成功</p><p><code>select * from structs_;</code></p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">address    |user                          |</span><br><span class="line"><span class="comment">-----------|------------------------------|</span></span><br><span class="line">192.168.1.1|[&#123;"name":"zhangsan","age":40&#125;]|</span><br><span class="line">192.168.1.2|[&#123;"name":"lisi","age":50&#125;]    |</span><br><span class="line">192.168.1.3|[&#123;"name":"wangwu","age":60&#125;]  |</span><br><span class="line">192.168.1.4|[&#123;"name":"zhaoliu","age":70&#125;] |</span><br></pre></td></tr></table></figure></li></ol><h4 id="对structs结构体的”取”"><a href="#对structs结构体的”取”" class="headerlink" title="对structs结构体的”取”"></a>对structs结构体的”取”</h4><ol><li><p>取出结构体中的元素</p><p><code>select address,user.name as name from structs_;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">address    |name    |</span><br><span class="line"><span class="comment">-----------|--------|</span></span><br><span class="line">192.168.1.1|zhangsan|</span><br><span class="line">192.168.1.2|lisi    |</span><br><span class="line">192.168.1.3|wangwu  |</span><br><span class="line">192.168.1.4|zhaoliu |</span><br></pre></td></tr></table></figure></li></ol><h2 id="行列互转"><a href="#行列互转" class="headerlink" title="行列互转"></a>行列互转</h2><p>这是一个复杂数据类型的综合练习</p><h3 id="准备数据-3"><a href="#准备数据-3" class="headerlink" title="准备数据"></a>准备数据</h3><ol><li><p>第一步准备数据</p><p>数据1: session点击广告记录</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">11      ad_101  2014-05-01 06:01:12.334+01</span><br><span class="line">22      ad_102  2014-05-01 07:28:12.342+01</span><br><span class="line">33      ad_103  2014-05-01 07:50:12.33+01</span><br><span class="line">11      ad_104  2014-05-01 09:27:12.33+01</span><br><span class="line">22      ad_103  2014-05-01 09:03:12.324+01</span><br><span class="line">33      ad_102  2014-05-02 19:10:12.343+01</span><br><span class="line">11      ad_101  2014-05-02 09:07:12.344+01</span><br><span class="line">35      ad_105  2014-05-03 11:07:12.339+01</span><br><span class="line">22      ad_104  2014-05-03 12:59:12.743+01</span><br><span class="line">77      ad_103  2014-05-03 18:04:12.355+01</span><br><span class="line">99      ad_102  2014-05-04 00:36:39.713+01</span><br><span class="line">33      ad_101  2014-05-04 19:10:12.343+01</span><br><span class="line">11      ad_101  2014-05-05 09:07:12.344+01</span><br><span class="line">35      ad_102  2014-05-05 11:07:12.339+01</span><br><span class="line">22      ad_103  2014-05-05 12:59:12.743+01</span><br><span class="line">77      ad_104  2014-05-05 18:04:12.355+01</span><br><span class="line">99      ad_105  2014-05-05 20:36:39.713+01</span><br></pre></td></tr></table></figure><p>数据2: 广告的详情</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ad_101  http://www.google.com   catalog8|catalog1</span><br><span class="line">ad_102  http://www.sohu.com     catalog6|catalog3</span><br><span class="line">ad_103  http://www.baidu.com    catalog7</span><br><span class="line">ad_104  http://www.qq.com       catalog5|catalog1|catalog4|catalog9</span><br><span class="line">ad_105  http://sina.com</span><br></pre></td></tr></table></figure></li><li><p>第二步创建表</p><p>表1: 动作表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> click_tbl(</span><br><span class="line">cookie_id <span class="keyword">string</span>,  </span><br><span class="line">ad_id <span class="built_in">int</span>,</span><br><span class="line"><span class="built_in">time</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure><p>表2: 广告表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ad_tbl(</span><br><span class="line">ad_id <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">catalogs <span class="built_in">array</span>&lt;<span class="keyword">String</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"|"</span></span><br></pre></td></tr></table></figure></li><li><p>第三步加载数据</p><p>给动作表加载数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/click_log.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> click_tbl;</span><br></pre></td></tr></table></figure><p>给广告表加载数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/ad_list.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> ad_tbl;</span><br></pre></td></tr></table></figure></li><li><p>第四步查询数据是否装载成功</p><p>动作表查询</p><p><code>select * from click_tbl;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id |time                      |</span><br><span class="line"><span class="comment">---------|------|--------------------------|</span></span><br><span class="line">       11|ad_101|2014-05-01 06:01:12.334+01|</span><br><span class="line">       22|ad_102|2014-05-01 07:28:12.342+01|</span><br><span class="line">       33|ad_103|2014-05-01 07:50:12.33+01 |</span><br><span class="line">       11|ad_104|2014-05-01 09:27:12.33+01 |</span><br><span class="line">       22|ad_103|2014-05-01 09:03:12.324+01|</span><br><span class="line">       33|ad_102|2014-05-02 19:10:12.343+01|</span><br><span class="line">       11|ad_101|2014-05-02 09:07:12.344+01|</span><br><span class="line">       35|ad_105|2014-05-03 11:07:12.339+01|</span><br><span class="line">       22|ad_104|2014-05-03 12:59:12.743+01|</span><br><span class="line">       77|ad_103|2014-05-03 18:04:12.355+01|</span><br><span class="line">       99|ad_102|2014-05-04 00:36:39.713+01|</span><br><span class="line">       33|ad_101|2014-05-04 19:10:12.343+01|</span><br><span class="line">       11|ad_101|2014-05-05 09:07:12.344+01|</span><br><span class="line">       35|ad_102|2014-05-05 11:07:12.339+01|</span><br><span class="line">       22|ad_103|2014-05-05 12:59:12.743+01|</span><br><span class="line">       77|ad_104|2014-05-05 18:04:12.355+01|</span><br><span class="line">       99|ad_105|2014-05-05 20:36:39.713+01|</span><br></pre></td></tr></table></figure><p>广告把查询</p><p><code>select * from ad_tbl;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ad_id |url                  |catalogs                               |</span><br><span class="line"><span class="comment">------|---------------------|---------------------------------------|</span></span><br><span class="line">ad_101|http://www.google.com|["catalog8|catalog1"]                  |</span><br><span class="line">ad_102|http://www.sohu.com  |["catalog6|catalog3"]                  |</span><br><span class="line">ad_103|http://www.baidu.com |["catalog7"]                           |</span><br><span class="line">ad_104|http://www.qq.com    |["catalog5|catalog1|catalog4|catalog9"]|</span><br><span class="line">ad_105|http://sina.com      |NULL                                   |</span><br></pre></td></tr></table></figure></li></ol><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><h4 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h4><p>查询每个人访问的广告</p><ol><li><p>去重(collect_set): </p><p><code>select cookie_id,collect_set(ad_id) ad_id from click_tbl group by cookie_id;</code></p></li></ol><ul><li><p>结果:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id                       |</span><br><span class="line"><span class="comment">---------|----------------------------|</span></span><br><span class="line">       11|["ad_101","ad_104"]         |</span><br><span class="line">       22|["ad_102","ad_103","ad_104"]|</span><br><span class="line">       33|["ad_103","ad_102","ad_101"]|</span><br><span class="line">       35|["ad_105","ad_102"]         |</span><br><span class="line">       77|["ad_103","ad_104"]         |</span><br><span class="line">       99|["ad_102","ad_105"]         |</span><br></pre></td></tr></table></figure></li></ul><ol start="2"><li><p>不去重(collect_list):</p><p><code>select cookie_id,collect_list(ad_id) ad_id from click_tbl group by cookie_id;</code></p></li></ol><ul><li><p>结果:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id                                |</span><br><span class="line"><span class="comment">---------|-------------------------------------|</span></span><br><span class="line">       11|["ad_101","ad_104","ad_101","ad_101"]|</span><br><span class="line">       22|["ad_102","ad_103","ad_104","ad_103"]|</span><br><span class="line">       33|["ad_103","ad_102","ad_101"]         |</span><br><span class="line">       35|["ad_105","ad_102"]                  |</span><br><span class="line">       77|["ad_103","ad_104"]                  |</span><br><span class="line">       99|["ad_102","ad_105"]                  |</span><br></pre></td></tr></table></figure></li></ul><h4 id="聚合查询"><a href="#聚合查询" class="headerlink" title="聚合查询"></a>聚合查询</h4><ol><li><p>查询每个人访问相同广告的次数</p><p><code>select cookie_id,ad_id,count(1) amount from click_tbl group by  cookie_id,ad_id;</code></p><p>结果:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id |amount|</span><br><span class="line"><span class="comment">---------|------|------|</span></span><br><span class="line">       11|ad_101|     3|</span><br><span class="line">       11|ad_104|     1|</span><br><span class="line">       22|ad_102|     1|</span><br><span class="line">       22|ad_103|     2|</span><br><span class="line">       22|ad_104|     1|</span><br><span class="line">       33|ad_101|     1|</span><br><span class="line">       33|ad_102|     1|</span><br><span class="line">       33|ad_103|     1|</span><br><span class="line">       35|ad_102|     1|</span><br><span class="line">       35|ad_105|     1|</span><br><span class="line">       77|ad_103|     1|</span><br><span class="line">       77|ad_104|     1|</span><br><span class="line">       99|ad_102|     1|</span><br><span class="line">       99|ad_105|     1|</span><br></pre></td></tr></table></figure></li><li><p>查询每个人访问的广告详情</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">click_tmp.cookie_id,click_tmp.ad_id,ad_tbl.url,ad_tbl.catalogs </span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">ad_tbl</span><br><span class="line"><span class="keyword">JOIN</span></span><br><span class="line">(<span class="keyword">select</span> cookie_id,ad_id,<span class="keyword">count</span>(<span class="number">1</span>) amount <span class="keyword">from</span> click_tbl <span class="keyword">group</span> <span class="keyword">by</span>  cookie_id,ad_id) click_tmp</span><br><span class="line"><span class="keyword">on</span> click_tmp.ad_id=ad_tbl.ad_id;</span><br></pre></td></tr></table></figure><p>结果:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id |url                  |catalogs                                     |</span><br><span class="line"><span class="comment">---------|------|---------------------|---------------------------------------------|</span></span><br><span class="line">       11|ad_101|http://www.google.com|["catalog8","catalog1"]                      |</span><br><span class="line">       11|ad_104|http://www.qq.com    |["catalog5","catalog1","catalog4","catalog9"]|</span><br><span class="line">       22|ad_102|http://www.sohu.com  |["catalog6","catalog3"]                      |</span><br><span class="line">       22|ad_103|http://www.baidu.com |["catalog7"]                                 |</span><br><span class="line">       22|ad_104|http://www.qq.com    |["catalog5","catalog1","catalog4","catalog9"]|</span><br><span class="line">       33|ad_101|http://www.google.com|["catalog8","catalog1"]                      |</span><br><span class="line">       33|ad_102|http://www.sohu.com  |["catalog6","catalog3"]                      |</span><br><span class="line">       33|ad_103|http://www.baidu.com |["catalog7"]                                 |</span><br><span class="line">       35|ad_102|http://www.sohu.com  |["catalog6","catalog3"]                      |</span><br><span class="line">       35|ad_105|http://sina.com      |NULL                                         |</span><br><span class="line">       77|ad_103|http://www.baidu.com |["catalog7"]                                 |</span><br><span class="line">       77|ad_104|http://www.qq.com    |["catalog5","catalog1","catalog4","catalog9"]|</span><br><span class="line">       99|ad_102|http://www.sohu.com  |["catalog6","catalog3"]                      |</span><br><span class="line">       99|ad_105|http://sina.com      |NULL                                         |</span><br></pre></td></tr></table></figure></li><li><p>把catalogs中的数据元素排序</p><p><code>select ad_id,url,sort_array(catalogs) as catalogs from ad_tbl;</code></p><p>结果:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ad_id |url                  |catalogs                                     |</span><br><span class="line"><span class="comment">------|---------------------|---------------------------------------------|</span></span><br><span class="line">ad_101|http://www.google.com|["catalog1","catalog8"]                      |</span><br><span class="line">ad_102|http://www.sohu.com  |["catalog3","catalog6"]                      |</span><br><span class="line">ad_103|http://www.baidu.com |["catalog7"]                                 |</span><br><span class="line">ad_104|http://www.qq.com    |["catalog1","catalog4","catalog5","catalog9"]|</span><br><span class="line">ad_105|http://sina.com      |NULL                                         |</span><br></pre></td></tr></table></figure></li></ol><h4 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h4><p>查询每个广告详情</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">ad_id,<span class="keyword">catalog</span></span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">ad_tbl</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(catalogs) t <span class="keyword">as</span> <span class="keyword">catalog</span>;</span><br></pre></td></tr></table></figure><p>结果:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ad_id |catalog |</span><br><span class="line"><span class="comment">------|--------|</span></span><br><span class="line">ad_101|catalog8|</span><br><span class="line">ad_101|catalog1|</span><br><span class="line">ad_102|catalog6|</span><br><span class="line">ad_102|catalog3|</span><br><span class="line">ad_103|catalog7|</span><br><span class="line">ad_104|catalog5|</span><br><span class="line">ad_104|catalog1|</span><br><span class="line">ad_104|catalog4|</span><br><span class="line">ad_104|catalog9|</span><br></pre></td></tr></table></figure><p><code>注意:</code> 如果ad_tbl的catalogs字段是String类型的，那么在explode炸开的时候要转换成数组，也就是用split把字段元素按’|’切分开返回一个数组</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">lateral view outer explode(split(catalogs,'\\|')) t as catalog</span><br></pre></td></tr></table></figure><h2 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h2><p>查用函数的用法查询: <code>desc function extended 函数名;</code></p><h3 id="时间函数"><a href="#时间函数" class="headerlink" title="时间函数"></a>时间函数</h3><p><code>current_date:</code> 返回当前日期</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">current_date</span><span class="comment">--2019-12-21</span></span><br></pre></td></tr></table></figure><p><code>current_timestamp:</code> 返回当前时间</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">current_timestamp</span><span class="comment">--2019-12-21 02:23:44</span></span><br></pre></td></tr></table></figure><p><code>unix_timestamp:</code> 时间转秒</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">unix_timestamp</span>(<span class="string">'2019-08-15 16:40:00'</span>,<span class="string">'yyyy-MM-dd HH:mm:ss'</span>)   <span class="comment">--1565858400</span></span><br></pre></td></tr></table></figure><p><code>from_unixtime:</code> 秒转时间</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">1565858389</span>,<span class="string">'yyyy-MM-dd HH:mm:ss'</span>)  <span class="comment">--2019-08-15 16:39:49</span></span><br><span class="line"><span class="keyword">select</span> from_unixtime(<span class="keyword">cast</span>(<span class="keyword">substr</span>(<span class="number">1553184000488</span>,<span class="number">1</span>,<span class="number">10</span>) <span class="keyword">as</span> <span class="built_in">int</span>),<span class="string">'yyyy-MM-dd HH:mm:ss'</span>)  <span class="comment">--2019-03-22 00:00:00</span></span><br><span class="line"><span class="keyword">select</span> from_unixtime(<span class="keyword">unix_timestamp</span>(),<span class="string">'yyyy-MM-dd HH:mm:ss'</span>)   <span class="comment">-- 2019-08-15 17:18:55</span></span><br></pre></td></tr></table></figure><p><code>to_date:</code> 返回日期</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">to_date</span>(<span class="string">'2009-07-30 04:17:52'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;<span class="comment">--2009-07-30</span></span><br></pre></td></tr></table></figure><p><code>year:</code> 返回年份</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">year</span>(<span class="string">'2009-07-30'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;<span class="comment">--2009</span></span><br></pre></td></tr></table></figure><p><code>month:</code> 返回月份</p><p><code>day:</code> 返回日期</p><p><code>hour:</code> 返回小时</p><p><code>minute:</code> 返回分钟</p><p><code>second:</code> 返回毫秒</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">second</span>(<span class="string">'2009-07-30 12:58:59'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>; <span class="comment">--59</span></span><br></pre></td></tr></table></figure><p><code>date_add:</code> 增加指定时间</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">date_add</span>(<span class="string">'2009-07-30'</span>, <span class="number">1</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;<span class="comment">--2009-07-31</span></span><br></pre></td></tr></table></figure><p><code>date_sub:</code> 减少指定时间</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">date_sub</span>(<span class="string">'2009-07-30'</span>, <span class="number">1</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;<span class="comment">--2009-07-29</span></span><br></pre></td></tr></table></figure><h3 id="数值函数"><a href="#数值函数" class="headerlink" title="数值函数"></a>数值函数</h3><p><code>round:</code> 返回指定范围的数值</p><p><code>ceil:</code> 返回天花板，取最大的整数值</p><p><code>floor:</code> 返回地板，去最小的整数值</p><p><code>abs:</code> 返回绝对值</p><p><code>least:</code> 数列中最小值 ==&gt; min</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">least</span>(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;<span class="comment">--1</span></span><br></pre></td></tr></table></figure><h3 id="字符串函数"><a href="#字符串函数" class="headerlink" title="字符串函数"></a>字符串函数</h3><p><code>substr:</code> 返回截取字符串</p><p><code>concat:</code> 返回连接字符串</p><p><code>concat_ws:</code> 根据特定格式组合字符串</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">concat_ws</span>(<span class="string">'.'</span>, <span class="string">'www'</span>, <span class="built_in">array</span>(<span class="string">'facebook'</span>, <span class="string">'com'</span>)) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;<span class="comment">--www.facebook.com</span></span><br></pre></td></tr></table></figure><p><code>length:</code> 返回字符串的长度，整数值</p><p><code>split:</code> 返回切分后的字符串数组</p><p><code>upper:</code> 字符转大写</p><p><code>lower:</code> 字符转小写</p><h3 id="Json处理函数"><a href="#Json处理函数" class="headerlink" title="Json处理函数"></a>Json处理函数</h3><ol><li><p>第一步准备数据</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"1"</span>,<span class="attr">"rate"</span>:<span class="string">"5"</span>,<span class="attr">"time"</span>:<span class="string">"978300760"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"2"</span>,<span class="attr">"rate"</span>:<span class="string">"4"</span>,<span class="attr">"time"</span>:<span class="string">"978702109"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"3"</span>,<span class="attr">"rate"</span>:<span class="string">"3"</span>,<span class="attr">"time"</span>:<span class="string">"978401968"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"4"</span>,<span class="attr">"rate"</span>:<span class="string">"4"</span>,<span class="attr">"time"</span>:<span class="string">"978300275"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"5"</span>,<span class="attr">"rate"</span>:<span class="string">"3"</span>,<span class="attr">"time"</span>:<span class="string">"978801091"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br></pre></td></tr></table></figure></li><li><p>第二步建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> rating_json(</span><br><span class="line"><span class="keyword">json</span> <span class="keyword">string</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p>第三步加载数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/rating.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> rating_json;</span><br></pre></td></tr></table></figure></li><li><p>第四步查看数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">json                                                    |</span><br><span class="line"><span class="comment">--------------------------------------------------------|</span></span><br><span class="line">&#123;"movie":"1","rate":"5","time":"978300760","userid":"1"&#125;|</span><br><span class="line">&#123;"movie":"2","rate":"4","time":"978702109","userid":"1"&#125;|</span><br><span class="line">&#123;"movie":"3","rate":"3","time":"978401968","userid":"1"&#125;|</span><br><span class="line">&#123;"movie":"4","rate":"4","time":"978300275","userid":"1"&#125;|</span><br><span class="line">&#123;"movie":"5","rate":"3","time":"978801091","userid":"1"&#125;|</span><br></pre></td></tr></table></figure></li><li><p>对数据进行处理</p><p><code>json_tuple:</code> 返回指定json文件的指定字段，返回的是字符串</p><p><code>select json_tuple(json,&quot;movie&quot;,&quot;rate&quot;,&quot;time&quot;,&quot;userid&quot;) as (movie,rate,time,userid) from rating_json;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">movie|rate|time     |userid|</span><br><span class="line"><span class="comment">-----|----|---------|------|</span></span><br><span class="line">1    |5   |978300760|1     |</span><br><span class="line">2    |4   |978702109|1     |</span><br><span class="line">3    |3   |978401968|1     |</span><br><span class="line">4    |4   |978300275|1     |</span><br><span class="line">5    |3   |978801091|1     |</span><br></pre></td></tr></table></figure><p><code>parse_url_tuple:</code> 返回指定url的指定字段，返回的是字符串</p><p><code>select parse_url_tuple(&quot;https://www.baidu.com/bigdate/spark?cookie_id=10&quot;,&#39;HOST&#39;,&#39;PATH&#39;,&#39;QUERY&#39;,&#39;QUERY:cookie_id&#39;);</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">c0           |c1            |c2          |c3|</span><br><span class="line"><span class="comment">-------------|--------------|------------|--|</span></span><br><span class="line">www.baidu.com|/bigdate/spark|cookie_id=10|10|</span><br></pre></td></tr></table></figure></li></ol><h3 id="Null值处理函数"><a href="#Null值处理函数" class="headerlink" title="Null值处理函数"></a>Null值处理函数</h3><p><code>isnull:</code> 指定字段的元素如果为null则返回true</p><p><code>isnotnull:</code> 指定字段的元素如果不为null则返回true</p><p><code>elt:</code> 指定返回的元素</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">elt</span>(<span class="number">1</span>, <span class="string">'face'</span>, <span class="string">'book'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;<span class="comment">--face</span></span><br></pre></td></tr></table></figure><p><code>nvl:</code> 替换null值</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> nvl(<span class="literal">null</span>,<span class="string">'bla'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;<span class="comment">--bla</span></span><br></pre></td></tr></table></figure><h3 id="其他函数"><a href="#其他函数" class="headerlink" title="其他函数"></a>其他函数</h3><p><code>cast:</code> 转换数据类型</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cast(time as bigint)<span class="comment">--时间转数值</span></span><br><span class="line">cast(string as date)<span class="comment">--字符串转日期</span></span><br></pre></td></tr></table></figure><p><code>注意:</code> binary只能转string</p><h2 id="静动态分区表"><a href="#静动态分区表" class="headerlink" title="静动态分区表"></a>静动态分区表</h2><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><ol><li><p>数据源</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">1,jack,shanghai,20190129</span><br><span class="line">2,kevin,beijing,20190130</span><br><span class="line">3,lucas,hangzhou,20190129</span><br><span class="line">4,lily,hangzhou,20190130</span><br></pre></td></tr></table></figure></li><li><p>创建源数据表（外表）</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> prit_tbl(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">address <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">day</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure></li><li><p>加载数据：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/partition.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> prit_tbl;</span><br></pre></td></tr></table></figure></li><li><p>创建分区表（外表）</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> prit_tbl(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">address <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">day</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure></li></ol><h3 id="静态分区加载数据"><a href="#静态分区加载数据" class="headerlink" title="静态分区加载数据"></a>静态分区加载数据</h3><ul><li><p>静态分区缺点：每次写入都要明确指定分区日期。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> prit_tbl <span class="keyword">partition</span>(<span class="keyword">day</span>=<span class="string">"20190129"</span>) <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span>,address <span class="keyword">from</span> test_tbl <span class="keyword">where</span> <span class="keyword">day</span> = <span class="string">"20190129"</span> ;</span><br></pre></td></tr></table></figure><p><code>注意:</code> 并且在查询处不能包含分区字段day，否则会报错</p></li></ul><h3 id="动态分区加载数据"><a href="#动态分区加载数据" class="headerlink" title="动态分区加载数据"></a>动态分区加载数据</h3><ul><li><p>查看表分区</p><p><code>show partitions prit_tbl;</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">partition   |</span><br><span class="line"><span class="comment">------------|</span></span><br><span class="line">day=20190129|</span><br><span class="line">day=20190130|</span><br></pre></td></tr></table></figure></li><li><p>自动识别分区，不需要明确指定</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> prit_tbl <span class="keyword">partition</span>(<span class="keyword">day</span>) <span class="keyword">select</span> * <span class="keyword">from</span> test_tbl;</span><br></pre></td></tr></table></figure></li><li><p>查询验证：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> prit_tbl;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> prit_tbl <span class="keyword">where</span> <span class="keyword">day</span> = <span class="number">20190129</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> prit_tbl <span class="keyword">where</span> <span class="keyword">day</span> = <span class="number">20190130</span>;</span><br></pre></td></tr></table></figure><p>HDFS web界面验证</p></li></ul><h3 id="分区注意"><a href="#分区注意" class="headerlink" title="分区注意"></a>分区注意</h3><ol><li><p>尽量不要是用动态分区，因为动态分区的时候，将会为每一个分区分配reducer数量，当分区数量多的时候，reducer数量将会增加，对服务器是一种灾难。</p></li><li><p>动态分区和静态分区的区别: 静态分区不管有没有数据都将会创建该分区，动态分区是有结果集将创建，否则不创建。</p></li><li><p>hive动态分区的严格模式和hive提供的<code>hive.mapred.mode的</code>严格模式,为了阻止用户不小心提交恶意<code>hql</code>。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive.mapred.mode=nostrict : strict</span><br></pre></td></tr></table></figure><p>如果该模式值为strict，将会阻止以下三种查询：</p><ol><li>对分区表查询，where中过滤字段不是分区字段。</li><li>笛卡尔积join查询，join查询语句，不带on条件 或者 where条件。</li><li>对order by查询，有order by的查询不带limit语句。</li></ol></li></ol><h2 id="桶表"><a href="#桶表" class="headerlink" title="桶表"></a>桶表</h2><h3 id="分桶的概念"><a href="#分桶的概念" class="headerlink" title="分桶的概念"></a>分桶的概念</h3><p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。</p><p>分桶是将数据集分解成更容易管理的若干部分的另一个技术。</p><p><code>分区针对的是数据的存储路径；分桶针对的是数据文件。</code></p><h3 id="分桶的好处"><a href="#分桶的好处" class="headerlink" title="分桶的好处"></a>分桶的好处</h3><ul><li>分桶规则：对分桶字段值进行哈希，哈希值除以桶的个数求余，余数决定了该条记录在哪个桶中，也就是余数相同的在一个桶中。</li><li>优点：<ol><li>提高join查询效率 </li><li>提高抽样效率</li></ol></li></ul><h3 id="分桶实践"><a href="#分桶实践" class="headerlink" title="分桶实践"></a>分桶实践</h3><ol start="2"><li><p>准备数据</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1,name1</span><br><span class="line">2,name2</span><br><span class="line">3,name3</span><br><span class="line">4,name4</span><br><span class="line">5,name5</span><br><span class="line">6,name6</span><br><span class="line">7,name7</span><br><span class="line">8,name8</span><br><span class="line">9,name9</span><br></pre></td></tr></table></figure></li><li><p>创建桶表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> bucket_tbl(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>) </span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure></li><li><p>创建中间表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> mid_tbl(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure></li><li><p>加载数据到中间表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/bucket.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> mid_tbl;</span><br></pre></td></tr></table></figure></li><li><p>设置强制分桶</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set hive.enforce.bucketing = true; --Hive 2.x 不需要这一步</span><br><span class="line">set mapreduce.job.reduces=-1;</span><br></pre></td></tr></table></figure><p>在 Hive 0.x and 1.x 版本，必须使用设置 <code>hive.enforce.bucketing = true</code>，表示强制分桶，允许程序根据表结构自动选择正确数量的 Reducer 和 cluster by column 来进行分桶。</p></li><li><p>插入数据到分桶表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> bucket_tbl <span class="keyword">select</span> * <span class="keyword">from</span> mid_tbl;</span><br></pre></td></tr></table></figure></li><li><p>查看结果</p><p>HDFS：<code>桶是以文件的形式存在的，而不是像分区那样以文件夹的形式存在。</code></p><p><img src="https://upload-images.jianshu.io/upload_images/12267859-d7e00bb44b332b5b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/format/webp" alt="分桶文件"></p></li></ol><h3 id="有序的分桶表"><a href="#有序的分桶表" class="headerlink" title="有序的分桶表"></a>有序的分桶表</h3><ul><li><p>如果要按id升序排序可以这样建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_bucket_sorted (</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'ID'</span>, </span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'名字'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">comment</span> <span class="string">'测试分桶'</span></span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>) </span><br><span class="line">sorted <span class="keyword">by</span> (<span class="keyword">id</span>) </span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span> ;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> bucket_tbl(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>) sorted <span class="keyword">by</span> (<span class="keyword">id</span>) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure><p><code>注意:</code> 同样需要中间表insert插入数据</p></li><li><p>好处:</p><p>因为每个桶内的数据是排序的，这样每个桶进行连接时就变成了高效的归并排序</p></li></ul><h3 id="分桶抽样查询"><a href="#分桶抽样查询" class="headerlink" title="分桶抽样查询"></a>分桶抽样查询</h3><p>对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。</p><h4 id="SQL示例"><a href="#SQL示例" class="headerlink" title="SQL示例"></a>SQL示例</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from test_bucket tablesample (bucket 1 out of 2);</span><br><span class="line">OK</span><br><span class="line">8   name8</span><br><span class="line">4   name4</span><br><span class="line">2   name2</span><br><span class="line">6   name6</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from test tablesample (bucket 1 out of 2 on id);</span><br><span class="line">OK</span><br><span class="line">2   name2</span><br><span class="line">8   name8</span><br><span class="line">4   name4</span><br><span class="line">6   name6</span><br></pre></td></tr></table></figure><h4 id="区别"><a href="#区别" class="headerlink" title="区别:"></a>区别:</h4><ul><li>分桶表后面可以不带on 字段名，不带时默认的是按分桶字段,也可以带，而没有分桶的表则必须带</li><li>按分桶字段取样时，因为分桶表是直接去对应的桶中拿数据，在表比较大时会提高取样效率</li></ul><h4 id="语法"><a href="#语法" class="headerlink" title="语法:"></a>语法:</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">tablesample (bucket x out of y on id);</span><br></pre></td></tr></table></figure><p>x表示从哪个桶开始，y代表分几个桶，也可以理解分x为分子，y为分母，及将表分为y份（桶），取第x份（桶）</p><p>所以这时对于分桶表是有要求的，y为桶数的倍数或因子，</p><ul><li><p>x=1,y=2，取2(4/y)个bucket的数据，分别桶1和桶3(1+y)</p></li><li><p>x=1,y=4, 取1(4/y)个bucket的数据，即桶1</p></li><li><p>x=2,y=8, 取1/2(4/y)个bucket的数据，即桶1的一半</p><p> x的值必须小于等于y的值</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive数据类型&amp;DDL数据定义(增删查改)&amp;DML数据操作(导入导出)</title>
      <link href="/2018/11/04/hive/4/"/>
      <url>/2018/11/04/hive/4/</url>
      
        <content type="html"><![CDATA[<h2 id="Hive数据类型"><a href="#Hive数据类型" class="headerlink" title="Hive数据类型"></a>Hive数据类型</h2><h3 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h3><table><thead><tr><th>Hive数据类型</th><th>Java数据类型</th><th>长度</th><th>例子</th></tr></thead><tbody><tr><td>TINYINT</td><td>byte</td><td>1byte有符号整数</td><td>20</td></tr><tr><td>SMALINT</td><td>short</td><td>2byte有符号整数</td><td>20</td></tr><tr><td>INT</td><td>int</td><td>4byte有符号整数</td><td>20</td></tr><tr><td>BIGINT</td><td>long</td><td>8byte有符号整数</td><td>20</td></tr><tr><td>BOOLEAN</td><td>boolean</td><td>布尔类型，true或者false</td><td>TRUE FALSE</td></tr><tr><td>FLOAT</td><td>float</td><td>单精度浮点数</td><td>3.14159</td></tr><tr><td>DOUBLE</td><td>double</td><td>双精度浮点数</td><td>3.14159</td></tr><tr><td>STRING</td><td>string</td><td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td><td>‘now is the time’ “for all good men”</td></tr><tr><td>TIMESTAMP</td><td></td><td>时间类型</td><td></td></tr><tr><td>BINARY</td><td></td><td>字节数组</td><td></td></tr></tbody></table><p>对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</p><h3 id="集合数据类型"><a href="#集合数据类型" class="headerlink" title="集合数据类型"></a>集合数据类型</h3><table><thead><tr><th>数据类型</th><th>描述</th><th>语法示例</th></tr></thead><tbody><tr><td>STRUCT</td><td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td><td>struct()  例如struct&lt;street:string,  city:string&gt;</td></tr><tr><td>MAP</td><td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td><td>map()  例如map&lt;string, int&gt;</td></tr><tr><td>ARRAY</td><td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’,  ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td><td>Array()  例如array<string></td></tr></tbody></table><p>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p><ul><li><p>案例实操</p><ol><li><p>假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问的格式为</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"name"</span>: <span class="string">"songsong"</span>,</span><br><span class="line">    <span class="attr">"friends"</span>: [<span class="string">"bingbing"</span> , <span class="string">"lili"</span>] ,       <span class="comment">//列表Array, </span></span><br><span class="line">    <span class="attr">"children"</span>: &#123;                      <span class="comment">//键值Map,</span></span><br><span class="line">        <span class="attr">"xiao song"</span>: <span class="number">18</span> ,</span><br><span class="line">        <span class="attr">"xiaoxiao song"</span>: <span class="number">19</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">"address"</span>: &#123;                      <span class="comment">//结构Struct,</span></span><br><span class="line">        <span class="attr">"street"</span>: <span class="string">"hui long guan"</span> ,</span><br><span class="line">        <span class="attr">"city"</span>: <span class="string">"beijing"</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>基于上述数据结构，我们在Hive里创建对应的表，并导入数据。 创建本地测试文件test.txt</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing</span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</span><br></pre></td></tr></table></figure><p>注意：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。</p></li><li><p>Hive上创建测试表test</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">test</span>(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">friends <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">children <span class="keyword">map</span>&lt;<span class="keyword">string</span>, <span class="built_in">int</span>&gt;,</span><br><span class="line">address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>, city:<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'_'</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">':'</span></span><br><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span>;<span class="comment">#默认"\n"</span></span><br></pre></td></tr></table></figure><p>字段解释：</p><p><code>row format delimited fields terminated by &#39;,&#39;</code>      – 列分隔符</p><p><code>collection items terminated by &#39;_&#39;</code>                             – MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</p><p><code>map keys terminated by &#39;:&#39;</code>                                              – MAP中的key与value的分隔符</p><p><code>lines terminated by &#39;\n&#39;;</code>                                                – 行分隔符</p></li><li><p>导入文本数据到测试表</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath ‘/opt/module/datas/test.txt’into table test</span><br></pre></td></tr></table></figure></li><li><p>访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select friends[1],children['xiao song'],address.city from test</span><br><span class="line">where name="songsong";</span><br><span class="line">OK</span><br><span class="line">_c0     _c1     city</span><br><span class="line">lili    18      beijing</span><br><span class="line">Time taken: 0.076 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure></li></ol></li></ul><h2 id="DDL数据定义"><a href="#DDL数据定义" class="headerlink" title="DDL数据定义"></a>DDL数据定义</h2><h3 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] database_name</span><br><span class="line">[<span class="keyword">COMMENT</span> database_comment]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[<span class="keyword">WITH</span> DBPROPERTIES (property_name=property_value, ...)];</span><br></pre></td></tr></table></figure><ul><li>创建一个数据库，数据库在HDFS上的默认存储路径是/user/hive/warehouse/*.db。</li></ul><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br></pre></td></tr></table></figure><ul><li>避免要创建的数据库已经存在错误，增加if not exists判断。（标准写法）</li></ul><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already exists</span><br><span class="line">hive (default)&gt; create database if not exists db_hive;</span><br></pre></td></tr></table></figure><ul><li>创建一个数据库，指定数据库在HDFS上存放的位置</li></ul><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive2 location '/db_hive2.db';</span><br></pre></td></tr></table></figure><h3 id="查询数据库"><a href="#查询数据库" class="headerlink" title="查询数据库"></a>查询数据库</h3><h4 id="显示数据库"><a href="#显示数据库" class="headerlink" title="显示数据库"></a>显示数据库</h4><ol><li><p>显示数据库</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure></li><li><p>过滤显示查询的数据库</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; show databases like 'db_hive*';</span><br><span class="line">OK</span><br><span class="line">db_hive</span><br><span class="line">db_hive_1</span><br></pre></td></tr></table></figure></li></ol><h4 id="查看数据库详情"><a href="#查看数据库详情" class="headerlink" title="查看数据库详情"></a>查看数据库详情</h4><ol><li><p>显示数据库信息</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; desc database db_hive;</span><br><span class="line">OK</span><br><span class="line">db_hivehdfs://aliyun:9000/user/hive/warehouse/db_hive.dbhadoopUSER</span><br></pre></td></tr></table></figure></li><li><p>显示数据库详细信息，extended</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br><span class="line">OK</span><br><span class="line">db_hivehdfs://aliyun:9000/user/hive/warehouse/db_hive.dbhadoopUSER</span><br></pre></td></tr></table></figure></li></ol><h4 id="切换当前数据库"><a href="#切换当前数据库" class="headerlink" title="切换当前数据库"></a>切换当前数据库</h4><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; use db_hive;</span><br></pre></td></tr></table></figure><h3 id="修改数据库"><a href="#修改数据库" class="headerlink" title="修改数据库"></a>修改数据库</h3><p>用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter database db_hive set dbproperties('createtime'='20170830');</span><br></pre></td></tr></table></figure><p>在hive中查看修改结果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br><span class="line">db_name <span class="keyword">comment</span> location        owner_name      owner_type      <span class="keyword">parameters</span></span><br><span class="line">db_hive         hdfs://aliyun:<span class="number">8020</span>/<span class="keyword">user</span>/hive/warehouse/db_hive.db    hadoop <span class="keyword">USER</span>    &#123;createtime=<span class="number">20170830</span>&#125;</span><br></pre></td></tr></table></figure><h3 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h3><ul><li><p>删除空数据库</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt;drop database db_hive2;</span><br></pre></td></tr></table></figure></li><li><p>如果删除的数据库不存在，最好采用 if exists判断数据库是否存在</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: SemanticException [Error 10072]: Database does not exist: db_hive</span><br><span class="line">hive&gt; drop database if exists db_hive2;</span><br></pre></td></tr></table></figure></li><li><p>如果数据库不为空，可以采用cascade命令，强制删除</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database db_hive is not empty. One or more tables exist.)</span><br><span class="line">hive&gt; drop database db_hive cascade;</span><br></pre></td></tr></table></figure></li></ul><h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><ul><li><p>建表语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name </span><br><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment] </span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) </span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS] </span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] </span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name=property_value, ...)]</span><br><span class="line">[<span class="keyword">AS</span> select_statement]</span><br></pre></td></tr></table></figure></li><li><p>字段解释说明 </p><ol><li><p><code>CREATE TABLE</code> 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。</p></li><li><p><code>EXTERNAL</code>关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实际数据的路径（LOCATION），在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</p></li><li><p><code>COMMENT</code>为表和列添加注释。</p></li><li><p><code>PARTITIONED BY</code>创建分区表</p></li><li><p><code>CLUSTERED BY</code>创建分桶表</p></li><li><p><code>SORTED BY</code>不常用，对桶中的一个或多个列另外排序</p></li><li><p><code>ROW FORMAT</code> </p><p><code>DELIMITED [FIELDS TERMINATED BY char]</code> </p><p><code>[COLLECTION ITEMS TERMINATED BY char]</code></p><p><code>[MAP KEYS TERMINATED BY char]</code> </p><p><code>[LINES TERMINATED BY char]</code> </p><p><code>SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</code></p><p>用户在建表的时候可以自定义<code>SerDe</code>或者使用自带的<code>SerDe</code>。如果没有指定<code>ROW FORMAT</code> 或者<code>ROW FORMAT DELIMITED</code>，将会使用自带的<code>SerDe</code>。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的<code>SerDe</code>，Hive通过<code>SerDe</code>确定表的具体的列的数据。</p><p><code>SerDe</code>是<code>Serialize</code>/<code>Deserilize</code>的简称， hive使用<code>Serde</code>进行行对象的序列与反序列化。</p></li><li><p><code>STORED AS</code>指定存储文件类型</p><p>常用的存储文件类型：<code>SEQUENCEFILE</code>（二进制序列文件）、<code>TEXTFILE</code>（文本）、<code>RCFILE</code>（列式存储格式文件）</p><p>如果文件数据是纯文本，可以使用<code>STORED AS TEXTFILE</code>。如果数据需要压缩，使用 <code>STORED AS</code> <code>SEQUENCEFILE</code>。</p></li><li><p><code>LOCATION</code> ：指定表在HDFS上的存储位置。</p></li><li><p><code>AS</code>：后跟查询语句，根据查询结果创建表。</p></li><li><p><code>LIKE</code>允许用户复制现有的表结构，但是不复制数据。</p></li></ol></li></ul><h4 id="管理表"><a href="#管理表" class="headerlink" title="管理表"></a>管理表</h4><ul><li><p>理论</p><p>默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。 当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据。</p></li><li><p>案例实操</p><ol><li><p>普通创建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student2(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile</span><br><span class="line">location <span class="string">'/user/hive/warehouse/student2'</span>;</span><br></pre></td></tr></table></figure></li><li><p>根据查询结果创建表（查询的结果会添加到新创建的表中）</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student3 <span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></li><li><p>根据已经存在的表结构创建表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student4 <span class="keyword">like</span> student;</span><br></pre></td></tr></table></figure></li><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure></li></ol></li></ul><h4 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h4><ul><li><p>理论</p><p>因为表是外部表，所以Hive并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。</p></li><li><p>管理表和外部表的使用场景</p><p>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p></li><li><p>案例实操</p><p>分别创建部门和员工外部表，并向表中导入数据。</p><ol><li><p>上传数据到HDFS</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir /student;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /student;</span><br></pre></td></tr></table></figure></li><li><p>建表语句</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create external table stu (</span><br><span class="line">id int, </span><br><span class="line">name string) </span><br><span class="line">row format delimited fields terminated by '\t' </span><br><span class="line">location '/student';</span><br></pre></td></tr></table></figure></li><li><p>查看创建的表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from stu_external;</span><br><span class="line">OK</span><br><span class="line">stu_external.id stu_external.name</span><br><span class="line">1001    lisi</span><br><span class="line">1002    wangwu</span><br></pre></td></tr></table></figure></li><li><p>查看表格式化数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted dept;</span><br><span class="line">Table Type:             EXTERNAL_TABLE</span><br></pre></td></tr></table></figure></li><li><p>删除外部表</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; drop table stu_external;</span><br></pre></td></tr></table></figure><p>外部表删除后，hdfs中的数据还在，但是metadata中stu_external的元数据已被删除</p></li></ol></li></ul><h4 id="管理表与外部表的互相转换"><a href="#管理表与外部表的互相转换" class="headerlink" title="管理表与外部表的互相转换"></a>管理表与外部表的互相转换</h4><ol><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure></li><li><p>修改内部表student2为外部表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'TRUE'</span>);</span><br></pre></td></tr></table></figure></li><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             EXTERNAL_TABLE</span><br></pre></td></tr></table></figure></li><li><p>修改外部表student2为内部表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'FALSE'</span>);</span><br></pre></td></tr></table></figure></li><li><p>查询表的类型</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure><p>注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写！</p></li></ol><h3 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h3><p>分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。</p><h4 id="分区表基本操作"><a href="#分区表基本操作" class="headerlink" title="分区表基本操作"></a>分区表基本操作</h4><ol><li><p>引入分区表（需要根据日期对日志进行管理）</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">/user/hive/warehouse/log_partition/20170702/20170702.log</span><br><span class="line">/user/hive/warehouse/log_partition/20170703/20170703.log</span><br><span class="line">/user/hive/warehouse/log_partition/20170704/20170704.log</span><br></pre></td></tr></table></figure></li><li><p>创建分区表语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table dept (</span><br><span class="line">deptno int, </span><br><span class="line">dname string, </span><br><span class="line">loc string</span><br><span class="line">)</span><br><span class="line">partitioned by (month string)</span><br><span class="line">row format delimited fields terminated by '\t';</span><br></pre></td></tr></table></figure><p><code>注意：</code> 分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。</p></li><li><p>加载数据到分区表中</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept partition(month='201709');</span><br><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept partition(month='201708');</span><br><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept partition(month='201707’);</span><br></pre></td></tr></table></figure><p><code>注意：</code> 分区表加载数据时，必须指定分区</p></li><li><p>查询分区表中数据</p><ol><li><p>单分区查询</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month='201709';</span><br></pre></td></tr></table></figure></li><li><p>多分区联合查询</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month='201709'</span><br><span class="line">              union all</span><br><span class="line">              <span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201708'</span></span><br><span class="line">              <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">              <span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201707'</span>;</span><br><span class="line"></span><br><span class="line">_u3.deptno      _u3.dname       _u3.loc _u3.month</span><br><span class="line">10      ACCOUNTING      NEW YORK        201707</span><br><span class="line">10      ACCOUNTING      NEW YORK        201708</span><br><span class="line">10      ACCOUNTING      NEW YORK        201709</span><br><span class="line">20      RESEARCH        DALLAS  201707</span><br><span class="line">20      RESEARCH        DALLAS  201708</span><br><span class="line">20      RESEARCH        DALLAS  201709</span><br><span class="line">30      SALES   CHICAGO 201707</span><br><span class="line">30      SALES   CHICAGO 201708</span><br><span class="line">30      SALES   CHICAGO 201709</span><br><span class="line">40      OPERATIONS      BOSTON  201707</span><br><span class="line">40      OPERATIONS      BOSTON  201708</span><br><span class="line">40      OPERATIONS      BOSTON  201709</span><br></pre></td></tr></table></figure></li></ol></li><li><p>增加分区</p><ol><li><p>创建单个分区</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(month='201706') ;</span><br></pre></td></tr></table></figure></li><li><p>同时创建多个分区</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(month='201705') partition(month='201704');</span><br></pre></td></tr></table></figure></li></ol></li><li><p>删除分区</p><ol><li><p>删除单个分区</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (month='201704');</span><br></pre></td></tr></table></figure></li><li><p>同时删除多个分区</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (month='201705'), partition (month='201706');</span><br></pre></td></tr></table></figure></li></ol></li><li><p>查看分区表有多少分区</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show partitions dept_partition;</span><br></pre></td></tr></table></figure></li><li><p>查看分区表结构</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; desc formatted dept_partition;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Partition Information          </span></span><br><span class="line"><span class="comment"># col_name              data_type               comment             </span></span><br><span class="line">month                   string</span><br></pre></td></tr></table></figure></li></ol><h4 id="分区表注意事项"><a href="#分区表注意事项" class="headerlink" title="分区表注意事项"></a>分区表注意事项</h4><ul><li><p>创建二级分区表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition2(</span><br><span class="line">               deptno int, </span><br><span class="line">               dname string, </span><br><span class="line">               loc string</span><br><span class="line">               )</span><br><span class="line">               partitioned by (month string, day string)</span><br><span class="line">               row format delimited fields terminated by '\t';</span><br></pre></td></tr></table></figure></li><li><p>正常的加载数据</p><ol><li><p>加载数据到二级分区表中</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table</span><br><span class="line"> default.dept_partition2 partition(month='201709', day='13');</span><br></pre></td></tr></table></figure></li><li><p>查询分区数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='13';</span><br></pre></td></tr></table></figure></li></ol></li><li><p>把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式</p><ol><li><p>方式一：上传数据后修复(<code>禁用</code>)</p><p>上传数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br></pre></td></tr></table></figure><p>查询数据（查询不到刚上传的数据）</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='12';</span><br></pre></td></tr></table></figure><p>执行修复命令</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; msck repair table dept_partition2;</span><br></pre></td></tr></table></figure><p>再次查询数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='12';</span><br></pre></td></tr></table></figure></li><li><p>方式二：上传数据后添加分区(<code>推介</code>)</p><p>上传数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br></pre></td></tr></table></figure><p>执行添加分区</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 add partition(month='201709',</span><br><span class="line"> day='11');</span><br></pre></td></tr></table></figure><p>查询数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='11';</span><br></pre></td></tr></table></figure></li><li><p>方式三：创建文件夹后load数据到分区</p><p>创建目录</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=10;</span><br></pre></td></tr></table></figure><p>上传数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table</span><br><span class="line"> dept_partition2 partition(month='201709',day='10');</span><br></pre></td></tr></table></figure><p>查询数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='10';</span><br></pre></td></tr></table></figure></li></ol></li></ul><h3 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h3><h4 id="重命名表"><a href="#重命名表" class="headerlink" title="重命名表"></a>重命名表</h4><ol><li><p>语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">RENAME</span> <span class="keyword">TO</span> new_table_name</span><br></pre></td></tr></table></figure></li><li><p>实操案例</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 rename to dept_partition3;</span><br></pre></td></tr></table></figure></li></ol><h4 id="增加、修改和删除表分区"><a href="#增加、修改和删除表分区" class="headerlink" title="增加、修改和删除表分区"></a>增加、修改和删除表分区</h4><p>详见分区表基本操作。</p><h4 id="增加-修改-替换列信息"><a href="#增加-修改-替换列信息" class="headerlink" title="增加/修改/替换列信息"></a>增加/修改/替换列信息</h4><ul><li><p>语法</p><ol><li><p>更新列</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">CHANGE</span> [<span class="keyword">COLUMN</span>] col_old_name col_new_name column_type [<span class="keyword">COMMENT</span> col_comment] [<span class="keyword">FIRST</span>|<span class="keyword">AFTER</span> column_name]</span><br></pre></td></tr></table></figure></li><li><p>增加和替换列</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">ADD</span>|<span class="keyword">REPLACE</span> <span class="keyword">COLUMNS</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)</span><br></pre></td></tr></table></figure></li></ol><p><code>注：</code> ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。</p></li><li><p>实操案例</p><ol><li><p>查询表结构</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure></li><li><p>添加列</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add columns(deptdesc string);</span><br></pre></td></tr></table></figure></li><li><p>查询表结构</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure></li><li><p>更新列</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition change column deptdesc desc int;</span><br></pre></td></tr></table></figure></li><li><p>查询表结构</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure></li><li><p>替换列</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition replace columns(deptno string, dname</span><br><span class="line"> string, loc string);</span><br></pre></td></tr></table></figure></li><li><p>查询表结构</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure></li></ol></li></ul><h3 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; drop table dept_partition;</span><br></pre></td></tr></table></figure><h2 id="DML数据操作"><a href="#DML数据操作" class="headerlink" title="DML数据操作"></a>DML数据操作</h2><h3 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h3><h4 id="向表中装载数据（Load）"><a href="#向表中装载数据（Load）" class="headerlink" title="向表中装载数据（Load）"></a>向表中装载数据（Load）</h4><ul><li><p>语法</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; load data [local] inpath '/opt/module/datas/student.txt' [overwrite] into table student [partition (partcol1=val1,…)];</span><br></pre></td></tr></table></figure><p><code>load data:</code> 表示加载数据</p><p><code>local:</code> 表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</p><p><code>inpath:</code> 表示加载数据的路径</p><p><code>overwrite:</code> 表示覆盖表中已有数据，否则表示追加</p><p><code>into table:</code> 表示加载到哪张表</p><p><code>student:</code> 表示具体的表</p><p><code>partition:</code> 表示上传到指定分区</p></li><li><p>实操案例</p><ol><li><p>创建一张表</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table student(id string, name string) row format delimited fields terminated by '\t';</span><br></pre></td></tr></table></figure></li><li><p>加载本地文件到hive</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/student.txt' into table default.student;</span><br></pre></td></tr></table></figure></li><li><p>加载HDFS文件到hive中</p><p>上传文件到HDFS</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/hadoop/hive;</span><br></pre></td></tr></table></figure><p>加载HDFS上数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data inpath '/user/hadoop/hive/student.txt' into table default.student;</span><br></pre></td></tr></table></figure></li><li><p>加载数据覆盖表中已有的数据</p><p>上传文件到HDFS</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/hadoop/hive;</span><br></pre></td></tr></table></figure><p>加载数据覆盖表中已有的数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data inpath '/user/hadoop/hive/student.txt' overwrite into table default.student;</span><br></pre></td></tr></table></figure></li></ol></li></ul><h4 id="通过查询语句向表中插入数据（Insert）"><a href="#通过查询语句向表中插入数据（Insert）" class="headerlink" title="通过查询语句向表中插入数据（Insert）"></a>通过查询语句向表中插入数据（Insert）</h4><ol><li><p>创建一张分区表</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table student(id int, name string) partitioned by (month string) row format delimited fields terminated by '\t';</span><br></pre></td></tr></table></figure></li><li><p>基本插入数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert into table  student partition(month='201709') values(1,'wangwu'),(2,’zhaoliu’);</span><br></pre></td></tr></table></figure></li><li><p>基本模式插入（根据单张表查询结果）</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite table student partition(month='201708')</span><br><span class="line">             <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br></pre></td></tr></table></figure><p><code>insert into：</code> 以追加数据的方式插入到表或分区，原有数据不会删除</p><p><code>insert overwrite：</code> 会覆盖表或分区中已存在的数据</p><p><code>注意：</code> insert不支持插入部分字段</p></li><li><p>多表（多分区）插入模式（根据多张表查询结果）</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; from student</span><br><span class="line">              <span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201707'</span>)</span><br><span class="line">              <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span></span><br><span class="line">              <span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201706'</span>)</span><br><span class="line">              <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br></pre></td></tr></table></figure></li></ol><h4 id="查询语句中创建表并加载数据（CTAS）"><a href="#查询语句中创建表并加载数据（CTAS）" class="headerlink" title="查询语句中创建表并加载数据（CTAS）"></a>查询语句中创建表并加载数据（CTAS）</h4><p>详见创建表。</p><p>根据查询结果创建表（查询的结果会添加到新创建的表中）</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student3 <span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><h4 id="创建表时通过Location指定加载数据路径"><a href="#创建表时通过Location指定加载数据路径" class="headerlink" title="创建表时通过Location指定加载数据路径"></a>创建表时通过Location指定加载数据路径</h4><ol><li><p>上传数据到hdfs上</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir /student;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /student;</span><br></pre></td></tr></table></figure></li><li><p>创建表，并指定在hdfs上的位置</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create external table if not exists student5(</span><br><span class="line">              id int, name string</span><br><span class="line">              )</span><br><span class="line">              row format delimited fields terminated by '\t'</span><br><span class="line">              location '/student;</span><br></pre></td></tr></table></figure></li><li><p>查询数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from student5;</span><br></pre></td></tr></table></figure></li></ol><h4 id="Import数据到指定Hive表中"><a href="#Import数据到指定Hive表中" class="headerlink" title="Import数据到指定Hive表中"></a>Import数据到指定Hive表中</h4><p>注意：先用export导出后，再将数据导入。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; import table student2 partition(month='201709') from</span><br><span class="line"> '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure><h3 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h3><h4 id="Insert导出"><a href="#Insert导出" class="headerlink" title="Insert导出"></a>Insert导出</h4><ol><li><p>将查询的结果导出到本地</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory '/opt/module/datas/export/student' select * from student;</span><br></pre></td></tr></table></figure></li><li><p>将查询的结果格式化导出到本地</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive(default)&gt;insert overwrite local directory '/opt/module/datas/export/student1' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' select * from student;</span><br></pre></td></tr></table></figure></li><li><p>将查询的结果导出到HDFS上(没有local)</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite directory '/user/hadoop/student2' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' select * from student;</span><br></pre></td></tr></table></figure></li></ol><h4 id="Hadoop命令导出到本地"><a href="#Hadoop命令导出到本地" class="headerlink" title="Hadoop命令导出到本地"></a>Hadoop命令导出到本地</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -get /user/hive/warehouse/student/month=201709/000000_0</span><br><span class="line">/opt/module/datas/export/student3.txt;</span><br></pre></td></tr></table></figure><h4 id="Hive-Shell-命令导出"><a href="#Hive-Shell-命令导出" class="headerlink" title="Hive Shell 命令导出"></a>Hive Shell 命令导出</h4><p>基本语法：（hive -f/-e 执行语句或者脚本 &gt; file）</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop102 hive]$ bin/hive -e 'select * from default.student;' &gt;</span><br><span class="line"> /opt/module/datas/export/student4.txt;</span><br></pre></td></tr></table></figure><h4 id="Export导出到HDFS上"><a href="#Export导出到HDFS上" class="headerlink" title="Export导出到HDFS上"></a>Export导出到HDFS上</h4><figure class="highlight"><table><tr><td class="code"><pre><span class="line">(defahiveult)&gt; export table default.student to '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure><p>export和import主要用于两个Hadoop平台集群之间Hive表迁移。</p><h4 id="Sqoop导出"><a href="#Sqoop导出" class="headerlink" title="Sqoop导出"></a>Sqoop导出</h4><p>后续的博客讲解</p><h3 id="清除表中数据（Truncate）"><a href="#清除表中数据（Truncate）" class="headerlink" title="清除表中数据（Truncate）"></a>清除表中数据（Truncate）</h3><p>注意：Truncate只能删除管理表，不能删除外部表中数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; truncate table student;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的部署和初始化工作&amp;验证Hive部署成功</title>
      <link href="/2018/11/03/hive/3/"/>
      <url>/2018/11/03/hive/3/</url>
      
        <content type="html"><![CDATA[<h3 id="Hive安装部署"><a href="#Hive安装部署" class="headerlink" title="Hive安装部署"></a>Hive安装部署</h3><h4 id="Hive安装及配置"><a href="#Hive安装及配置" class="headerlink" title="Hive安装及配置"></a>Hive安装及配置</h4><ol><li><p>把apache-hive-1.2.1-bin.tar.gz上传到linux的/opt/software目录下</p></li><li><p>解压apache-hive-1.2.1-bin.tar.gz到/opt/module/目录下面</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun software]$ tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li><li><p>修改apache-hive-1.2.1-bin.tar.gz的名称为hive</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun module]$ mv apache-hive-1.2.1-bin/ hive</span><br></pre></td></tr></table></figure></li><li><p>修改/opt/module/hive/conf目录下的hive-env.sh.template名称为hive-env.sh</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun conf]$ mv hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure></li><li><p>配置hive-env.sh文件</p><ol><li><p>配置HADOOP_HOME路径</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure></li><li><p>配置HIVE_CONF_DIR路径</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HIVE_CONF_DIR=/opt/module/hive/conf</span><br></pre></td></tr></table></figure></li></ol></li></ol><h4 id="Hadoop集群配置"><a href="#Hadoop集群配置" class="headerlink" title="Hadoop集群配置"></a>Hadoop集群配置</h4><ol><li><p>必须启动hdfs和yarn</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[hadoop@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></li><li><p>在HDFS上创建/tmp和/user/hive/warehouse两个目录并修改他们的同组权限可写</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop-2.7.2]$ bin/hadoop fs -mkdir /tmp</span><br><span class="line">[hadoop@aliyun hadoop-2.7.2]$ bin/hadoop fs -mkdir -p /user/hive/warehouse</span><br><span class="line">[hadoop@aliyun hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /tmp</span><br></pre></td></tr></table></figure></li></ol><h4 id="Hive基本操作"><a href="#Hive基本操作" class="headerlink" title="Hive基本操作"></a>Hive基本操作</h4><ol><li><p>启动hive</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive</span><br></pre></td></tr></table></figure></li><li><p>查看数据库</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure></li><li><p>打开默认数据库</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; use default;</span><br></pre></td></tr></table></figure></li><li><p>显示default数据库中的表</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure></li><li><p>创建一张表</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; create table student(id int, name string);</span><br></pre></td></tr></table></figure></li><li><p>显示数据库中有几张表</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure></li><li><p>查看表的结构</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc student;</span><br></pre></td></tr></table></figure></li><li><p>向表中插入数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; insert into student values(1000,"ss");</span><br></pre></td></tr></table></figure></li><li><p>查询表中数据</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from student;</span><br></pre></td></tr></table></figure></li><li><p>退出hive</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; quit;</span><br></pre></td></tr></table></figure></li></ol><ul><li>说明：<br><code>数据库：</code>在hdfs中表现为${hive.metastore.warehouse.dir}目录下一个文件夹<br><code>表：</code>在hdfs中表现所属db目录下一个文件夹，文件夹中存放该表中的具体数据</li></ul><h3 id="MySql安装"><a href="#MySql安装" class="headerlink" title="MySql安装"></a>MySql安装</h3><ul><li>安装过程请看我的另外一个博客: <a href="https://yerias.github.io/2018/10/01/DataWarehouse/mysql/1/">https://yerias.github.io/2018/10/01/DataWarehouse/mysql/1/</a></li></ul><h3 id="Hive元数据配置到MySql"><a href="#Hive元数据配置到MySql" class="headerlink" title="Hive元数据配置到MySql"></a>Hive元数据配置到MySql</h3><h4 id="驱动拷贝"><a href="#驱动拷贝" class="headerlink" title="驱动拷贝"></a>驱动拷贝</h4><ol><li><p>在/opt/software/mysql-libs目录下解压mysql-connector-java-5.1.27.tar.gz驱动包</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun mysql-libs]# tar -zxvf mysql-connector-java-5.1.27.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>拷贝/opt/software/mysql-libs/mysql-connector-java-5.1.27目录下的mysql-connector-java-5.1.27-bin.jar到/opt/module/hive/lib/</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun mysql-connector-java-5.1.27]# cp mysql-connector-java-5.1.27-bin.jar</span><br><span class="line">/opt/module/hive/lib/</span><br></pre></td></tr></table></figure></li></ol><h4 id="配置Metastore到MySql"><a href="#配置Metastore到MySql" class="headerlink" title="配置Metastore到MySql"></a>配置Metastore到MySql</h4><ol><li><p>在/opt/module/hive/conf目录下创建一个hive-site.xml</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun conf]$ touch hive-site.xml</span><br><span class="line">[hadoop@aliyun conf]$ vi hive-site.xml</span><br></pre></td></tr></table></figure></li><li><p>根据官方文档配置参数，拷贝数据到hive-site.xml文件中<br><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;?xml version="1.0"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">&lt;value&gt;jdbc:mysql://aliyun:3306/metastore?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">&lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;root&lt;/value&gt;</span><br><span class="line">&lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">&lt;value&gt;000000&lt;/value&gt;</span><br><span class="line">&lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li></ol><h4 id="多窗口启动Hive测试"><a href="#多窗口启动Hive测试" class="headerlink" title="多窗口启动Hive测试"></a>多窗口启动Hive测试</h4><ol><li><p>先启动MySQL</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun mysql-libs]$ mysql -uroot -p000000</span><br></pre></td></tr></table></figure><p>查看有几个数据库</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; show databases;</span><br><span class="line">   +<span class="comment">--------------------+</span></span><br><span class="line">   | Database |</span><br><span class="line">   +<span class="comment">--------------------+</span></span><br><span class="line">   | information_schema |</span><br><span class="line">   | mysql |</span><br><span class="line">   | performance_schema |</span><br><span class="line">   | test |</span><br><span class="line">   +<span class="comment">--------------------+</span></span><br></pre></td></tr></table></figure></li><li><p>再次打开多个窗口，分别启动hive</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive</span><br></pre></td></tr></table></figure></li><li><p>启动hive后，回到MySQL窗口查看数据库，显示增加了metastore数据库</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; show databases;</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line">| Database |</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line">| information_schema |</span><br><span class="line">| metastore |</span><br><span class="line">| mysql |</span><br><span class="line">| performance_schema |</span><br><span class="line">| test |</span><br><span class="line">+<span class="comment">--------------------+</span></span><br></pre></td></tr></table></figure></li></ol><h4 id="HiveJDBC访问"><a href="#HiveJDBC访问" class="headerlink" title="HiveJDBC访问"></a>HiveJDBC访问</h4><ol><li><p>启动hiveserver2服务</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hiveserver2</span><br></pre></td></tr></table></figure></li><li><p>启动beeline</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/beeline</span><br><span class="line">Beeline version 1.2.1 by Apache Hive</span><br><span class="line"><span class="meta">beeline&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>连接hiveserver2</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">beeline&gt;</span><span class="bash"> !connect jdbc:hive2://aliyun:10000（回车）</span></span><br><span class="line">Connecting to jdbc:hive2://aliyun:10000</span><br><span class="line">Enter username for jdbc:hive2://aliyun:10000: hadoop（回车）</span><br><span class="line">Enter password for jdbc:hive2://aliyun:10000: （直接回车）</span><br><span class="line">Connected to: Apache Hive (version 1.2.1)</span><br><span class="line">Driver: Hive JDBC (version 1.2.1)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">0: jdbc:hive2://aliyun:10000&gt; show databases;</span><br><span class="line">+----------------+--+</span><br><span class="line">| database_name |</span><br><span class="line">+----------------+--+</span><br><span class="line">| default |</span><br><span class="line">| hive_db2 |</span><br><span class="line">+----------------+--+</span><br></pre></td></tr></table></figure></li></ol><h4 id="Hive常用交互命令"><a href="#Hive常用交互命令" class="headerlink" title="Hive常用交互命令"></a>Hive常用交互命令</h4><ol><li><p>Hive命令帮助</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive -help</span><br><span class="line">usage: hive</span><br><span class="line">-d,--define &lt;key=value&gt; Variable subsitution to apply to hive</span><br><span class="line">commands. e.g. -d A=B or --define A=B</span><br><span class="line">--database &lt;databasename&gt; Specify the database to use</span><br><span class="line">-e &lt;quoted-query-string&gt; SQL from command line</span><br><span class="line">-f &lt;filename&gt; SQL from files</span><br><span class="line">-H,--help Print help information</span><br><span class="line">--hiveconf &lt;property=value&gt; Use value for given property</span><br><span class="line">--hivevar &lt;key=value&gt; Variable subsitution to apply to hive</span><br><span class="line">commands. e.g. --hivevar A=B</span><br><span class="line">-i &lt;filename&gt; Initialization SQL file</span><br><span class="line">-S,--silent Silent mode in interactive shell</span><br><span class="line">-v,--verbose Verbose mode (echo executed SQL to the console)</span><br></pre></td></tr></table></figure></li><li><p><code>&quot;-e&quot;</code>不进入hive的交互窗口执行sql语句</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive -e "select id from student;"</span><br></pre></td></tr></table></figure></li><li><p><code>&quot;-f&quot;</code>执行脚本中sql语句</p><ol><li><p>在/opt/module/datas目录下创建hivef.sql文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun datas]$ touch hivef.sql</span><br></pre></td></tr></table></figure><p>文件中写入正确的sql语句</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">select * from student;</span><br></pre></td></tr></table></figure></li><li><p>执行文件中的sql语句</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive -f /opt/module/datas/hivef.sql</span><br></pre></td></tr></table></figure></li><li><p>执行文件中的sql语句并将结果写入文件中</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive -f /opt/module/datas/hivef.sql &gt; /opt/module/datas/hive_result.txt</span><br></pre></td></tr></table></figure></li></ol></li></ol><h4 id="Hive其他命令操作"><a href="#Hive其他命令操作" class="headerlink" title="Hive其他命令操作"></a>Hive其他命令操作</h4><ol><li><p>退出hive窗口：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash"><span class="built_in">exit</span>;</span></span><br><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">quit;</span></span><br></pre></td></tr></table></figure><p>在新版的hive中没区别了，在以前的版本是有的：<br>exit:先隐性提交数据，再退出；<br>quit:不提交数据，退出；</p></li><li><p>在hive cli命令窗口中如何查看hdfs文件系统</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">dfs -ls /;</span></span><br></pre></td></tr></table></figure></li><li><p>在hive cli命令窗口中如何查看本地文件系统</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">! ls /opt/module/datas;</span></span><br></pre></td></tr></table></figure></li><li><p>查看在hive中输入的所有历史命令</p><ol><li><p>进入到当前用户的根目录/root或/home/hadoop</p></li><li><p>查看. hivehistory文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ cat .hivehistory</span><br></pre></td></tr></table></figure></li></ol></li></ol><h3 id="Hive常见属性配置"><a href="#Hive常见属性配置" class="headerlink" title="Hive常见属性配置"></a>Hive常见属性配置</h3><h4 id="Hive数据仓库位置配置"><a href="#Hive数据仓库位置配置" class="headerlink" title="Hive数据仓库位置配置"></a>Hive数据仓库位置配置</h4><ol><li><p>Default数据仓库的最原始位置是在hdfs上的：/user/hive/warehouse路径下。</p></li><li><p>在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹。</p></li><li><p>修改default数据仓库原始位置（将hive-default.xml.template如下配置信息拷贝到hive-site.xml文件中）。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">&lt;description&gt;location of default database for the warehouse&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>配置同组用户有执行权限</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/hdfs dfs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure></li></ol><h4 id="查询后信息显示配置"><a href="#查询后信息显示配置" class="headerlink" title="查询后信息显示配置"></a>查询后信息显示配置</h4><ol><li><p>在hive-site.xml文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.cli.print.header&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.cli.print.current.db&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li><li><p>重新启动hive，对比配置前后差异。</p></li></ol><h4 id="Hive运行日志信息配置"><a href="#Hive运行日志信息配置" class="headerlink" title="Hive运行日志信息配置"></a>Hive运行日志信息配置</h4><ol><li><p>Hive的log默认存放在/tmp/hadoop/hive.log目录下（当前用户名下）</p></li><li><p>修改hive的log存放日志到/opt/module/hive/logs</p><ol><li><p>修改/opt/module/hive/conf/hive-log4j.properties.template文件名称为hive-log4j.properties</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun conf]$ pwd</span><br><span class="line">/opt/module/hive/conf</span><br><span class="line">[hadoop@aliyun conf]$ mv hive-log4j.properties.template hive-log4j.properties</span><br></pre></td></tr></table></figure></li></ol></li><li><p>在hive-log4j.properties文件中修改log存放位置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive.log.dir=/opt/module/hive/logs</span><br></pre></td></tr></table></figure></li></ol><h4 id="参数配置方式"><a href="#参数配置方式" class="headerlink" title="参数配置方式"></a>参数配置方式</h4><ol><li><p>查看当前所有的配置信息</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"><span class="built_in">set</span>;</span></span><br></pre></td></tr></table></figure></li><li><p>参数的配置三种方式</p><ol><li><p>配置文件方式<br>默认配置文件：hive-default.xml<br>用户自定义配置文件：hive-site.xml</p><p><code>注意：</code>用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。</p></li><li><p>命令行参数方式<br>启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。<br>例如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10;</span><br></pre></td></tr></table></figure><p>注意：仅对本次hive启动有效<br>查看参数设置：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure></li><li><p>参数声明方式<br>可以在HQL中使用SET关键字设定参数<br>例如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks=100;</span><br></pre></td></tr></table></figure><p>注意：仅对本次hive启动有效。<br>查看参数设置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure></li></ol><p>上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷粒影音8道SQL题(各种Top N)</title>
      <link href="/2018/11/02/hive/2/"/>
      <url>/2018/11/02/hive/2/</url>
      
        <content type="html"><![CDATA[<h4 id="data表字段"><a href="#data表字段" class="headerlink" title="data表字段"></a>data表字段</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">videoId string <span class="keyword">comment</span> <span class="string">"视频唯一id"</span>, </span><br><span class="line">uploader <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">"视频上传者"</span>,</span><br><span class="line">age <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">"视频年龄"</span>,</span><br><span class="line"><span class="keyword">category</span> <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;<span class="keyword">comment</span> <span class="string">"视频类别"</span>,</span><br><span class="line"><span class="keyword">length</span> <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">"视频长度"</span>,</span><br><span class="line">views <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">"观看次数"</span>,</span><br><span class="line">rate <span class="built_in">float</span> <span class="keyword">comment</span> <span class="string">"视频评分"</span>,</span><br><span class="line">ratings <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">"流量"</span>,</span><br><span class="line">comments <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">"评论数"</span>,</span><br><span class="line">relatedId <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;<span class="keyword">comment</span> <span class="string">"相关视频id"</span></span><br></pre></td></tr></table></figure><h4 id="user表字段"><a href="#user表字段" class="headerlink" title="user表字段"></a>user表字段</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">uploader String<span class="keyword">comment</span> <span class="string">"上传者用户名"</span>,</span><br><span class="line">videos <span class="built_in">int</span><span class="keyword">comment</span> <span class="string">"上传视频数"</span>,</span><br><span class="line">friends <span class="built_in">int</span><span class="keyword">comment</span> <span class="string">"朋友数量"</span>,</span><br></pre></td></tr></table></figure><h4 id="8道题目-思路"><a href="#8道题目-思路" class="headerlink" title="8道题目(思路)"></a>8道题目(思路)</h4><p>统计硅谷影音视频网站的常规指标，各种TopN指标：</p><ol><li>统计视频观看数Top10</li><li>统计视频类别热度Top10</li><li>统计视频观看数Top20所属类别</li><li>统计视频观看数Top50所关联视频的所属类别Rank</li><li>统计每个类别中的视频热度Top10</li><li>统计每个类别中视频流量Top10</li><li>统计上传视频最多的用户Top10以及他们上传的视频</li><li>统计每个类别视频观看数Top10</li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive中的字符集编码若干问题</title>
      <link href="/2018/11/01/hive/1/"/>
      <url>/2018/11/01/hive/1/</url>
      
        <content type="html"><![CDATA[<h3 id="个人初始开发环境的基本情况以及Hive元数据库说明"><a href="#个人初始开发环境的基本情况以及Hive元数据库说明" class="headerlink" title="个人初始开发环境的基本情况以及Hive元数据库说明"></a>个人初始开发环境的基本情况以及Hive元数据库说明</h3><ol><li><p>hive的元数据库改成了mysql(安装完mysql之后也没有进行其它别的设置)</p></li><li><p>hive-site.xml中设置元数据库对应的配置为  <code>jdbc:mysql://ip:3306/metastore?createDatabaseIfNotExist=true</code></p></li><li><p>普通情况下咱们的mysql默认编码是latin1,但是我们在日常开发中大多数情况下需要用到utf-8编码,如果是默认latin1的话,咱们的中文存储进去容易乱码,所以说大家在遇到一些数据乱码的情况话,最好把mysql的编码改成utf-8.</p></li></ol><p><code>注意:</code> 但是在这里要非常严重强调的一点:hive的元数据metastore在mysql的数据库,不管是数据库本身,还是里面的表编码都必须是latin1(CHARACTER SET latin1 COLLATE latin1_bin)!!!!!</p><h4 id="验证方式"><a href="#验证方式" class="headerlink" title="验证方式:"></a>验证方式:</h4><p>可以通过客户端软件在数据库上右键属性查看,也可以通过命令查看</p><p>mysql&gt; show create database hive_cz3q;</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">-----------+-----------------------------------------------------------------------------------------+</span></span><br><span class="line">| Database  | <span class="keyword">Create</span> <span class="keyword">Database</span>                                                                         |</span><br><span class="line">+<span class="comment">-----------+-----------------------------------------------------------------------------------------+</span></span><br><span class="line">| hive_cz3q | <span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> <span class="string">`hive_cz3q`</span> <span class="comment">/*!40100 DEFAULT CHARACTER SET latin1 COLLATE latin1_bin */</span> |</span><br><span class="line">+<span class="comment">-----------+-----------------------------------------------------------------------------------------+</span></span><br></pre></td></tr></table></figure><p>不然会有类似如下的错误:</p><p> <img src="https://yerias.github.io/hive_img/610238-20170903131046858-396716990.png" alt="图片1"></p><p>那么怎么修改mysql的编码为utf8呢?这里提供了在线安装修改和离线方式安装下的修改方式供大家选择!</p><h3 id="乱码的情况"><a href="#乱码的情况" class="headerlink" title="乱码的情况:"></a>乱码的情况:</h3><p> 向hive的表中 创建表,表语句部分如下:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ods.ods_order</span><br><span class="line">(</span><br><span class="line">   ORDER_ID             <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'订单ID'</span>,</span><br><span class="line">   ORDER_NO             <span class="built_in">varchar</span>(<span class="number">30</span>)  <span class="keyword">comment</span> <span class="string">'订单编号（唯一字段），</span></span><br><span class="line"><span class="string">   DEALER_ID            int comment '</span>门店<span class="keyword">ID</span><span class="string">',</span></span><br><span class="line"><span class="string">   CUST_ID              int comment '</span>客户<span class="keyword">ID</span><span class="string">',</span></span><br></pre></td></tr></table></figure><p>在创建表的时候，字段可以有 comment，但是 comment 建议不要用中文说明，因为我们说过，hive 的 metastore 支持的字符集是 latin1，所以中文写入的时候会有编码问题，如下图！ </p><p>然后通过desc ods_order 查看 对应的comment中是中文的地方,通过Xshell显示全部都是 “?” 问号.  同时确认了Xshell支持显示中文(排除Xshell的问题).</p><p><code>以上就是说Hive在字段定义时的Comment中文乱码问题.</code></p><p><img src="https://yerias.github.io/hive_img/610238-20170903134309687-1604377616.png" alt="图片2"></p><p>有了上述的问题，那么我们该如何去解决注释中文乱码问题呢？ </p><h3 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h3><h4 id="首先进行Mysql的编码设置"><a href="#首先进行Mysql的编码设置" class="headerlink" title="首先进行Mysql的编码设置"></a>首先进行Mysql的编码设置</h4><h5 id="离线安装mysql的修改方式"><a href="#离线安装mysql的修改方式" class="headerlink" title="离线安装mysql的修改方式"></a>离线安装mysql的修改方式</h5><ol><li><p>修改编码,设置为utf8</p><p>拷贝 mysql 的配置文件/usr/share/mysql/my-small.cnf 到/etc/my.cnf </p><p>在mysql 配置文件/etc/my.cnf 中增加以下内容</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[client]下面增加</span><br><span class="line">default-character-set=utf8</span><br><span class="line">在[mysqld]下面增加</span><br><span class="line">default-character-set=utf8</span><br><span class="line">init_connect='SET NAMES utf8'</span><br></pre></td></tr></table></figure><p><em>2020/03/09更新</em>：<code>default-character-set=utf8</code>，如果这样改会导致5.7版本mysql无法打开所以要改为 <code>character-set-server=utf8</code>，(可选)改完后，要删除数据库中所有数据，才能使用。``````````</p></li><li><p>重启mysql 服务(这样确保缺省编码是utf8)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">service mysqld restart</span><br></pre></td></tr></table></figure></li><li><p>验证编码是否改成了utf8:</p><p>输入命令 “\s”</p><p><img src="https://yerias.github.io/hive_img/610238-20170903132119671-1333763157.png" alt="图片3"></p><p>输入命令:show variables like ‘char%’</p><p><img src="https://yerias.github.io/hive_img/610238-20170903132210124-717565896.png" alt="图片4"></p><p>输入命令:show variables like “colla%”;</p><p> <img src="https://yerias.github.io/hive_img/610238-20170903132350968-1341436179.png" alt="图片5"></p><p>OK修改成功!</p></li><li><p>这样在启动hive,向hive中插入的表中comment等有汉字的情况,就可以正常的显示(如下为本人测试的部分显示结果):</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://localhost:10000/ods&gt; desc ods_order;</span><br><span class="line">+<span class="comment">--------------------------+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--+</span></span><br><span class="line">|         col_name         |       data_type       |                                                                   <span class="keyword">comment</span>                                                                   |</span><br><span class="line">+<span class="comment">--------------------------+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--+</span></span><br><span class="line">| order_id                 | <span class="built_in">int</span>                   | 订单<span class="keyword">ID</span>                                                                                                                                        |</span><br><span class="line">| order_no                 | <span class="built_in">varchar</span>(<span class="number">30</span>)           | 订单编号（唯一字段），前缀字符表示订单来源：a，Andriod；b，微博；c，WEB；e，饿了么；i，Iphone；m，Mobile；x，微信； z，中粮我买网；l，其它。 接着<span class="number">3</span>位数字代表订单城市编号；接着字符z与后面的真正订单编号分隔。这套机制从<span class="number">2014</span>年<span class="number">12</span>月开始实施。</span><br></pre></td></tr></table></figure></li></ol><h5 id="在线安装mysql的修改方式"><a href="#在线安装mysql的修改方式" class="headerlink" title="在线安装mysql的修改方式"></a>在线安装mysql的修改方式</h5><ol><li><p>修改编码,设置为utf-8</p><p> 在 mysql 配置文件/etc/my.cnf（不需要拷贝）中[mysqld]的下面增加以下内容</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">init_connect='SET collation_connection = utf8_unicode_ci'</span><br><span class="line">init_connect='SET NAMES utf8'</span><br><span class="line">character-set-server=utf8</span><br><span class="line">collation-server=utf8_unicode_ci</span><br><span class="line">skip-character-set-client-handshake</span><br></pre></td></tr></table></figure><p> <img src="https://yerias.github.io/hive_img/610238-20170903133604796-492434925.png" alt="图片6"></p></li><li><p>重启mysqld服务</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">service mysqld restart</span><br></pre></td></tr></table></figure></li><li><p>和离线方式一样验证编码是否确实修改;</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">show variables like 'char%';</span><br></pre></td></tr></table></figure><p> <img src="https://yerias.github.io/hive_img/610238-20170903133820296-974086333.png" alt="图片7"></p></li></ol><h4 id="针对元数据库metastore中的表-分区-视图的编码设置"><a href="#针对元数据库metastore中的表-分区-视图的编码设置" class="headerlink" title="针对元数据库metastore中的表,分区,视图的编码设置"></a>针对元数据库metastore中的表,分区,视图的编码设置</h4><p>因为我们知道 metastore 支持数据库级别，表级别的字符集是 latin1，那么我们<code>只需要把相应注释的地方的字符集由 latin1 改成 utf-8</code>，就可以了。用到注释的就三个地方，表、分区、视图。如下修改分为两个步骤：</p><ol><li><p>进入数据库 Metastore 中执行以下 5 条 SQL 语句 </p><ol><li>修改表字段注解和表注解<br>alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8<br>alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8</li><li>修改分区字段注解：<br>alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8 ;<br>alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8</li><li>修改索引注解：<br>alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</li></ol></li><li><p>修改 metastore 的连接 URL</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;jdbc:mysql://IP:3306/db_name?createDatabaseIfNotExist=true&amp;amp;useUnicode=true&amp;amp;characterEncoding=UTF-8&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li></ol><p><strong>测试结果：</strong></p><p><img src="https://yerias.github.io/hive_img/610238-20170903134604093-93033588.png" alt="图片8"></p><p>以上就能完美的解决这个问题.</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>有时候在使用xml作为配置文件的时候，应该要使用xml的编码规则来进行适当的设置。如<code>&amp;</code>在xml文件中应该写为<code>&amp;amp;</code></p><p>下面给出xml中一些特殊符号的编码转换：</p><table><thead><tr><th>代码</th><th>符号</th><th>描述</th></tr></thead><tbody><tr><td><code>&amp;lt;</code></td><td>&lt;</td><td>小于号</td></tr><tr><td><code>&amp;gt;</code></td><td>&gt;</td><td>大于号</td></tr><tr><td><code>&amp;amp;</code></td><td>&amp;</td><td>and字符</td></tr><tr><td><code>&amp;apos;</code></td><td>‘</td><td>单引号</td></tr><tr><td><code>&amp;quot;</code></td><td>“</td><td>双引号</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MR调优之压缩</title>
      <link href="/2018/10/16/hadoop/12/"/>
      <url>/2018/10/16/hadoop/12/</url>
      
        <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>什么是压缩</li><li>压缩的好处与坏处</li><li>常见的压缩格式</li><li>优缺点比较</li><li>如何选择压缩格式</li><li>MR配置文件压缩格式</li><li>Hive配置文件压缩格式</li></ol><h2 id="什么是压缩"><a href="#什么是压缩" class="headerlink" title="什么是压缩"></a>什么是压缩</h2><p>压缩就是通过某种技术（算法）把原始文件变小，相应的解压就是把压缩后的文件变成原始文件。嘿嘿是不是又可以变大又可以变小。</p><h2 id="压缩的好处与坏处"><a href="#压缩的好处与坏处" class="headerlink" title="压缩的好处与坏处"></a>压缩的好处与坏处</h2><p><strong>好处</strong></p><ul><li>减少存储磁盘空间</li><li>降低IO(网络的IO和磁盘的IO)</li><li>加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度</li></ul><p><strong>坏处</strong></p><ul><li>由于使用数据时，需要先将数据解压，加重<strong>CPU</strong>负荷</li></ul><h2 id="常见的压缩格式"><a href="#常见的压缩格式" class="headerlink" title="常见的压缩格式"></a>常见的压缩格式</h2><p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9.png" alt="压缩"></p><p>一个简单的案例对于集中压缩方式之间的压缩大小和压缩时间进行一个感观性的认识</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">测试环境:</span><br><span class="line">8 core i7 cpu </span><br><span class="line">8GB memory</span><br><span class="line">64 bit CentOS</span><br><span class="line">1.4GB Wikipedia Corpus 2-gram text input</span><br></pre></td></tr></table></figure><p>压缩比</p><p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E6%AF%94.png" alt="压缩比"></p><p>压缩时间比</p><p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E6%97%B6%E9%97%B4.png" alt="压缩时间"></p><p>可以看出，压缩比越高，压缩时间越长</p><h2 id="优缺点比较"><a href="#优缺点比较" class="headerlink" title="优缺点比较"></a>优缺点比较</h2><table><thead><tr><th align="left">压缩格式</th><th align="left">优点</th><th align="left">缺点</th><th></th></tr></thead><tbody><tr><td align="left"><strong>gzip</strong></td><td align="left">压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便</td><td align="left"><strong>不支持split</strong></td><td></td></tr><tr><td align="left"><strong>lzo</strong></td><td align="left">压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便</td><td align="left">压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）</td><td></td></tr><tr><td align="left"><strong>snappy</strong></td><td align="left">压缩速度快；支持hadoop native库</td><td align="left"><strong>不支持split</strong>；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令d. bzip2</td><td></td></tr><tr><td align="left"><strong>bzip2</strong></td><td align="left">支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便</td><td align="left">压缩/解压速度慢；不支持native</td><td></td></tr></tbody></table><h2 id="如何选择压缩格式"><a href="#如何选择压缩格式" class="headerlink" title="如何选择压缩格式"></a>如何选择压缩格式</h2><p>从两方面考虑：Storage + Compute；</p><ol><li>Storage ：基于HDFS考虑，减少了存储文件所占空间，提升了数据传输速率；如gzip、bzip2。</li><li>Compute：基于YARN上的计算(MapReduce/Hive/Spark/….)速度的提升；如lzo、lz4、snappy。</li></ol><p>不同的场景选择不同的压缩方式，肯定没有一个一劳永逸的方法，如果选择高压缩比，那么对于cpu的性能要求要高，同时压缩、解压时间耗费也多；选择压缩比低的，对于磁盘io、网络io的时间要多，空间占据要多；<strong>对于支持分割的，可以实现并行处理</strong>。</p><ol><li>IO密集型：使用压缩</li><li>运算密集型：慎用压缩</li></ol><h2 id="压缩在MapReduce中的应用场景"><a href="#压缩在MapReduce中的应用场景" class="headerlink" title="压缩在MapReduce中的应用场景"></a>压缩在MapReduce中的应用场景</h2><p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E5%9C%A8MR%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF.png" alt="压缩在MR的应用场景"></p><p>压缩在hadoop中的应用场景总结在三方面：<strong>输入</strong>，<strong>中间</strong>，<strong>输出</strong>。</p><p>整体思路：hdfs ==&gt; map ==&gt; shuffle ==&gt; reduce</p><ol><li>Use Compressd Map Input: 从HDFS中读取文件进行Mapreuce作业，如果数据很大，可以使用压缩并且<strong>选择支持分片的压缩方式（Bzip2,LZO）</strong>，可以实现并行处理，提高效率，减少磁盘读取时间，同时选择合适的存储格式例如Sequence Files，RC,ORC等；</li><li>Compress Intermediate Data: Map输出作为Reducer的输入，需要经过shuffle这一过程，需要把数据读取到一个环形缓冲区，然后读取到本地磁盘，所以选择压缩可以减少了存储文件所占空间，提升了数据传输速率，建议<strong>使用压缩速度快的压缩方式，例如Snappy和LZO.</strong></li><li>Compress Reducer Output: 进行归档处理或者链式Mapreduce的工作（该作业的输出作为下个作业的输入），压缩可以减少了存储文件所占空间，提升了数据传输速率，如果作为归档处理，可以<strong>采用高的压缩比（Gzip,Bzip2）</strong>，如果作为下个作业的输入，考虑<strong>是否要分片</strong>进行选择。</li></ol><h2 id="MR配置文件压缩格式"><a href="#MR配置文件压缩格式" class="headerlink" title="MR配置文件压缩格式"></a>MR配置文件压缩格式</h2><p>hadoop自带不支持split的gzip和支持split的bzip2，我们还手动安装了lzo的压缩方式</p><ol><li><p>修改<code>core-site.xml</code>文件</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">    org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">    org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">    org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">    com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">    com.hadoop.compression.lzo.LzopCodec,</span><br><span class="line">    org.apache.hadoop.io.compress.Lz4Codec,</span><br><span class="line">    org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">    <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>修改<code>mapred-site.xml</code>文件</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">#开启支持压缩</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.output.fileoutputformat.compress<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">#压缩方式/最终输出的压缩方式</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.output.fileoutputformat.compress.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.compress.BZip2Codec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line">#中间压缩(可选，Snappy需要手动安装)</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.map.output.compression.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.compress.SnappyCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>中间压缩</strong>：中间压缩就是处理作业map任务和reduce任务之间的数据，对于中间压缩，最好选择一个节省CPU耗时的压缩方式（快）</p><p><strong>最终压缩</strong>：可以选择高压缩比，减少了存储文件所占空间，提升了数据传输速率<br><code>mapred-site.xml</code> 中设置</p></li><li><p>验证，跑个wc看最终输出文件的后缀</p></li></ol><p><strong>更换压缩方式只需要修改中间输出或者最终输出的压缩类即可</strong></p><h2 id="Hive配置文件压缩格式"><a href="#Hive配置文件压缩格式" class="headerlink" title="Hive配置文件压缩格式"></a>Hive配置文件压缩格式</h2><ol><li><p>配置压缩功能</p><p>hive配置文件压缩格式只需要配置两个参数</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">//开启压缩功能</span><br><span class="line">SET hive.exec.compress.output=true; </span><br><span class="line">//设置最终以bz2格式存储</span><br><span class="line">SET mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Code;</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：不建议再配置文件中设置</p></li><li><p>使用压缩</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">#创建表：</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载数据：</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/page_views.dat"</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> page_views;  </span><br><span class="line"></span><br><span class="line"><span class="comment">#配置压缩格式</span></span><br><span class="line">hive：</span><br><span class="line"><span class="keyword">SET</span> hive.exec.compress.output=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">SET</span> mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建压缩表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_bzip2 <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span> <span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> MapReduce </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HADOOP安装LZO压缩</title>
      <link href="/2018/10/15/hadoop/13/"/>
      <url>/2018/10/15/hadoop/13/</url>
      
        <content type="html"><![CDATA[<h3 id="编译安装lzo与lzop"><a href="#编译安装lzo与lzop" class="headerlink" title="编译安装lzo与lzop"></a>编译安装lzo与lzop</h3><p> <strong>在集群的每一台主机上都需要编译安装！！！</strong></p><ol><li><p>下载编译安装lzo文件，<a href="http://www.oberhumer.com/opensource/lzo/download" target="_blank" rel="noopener"><strong>版本可以下载最新的</strong></a> </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget http://www.oberhumer.com/opensource/lzo/download/lzo-2.10.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>编译安装(保证主机上有gcc与g++)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xvzf   lzo-2.10.tar.gz</span><br><span class="line">cd lzo-2.10</span><br><span class="line">./configure --enable-shared</span><br><span class="line">make -j 10</span><br><span class="line">make install</span><br><span class="line">cp /usr/local/lib/*lzo* /usr/lib</span><br></pre></td></tr></table></figure><p><strong>安装完成后需要将 cp部分文件到/usr/lib中，这个步骤不做会抛</strong>   lzop: error while loading shared libraries: liblzo2.so.2: cannot open shared object file: No such file or directory</p></li><li><p>下载编译lzop，<a href="http://www.lzop.org/download/" target="_blank" rel="noopener"><strong>最新版选择</strong></a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget http://www.lzop.org/download/lzop-1.04.tar.gz</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xvzf lzop-1.04.tar.gz </span><br><span class="line">cd lzop-1.04 </span><br><span class="line">./configure </span><br><span class="line">make -j 10 </span><br><span class="line">make install</span><br></pre></td></tr></table></figure></li></ol><h3 id="安装、编译hadoop-lzo-master"><a href="#安装、编译hadoop-lzo-master" class="headerlink" title="安装、编译hadoop-lzo-master"></a>安装、编译hadoop-lzo-master</h3><p>需在linux环境中安装,在windows上编译不过</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget https://github.com/twitter/hadoop-lzo/archive/master.zip</span><br></pre></td></tr></table></figure><ol><li><p>解压</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">unzip master.zip </span><br><span class="line">cd hadoop-lzo-master/</span><br></pre></td></tr></table></figure></li><li><p>编辑<em>pom.xm</em>l修改hadoop的版本号与你集群中hadoop版本一致   </p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">hadoop.current.version</span>&gt;</span>2.6.0-cdh5.16.2<span class="tag">&lt;/<span class="name">hadoop.current.version</span>&gt;</span></span><br></pre></td></tr></table></figure><p>pom文件增加cloudera的仓库地址</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--允许发布版本，禁止快照版--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">releases</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">releases</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>false<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>检查所在主机是否有maven,如果没有需要安装,如下(安装了maven即可跳过):</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget http://mirrors.hust.edu.cn/apache/maven/maven-3/3.5.4/binaries/apache-maven-3.5.4-bin.tar.gz</span><br><span class="line"></span><br><span class="line">tar -zxvf apache-maven-3.5.4-bin.tar.gz</span><br><span class="line"></span><br><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line">添加环境变量:</span><br><span class="line">MAVEN_HOME=/usr/local/apache-maven-3.5.4</span><br><span class="line">export MAVEN_HOME</span><br><span class="line">export PATH=$&#123;PATH&#125;:$&#123;MAVEN_HOME&#125;/bin</span><br><span class="line"></span><br><span class="line">保存退出profile</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>在maven中配置阿里云和cloudera的仓库</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>nexus-aliyun<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>*,!cloudera<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Nexus aliyun<span class="tag">&lt;/<span class="name">name</span>&gt;</span>                     </span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span></span><br><span class="line">        http://maven.aliyun.com/nexus/content/groups/public</span><br><span class="line">    <span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>导入hadoop-lzo编译时需要路径信息</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export CFLAGS=-m64</span><br><span class="line">export CXXFLAGS=-m64</span><br><span class="line">export C_INCLUDE_PATH=/home/hadoop/app/hadoop/lzo/include</span><br><span class="line">export LIBRARY_PATH=/home/hadoop/app/hadoop/lzo/lib</span><br></pre></td></tr></table></figure></li><li><p>maven编译安装</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mvn clean package -Dmaven.test.skip=true</span><br></pre></td></tr></table></figure><p><strong>编译安装没有异常结束后往下继续   PS:如果在mvn这里出现异常，请解决后再继续，注意权限问题</strong></p></li><li><p>编译成功后会有target文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd target/native/Linux-amd64-64/</span><br><span class="line">mkdir ~/hadoop-lzo-files</span><br><span class="line">tar -cBf - -C lib . | tar -xBvf - -C ~/hadoop-lzo-files</span><br></pre></td></tr></table></figure></li><li><p>在 ~/hadoop-lzo-files 目录下产生几个文件,执行cp</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp ~/hadoop-lzo-files/libgplcompression*  $HADOOP_HOME/lib/native/</span><br></pre></td></tr></table></figure><p><strong>注意！！！上面这一步的cp文件也要同步到集群其他主机的hadoop的对应目录下</strong></p></li><li><p>cp  hadoop-lzo的jar包到hadoop目录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp target/hadoop-lzo-0.4.21-SNAPSHOT.jar $HADOOP_HOME/share/hadoop/common/</span><br></pre></td></tr></table></figure><p><strong>注意！！！上面这一步的cp文件也要同步到集群其他主机的hadoop的对应目录下</strong></p></li></ol><h3 id="配置hadoop配置文件"><a href="#配置hadoop配置文件" class="headerlink" title="配置hadoop配置文件"></a>配置hadoop配置文件</h3><ol><li><p>修改<strong>core-site.xml</strong>(<em>如果配置过了不需要配置</em>)</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">        org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">        com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzopCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.Lz4Codec,</span><br><span class="line">        org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">    <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>修改<strong>mapred-site.xml</strong>中的压缩方式</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">#开启压缩</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.compress.map.output<span class="tag">&lt;/<span class="name">name</span>&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span>    </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">#配置压缩方式</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.map.output.compression.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span>    </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>跑个wc验证输出文件是否压缩</p></li><li><p>创建索引</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop jar $HADOOP_HOME/share/hadoop/common/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer /data/lzo-index/large_wc.txt.lzo</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce使用压缩以及在MR中的通用做法</title>
      <link href="/2018/10/15/hadoop/14/"/>
      <url>/2018/10/15/hadoop/14/</url>
      
        <content type="html"><![CDATA[<p>上一步中我们在Hadoop中安装了lzo的压缩方式，现在将测试如何在MapReduce程序中使用压缩</p><ol><li><p>在MapReduce中使用压缩，要注意三个位置，分别是map输入文件的压缩格式，map输出的压缩格式，和reduce最终输出的压缩格式</p><ul><li><p>首先配置使用压缩</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">mapreduce.output.fileoutputformat.compress=<span class="keyword">true</span></span><br></pre></td></tr></table></figure></li><li><p>第二步配置输入文件的压缩格式(如lzo):</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">mapreduce.job.inputformat<span class="class">.<span class="keyword">class</span></span>=com.hadoop.mapreduce.LzoTextInputFormat</span><br></pre></td></tr></table></figure></li><li><p>第三步配置map输出的文件压缩格式(如snappy):</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">mapred.map.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec</span><br></pre></td></tr></table></figure></li><li><p>第四步配置reduce输出的文件压缩格式(可不配置，这里配置lzo):</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Codec</span><br></pre></td></tr></table></figure></li></ul></li><li><p>我们随意找个表，查看表结构(这里只拿出来了我们想看的)</p><p>desc formatted emp;</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Storage Information  </span><br><span class="line">SerDe Library:      org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe </span><br><span class="line">InputFormat:        org.apache.hadoop.mapred.TextInputFormat </span><br><span class="line">OutputFormat:       org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</span><br></pre></td></tr></table></figure><p>由于hive底层也是跑的MapReduce，现在我们就能知道为什么要设置InputFormat和OutputFormat了。</p></li><li><p>一般不固定写在配置文件中，而是提交作业的时候手动指定，通过-D 指定参数</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop jar hadoop-mapreduce-examples-<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.16</span><span class="number">.2</span>.jar wordcount \</span><br><span class="line"> -D mapreduce.output.fileoutputformat.compress=<span class="keyword">true</span> \</span><br><span class="line"> -D io.compression.codec.lzo<span class="class">.<span class="keyword">class</span></span>=com.hadoop.compression.lzo.LzoCodec \</span><br><span class="line"> -D mapreduce.job.inputformat<span class="class">.<span class="keyword">class</span></span>=com.hadoop.mapreduce.LzoTextInputFormat \</span><br><span class="line"> -D mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzopCodec \</span><br><span class="line"> /data/lzo-index/  /out</span><br></pre></td></tr></table></figure></li><li><p>给.lzo文件创建索引</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop jar $HADOOP_HOME/share/hadoop/common/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer /data/lzo-index/large_wc.txt.lzo</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据倾斜&amp;MRchain解决数据倾斜&amp;大小表Reduce Join&amp;大小表Map Join&amp;SQL的执行计划</title>
      <link href="/2018/10/14/hadoop/11/"/>
      <url>/2018/10/14/hadoop/11/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>数据倾斜</li><li>MRchain解决数据倾斜</li><li>大小表Reduce Join</li><li>大小表Map Join</li><li>SQL的执行计划</li></ol><h2 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h2><ol><li><p>数据倾斜怎么造成的</p><p>mapreduce计算是将map相同的key丢到reduce，在reduce中进行聚合操作,在map和reduce中间有个shuffle操作，shuffle会将map阶段相同的key划分到reduce阶段中的一个reduce中去，数据倾斜就是数据的key 的分化严重不均，造成一部分数据很多，一部分数据很少的局面。</p></li><li><p>数据倾斜产生的问题</p><ul><li><p>有一个或多个reduce卡住</p></li><li><p>各种container报错OOM</p></li><li><p>读写的数据量极大，至少远远超过其它正常的reduce</p></li><li><p>伴随着数据倾斜，会出现任务被kill等各种诡异的表现。</p></li></ul></li><li><p>原因和解决方法</p><p>原因:</p><ul><li>单个值有大量记录(1.内存的限制存在，2.可能会对集群其他任务的运行产生不稳定的影响)</li><li>唯一值较多(单个唯一值的记录数占用内存不会超过分配给reduce的内存)</li></ul><p>解决办法:</p><ul><li><p>增加reduce个数</p></li><li><p>使用自定义partitioner</p></li><li><p>增加reduce 的jvm内存（效果不好）</p></li><li><p>map 阶段将造成倾斜的key 先分成多组加随机数并且在reduce阶段去除随机数</p></li><li><p>从业务和数据上解决数据倾斜</p><p>我们通过设计的角度尝试解决它</p><ul><li>数据预处理，过滤掉异常值</li><li>将数据打散让它的并行度变大，再汇集</li></ul></li><li><p>平台的优化方法</p><ul><li>join 操作中，使用 map join 在 map 端就先进行 join ，免得到reduce 时卡住</li><li>能先进行 group 操作的时候先进行 group 操作，把 key 先进行一次 reduce,之后再进行 count 或者 distinct count 操作</li><li>设置map端输出、中间结果压缩</li></ul></li></ul></li></ol><h2 id="MRchain解决数据倾斜"><a href="#MRchain解决数据倾斜" class="headerlink" title="MRchain解决数据倾斜"></a>MRchain解决数据倾斜</h2><p>核心思想: 第一个mapredue把具有数据倾斜特性的数据加盐(随机数)，进行聚合；第二个mapreduce把第一个mapreduce的加盐结果进行去盐，再聚合，问题是两个MR IO高。</p><p>参考代码:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.item;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.utils.FileUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.chain.ChainMapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.chain.ChainReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.Random;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-02-04 14:50</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ChainMRDriver</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Random r;</span><br><span class="line">    String in = <span class="string">"data/skew/access.txt"</span>;</span><br><span class="line">    String out1 = <span class="string">"out/mr1"</span>;</span><br><span class="line">    String out2 = <span class="string">"out/mr2"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(conf, <span class="keyword">new</span> ChainMRDriver(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">super</span>.getConf();</span><br><span class="line">        FileUtil.checkFileIsExists(conf, out1);</span><br><span class="line">        FileUtil.checkFileIsExists(conf, out2);</span><br><span class="line"></span><br><span class="line">        Job job1 = Job.getInstance(conf);</span><br><span class="line">        Job job2 = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        job1.setJarByClass(ChainMRDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job2.setJarByClass(ChainMRDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        ChainMapper.addMapper(job1, ChainMRDriver.incRanDomMapper<span class="class">.<span class="keyword">class</span>, <span class="title">LongWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line">        ChainReducer.setReducer(job1, ChainMRDriver.incRanDomReduver<span class="class">.<span class="keyword">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line"></span><br><span class="line">        ChainMapper.addMapper(job2, ChainMRDriver.decRanDomMapper<span class="class">.<span class="keyword">class</span>, <span class="title">LongWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line">        ChainReducer.setReducer(job2, ChainMRDriver.decRanDomReduver<span class="class">.<span class="keyword">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job1, <span class="keyword">new</span> Path(in));</span><br><span class="line">        FileOutputFormat.setOutputPath(job1, <span class="keyword">new</span> Path(out1));</span><br><span class="line">        FileInputFormat.setInputPaths(job2, <span class="keyword">new</span> Path(out1));</span><br><span class="line">        FileOutputFormat.setOutputPath(job2, <span class="keyword">new</span> Path(out2));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//提交job1和job2 job1--&gt;job2 必须按照顺序提交</span></span><br><span class="line">        System.out.println(<span class="string">"=============第一阶段=============="</span>);</span><br><span class="line">        <span class="keyword">boolean</span> b = job1.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">if</span> (b) &#123;</span><br><span class="line">            System.out.println(<span class="string">"=============第二阶段=============="</span>);</span><br><span class="line">            <span class="keyword">boolean</span> b1 = job2.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">            <span class="keyword">return</span> b1 ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">incRanDomMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//创建对象</span></span><br><span class="line">            r = <span class="keyword">new</span> Random();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//把数据读出来，加盐  www.baidu.com   2</span></span><br><span class="line">            String[] line = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">            String incR = r.nextInt(<span class="number">10</span>) +<span class="string">"_"</span>+line[<span class="number">0</span>];</span><br><span class="line">            <span class="keyword">int</span> number = Integer.parseInt(line[<span class="number">1</span>]);</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(incR), <span class="keyword">new</span> IntWritable(number));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (r != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">//回收对象</span></span><br><span class="line">                r = <span class="keyword">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">incRanDomReduver</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">int</span> sums = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (IntWritable sum : values) &#123;</span><br><span class="line">                sums += sum.get();</span><br><span class="line">            &#125;</span><br><span class="line">            context.write(key, <span class="keyword">new</span> IntWritable(sums));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">decRanDomMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="comment">//去盐 聚合</span></span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            String[] line = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">            String decWord = line[<span class="number">0</span>].split(<span class="string">"_"</span>)[<span class="number">1</span>];</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(decWord), <span class="keyword">new</span> IntWritable(Integer.parseInt(line[<span class="number">1</span>])));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">decRanDomReduver</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">int</span> sums = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (IntWritable sum : values) &#123;</span><br><span class="line">                sums += sum.get();</span><br><span class="line">            &#125;</span><br><span class="line">            context.write(key, <span class="keyword">new</span> IntWritable(sums));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="SQL的执行计划"><a href="#SQL的执行计划" class="headerlink" title="SQL的执行计划"></a>SQL的执行计划</h2><p>如何运行SQL的执行计划</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">EXPLAIN</span> [<span class="keyword">EXTENDED</span>] Syntax</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">EXPLAIN</span> <span class="keyword">select</span> e.empno,e.ename,d.deptno,d.dname <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure><p>解析这句SQL的执行计划</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">+-----------------------------------------------------------------+--+</span><br><span class="line">                      Explain                       </span><br><span class="line">+-----------------------------------------------------------------+--+</span><br><span class="line"> STAGE DEPENDENCIES:     <span class="comment">//阶段性依赖                           </span></span><br><span class="line">   Stage-<span class="number">4</span> is a root stage   <span class="comment">//这是一个根依赖                       </span></span><br><span class="line">   Stage-<span class="number">3</span> depends on stages: Stage-<span class="number">4</span>    <span class="comment">//Stage-3依赖Stage-4           </span></span><br><span class="line">   Stage-<span class="number">0</span> depends on stages: Stage-<span class="number">3</span>    <span class="comment">//Stage-0依赖Stage-3           </span></span><br><span class="line">                                                    </span><br><span class="line"> STAGE PLANS:   <span class="comment">// 阶段性计划                                    </span></span><br><span class="line">   Stage: Stage-<span class="number">4</span>     <span class="comment">//阶段4                              </span></span><br><span class="line">     Map Reduce Local Work   <span class="comment">//这是一个本地作业                    </span></span><br><span class="line">       Alias -&gt; Map Local Tables:    <span class="comment">// Map本地表的别名为d 即表dept              </span></span><br><span class="line">         d                                          </span><br><span class="line">           Fetch Operator   <span class="comment">//抓取                        </span></span><br><span class="line">             limit: -<span class="number">1</span>    <span class="comment">//limit为-1，即把数据全部读出来了                   </span></span><br><span class="line">       Alias -&gt; Map Local Operator Tree:  <span class="comment">//Map本地操作树          </span></span><br><span class="line">         d                                          </span><br><span class="line">           TableScan     <span class="comment">//表扫描                           </span></span><br><span class="line">             alias: d    <span class="comment">//别名d                           </span></span><br><span class="line">             Statistics: Num rows: <span class="number">2</span> Data size: <span class="number">284</span> Basic stats: PARTIAL Column stats: NONE <span class="comment">//统计 </span></span><br><span class="line">             Filter Operator  <span class="comment">//过滤                      </span></span><br><span class="line">               predicate: <span class="function">deptno is not <span class="title">null</span> <span class="params">(type: <span class="keyword">boolean</span>)</span>  <span class="comment">//断定：deptno字段 不为空</span></span></span><br><span class="line"><span class="function">               Statistics: Num rows: 1 Data size: 142 Basic stats: COMPLETE Column stats: NONE    <span class="comment">//统计</span></span></span><br><span class="line"><span class="function">               HashTable Sink Operator  <span class="comment">//输出类型为HashTable           </span></span></span><br><span class="line"><span class="function">                 keys:                              </span></span><br><span class="line"><span class="function">                   0 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                   1 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                                                    </span></span><br><span class="line"><span class="function">   Stage: Stage-3   <span class="comment">//阶段3                                </span></span></span><br><span class="line"><span class="function">     Map Reduce                                     </span></span><br><span class="line"><span class="function">       Map Operator Tree:    <span class="comment">//Map操作树                       </span></span></span><br><span class="line"><span class="function">           TableScan   <span class="comment">//表扫描                             </span></span></span><br><span class="line"><span class="function">             alias: e  <span class="comment">//e表 即emp表                             </span></span></span><br><span class="line"><span class="function">             Statistics: Num rows: 6 Data size: 657 Basic stats: COMPLETE Column stats: NONE  <span class="comment">//统计</span></span></span><br><span class="line"><span class="function">             Filter Operator    <span class="comment">//过滤                    </span></span></span><br><span class="line"><span class="function">               predicate: deptno is not <span class="title">null</span> <span class="params">(type: <span class="keyword">boolean</span>)</span> <span class="comment">//断定：deptno字段 不为空</span></span></span><br><span class="line"><span class="function">               Statistics: Num rows: 3 Data size: 328 Basic stats: COMPLETE Column stats: NONE   <span class="comment">//统计</span></span></span><br><span class="line"><span class="function">               Map Join Operator    <span class="comment">// Map Join  操作               </span></span></span><br><span class="line"><span class="function">                 condition map:   <span class="comment">//Map条件                 </span></span></span><br><span class="line"><span class="function">                      Inner Join 0 to 1             </span></span><br><span class="line"><span class="function">                 keys:                              </span></span><br><span class="line"><span class="function">                   0 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                   1 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                 outputColumnNames: _col0, _col1, _col11, _col12  <span class="comment">//数值字段在表中的位置</span></span></span><br><span class="line"><span class="function">                 Statistics: Num rows: 3 Data size: 360 Basic stats: COMPLETE Column stats: NONE<span class="comment">//统计 </span></span></span><br><span class="line"><span class="function">                 Select Operator   <span class="comment">//Select操作                 </span></span></span><br><span class="line"><span class="function">                   expressions: <span class="title">_col0</span> <span class="params">(type: <span class="keyword">int</span>)</span>, <span class="title">_col1</span> <span class="params">(type: string)</span>, <span class="title">_col11</span> <span class="params">(type: <span class="keyword">int</span>)</span>, <span class="title">_col12</span> <span class="params">(type: string)</span> <span class="comment">//表达式</span></span></span><br><span class="line"><span class="function">                   outputColumnNames: _col0, _col1, _col2, _col3 <span class="comment">//数值字段在表中的位置</span></span></span><br><span class="line"><span class="function">                   Statistics: Num rows: 3 Data size: 360 Basic stats: COMPLETE Column stats: NONE   <span class="comment">//统计</span></span></span><br><span class="line"><span class="function">                   File Output Operator   <span class="comment">//文件输出操作          </span></span></span><br><span class="line"><span class="function">                     compressed: <span class="keyword">false</span>    <span class="comment">//是否压缩：否          </span></span></span><br><span class="line"><span class="function">                     Statistics: Num rows: 3 Data size: 360 Basic stats: COMPLETE Column stats: NONE<span class="comment">//统计 </span></span></span><br><span class="line"><span class="function">                     table:   <span class="comment">//表文件的输入、输出、序列化类型                      </span></span></span><br><span class="line"><span class="function">                         input format: org.apache.hadoop.mapred.TextInputFormat <span class="comment">//文件输入格式</span></span></span><br><span class="line"><span class="function">                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat <span class="comment">//文件输出格式</span></span></span><br><span class="line"><span class="function">                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe <span class="comment">//反序列化</span></span></span><br><span class="line"><span class="function">       Local Work:                                  </span></span><br><span class="line"><span class="function">         Map Reduce Local Work                      </span></span><br><span class="line"><span class="function">                                                    </span></span><br><span class="line"><span class="function">   Stage: Stage-0   <span class="comment">//阶段0                                </span></span></span><br><span class="line"><span class="function">     Fetch Operator                                 </span></span><br><span class="line"><span class="function">       limit: -1     <span class="comment">// limit设置的值                               </span></span></span><br><span class="line"><span class="function">       Processor Tree:                              </span></span><br><span class="line"><span class="function">         ListSink                                   </span></span><br><span class="line"><span class="function">                                                    </span></span><br><span class="line"><span class="function">+-----------------------------------------------------------------+--+</span></span><br></pre></td></tr></table></figure><p>从执行计划得知，hive中执行SQL语句底层执行的是MapReduce。</p><p>我们在SQL中关联了两张表分别是emp dept，并从两张表中取出某些字段，在SQL执行计划中共分为三个阶段，分别是stage4、stage3、stage0。</p><p>stage4是根stage，stage3依赖stage4，同时stage0依赖stage3。</p><p>stage4是一个本地作业，读取的是dept表，输出一个Map类型的hashTable，关联的key是两张表的deptno，在执行计划中表现为0 deptno和 1 deptno，即执行的MapReduce中的Key是deptno字段。</p><p>stage3是MapReduce中的Map阶段，扫描emp表，执行一个Map Join操作，条件是两张表的dept字段相等(内连接)，现在我们得到的是一张包含所有字段的大表，得到需要的字段的对应位置，并且匹配字段的类型，在输出的时候检查是否需要压缩，以及输入、输出、和序列化类型</p><p>Stage-0阶段取出limit中指定的记录数</p><p>总结: 我们发现执行该SQL没有Reduce阶段，在现有的版本中默认设置<code>hive.auto.convert.join</code>(是否自动转换为mapjoin)为true，该参数配合<code>hive.mapjoin.smalltable.filesize</code>参数(小表的最大文件大小)默认为25M。即小于25M的表为小表，自动转为mapjoin，小表上传到hadoop缓存，提供给各个大表join使用。大表和小表根据关联的key形成一张大表，取出select需要的字段，最后根据limit设置的值取出对应的记录数。</p><p>参考参数：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--是否自动转换为mapjoin</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">true</span>;</span><br><span class="line"><span class="comment">--小表的最大文件大小，默认为25000000，即25M</span></span><br><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize = <span class="number">25000000</span>;</span><br><span class="line"><span class="comment">--是否将多个mapjoin合并为一个</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask = <span class="literal">true</span>;</span><br><span class="line"><span class="comment">--多个mapjoin转换为1个时，所有小表的文件大小总和的最大值。</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask.size = <span class="number">10000000</span>;</span><br></pre></td></tr></table></figure><h2 id="大小表Reduce-Join-emp、dept"><a href="#大小表Reduce-Join-emp、dept" class="headerlink" title="大小表Reduce Join(emp、dept)"></a>大小表Reduce Join(emp、dept)</h2><p>Reduce Join的核心思路是定义输出字段作为一个实体类，用来作为输出，实体类中定义一个标志用来区分表的来源</p><ol><li><p>将大小两个表在SQL中join的字段作为MapReduce中的key，原因是MapReduce中的key具有排序和分区的作用</p></li><li><p>Map中获取context中切片所在的文件名，按行获取文件中的数据并且根据获取的文件名分别将数据set到对象中，并写出Map。</p></li><li><p>Reduce中每次获取key相同的一组value值数据，这组value值既有dept中的数据，也</p></li></ol><p>   有emp中的数据，只要他们有相同的key，就会在shuffle中丢到一个reduce，这时候获取这组数据的值，根据flag来判断来自哪个表，如果是dept表则将数据设置到新new出来的对象中，添加到List列表中，同时创建一个保存emp表中数据的变量，由于emp表是小表，emp表中需要的数据对应dept/emp中的key的字段是唯一的，所以只需要把value中所有的对象都遍历循环出来，dept表数据添加到了List列表，emp表的数据添加到了变量中，最后循环List列表把变量set到每一个对象中，即完成了全部对象的全部成员属性。最后输出即可。</p><p>   参考代码:</p>   <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.hadoop.join;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.pojo.JoinMain;</span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.utils.FileUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-01-29 16:39</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReduceJoinDriver</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    String in = <span class="string">"data/join/"</span>;</span><br><span class="line">    String out = <span class="string">"out/"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(conf, <span class="keyword">new</span> ReduceJoinDriver(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//获得configuration</span></span><br><span class="line">        Configuration conf = <span class="keyword">super</span>.getConf();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//检查文件夹</span></span><br><span class="line">        FileUtil.checkFileIsExists(conf, out);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用新方法这里怎么操作?</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置驱动类</span></span><br><span class="line">        job.setJarByClass(ReduceJoinDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置Map/Reducer类</span></span><br><span class="line">        job.setMapperClass(JoinMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(JoinReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置Map参数类型</span></span><br><span class="line">        job.setMapOutputKeyClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(JoinMain<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//job.setNumReduceTasks(3);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置Reducer参数类型</span></span><br><span class="line">        job.setOutputKeyClass(JoinMain<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置文件的输入输出</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(in));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(out));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//提交任务</span></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> b ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">JoinMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">JoinMain</span>&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            FileSplit fileSplit = (FileSplit) context.getInputSplit();</span><br><span class="line">            name = fileSplit.getPath().getName();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//做一个传入的表的判断</span></span><br><span class="line">            <span class="keyword">if</span> (name.contains(<span class="string">"emp"</span>))&#123;  <span class="comment">//emp</span></span><br><span class="line">                <span class="comment">//读取数据</span></span><br><span class="line">                String[] lines = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">                <span class="keyword">if</span> (lines.length ==<span class="number">8</span>)&#123;</span><br><span class="line">                    <span class="comment">//细粒度划分</span></span><br><span class="line">                    Integer empno = Integer.parseInt(lines[<span class="number">0</span>].trim());</span><br><span class="line">                    String ename = lines[<span class="number">1</span>];</span><br><span class="line">                    Integer deptno = Integer.parseInt(lines[lines.length-<span class="number">1</span>].trim());</span><br><span class="line">                    <span class="comment">//写入数据</span></span><br><span class="line">                    context.write(<span class="keyword">new</span> IntWritable(deptno),<span class="keyword">new</span> JoinMain(empno,ename,deptno,<span class="string">""</span>,<span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;      <span class="comment">//dept</span></span><br><span class="line">                <span class="comment">//读取数据</span></span><br><span class="line">                String[] lines = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">                <span class="keyword">if</span> (lines.length ==<span class="number">3</span>)&#123;</span><br><span class="line">                    <span class="keyword">int</span> deptno = Integer.parseInt(lines[<span class="number">0</span>].trim());</span><br><span class="line">                    String dname = lines[<span class="number">1</span>];</span><br><span class="line">                    context.write(<span class="keyword">new</span> IntWritable(deptno),<span class="keyword">new</span> JoinMain(<span class="number">0</span>, <span class="string">""</span>, deptno, dname, <span class="number">2</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">super</span>.cleanup(context);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">JoinReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">IntWritable</span>, <span class="title">JoinMain</span>,<span class="title">JoinMain</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="comment">//核心思路在 每个deptno组 进一次reduce ，前提是map中的key是deptno</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(IntWritable key, Iterable&lt;JoinMain&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            List&lt;JoinMain&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            String dname=<span class="string">""</span>;</span><br><span class="line">            <span class="comment">// 1.取出map中每行数据，判断flag值</span></span><br><span class="line">            <span class="comment">// 2.取出所有的emp中数据放入list中</span></span><br><span class="line">            <span class="comment">// 3.取出dept中的dname赋值给变量</span></span><br><span class="line">            <span class="comment">// 4.取出属于这个deptno中的所有数据，并给dname赋值</span></span><br><span class="line">            <span class="comment">// 5.每条赋值dname的数据写入reduce</span></span><br><span class="line">            <span class="keyword">for</span> (JoinMain main : values)&#123;</span><br><span class="line">                <span class="comment">// emp表</span></span><br><span class="line">                <span class="keyword">if</span> (main.getFlag() == <span class="number">1</span>)&#123;</span><br><span class="line">                    <span class="comment">//给emp表全部行重新赋值</span></span><br><span class="line">                    JoinMain m = <span class="keyword">new</span> JoinMain();</span><br><span class="line">                    m.setDeptno(main.getDeptno());</span><br><span class="line">                    m.setEmpno(main.getEmpno());</span><br><span class="line">                    m.setEname(main.getEname());</span><br><span class="line">                    <span class="comment">//写出到list</span></span><br><span class="line">                    list.add(m);</span><br><span class="line">                &#125;<span class="keyword">else</span> <span class="keyword">if</span> (main.getFlag() ==<span class="number">2</span> )&#123; <span class="comment">//dept</span></span><br><span class="line">                    <span class="comment">//拿到dept表中的dname</span></span><br><span class="line">                dname = main.getDname();</span><br><span class="line">            &#125;&#125;</span><br><span class="line">            <span class="comment">//循环赋值</span></span><br><span class="line">            <span class="keyword">for</span> (JoinMain bean : list) &#123;</span><br><span class="line">                bean.setDname(dname);</span><br><span class="line">                context.write(bean,NullWritable.get());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="大小表Map-Join-emp、dept"><a href="#大小表Map-Join-emp、dept" class="headerlink" title="大小表Map Join(emp、dept)"></a>大小表Map Join(emp、dept)</h2><p>Map Join的核心思想是把小表添加到缓存中(Map中)，在map中读取大表每行数据时set到对象值时取出小表(Map)对应key的值即可</p><ol><li><p>setup中，通过context获取小表文件切片的路径，然后通过读取流的方式读取为字符，按行获取到字符后切分，使用HashMap结构设置key和value分别为map方法中大表join时需要的键和值。</p></li><li><p>在map方法中读取文件数据，并且根据key取出HashMap(小表)中的value，一起set到对象中即可，最后写出，写出时，可以把value设置为NullWritable。</p><p>参考代码:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.hadoop.join;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.pojo.JoinMain;</span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.utils.FileUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"><span class="keyword">import</span> org.codehaus.groovy.runtime.wrappers.LongWrapper;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-02-01 23:10</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapJoinDriver</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String in = <span class="string">"data/join/emp.txt"</span>;</span><br><span class="line">    <span class="keyword">private</span> String out = <span class="string">"out"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(conf, <span class="keyword">new</span> MapJoinDriver(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> strings</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> : int</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@describe</span> : 设置配置文件，不用设置reduce</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@author</span> : tunan</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@date</span> : 2020/2/1 23:14</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">super</span>.getConf();</span><br><span class="line">        FileUtil.checkFileIsExists(conf,out);</span><br><span class="line"></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(MapJoinDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"data/join/dept.txt"</span>));</span><br><span class="line"></span><br><span class="line">        job.setMapperClass(MapperJoin<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(JoinMain<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(in));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(out));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> b ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MapperJoin</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">JoinMain</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> HashMap&lt;Integer, String&gt; hashMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//得到缓存文件路径</span></span><br><span class="line">            <span class="comment">//String path = context.getCacheFiles()[0].getPath().toString();</span></span><br><span class="line">            String path = context.getCacheFiles()[<span class="number">0</span>].getPath();</span><br><span class="line">            <span class="comment">/*URI[] files = context.getCacheFiles();   //URI 通过getPath()解码 没有toString()方法</span></span><br><span class="line"><span class="comment">            String s = files[0].getPath();*/</span></span><br><span class="line">            <span class="comment">//得到文件</span></span><br><span class="line">            <span class="comment">//File file = new File(cacheFiles[0]);</span></span><br><span class="line">            <span class="comment">//String path = file.getPath();</span></span><br><span class="line">            <span class="comment">//得到文件的流        InputStreamReader将字节转换为字符</span></span><br><span class="line">            BufferedReader br = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(<span class="keyword">new</span> FileInputStream(path)));</span><br><span class="line">            <span class="comment">//读取文件为字符串</span></span><br><span class="line">            String line ;</span><br><span class="line">            <span class="keyword">while</span>(StringUtils.isNotEmpty(line=br.readLine()))&#123;</span><br><span class="line">                <span class="comment">//切分字符串得到字符串数组</span></span><br><span class="line">                String[] split = line.split(<span class="string">"\t"</span>);</span><br><span class="line">                hashMap.put(Integer.parseInt(split[<span class="number">0</span>]),split[<span class="number">1</span>]);</span><br><span class="line">            &#125;</span><br><span class="line">            IOUtils.closeStream(br);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@describe</span> :</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@author</span> : tunan</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> key</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@return</span> : void</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@date</span> : 2020/2/1 23:38</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            String[] line = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">            <span class="keyword">if</span> (line.length &gt;= <span class="number">8</span>)&#123;</span><br><span class="line">                Integer empno = Integer.parseInt(line[<span class="number">0</span>].trim());</span><br><span class="line">                String ename = line[<span class="number">1</span>];</span><br><span class="line">                Integer deptno = Integer.parseInt(line[line.length-<span class="number">1</span>].trim());</span><br><span class="line">                String dname = hashMap.get(deptno);</span><br><span class="line">                context.write(<span class="keyword">new</span> JoinMain(empno,ename,deptno,dname),NullWritable.get());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">super</span>.cleanup(context);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> MapReduce </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>InputFormat&amp;Partitioner&amp;Conbiner&amp;Sort&amp;OutputFormat</title>
      <link href="/2018/10/13/hadoop/10/"/>
      <url>/2018/10/13/hadoop/10/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>InputFormat</li><li>Partitioner</li><li>Conbiner</li><li>Sort</li><li>OutputFormat</li></ol><h2 id="InputFormat"><a href="#InputFormat" class="headerlink" title="InputFormat"></a>InputFormat</h2><p>在数据进入map之前，会进过一系列的格式化操作</p><ol><li>在客户端submitJob()方法提交作业前，会获取配置信息，形成一个任务分配的规划</li><li>提交文件分片(文件夹)和应用程序jar包</li><li>MR运行MapTask根据InputFormat读取文件，这里将详细介绍InputFormat</li></ol><p>InputFormat是一个抽象类，MR默认是用TextInputFormat方法读取文件</p><p>TextInputFormat是按行读取文件中的数据，实际上TextInputFormat中只实现了createRecordReader()和isSplitable()两个方法，它的具体实现在FileInputFormat中就已经实现的，FileInputFormat也是一个抽象类。</p><p>NLineInputFormat也继承于FileInputFormat，它的特点是按特定的行数读取数据</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 设置指定的InputFormat(重点)</span></span><br><span class="line">NLineInputFormat.setNumLinesPerSplit(job, <span class="number">3</span>);</span><br><span class="line">job.setInputFormatClass(NLineInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure><p>DBInputFormat继承于DBInputFormat的同时实现了InputFormat，这个方法可以从数据库中读取数据，写入HDFS，类似于Sqoop，需要注意的是它的实体类要同时继承DBWritable和Writable，提交到HDFS上执行的时候需要指定jdbc的jar包(不推介使用)。</p><h2 id="Partitioner"><a href="#Partitioner" class="headerlink" title="Partitioner"></a>Partitioner</h2><p>MR的默认分区规则是按照key分区，相同的key到一个reduce方法中去，<strong>Partitoner可以自定义分区规则</strong>，自定义类继承Partitioner&lt;Text, Flow&gt;，泛型是map输出的key和value类型</p><p>参考代码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Partitoner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">Flow</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, Flow flow, <span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">        String phone = text.toString();</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"137"</span>.equals(phone.substring(<span class="number">0</span>,<span class="number">3</span>)))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"136"</span>.equals(phone.substring(<span class="number">0</span>,<span class="number">3</span>)))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>要在Driver中指定Partitioner的类，并且指定reduce的个数，这里的<strong>reduce设置的个数一定要和Partitoner分区中返回的分区个数相同</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 设置Partitoner</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">3</span>);</span><br><span class="line">job.setPartitionerClass(Partitoner<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure><p>如果reduce设置的数量大于分区个数，则会产生空的输出文件，即空的reduce。</p><p>如果reduce设置的个数小于分区个数，则会报错，表示多余的数据没有分区可去。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">java.io.IOException: Illegal partition <span class="keyword">for</span> <span class="number">13826544101</span> (<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h2 id="Conbiner"><a href="#Conbiner" class="headerlink" title="Conbiner"></a>Conbiner</h2><p>Conbiner是合并，即是map阶段的reduce，可以自定义，也可以直接使用reduce方法，需要在Driver中指定，需要注意的是不能改变业务逻辑(不适用于乘积)</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//设置conbiner</span></span><br><span class="line">job.setCombinerClass(FlowReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure><h2 id="Sort"><a href="#Sort" class="headerlink" title="Sort"></a>Sort</h2><p>Sort是分区排序，需要知道的是MR的key默认是有序的，如果要自定义排序规则需要将实体类实现<code>WritableComparable&lt;FlowSort&gt;</code>接口，泛型就传入实体类的类名，WritableComparable实际上是继承了Writable和Comparable<T>，Writable是Hadoop自己实现的，Comparable是Java中的类</p><p>实体类参考代码</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowSort o)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Long.compare(o.sum, <span class="keyword">this</span>.sum);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>需要注意的是使用自定义排序的实体类要放到mapreduce方法key的位置，使之有序。</p><p>主要注意的是Sort是每个reduce中有序，如果设置了多个reduce，则只能保证每个reduce内部有序</p><h2 id="OutputFormat"><a href="#OutputFormat" class="headerlink" title="OutputFormat"></a>OutputFormat</h2><p>有一类很常见的需求：按照一定的规则把数据给我写到某个文件中去</p><p>OutputFormat是一个接口，实现它的类有FileOutputFormat和DBOutputFormat，使用和InputFormat差不多，用的不多，不写了</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> MapReduce </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark、IDEA和Maven的环境准备&amp;Hadoop的依赖以及常用API&amp;WordCount Debug流程&amp;map、reduce方法的参数类型和作用&amp;瘦包在服务器上的jar包依赖</title>
      <link href="/2018/10/12/hadoop/9/"/>
      <url>/2018/10/12/hadoop/9/</url>
      
        <content type="html"><![CDATA[<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>Spark、IDEA和Maven的环境准备</li><li>hadoop的依赖以及常用API</li><li>WordCount Debug流程</li><li>map、reduce方法的参数类型和作用</li><li>Writable和WritableComparable的作用</li><li>瘦包在服务器上的jar包依赖</li></ol><h2 id="Spark、IDEA和Maven的环境准备"><a href="#Spark、IDEA和Maven的环境准备" class="headerlink" title="Spark、IDEA和Maven的环境准备"></a>Spark、IDEA和Maven的环境准备</h2><p>环境:</p><ol><li>Spark3.0</li><li>IDEA19.3</li><li>Maven3.6.3(安装配置阿里云的镜像)</li></ol><h2 id="Hadoop的依赖以及常用API"><a href="#Hadoop的依赖以及常用API" class="headerlink" title="Hadoop的依赖以及常用API"></a>Hadoop的依赖以及常用API</h2><p>依赖:</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.6.0-cdh5.16.2&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>常用API:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">FileSystem fileSystem; <span class="comment">//核心</span></span><br><span class="line">open()<span class="comment">//打开文件返回流</span></span><br><span class="line">mkdirs()<span class="comment">//创建目录</span></span><br><span class="line">create()<span class="comment">//创建文件</span></span><br><span class="line">copyFromLocalFile()<span class="comment">//从本地复制文件到hdfs，类似于get</span></span><br><span class="line">copyToLocalFile()<span class="comment">//从hdfs复制文件到本地，类似于put</span></span><br><span class="line">listFiles()<span class="comment">//列出目录下的所有文件，可以迭代</span></span><br><span class="line">delete()<span class="comment">//删除文件，不存在报错</span></span><br><span class="line">deleteOnExit()<span class="comment">//删除存在的文件,不存在不报错</span></span><br></pre></td></tr></table></figure><h2 id="WordCount-Debug流程"><a href="#WordCount-Debug流程" class="headerlink" title="WordCount Debug流程"></a>WordCount Debug流程</h2><ol><li><p>编译</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">job.waitForCompletion(<span class="keyword">true</span>);</span><br></pre></td></tr></table></figure></li><li><p>提交</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (state == JobState.DEFINE) &#123;</span><br><span class="line">   submit();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>兼容新老API</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">setUseNewAPI();</span><br></pre></td></tr></table></figure></li><li><p>本地连接/服务器连接</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">connect();</span><br></pre></td></tr></table></figure></li><li><p>检查配置、输出路径等</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> JobSubmitter submitter = getJobSubmitter(cluster.getFileSystem(),cluster.getClient());</span><br><span class="line"><span class="function"><span class="keyword">public</span> JobStatus <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException,ClassNotFoundException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster);</span><br><span class="line">&#125;</span><br><span class="line">checkSpecs(job);<span class="comment">//validate the jobs output specs</span></span><br></pre></td></tr></table></figure></li><li><p>把该作业的配置信息加到分布式缓存中</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Configuration conf = job.getConfiguration();</span><br><span class="line">addMRFrameworkToDistributedCache(conf);</span><br></pre></td></tr></table></figure></li><li><p>创建该Job对应的存放目录</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br></pre></td></tr></table></figure></li><li><p>拿到该Job对应的ID(local/application)</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">JobID jobId = submitClient.getNewJobID();</span><br></pre></td></tr></table></figure></li><li><p>jobStagingArea/jobid</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Path submitJobDir = <span class="keyword">new</span> Path(jobStagingArea, jobId.toString());</span><br></pre></td></tr></table></figure></li><li><p>拷贝job对应的信息到jobStagingArea/jobid</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">copyAndConfigureFiles(job, submitJobDir);</span><br></pre></td></tr></table></figure></li><li><p>完成我们输入数据的切片(默认128MB，预留10%浮动空间)</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> maps = writeSplits(job, submitJobDir);</span><br></pre></td></tr></table></figure></li><li><p>作业文件提交到指定目录</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">writeConf(conf, submitJobFile);</span><br></pre></td></tr></table></figure></li><li><p>提交作业</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure></li></ol><h2 id="map、reduce方法的参数类型和作用"><a href="#map、reduce方法的参数类型和作用" class="headerlink" title="map、reduce方法的参数类型和作用"></a>map、reduce方法的参数类型和作用</h2><ul><li><p>继承Mapper后实现map方法</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br></pre></td></tr></table></figure><p>该方法中的参数分别是<code>LongWritable key, Text value, Context context</code></p><p>前两个参数是map方法中输入的键和值，输入的键和值必须是LongWritable类型和Text类型，因为是按offset读取每行本文数据，最后一个参数context，它是MapReduce的上下文。什么都可以拿出来。</p></li><li><p>继承Reducer后实现reduce方法</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span></span></span><br></pre></td></tr></table></figure><p>该方法中的参数分别是<code>Text key, Iterable&lt;IntWritable&gt; values, Context context</code></p><p>前两个参数是reduce方法中输入的键和值，输入的键和值对应map中输出的键值类型，并且值是一个Iterable类型，因为在shuffle阶段相同key的value分到了一起，是一个可迭代的参数。因为是按offset读取每行本文数据，最后一个参数context，它是MapReduce的上下文。什么都可以拿出来。</p></li></ul><h2 id="Writable和WritableComparable的作用"><a href="#Writable和WritableComparable的作用" class="headerlink" title="Writable和WritableComparable的作用"></a>Writable和WritableComparable的作用</h2><p>Writable是hadoop中的序列化接口，是一个接口，只定义了两个方法，分别是<code>write()</code>和<code>readFields()</code>方法，用于hadoop序列化时的读和写；<code>WritableComparable</code>也是一个序列化接口，只是在序列化的同时同时实现了java中的<code>Comparable&lt;T&gt;</code>接口，具有排序的特性。</p><p>hadoop是java写的，那么为什么hadoop要实现自己的序列化接口</p><ul><li>java序列化数据结果比较大、传输效率比较低、不能跨语言对接</li></ul><p>hadoop使用的是RPC协议传送数据，且hadoop是应用在大集群上，所以hadoop的序列化必须做到</p><ul><li>占用空间更小</li><li>传输速度更快</li><li>扩展性更强，支持多种格式的序列化</li><li>兼容性更好，需要支持多种语言，如java、scala、python等</li></ul><p>所以hadoop实现了自己的序列化接口Writable：<code>压缩</code>、<code>速度</code>、<code>扩展性</code>、<code>兼容性</code>都比java更优秀</p><p>另外:</p><ol><li><em>序列化的对象，他们超越了JVM的生死，不顾生他们的母亲，化作永恒。</em>static和transient修饰的属性除外，因为static修饰的属性是在编译时静态生成的，而对象是动态生成的，又因为transient修饰的属性禁止了属性的序列化。</li><li><em>把“活的”对象序列化，就是把“活的”对象转化成一串字节，而“反序列化”，就是从一串字节里解析出“活的”对象。</em></li></ol><h2 id="瘦包在服务器上的jar包依赖"><a href="#瘦包在服务器上的jar包依赖" class="headerlink" title="瘦包在服务器上的jar包依赖"></a>瘦包在服务器上的jar包依赖</h2><p>打包好的mapreduce程序上传到云主机，由于是瘦包，缺少某些依赖，比如连接mysql的的jar包，现在我们就解决缺少依赖的问题</p><ol><li><p>将下载好的jar包上传到云主机上</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">cp mysql-connector-java-<span class="number">5.1</span><span class="number">.27</span>-bin.jar ~/lib/</span><br></pre></td></tr></table></figure></li><li><p>将jar包加载到hadoop的classpath中</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:~/lib/mysql-connector-java-<span class="number">5.1</span><span class="number">.27</span>-bin.jar</span><br></pre></td></tr></table></figure></li><li><p>用hadoop jar 执行jar文件时，加上-libjars参数</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop jar hadoop-mapreduce-examples-<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.16</span><span class="number">.2</span>.jar wordcount -libjars /home/hadoop/lib/mysql-connector-java-<span class="number">5.1</span><span class="number">.27</span>-bin.jar  /<span class="number">1</span>.txt /out</span><br></pre></td></tr></table></figure></li></ol><p>如果上诉方法有问题可以使用hadoop的分布式缓存</p><ol><li><p>把jar包传到集群上，命令如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop fs -put mysql-connector-java-<span class="number">5.1</span><span class="number">.27</span>.jar /lib</span><br></pre></td></tr></table></figure></li><li><p>在mr程序提交job前，添加一下语句：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">job.addArchiveToClassPath(<span class="keyword">new</span> Path(<span class="string">"hdfs://aliyun:9000/lib/mysql-connector-java-5.1.27.jar"</span>));</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> MapReduce </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YARN的调优&amp;YARN的三种调度器</title>
      <link href="/2018/10/11/hadoop/8/"/>
      <url>/2018/10/11/hadoop/8/</url>
      
        <content type="html"><![CDATA[<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><ol><li>梳理YARN资源调优参数</li><li>调度器整理三种，区别是什么，CDH默认是什么</li></ol><h3 id="YARN的资源调优"><a href="#YARN的资源调优" class="headerlink" title="YARN的资源调优"></a>YARN的资源调优</h3><p><code>背景:</code> 假设每台服务器拥有内存128G 16物理core，怎么分配？</p><ol><li><p>装完CentOS，消耗内存1G</p></li><li><p>系统预览15%-20%内存(包含1.1)，以防全部使用导致系统夯住 和 oom机制事件，或者给未来部署组件预览点空间(<code>128*20%=25.6G==26G</code>)</p></li><li><p>假设只有DN NM节点，余下内存: <code>128-26=102G</code></p><ol><li><p>给DN进程(自身)2G，给NM进程(自身)4G，剩余<code>102-2-4=96G</code></p></li><li><p>container内存的分配共96G</p><ul><li><p><code>yarn.nodemanager.resource.memory-mb</code>     共 96G</p></li><li><p><code>yarn.scheduler.minimum-allocation-mb</code>    最少1G    极限情况下，只有96个container 内存1G</p></li><li><p><code>yarn.scheduler.maximum-allocation-mb</code>    最多96G  极限情况下，只有1个container 内存96G</p><p>container的内存会自动增加，默认1G递增，那么container的个数的范围: 1-96个</p></li></ul></li><li><p>container物理核分配 (物理核:虚拟核 =1:2 ==&gt;16:32)</p><ul><li><p><code>yarn.nodemanager.resource.cpu-vcores</code>            共 32个</p></li><li><p><code>yarn.scheduler.minimum-allocation-vcores</code>     最少1个   极限情况下，只有32个container    </p></li><li><p><code>yarn.scheduler.maximum-allocation-vcores</code>     最多32个 极限情况下，只有1个container</p><p>container的物理核会自动增加，默认1个递增，那么container的个数的范围: :1-32个</p></li></ul></li><li><p><code>关键:</code> cloudera公司推荐，一个container的vcore最好不要超过5，那么我们设置4</p><ul><li><p><code>yarn.scheduler.maximum-allocation-vcores</code>    4   </p><p>目前为止，极限情况下，共有8个container (32/4)</p></li></ul></li><li><p>综合memory+vcore的分配</p><ol><li><p>一共有32个vcore，一个container的vcore是4个，那么分配container一共有8个</p></li><li><p>重新分配核</p><ul><li><p><code>yarn.nodemanager.resource.cpu-vcores</code>            共32个</p></li><li><p><code>yarn.scheduler.minimum-allocation-vcores</code>    最少4个    </p></li><li><p><code>yarn.scheduler.maximum-allocation-vcores</code>    最多4个   极限container 8个</p></li></ul></li><li><p>根据物理核重新分配内存</p><ul><li><p><code>yarn.nodemanager.resource.memory-mb</code>      共96G</p></li><li><p><code>yarn.scheduler.minimum-allocation-mb</code>    最少12G  </p></li><li><p><code>yarn.scheduler.maximum-allocation-mb</code>    最多12G   (极限container 8个)</p></li></ul></li><li><p>分配后的每个container的物理核数是4个，内存大小是12G，当然spark计算时内存不够大，这个参数肯定要调大，那么这种理想化的设置个数必然要打破，以memory为主</p></li></ol></li><li><p>假如 256G内存 56core，请问参数如何设置</p><ol><li><p>首先减去系统内存开销和其他进程开销，</p><ul><li><p>系统开销: 256*0.2=52G</p></li><li><p>DN开销: 2G</p></li><li><p>NM开销: 4G</p></li><li><p>Hbase开销: 暂无</p></li><li><p>升序内存容量: 256-52-2-4=198G</p></li></ul></li><li><p>确定每个container的物理核数量是4个，56/4=14个container容器</p></li><li><p>确定了最多分配14个container容器，每个容器的内存应该分配的容量是: 198/14==&gt;14G</p><p><strong>那么每个container的最大核数设置4，最大内存数设置14G</strong></p></li></ol></li><li><p>假如该节点还有组件，比如hbase regionserver进程，那么该如何设置？</p><p>总容量减就完事了。    </p></li></ol><p>所有的配置信息在<code>yarn-default.xm</code>l文件中</p><h4 id="内存参数默认值"><a href="#内存参数默认值" class="headerlink" title="内存参数默认值:"></a>内存参数默认值:</h4><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>yarn.nodemanager.resource.memory-mb</td><td>-1</td><td>可以分配给容器的物理内存总量(以MB为单位)。</td></tr><tr><td>yarn.scheduler.minimum-allocation-mb</td><td>1024</td><td>RM上每个容器请求的最小分配</td></tr><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>8192</td><td>RM上每个容器请求的最大分配</td></tr></tbody></table><h4 id="核数参数默认值"><a href="#核数参数默认值" class="headerlink" title="核数参数默认值:"></a>核数参数默认值:</h4><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>yarn.nodemanager.resource.cpu-vcores</td><td>-1</td><td>可以为容器分配的vcore总数量。</td></tr><tr><td>yarn.scheduler.minimum-allocation-vcores</td><td>1</td><td>RM上每个容器请求的最小虚拟CPU核心分配。</td></tr><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>4</td><td>RM上每个容器请求的最大虚拟CPU核心分配。</td></tr></tbody></table></li></ol><h3 id="Yarn的三种调度器"><a href="#Yarn的三种调度器" class="headerlink" title="Yarn的三种调度器"></a>Yarn的三种调度器</h3><ul><li><p>Apache hadoop2.x的默认调度器是Capacity Scheduler(计算调度器)</p></li><li><p>CDH的默认调度器是Fair Scheduler(公平调度器)</p></li></ul><h4 id="Yarn三种调度策略对比"><a href="#Yarn三种调度策略对比" class="headerlink" title="Yarn三种调度策略对比"></a>Yarn三种调度策略对比</h4><p>在Yarn中有三种调度器可以选择：FIFO Scheduler ，Capacity Scheduler，FairScheduler。</p><ol><li><p>FIFO Scheduler</p><p><img src="https://img-blog.csdnimg.cn/20181101090612286.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MzgyNDUz,size_16,color_FFFFFF,t_70" alt="队列调度器"></p><p>FIFO Scheduler把应用按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的应用进行分配资源，待最头上的应用需求满足后再给下一个分配，以此类推。</p><p>FIFO Scheduler是最简单也是最容易理解的调度器，也不需要任何配置，但它并不适用于共享集群。大的应用可能会占用所有集群资源，这就导致其它应用被阻塞。在共享集群中，更适合采用Capacity Scheduler或Fair Scheduler，这两个调度器都允许大任务和小任务在提交的同时获得一定的系统资源。从图中可以看出，在FIFO 调度器中，小任务会被大任务阻塞。</p></li><li><p>Capacity Scheduler</p><p><img src="https://img-blog.csdnimg.cn/20181101091012607.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MzgyNDUz,size_16,color_FFFFFF,t_70" alt="计算调度器"></p><p>而对于Capacity调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间。</p></li><li><p>Fair Scheduler</p><p><img src="https://img-blog.csdnimg.cn/20181101091843173.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MzgyNDUz,size_16,color_FFFFFF,t_70" alt="公平调度器"></p><p>在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。如上图所示，当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。</p><p>需要注意的是，在上图Fair调度器中，从第二个任务提交到获得资源会有一定的延迟，因为它需要等待第一个任务释放占用的Container。小任务执行完成之后也会释放自己占用的资源，大任务又获得了全部的系统资源。最终的效果就是Fair调度器即得到了高的资源利用率又能保证小任务及时完成。</p></li></ol><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn application -kill &lt;Application ID&gt;#杀死进程</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> Yarn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> Yarn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MR的执行流程&amp;初探文件压缩&amp;初探文件格式&amp;分片数与任务数&amp;shuffle的执行流程&amp;WordCount的执行流程</title>
      <link href="/2018/10/10/hadoop/7/"/>
      <url>/2018/10/10/hadoop/7/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 mr on yarn流程 </li><li>整理 文件格式有哪些 优缺点 </li><li>整理 压缩格式有哪些 优缺点 </li><li>spilt–&gt;map task关系 </li><li>wordcount的剖解图 </li><li>shuffle的理解 </li></ol><h3 id="mr-on-yarn流程"><a href="#mr-on-yarn流程" class="headerlink" title="mr on yarn流程"></a>mr on yarn流程</h3><p><img src="https://yerias.github.io/hadoop_img/rm_on_yarn.JPG" alt="rm_on_yarn"></p><h4 id="mr-on-yarn的工作流程简略分为两步"><a href="#mr-on-yarn的工作流程简略分为两步" class="headerlink" title="mr on yarn的工作流程简略分为两步:"></a>mr on yarn的工作流程简略分为两步:</h4><ol><li>启动应用程序管理器，申请资源。</li><li>运行任务，直到任务运行完成。</li></ol><h4 id="mr-on-yarn的工作流程详细分为八步"><a href="#mr-on-yarn的工作流程详细分为八步" class="headerlink" title="mr on yarn的工作流程详细分为八步:"></a>mr on yarn的工作流程详细分为八步:</h4><ol><li>用户向资源管理器(ResourceManager)提交作业，作业包括MapReduce应用程序管理器，启动MapReduce应用程序管理器的程序和用户自己编写的MapReduce程序。用于提交的所有作业都由ApplicationManager(全局应用程序管理器)管理。</li><li>资源管理器为该应用程序分配一个容器(Container)，并与对应的节点管理器(NodeManager)通信，要求它在这个容器中启动MapReduce应用程序管理器。</li><li>MapReduce应用程序管理器首先向资源管理器注册，这样用户可以直接通过资源管理器查看应用程序的运行状态，然后它将为各个任务申请资源，并监控他们的运行状态，直到运行结束，即重复步骤4-7。</li><li>MapReduce应用程序管理器采用轮询的方式通过RPC协议向资源管理器申请和领取资源。</li><li>MapReduce应用程序管理器申请到资源后，便与对应的节点管理器通信，要求启动任务。</li><li>节点管理器为任务设置好运行环境，包括环境变量、Jar包、二进制程序等，然后将任务启动命令写到另外一个脚本中，并通过该脚本启动任务。</li><li>各个任务通过RPC协议向MapReduce应用程序管理器汇报自己的状态和进度，MapReduce应用程序随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。在应用程序运行过程中，用户可以随时通过RPC协议向MapReduce应用程序管理器查询应用程序当前的运行状态。</li><li>应用程序运行完成后，MapReduce应用程序管理器向资源管理器注销并关闭自己。</li></ol><h3 id="文件格式有哪些-优缺点"><a href="#文件格式有哪些-优缺点" class="headerlink" title="文件格式有哪些 优缺点"></a>文件格式有哪些 优缺点</h3><p><strong>Hadoop中的文件格式大致上分为面向行和面向列两类：</strong></p><ol><li><p>面向行：同一行的数据存储在一起，即连续存储。SequenceFile,MapFile,Avro Datafile。采用这种方式，如果只需要访问行的一小部分数据，亦需要将整行读入内存，推迟序列化一定程度上可以缓解这个问题，但是从磁盘读取整行数据的开销却无法避免。面向行的存储适合于整行数据需要同时处理的情况。</p></li><li><p>面向列：整个文件被切割为若干列数据，每一列数据一起存储。Parquet , RCFile,ORCFile。面向列的格式使得读取数据时，可以跳过不需要的列，适合于只处于行的一小部分字段的情况。但是这种格式的读写需要更多的内存空间，因为需要缓存行在内存中（为了获取多行中的某一列）。同时不适合流式写入，因为一旦写入失败，当前文件无法恢复，而面向行的数据在写入失败时可以重新同步到最后一个同步点，所以Flume采用的是面向行的存储格式。</p></li></ol><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-0c5b3f7a2eceaaef.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="普通二维表"></p><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-dbe2595ee1e293b1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="行存储和列存储"></p><p>下面介绍几种相关的文件格式，它们在Hadoop体系上被广泛使用：</p><h4 id="SequenceFile"><a href="#SequenceFile" class="headerlink" title="SequenceFile"></a>SequenceFile</h4><p>SequenceFile是Hadoop API 提供的一种二进制文件,它将数据以的形式序列化到文件中。这种二进制文件内部使用Hadoop 的标准的Writable 接口实现序列化和反序列化。它与Hadoop API中的MapFile 是互相兼容的。Hive 中的SequenceFile 继承自Hadoop API 的SequenceFile,不过它的key为空,使用value 存放实际的值, 这样是为了避免MR 在运行map 阶段的排序过程。如果你用Java API 编写SequenceFile,并让Hive 读取的话,请确保使用value字段存放数据,否则你需要自定义读取这种SequenceFile 的InputFormat class 和OutputFormat class。</p><p>SequenceFile的文件结构如下：</p><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-3f5cd8d90742ec24.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="SequenceFile详解图1"></p><p>根据是否压缩，以及采用记录压缩还是块压缩，存储格式有所不同：</p><ul><li><p>不压缩:</p><p>按照记录长度、Key长度、Value程度、Key值、Value值依次存储。长度是指字节数。采用指定的Serialization进行序列化。</p></li><li><p>Record压缩:</p><p>只有value被压缩，压缩的codec保存在Header中。</p></li><li><p>Block压缩:</p><p>多条记录被压缩在一起，可以利用记录之间的相似性，更节省空间。Block前后都加入了同步标识。Block的最小值由io.seqfile.compress.blocksize属性设置。 </p></li></ul><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-d21745547eb4c021.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="SequenceFile详解图1"></p><h4 id="Avro"><a href="#Avro" class="headerlink" title="Avro"></a>Avro</h4><p>Avro是一种用于支持数据密集型的二进制文件格式。它的文件格式更为紧凑,若要读取大量数据时,Avro能够提供更好的序列化和反序列化性能。并 且Avro数据文件天生是带Schema定义的,所以它不需要开发者在API 级别实现自己的Writable对象。最近多个Hadoop 子项目都支持Avro 数据格式,如Pig 、Hive、Flume、Sqoop和Hcatalog。</p><h4 id="RCFile"><a href="#RCFile" class="headerlink" title="RCFile"></a>RCFile</h4><p>RCFile是Hive推出的一种专门面向列的数据格式。 它遵循“先按列划分,再垂直划分”的设计理念。当查询过程中,针对它并不关心的列时,它会在IO上跳过这些列。需要说明的是,RCFile在map阶段从 远端拷贝仍然是拷贝整个数据块,并且拷贝到本地目录后RCFile并不是真正直接跳过不需要的列,并跳到需要读取的列, 而是通过扫描每一个row group的头部定义来实现的,但是在整个HDFS Block 级别的头部并没有定义每个列从哪个row group起始到哪个row group结束。所以在读取所有列的情况下,RCFile的性能反而没有SequenceFile高。</p><p>Hive的Record Columnar File,这种类型的文件先将数据按行划分成Row Group，在Row Group内部，再将数据按列划分存储。其结构如下：</p><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-0a6f19b8bb6ee4e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/688/format/webp" alt="RCFile详解图1"></p><p>相比较于单纯地面向行和面向列：</p><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-0df474935c56807d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/649/format/webp" alt="RCFile详解图2"></p><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-6d56c39e3445288e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/615/format/webp" alt="RCFile详解图3"></p><h4 id="ORC"><a href="#ORC" class="headerlink" title="ORC"></a>ORC</h4><p>ORC（Optimized Record Columnar File)提供了一种比RCFile更加高效的文件格式。其内部将数据划分为默认大小为250M的Stripe。每个Stripe包括索引、数据和Footer。索引存储每一列的最大最小值，以及列中每一行的位置。</p><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-1bb66728d866b469.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/580/format/webp" alt="ORCFile详解图"></p><p>在Hive中，如下命令用于使用ORCFile：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE ... STORED AS ORC</span><br><span class="line"></span><br><span class="line">ALTER TABLE ... SET FILEFORMAT ORC</span><br><span class="line"></span><br><span class="line">SET hive.default.fileformat=ORC</span><br></pre></td></tr></table></figure><h4 id="Parquet"><a href="#Parquet" class="headerlink" title="Parquet"></a>Parquet</h4><p>一种通用的面向列的存储格式，基于Google的Dremel。特别擅长处理深度嵌套的数据。</p><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-b45e32049ab54cbb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1124/format/webp" alt="img"></p><p>对于嵌套结构，Parquet将其转换为平面的列存储，嵌套结构通过Repeat Level和Definition Level来表示（R和D），在读取数据重构整条记录的时候，使用元数据重构记录的结构。下面是R和D的一个例子：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">AddressBook &#123;</span><br><span class="line"> contacts: &#123;</span><br><span class="line">  phoneNumber: &quot;555 987 6543&quot;</span><br><span class="line"> &#125;</span><br><span class="line"> contacts: &#123;</span><br><span class="line">&#125;&#125;</span><br><span class="line">AddressBook &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-36b68fc1d8e2b99b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/577/format/webp" alt="Parquet详解图"></p><h4 id="文件存储大小比较与分析"><a href="#文件存储大小比较与分析" class="headerlink" title="文件存储大小比较与分析"></a>文件存储大小比较与分析</h4><p>我们选取一个TPC-H标准测试来说明不同的文件格式在存储上的开销。因为此数据是公开的,所以读者如果对此结果感兴趣,也可以对照后面的实验自行 做一遍。Orders 表文本格式的原始大小为1.62G。 我们将其装载进Hadoop 并使用Hive 将其转化成以上几种格式,在同一种LZO 压缩模式下测试形成的文件的大小</p><p><img src="https:////upload-images.jianshu.io/upload_images/6450093-34e05b3cb0e72740.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/849/format/webp" alt="文件存储大小比较与分析"></p><h4 id="不同格式文件大小对比"><a href="#不同格式文件大小对比" class="headerlink" title="不同格式文件大小对比"></a>不同格式文件大小对比</h4><ul><li><p>从上述实验结果可以看到,SequenceFile无论在压缩和非压缩的情况下都比原始纯文本TextFile大,其中非压缩模式下大11%, 压缩模式下大6.4%。这跟SequenceFile的文件格式的定义有关: SequenceFile在文件头中定义了其元数据,元数据的大小会根据压缩模式的不同略有不同。一般情况下,压缩都是选取block 级别进行的,每一个block都包含key的长度和value的长度,另外每4K字节会有一个sync-marker的标记。对于TextFile文件格 式来说不同列之间只需要用一个行间隔符来切分,所以TextFile文件格式比SequenceFile文件格式要小。但是TextFile 文件格式不定义列的长度,所以它必须逐个字符判断每个字符是不是分隔符和行结束符。因此TextFile 的反序列化开销会比其他二进制的文件格式高几十倍以上。</p></li><li><p>RCFile文件格式同样也会保存每个列的每个字段的长度。但是它是连续储存在头部元数据块中,它储存实际数据值也是连续的。另外RCFile 会每隔一定块大小重写一次头部的元数据块(称为row group,由hive.io.rcfile.record.buffer.size控制,其默认大小为4M),这种做法对于新出现的列是必须的,但是如 果是重复的列则不需要。RCFile 本来应该会比SequenceFile 文件大,但是RCFile 在定义头部时对于字段长度使用了Run Length Encoding进行压缩,所以RCFile 比SequenceFile又小一些。Run length Encoding针对固定长度的数据格式有非常高的压缩效率,比如Integer、Double和Long等占固定长度的数据类型。在此提一个特例—— Hive 0.8引入的TimeStamp 时间类型,如果其格式不包括毫秒,可表示为”YYYY-MM-DD HH:MM:SS”,那么就是固定长度占8个字节。如果带毫秒,则表示为”YYYY-MM-DD HH:MM:SS.fffffffff”,后面毫秒的部分则是可变的。</p></li><li><p>Avro文件格式也按group进行划分。但是它会在头部定义整个数据的模式(Schema), 而不像RCFile那样每隔一个row group就定义列的类型,并且重复多次。另外,Avro在使用部分类型的时候会使用更小的数据类型,比如Short或者Byte类型,所以Avro的数 据块比RCFile 的文件格式块更小。</p></li></ul><h3 id="压缩格式有哪些-优缺点"><a href="#压缩格式有哪些-优缺点" class="headerlink" title="压缩格式有哪些 优缺点"></a>压缩格式有哪些 优缺点</h3><h4 id="压缩的好处和坏处"><a href="#压缩的好处和坏处" class="headerlink" title="压缩的好处和坏处"></a>压缩的好处和坏处</h4><p><strong>好处</strong></p><ul><li>减少存储磁盘空间</li><li>降低IO(网络的IO和磁盘的IO)</li><li>加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度</li></ul><p><strong>坏处</strong></p><ul><li>由于使用数据时，需要先将数据解压，加重CPU负荷</li></ul><h4 id="压缩格式"><a href="#压缩格式" class="headerlink" title="压缩格式"></a>压缩格式</h4><p><img src="https://ruozedata.github.io/assets/blogImg/%E5%8E%8B%E7%BC%A91.png" alt="压缩格式"></p><p><img src="https://ruozedata.github.io/assets/blogImg/%E5%8E%8B%E7%BC%A92.png" alt="压缩空间比较"></p><p><img src="https://ruozedata.github.io/assets/blogImg/yasuo3.png" alt="压缩时间比较"></p><p>可以看出，压缩空间比值越高，压缩时间越长，压缩比：<code>Snappy&gt;LZ4&gt;LZO&gt;GZIP&gt;BZIP2</code></p><table><thead><tr><th align="left">压缩格式</th><th align="left">优点</th><th align="left">缺点</th><th></th></tr></thead><tbody><tr><td align="left"><strong>gzip</strong></td><td align="left">压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便</td><td align="left">不支持split</td><td></td></tr><tr><td align="left"><strong>lzo</strong></td><td align="left">压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便</td><td align="left">压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）</td><td></td></tr><tr><td align="left"><strong>snappy</strong></td><td align="left">压缩速度快；支持hadoop native库</td><td align="left">不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令d. bzip2</td><td></td></tr><tr><td align="left"><strong>bzip2</strong></td><td align="left">支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便</td><td align="left">压缩/解压速度慢；不支持native</td><td></td></tr></tbody></table><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>不同的场景选择不同的压缩方式，肯定没有一个一劳永逸的方法，如果选择高压缩比，那么对于cpu的性能要求要高，同时压缩、解压时间耗费也多；选择压缩比低的，对于磁盘io、网络io的时间要多，空间占据要多；对于支持分割的，可以实现并行处理。</p><h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><p>一般在HDFS 、Hive、HBase中会使用；<br>当然一般较多的是结合Spark 来一起使用。</p><h3 id="Spilt–-gt-Map-Task关系"><a href="#Spilt–-gt-Map-Task关系" class="headerlink" title="Spilt–&gt;Map Task关系"></a>Spilt–&gt;Map Task关系</h3><p>Reduce Task默认是1个，Map Task默认是2个，但是实际运行场景下，Map Task的个数和切片的个数保持一致，而切片的个数又与文件数和文件大小相关联。切片默认大小决定文件被分成多少个切片，执行多少个Map Task。</p><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>mapreduce.job.maps</td><td>2</td><td>The default number of map tasks per job.</td></tr><tr><td>mapreduce.job.reduces</td><td>1</td><td>The default number of reduce tasks per job.</td></tr></tbody></table><h3 id="shuffle的理解"><a href="#shuffle的理解" class="headerlink" title="shuffle的理解"></a>shuffle的理解</h3><p>俩字: 洗牌</p><p>shuffle阶段又可以分为Map端的shuff和reduce端的shuffle</p><p><img src="https:/yerias.github.io/hadoop_img/shuffer.jpg" alt="shuffe过程"></p><h4 id="map端的shuffle"><a href="#map端的shuffle" class="headerlink" title="map端的shuffle"></a>map端的shuffle</h4><ul><li>map端会处理出入数据并产生中间结果，这个中间结果会写到本地磁盘，而不是HDFS。每个map的输出会先写到内存缓冲区中，当写入的数据达到设定的阈值时，系统将会启动一个线程将缓冲区的数据写到磁盘，这个过程叫做spill(溢写)。</li><li>在spill之前，会先进行两次排序，首先根据数据所属的partition进行排序，然后每个partition中的数据再按key来排序，partition的目的是将记录划分到不同的reduce上，以期望能达到负载均衡，以后的reduce就会根据partition来读取自己对应的数据。接着运行combiner(如果设置了的话)，combiner的本质也是一个reduce，其目的是对将要写入到磁盘的文件先进行一次处理，这样，写入到磁盘的数据量就会减少。最后将数据写到本地磁盘产生spill文件(spill文件保存在{mapred.local.dir}指定的目录中，map任务结束后就会被删除)。</li><li>最后，每个map任务可能产生多个spill文件，在每个map任务完成前，会通过多路归并算法将这些spill文件合并成一个文件。至此，map的shuffle过程就结束了。</li></ul><h4 id="reduce端的shuffle"><a href="#reduce端的shuffle" class="headerlink" title="reduce端的shuffle"></a>reduce端的shuffle</h4><ul><li>reduce端的shuffle主要包括三个阶段，copy、sort(merge)和reduce</li><li>首先将map端产生的输出文件拷贝到reduce端，但每个reduce如何知道自己应该处理哪些数据呢？因为map端进行partition的时候，实际上就相当于指定了每个reduce要处理的数据(partition就对应了reduce)，所以reduce在拷贝的数据的时候只需拷贝与自己对应的partition中的数据即可。每个reduce会处理一个或多个partiton，但需要先将自己对应的partition中的数据从每个map的输出结果中拷贝出来。</li><li>接下来就是sort阶段，也称为merge阶段，因为这个阶段的主要工作是执行了归并排序。从map端拷贝到reduce端的数据都是有序的，所以很适合归并排序。最终在reduce端生产一个较大的文件作为reduce的输入。</li><li>最后就是reduce阶段了，在这个过程中产生最终的输出结果，并将其写到HDFS上。</li></ul><h3 id="WordCount的剖解图"><a href="#WordCount的剖解图" class="headerlink" title="WordCount的剖解图"></a>WordCount的剖解图</h3><p><img src="https:/yerias.github.io/hadoop_img/wordcount.jpg" alt="wordcount执行流程"></p><h4 id="Map任务处理"><a href="#Map任务处理" class="headerlink" title="Map任务处理"></a>Map任务处理</h4><ol><li>读取HDFS中的文件，每一行解析成一个&lt;K,V&gt;值。每个键值对调用一次map函数。</li><li>重写map()方法，接收1产生的&lt;K,V&gt;值进行处理，转为新的&lt;K,V&gt;输出。</li><li>对2输出的&lt;K,V&gt;值进行分区，默认一盒分区。</li><li>对不同分区中的数据进行排序(按照K)、分组。分组指的是相同Key的Value放到一个集合中。</li><li>(可选)对分组后的数据进行合并。</li></ol><h4 id="Reduce任务处理"><a href="#Reduce任务处理" class="headerlink" title="Reduce任务处理"></a>Reduce任务处理</h4><ol><li>多个Map任务的输出，按照不同的分区，通过网络copy到不同的Reduce节点上。</li><li>对多个map的输出进行合并、排序。重写reduce()方法，接收的是分组后的数据，实现自己的业务逻辑，处理后产生新的&lt;K,V&gt;值输出</li><li>对reduce输出的&lt;K,V&gt;写到HDFS中。</li></ol><hr><p>整理来自：<a href="https://ruozedata.github.io/2018/04/18/、https://www.jianshu.com/p/43630456a18a" target="_blank" rel="noopener">https://ruozedata.github.io/2018/04/18/、https://www.jianshu.com/p/43630456a18a</a></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> MapReduce </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HDFS </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS Block损坏恢复</title>
      <link href="/2018/10/10/PE/2/"/>
      <url>/2018/10/10/PE/2/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">上传:</span><br><span class="line">-bash-4.2$ hdfs dfs -mkdir /blockrecover</span><br><span class="line">-bash-4.2$ echo &quot;www.ruozedata.com&quot; &gt; ruozedata.md</span><br><span class="line"></span><br><span class="line">-bash-4.2$ hdfs dfs -put ruozedata.md /blockrecover</span><br><span class="line">-bash-4.2$ hdfs dfs -ls /blockrecover</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 hdfs supergroup         18 2019-03-03 14:42 /blockrecover/ruozedata.md</span><br><span class="line">-bash-4.2$ </span><br><span class="line"></span><br><span class="line">校验: 健康状态</span><br><span class="line">-bash-4.2$ hdfs fsck /</span><br><span class="line">Connecting to namenode via http://yws76:50070/fsck?ugi=hdfs&amp;path=%2F</span><br><span class="line">FSCK started by hdfs (auth:SIMPLE) from /192.168.0.76 for path / at Sun Mar 03 14:44:44 CST 2019</span><br><span class="line">...............................................................................Status: HEALTHY</span><br><span class="line"> Total size:    50194618424 B</span><br><span class="line"> Total dirs:    354</span><br><span class="line"> Total files:   1079</span><br><span class="line"> Total symlinks:                0</span><br><span class="line"> Total blocks (validated):      992 (avg. block size 50599413 B)</span><br><span class="line"> Minimally replicated blocks:   992 (100.0 %)</span><br><span class="line"> Over-replicated blocks:        0 (0.0 %)</span><br><span class="line"> Under-replicated blocks:       0 (0.0 %)</span><br><span class="line"> Mis-replicated blocks:         0 (0.0 %)</span><br><span class="line"> Default replication factor:    3</span><br><span class="line"> Average block replication:     3.0</span><br><span class="line"> Corrupt blocks:                0</span><br><span class="line"> Missing replicas:              0 (0.0 %)</span><br><span class="line"> Number of data-nodes:          3</span><br><span class="line"> Number of racks:               1</span><br><span class="line">FSCK ended at Sun Mar 03 14:44:45 CST 2019 in 76 milliseconds</span><br><span class="line"></span><br><span class="line">The filesystem under path &apos;/&apos; is HEALTHY</span><br><span class="line">-bash-4.2$</span><br></pre></td></tr></table></figure><h3 id="直接DN节点上删除文件一个block的一个副本-3副本"><a href="#直接DN节点上删除文件一个block的一个副本-3副本" class="headerlink" title="直接DN节点上删除文件一个block的一个副本(3副本)"></a>直接DN节点上删除文件一个block的一个副本(3副本)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">删除块和meta文件:</span><br><span class="line">[root@yws87 subdir135]# rm -rf blk_1075808214 blk_1075808214_2068515.meta</span><br><span class="line"></span><br><span class="line">直接重启HDFS，直接模拟损坏效果，然后fsck检查:</span><br><span class="line">-bash-4.2$ hdfs fsck /</span><br><span class="line">Connecting to namenode via http://yws77:50070/fsck?ugi=hdfs&amp;path=%2F</span><br><span class="line">FSCK started by hdfs (auth:SIMPLE) from /192.168.0.76 for path / at Sun Mar 03 16:02:04 CST 2019</span><br><span class="line">.</span><br><span class="line">/blockrecover/ruozedata.md:  Under replicated BP-1513979236-192.168.0.76-1514982530341:blk_1075808214_2068515. Target Replicas is 3 but found 2 live replica(s), 0 decommissioned replica(s), 0 decommissioning replica(s).</span><br><span class="line">...............................................................................Status: HEALTHY</span><br><span class="line"> Total size:    50194618424 B</span><br><span class="line"> Total dirs:    354</span><br><span class="line"> Total files:   1079</span><br><span class="line"> Total symlinks:                0</span><br><span class="line"> Total blocks (validated):      992 (avg. block size 50599413 B)</span><br><span class="line"> Minimally replicated blocks:   992 (100.0 %)</span><br><span class="line"> Over-replicated blocks:        0 (0.0 %)</span><br><span class="line"> Under-replicated blocks:       1 (0.10080645 %)</span><br><span class="line"> Mis-replicated blocks:         0 (0.0 %)</span><br><span class="line"> Default replication factor:    3</span><br><span class="line"> Average block replication:     2.998992</span><br><span class="line"> Corrupt blocks:                0</span><br><span class="line"> Missing replicas:              1 (0.033602152 %)</span><br><span class="line"> Number of data-nodes:          3</span><br><span class="line"> Number of racks:               1</span><br><span class="line">FSCK ended at Sun Mar 03 16:02:04 CST 2019 in 148 milliseconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The filesystem under path &apos;/&apos; is HEALTHY</span><br><span class="line">-bash-4.2$</span><br></pre></td></tr></table></figure><h3 id="手动修复hdfs-debug"><a href="#手动修复hdfs-debug" class="headerlink" title="手动修复hdfs debug"></a>手动修复hdfs debug</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-bash-4.2$ hdfs |grep debug</span><br><span class="line">没有输出debug参数的任何信息结果！</span><br><span class="line">故hdfs命令帮助是没有debug的，但是确实有hdfs debug这个组合命令，切记。</span><br><span class="line"></span><br><span class="line">修复命令:</span><br><span class="line">-bash-4.2$ hdfs debug  recoverLease  -path /blockrecover/ruozedata.md -retries 10</span><br><span class="line">recoverLease SUCCEEDED on /blockrecover/ruozedata.md</span><br><span class="line">-bash-4.2$ </span><br><span class="line"></span><br><span class="line">直接DN节点查看，block文件和meta文件恢复:</span><br><span class="line">[root@yws87 subdir135]# ll</span><br><span class="line">total 8</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 56 Mar  3 14:28 blk_1075808202</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 14:28 blk_1075808202_2068503.meta</span><br><span class="line">[root@yws87 subdir135]# ll</span><br><span class="line">total 24</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 56 Mar  3 14:28 blk_1075808202</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 14:28 blk_1075808202_2068503.meta</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 18 Mar  3 15:23 blk_1075808214</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 15:23 blk_1075808214_2068515.meta</span><br></pre></td></tr></table></figure><h3 id="自动修复"><a href="#自动修复" class="headerlink" title="自动修复"></a>自动修复</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">当数据块损坏后，DN节点执行directoryscan操作之前，都不会发现损坏；</span><br><span class="line">也就是directoryscan操作是间隔6h</span><br><span class="line">dfs.datanode.directoryscan.interval : 21600</span><br><span class="line"></span><br><span class="line">在DN向NN进行blockreport前，都不会恢复数据块;</span><br><span class="line">也就是blockreport操作是间隔6h</span><br><span class="line">dfs.blockreport.intervalMsec : 21600000</span><br><span class="line"></span><br><span class="line">当NN收到blockreport才会进行恢复操作。</span><br></pre></td></tr></table></figure><p>具体参考生产上HDFS（CDH5.12.0）对应的版本的文档参数:<a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.12.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.12.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>生产上本人一般倾向于使用 手动修复方式，但是前提要手动删除损坏的block块。</p><p>切记，是删除损坏block文件和meta文件，而不是删除hdfs文件。</p><p>当然还可以先把文件get下载，然后hdfs删除，再对应上传。</p><p>切记删除不要执行: hdfs fsck / -delete 这是删除损坏的文件， 那么数据不就丢了嘛；除非无所谓丢数据，或者有信心从其他地方可以补数据到hdfs！</p><h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><ul><li>那么如何确定一个文件的损失的块位置，哪几种方法呢？</li><li>CDH的配置里搜索没有这两个参数，怎么调整生效呢？</li></ul><p>转载来源: [<a href="https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/]" target="_blank" rel="noopener">https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/]</a>(<a href="https://ruozedata.github.io/2019/06/06/生产HDFS" target="_blank" rel="noopener">https://ruozedata.github.io/2019/06/06/生产HDFS</a> Block损坏恢复最佳实践(含思考题)/)</p>]]></content>
      
      
      <categories>
          
          <category> 生产故障案例 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生产故障案例 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DataNode OOM溢出</title>
      <link href="/2018/10/09/PE/1/"/>
      <url>/2018/10/09/PE/1/</url>
      
        <content type="html"><![CDATA[<h3 id="DataNode的内存溢出报错"><a href="#DataNode的内存溢出报错" class="headerlink" title="DataNode的内存溢出报错"></a>DataNode的内存溢出报错</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2017-12-17 23:58:14,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1437036909-![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.36-1509097205664:blk_1074725940_987917, type=HAS_DOWNSTREAM_IN_PIPELINE terminating</span><br><span class="line">2017-12-17 23:58:31,425 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode is out of memory. Will retry in 30 seconds.</span><br><span class="line">java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">at java.lang.Thread.start0(Native Method)</span><br><span class="line">at java.lang.Thread.start(Thread.java:714)</span><br><span class="line">at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:154)</span><br><span class="line">at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">2017-12-17 23:59:01,426 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode is out of memory. Will retry in 30 seconds.</span><br><span class="line">java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">at java.lang.Thread.start0(Native Method)</span><br><span class="line">at java.lang.Thread.start(Thread.java:714)</span><br><span class="line">at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:154)</span><br><span class="line">at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">2017-12-17 23:59:05,520 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode is out of memory. Will retry in 30 seconds.</span><br><span class="line">java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">at java.lang.Thread.start0(Native Method)</span><br><span class="line">at java.lang.Thread.start(Thread.java:714)</span><br><span class="line">at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:154)</span><br><span class="line">at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">2017-12-17 23:59:31,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1437036909-![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.36-1509097205664:blk_1074725951_987928 src: /![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.54:40478 dest: /![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.48:50010</span><br></pre></td></tr></table></figure><h3 id="CDH查看DataNode的内存情况"><a href="#CDH查看DataNode的内存情况" class="headerlink" title="CDH查看DataNode的内存情况"></a>CDH查看DataNode的内存情况</h3><p><img src="https://yerias.github.io/hadoop_img/20191205092342.jpg" alt="datanode内存使用图"></p><h3 id="明明1G的内存都没有使用，为什么会报OOM？"><a href="#明明1G的内存都没有使用，为什么会报OOM？" class="headerlink" title="明明1G的内存都没有使用，为什么会报OOM？"></a>明明1G的内存都没有使用，为什么会报OOM？</h3><p>可以确定是操作系统哪里设置错了，我想应该是把产品环境的某个参数配置错了，系统本身的影响肯定不会有了，因为产品环境上我们只create了800左右个线程，就OOM了，那应该就是配置的问题了</p><p>解决方法：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1.</span><br><span class="line"></span><br><span class="line">echo "kernel.threads-max=196605" &gt;&gt; /etc/sysctl.conf</span><br><span class="line">echo "kernel.pid_max=196605" &gt;&gt; /etc/sysctl.conf</span><br><span class="line">echo "vm.max_map_count=393210" &gt;&gt; /etc/sysctl.conf</span><br><span class="line">sysctl -p </span><br><span class="line"></span><br><span class="line">2.</span><br><span class="line"></span><br><span class="line">/etc/security/limits.conf</span><br><span class="line">* soft nofile 196605</span><br><span class="line">* hard nofile 196605</span><br><span class="line">* soft nproc 196605</span><br><span class="line">* hard nproc 196605</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 生产故障案例 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生产故障案例 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop的Pid文件</title>
      <link href="/2018/10/09/hadoop/6/"/>
      <url>/2018/10/09/hadoop/6/</url>
      
        <content type="html"><![CDATA[<h3 id="存储位置"><a href="#存储位置" class="headerlink" title="存储位置"></a>存储位置</h3><p>hadoop启动之后，pid文件是存储哪里？<br>我们可以通过查看 hadoop-env.sh文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat etc/hadoop/hadoop-env.sh从下图可以看出hadoop默认的pid文件是存储到/tmp目录的</span><br></pre></td></tr></table></figure><p>从下图可以看出hadoop默认的pid文件是存储到/tmp目录的</p><p>从下图可以看出hadoop默认的pid文件是存储到/tmp目录的</p><p><img src="https://img-blog.csdnimg.cn/20190729214437420.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="hadoop-env.sh文件"></p><p>从下图可以看出，后缀名是.pid的就是hadoop的pid文件</p><p><img src="https://img-blog.csdnimg.cn/20190729214915879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="/tmp目录"></p><h3 id="启动和停止"><a href="#启动和停止" class="headerlink" title="启动和停止"></a>启动和停止</h3><p>我们启动的时候，是执行sbin/start-df.sh文件，我们看一看这个文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190729215631410.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="start-df.sh文件"></p><p>从上面这个图可以看出，启动namenode节点的时候，调用了hadoop-daemons.sh文件了，我们再看看这个文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat sbin/hadoop-daemons.sh</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190729215911670.png" alt="hadoop-daemons.sh文件"></p><p>从上图可以看出，在最后一行又调用了hadoop-daemon.sh文件，我们在看看这个文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat sbin/hadoop-daemon.sh</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190729220412390.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="hadoop-daemon.sh文件"></p><p><strong>从上面两张图可以得出结论:</strong></p><ol><li>hadoop启动的时候，会生成pid文件，并把进程号写入到pid文件</li><li>hadoop停止的时候，会到pid文件中获取进程号，然后停止进程，最后删除pid文件</li></ol><p><strong>下面我们做一下验证：</strong></p><ol><li><p>看下namenode的进程号是不是和pid文件里的进程号一样</p><p><img src="https://img-blog.csdnimg.cn/20190729221743612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="start-jps命令"></p><p>从上图可以看出，进程号是一样的，说明我们前面的推理是正确的</p></li><li><p>我们把生成号的namenode的pid文件名字改一下，停止的时候脚本会找不到pid文件，也就不会停止namenode进程了</p><p><img src="https://img-blog.csdnimg.cn/20190729222300551.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="stop-jps命令"></p><p>从上图可以看出，我们的两个推理是正确的</p></li><li><p>tmp目录的弊端<br>linux的/tmp目录会自动清理一段时间没有访问的文件，一般都是30天，假如hadoop启动了30天以上，那么pid文件会被删除，再调用停止的时候会停止不了，生产上一般不会放在/tmp目录下，下面我们自己创建个目录存放pid文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">创建文件夹</span></span><br><span class="line">mkdir -p /data/tmp</span><br><span class="line"><span class="meta">#</span><span class="bash">赋予权限</span></span><br><span class="line">chmod 777 -R /data/tmp</span><br><span class="line">然后修改etc/hadoop/hadoop-env.sh文件</span><br></pre></td></tr></table></figure><p>然后修改etc/hadoop/hadoop-env.sh文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vi /etc/hadoop/hadoop-env.sh</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190730105216506.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="修改hadoop-env.sh文件的pid目录"></p><p>然后启动</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p>启动之后，我们查看pid文件</p><p><img src="https://img-blog.csdnimg.cn/20190730105343192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="新的pid目录"></p></li></ol><hr><p>原文链接：<a href="https://blog.csdn.net/u010452388/article/details/97686380" target="_blank" rel="noopener">https://blog.csdn.net/u010452388/article/details/97686380</a></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop2.7.6之前和Hadoop2.8.4之后的副本存放策略</title>
      <link href="/2018/10/08/hadoop/5/"/>
      <url>/2018/10/08/hadoop/5/</url>
      
        <content type="html"><![CDATA[<h3 id="新旧版本的副本存放策略比较"><a href="#新旧版本的副本存放策略比较" class="headerlink" title="新旧版本的副本存放策略比较"></a>新旧版本的副本存放策略比较</h3><p>Hadoop2.7.6及以下版本是按照旧的策略进行副本存放的，官网文档描述如下：</p><p><img src="https://img-blog.csdnimg.cn/20191017161508850.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FjY3B0YW5nZ2FuZw==,size_16,color_FFFFFF,t_70" alt="旧版本的副本存放策略"></p><p>在常见情况下，当复制因子为3时，HDFS的放置策略是将一个副本放置在本地机架中的一个节点上，将另一个副本放置在本地机架中的另一个节点上，最后一个副本放置在不同机架中的另一个节点上。</p><p>Hadoop2.8.4及以上版本是按照新的策略进行副本存放的，官网文档描述如下：  </p><p><img src="https://img-blog.csdnimg.cn/20191017161742230.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FjY3B0YW5nZ2FuZw==,size_16,color_FFFFFF,t_70" alt="新版本的副本存放策略"></p><p>在常见情况下，当复制因子为3时，HDFS的放置策略是：如果写入器在数据节点上，则将一个副本放置在本地计算机上；否则，在随机数据节点上，HDFS将另一个副本放置在不同（远程）机架中的节点上的，最后一个位于同一远程机架中的其他节点上。</p><h3 id="新版本的副本存放策略思想"><a href="#新版本的副本存放策略思想" class="headerlink" title="新版本的副本存放策略思想"></a>新版本的副本存放策略思想</h3><p>最后，再把新版本的副本存放策略的基本思想描述如下：</p><p>第一个副本存放Client所在的节点上（假设Client不在集群的范围内，则第一个副本存储节点是随机选取的。当然系统会不选择那些太满或者太忙的节点）</p><p>第二个副本存放在与第一个节点不同机架中的一个节点上。</p><p>第三个副本和第二个在同一个机架，随机放在不同的节点上。</p><p>如果还有很多其他的副本就随机放在集群中的各个节点上。</p><hr><p>原文链接：<a href="https://blog.csdn.net/accptanggang/article/details/102609318" target="_blank" rel="noopener">https://blog.csdn.net/accptanggang/article/details/102609318</a></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux下MySQL进程死掉的可能解决方案</title>
      <link href="/2018/10/08/mysql/5/"/>
      <url>/2018/10/08/mysql/5/</url>
      
        <content type="html"><![CDATA[<p>linux下mysql进程死掉，且无法启动mysql服务，查看myql日志，发现如下日志：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">2019-10-10 18:11:03 9772 [Note] InnoDB: Initializing buffer pool, size = 128.0M</span><br><span class="line">InnoDB: mmap(136019968 bytes) failed; errno 12</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] InnoDB: Cannot allocate memory for the buffer pool</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Plugin 'InnoDB' init function returned error.</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Unknown/unsupported storage engine: InnoDB</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Aborting</span><br></pre></td></tr></table></figure><p>其中InnoDB: mmap(136019968 bytes) failed; errno 12是关键的错误信息。<br>从网上查资料，有人说修改innodb_buffer_pool_size，经过测试无效。<br>有人说是swap分区为0导致的此错误，使用free -m命令查看系统内存，发现swap确实为0。使用如下命令建立一个临时的swap分区：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">d if=/dev/zero of=/swap bs=1M count=512  //创建一个swap文件，大小为512M</span><br><span class="line">mkswap /swap                              //将swap文件变为swap分区文件</span><br><span class="line">swapon /swap                              //将其映射为swap分区</span><br></pre></td></tr></table></figure><p>此时使用<code>free -m</code>命令即可看到swap分区已存在了，然后启动mysql服务即可。<br>为了保证下次系统启动后，此swap分区被自动加载，需要修改系统的fstab文件，操作如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">vi /etc/fstab</span><br><span class="line">//在其中添加如下一行</span><br><span class="line">/swap swap swap defaults 0 0</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL中的Top N</title>
      <link href="/2018/10/08/mysql/4/"/>
      <url>/2018/10/08/mysql/4/</url>
      
        <content type="html"><![CDATA[<h3 id="切入点"><a href="#切入点" class="headerlink" title="切入点"></a>切入点</h3><p>MySQL没有获取Top N的这种函数，但是在MySQL中求Top N又是必须掌握的点</p><p>比如查询分组后的最大值、最小值所在的整行记录或者分组后的Top N行记录</p><p>下面我们就如何在MySQL中求Top N做出深度的思考和验证</p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>测试表结构如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">&gt;&gt; CREATE TABLE `student` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `name` varchar(20) DEFAULT NULL,</span><br><span class="line">  `course` varchar(20) DEFAULT NULL,</span><br><span class="line">  `score` int(11) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8</span><br></pre></td></tr></table></figure><p> 插入数据：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">&gt;&gt; insert into student(name,course,score)</span><br><span class="line"><span class="keyword">values</span></span><br><span class="line">(<span class="string">'张三'</span>,<span class="string">'语文'</span>,<span class="number">80</span>),</span><br><span class="line">(<span class="string">'李四'</span>,<span class="string">'语文'</span>,<span class="number">90</span>),</span><br><span class="line">(<span class="string">'王五'</span>,<span class="string">'语文'</span>,<span class="number">93</span>),</span><br><span class="line">(<span class="string">'张三'</span>,<span class="string">'数学'</span>,<span class="number">77</span>),</span><br><span class="line">(<span class="string">'李四'</span>,<span class="string">'数学'</span>,<span class="number">68</span>),</span><br><span class="line">(<span class="string">'王五'</span>,<span class="string">'数学'</span>,<span class="number">99</span>),</span><br><span class="line">(<span class="string">'张三'</span>,<span class="string">'英语'</span>,<span class="number">90</span>),</span><br><span class="line">(<span class="string">'李四'</span>,<span class="string">'英语'</span>,<span class="number">50</span>),</span><br><span class="line">(<span class="string">'王五'</span>,<span class="string">'英语'</span>,<span class="number">89</span>);</span><br></pre></td></tr></table></figure><p>查看结果：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">&gt;&gt; select * from student;</span><br><span class="line">+<span class="comment">----+--------+--------+-------+</span></span><br><span class="line">| id | name   | course | score |</span><br><span class="line">+<span class="comment">----+--------+--------+-------+</span></span><br><span class="line">|  1 | 张三   | 语文   |    80 |</span><br><span class="line">|  2 | 李四   | 语文   |    90 |</span><br><span class="line">|  3 | 王五   | 语文   |    93 |</span><br><span class="line">|  4 | 张三   | 数学   |    77 |</span><br><span class="line">|  5 | 李四   | 数学   |    68 |</span><br><span class="line">|  6 | 王五   | 数学   |    99 |</span><br><span class="line">|  7 | 张三   | 英语   |    90 |</span><br><span class="line">|  8 | 李四   | 英语   |    50 |</span><br><span class="line">|  9 | 王五   | 英语   |    89 |</span><br><span class="line">+<span class="comment">----+--------+--------+-------+</span></span><br></pre></td></tr></table></figure><h3 id="TOP-1"><a href="#TOP-1" class="headerlink" title="TOP 1"></a>TOP 1</h3><p>查询每门课程分数最高的学生以及成绩</p><ol><li><p>我们先拆分题目，这是一题查询分组求最大值的题目，拆分后的题目是：查询 每门课程 分数最高 的学生以及成绩</p><p>我们首先按照常规思路来写SQL: </p><p>select 学生姓名，学生分数</p><p>group by 课程</p><p>max(分数) </p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">s.name,s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">s.course;</span><br></pre></td></tr></table></figure><p>得出的查询结果是:</p><table><thead><tr><th>name</th><th>course</th><th>score</th></tr></thead><tbody><tr><td>张三</td><td>数学</td><td>99</td></tr><tr><td>张三</td><td>英语</td><td>90</td></tr><tr><td>张三</td><td>语文</td><td>93</td></tr></tbody></table><h4 id="问题-为什么姓名都是张三？课程和对应的成绩又全是对的？"><a href="#问题-为什么姓名都是张三？课程和对应的成绩又全是对的？" class="headerlink" title="问题: 为什么姓名都是张三？课程和对应的成绩又全是对的？"></a>问题: 为什么姓名都是张三？课程和对应的成绩又全是对的？</h4><p>我预测是因为没有把姓名加入group的分组字段，那么我们把姓名加入group的分组字段后试试看</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">s.name,s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">s.name,s.course;</span><br></pre></td></tr></table></figure><p>得出的查询结果是:</p><table><thead><tr><th>name</th><th>course</th><th>score</th></tr></thead><tbody><tr><td>张三</td><td>数学</td><td>77</td></tr><tr><td>张三</td><td>英语</td><td>90</td></tr><tr><td>张三</td><td>语文</td><td>80</td></tr><tr><td>李四</td><td>数学</td><td>68</td></tr><tr><td>李四</td><td>英语</td><td>50</td></tr><tr><td>李四</td><td>语文</td><td>90</td></tr><tr><td>王五</td><td>数学</td><td>99</td></tr><tr><td>王五</td><td>英语</td><td>89</td></tr><tr><td>王五</td><td>语文</td><td>93</td></tr></tbody></table><p>结果还是不对，这次把所有的字段都查询出来了，字段的排序规则是先按姓名分组，再按课程分组，因为课程是唯一的，所以跟直接查询的结果一样。</p></li><li><p>我们回到上一步，上一步的课程和成绩对应上了，姓名没有对应上，我们干脆就不要姓名和，拿课程和成绩作为一张表再和自己联结一次，以课程和成绩作为过滤字段，说不定就能得到想要的姓名字段。</p><p>思路：</p><ol><li><p>先课程分组求出最高的分数</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">s.course;</span><br></pre></td></tr></table></figure></li><li><p>把前面得出的结果作为表t再自联结一次</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">s.name,s.score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student s</span><br><span class="line"><span class="keyword">join</span> t </span><br><span class="line"><span class="keyword">on</span> s.course=t.course <span class="keyword">and</span> s.score=t.score;</span><br></pre></td></tr></table></figure></li><li><p>把t替换成查询出来的结果</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">s.name,s.course,s.score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student s</span><br><span class="line"><span class="keyword">join</span> (<span class="keyword">select</span> </span><br><span class="line">s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">s.course) t </span><br><span class="line"><span class="keyword">on</span> s.course=t.course <span class="keyword">and</span> s.score=t.score;</span><br></pre></td></tr></table></figure><p>得出的查询结果是:</p><table><thead><tr><th>name</th><th>course</th><th>score</th></tr></thead><tbody><tr><td>王五</td><td>语文</td><td>93</td></tr><tr><td>王五</td><td>数学</td><td>99</td></tr><tr><td>张三</td><td>英语</td><td>90</td></tr></tbody></table><p>和原数据比较，这就是我们要得到的每门课程的top1。</p></li></ol></li></ol><h3 id="TOP-N"><a href="#TOP-N" class="headerlink" title="TOP N"></a>TOP N</h3><p>查询每门课程前两名的学生以及成绩</p><p>首先求Top 1的方法不适用与Top N，然后毫无头绪。。。</p><p>翻看其他人的博客后，发现求Top N的核心是: <code>自联结表的需求字段比较，也就是自己跟自己比较，然后把比较的结果求count()，最后控制过滤的记录数即可</code></p><h4 id="注意-自己和自己比较，可以通过联结和子查询，我们这里使用子查询"><a href="#注意-自己和自己比较，可以通过联结和子查询，我们这里使用子查询" class="headerlink" title="注意:  自己和自己比较，可以通过联结和子查询，我们这里使用子查询"></a>注意:  自己和自己比较，可以通过联结和子查询，我们这里使用子查询</h4><p>思路:</p><ol><li><p>首先是子查询，然后是自己和自己比较，得出一个count()值，最后使用where过滤这个count值</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">a.name,a.course,a.score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">student a</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line"><span class="number">2</span>&gt;(<span class="keyword">select</span> <span class="keyword">count</span>(b.score) <span class="keyword">from</span> student b <span class="keyword">where</span> a.course=b.course <span class="keyword">and</span> a.score&lt;b.score)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> a.course <span class="keyword">desc</span>,a.score <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><p>梳理这段SQL，select字段不难，from字段不难，order by字段不难，难就难在where字段，我们先不看为什么使用2大于这个子查询，先把注意力放在子查询的where字段中的两个表成绩比较，实际上理解了为什么这么比较这题就解出来了。</p></li><li><p>我们画图来解释。。。只是做为示范，所以只取一个课程的成绩比较，其他的一样</p><p><img src="https://yerias.github.io/hadoop_img/20191205012529.jpg" alt="a.score&lt;b.score比较图"></p><p>可以看出，表a中的成绩越大，满足<code>a.score&lt;b.score</code>的次数越少，<code>where条件过滤count()的值越少越满足Top N的条件，可以根据where条件灵活控制过滤的记录数,我们这里是2，即取Top 2的记录。</code></p><h4 id="再提出一个问题，为什么要用a-score-lt-b-score而不是a-score-gt-b-score？"><a href="#再提出一个问题，为什么要用a-score-lt-b-score而不是a-score-gt-b-score？" class="headerlink" title="再提出一个问题，为什么要用a.score&lt;b.score而不是a.score&gt;b.score？"></a>再提出一个问题，为什么要用<code>a.score&lt;b.score</code>而不是<code>a.score&gt;b.score</code>？</h4><p>通过结果可以倒推出来，我们<code>select</code>语句中要的是表a，根据题意表a必定是比较中较大的值。如果使用<code>a.score&gt;b.score</code>，where条件限制的是满足最少的条件，把表a中最大的值给过滤了，那么得出的的count()结果是反的。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataWarehouse </tag>
            
            <tag> MySQL </tag>
            
            <tag> Rank </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS的副本存放策略&amp;HDFS的读写流程&amp;Pid文件详解&amp;HDFS常用命令&amp;HDFS的回收站机制&amp;安全模式详解&amp;单、多节点的磁盘均衡策略</title>
      <link href="/2018/10/07/hadoop/4/"/>
      <url>/2018/10/07/hadoop/4/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理副本放置策略</li><li>整理读写流程</li><li>整理pid文件</li><li>整理hdfs dfs 常用命令</li><li>整理多节点，单节点的磁盘均衡</li><li>整理安全模式</li></ol><h3 id="副本放置策略"><a href="#副本放置策略" class="headerlink" title="副本放置策略"></a>副本放置策略</h3><p>副本存放策略存在新旧两个版本</p><p>具体可参考我的另一个博客:<a href="https://yerias.github.io/2018/10/11/DataWarehouse/hadoop/5/">https://yerias.github.io/2018/10/11/DataWarehouse/hadoop/5/</a></p><h4 id="hadoop2-7-6之前的副本存放策略"><a href="#hadoop2-7-6之前的副本存放策略" class="headerlink" title="hadoop2.7.6之前的副本存放策略"></a>hadoop2.7.6之前的副本存放策略</h4><ul><li>副本一：同机架的不同节点 </li><li>副本二：同机架的另一节点 </li><li>副本三：不同机架的另一节点 </li><li>其他副本：随机挑选</li></ul><h4 id="hadoop2-8-4之后的副本存放策略"><a href="#hadoop2-8-4之后的副本存放策略" class="headerlink" title="hadoop2.8.4之后的副本存放策略"></a>hadoop2.8.4之后的副本存放策略</h4><ul><li>副本一：同Client的节点上 </li><li>副本二：不同机架中的节点上 </li><li>副本三：同第二个副本的机架中的另一个节点上</li><li>其他副本：随机挑选</li></ul><h4 id="副本存放策略优点"><a href="#副本存放策略优点" class="headerlink" title="副本存放策略优点"></a>副本存放策略优点</h4><ul><li>提高系统的可靠性</li><li>提供负载均衡</li><li>提高访问效率</li></ul><h3 id="读写流程"><a href="#读写流程" class="headerlink" title="读写流程"></a>读写流程</h3><h4 id="读取流程-FSData-InputStream"><a href="#读取流程-FSData-InputStream" class="headerlink" title="读取流程(FSData InputStream)"></a>读取流程(FSData InputStream)</h4><p><img src="http://yerias.github.io/hadoop_img/read.jpg" alt="hdfs读取数据流程"></p><ol><li>首先调用FileSystem对象的open()方法，获得一个分布式文件系统(DistributedFileSystem)的实例。</li><li>分布式文件系统(DistributedFileSystem)通过RPC获得文件的第一批块(Block)的位置信息，<a href="">同一个块按照副本数会返回多个位置信息</a>，这些位置信息按照Hadoop拓扑结构排序，距离客户端近的排在前面。</li><li>前两步会返回一个文件系统数据输入流(FSDataInputStream)对象，该对象会被封装为分布式文件系统输入流(DFSInputStream)对象，DFSInputStream可以方便地管理DataNode和NameNode的数据流。客户端调用read方法，DFSInputStream会找出离客户端最近的DataNode并连接。</li><li>数据从DataNode源源不断地流向客户端</li><li>如果第一个块的数据读完了，就会管理指向第一个块的DataNode的连接，接着读取下一个块。这些操作对客户端来说是透明的，从客户端的角度看来只是在读一个持续不断的数据流。</li><li>如果第一批块都读取完了，DFSInputStream就会去NameNode拿下一批块的位置信息，然后继续读，如果所有的块都读完了，这时就会关闭掉所有的流。</li></ol><p><code>注意:</code> 如果在读数据的时候，DFSInputStream和DataNode的通信发生异常，就会尝试连接正在读的块的排序第二近的DataNode，并且会记录哪个DataNode发生错误，剩余的块读的时候就会直接跳过该DataNode。DFSInputStream也会检查块的校验和，如果发现一个坏的块，就会先报告到NameNode，然后DFSIputStream在其它的DataNode上读取该块的数据。</p><h4 id="写入流程-FSData-OutputStream"><a href="#写入流程-FSData-OutputStream" class="headerlink" title="写入流程(FSData OutputStream)"></a>写入流程(FSData OutputStream)</h4><p><img src="http://yerias.github.io/hadoop_img/write.jpg" alt="hdfs写入数据流程"></p><ol><li>客户端在同过调用分布式文件系统(DistributedFileSystem)的create()方法创建新文件</li><li>DistributedFileSystem通过RPC调用NameNode去创建一个没有块关联的新文件，创建前NameNode会做各种校验，比如文件是否存在，客户端有没有权限等。如果通过校验，NameNode就会记录下新文件，否则就会抛出I/O异常。</li><li>前两步结合，会返回文件系统数据输出流(FSDataOutputStream)的对象，与读文件的时候相似，DistributedFileSystem被封装成分布式文件系统的输出流(DFSOutputStream)。DFSOutputStream可以协调NameNode和DataNode的通信。客户端开始写数据到DFSOutputStream，DFSOutputStream会把数据切分成一个个的数据包(packet)，然后排成数据队列(data quenc)</li><li>接下来，数据队列中的数据包首先输出到数据管道(多个datanode节点组成数据管道)中的第一个DataNode(写数据包)，第一个DataNode又把数据包输出到第二个DataNode中，依次类推。</li><li>DFSOutputStream还维护着一个队列叫做确认队列(ack quenc)，这个队列也是由数据包组成，用于等待DataNode收到数据返回确认数据包，当数据管道中的所有DataNode都表示已经收到了确认信息的时候，这时ack quenc才会把对应的数据包移除掉。</li><li>客户端完成写数据后，调用close()方法关闭写入数据流。</li><li>客户端通知NameNode把文件标记为已完成。然后NameNode把文件写成功的结果反馈给客户端。此时就表示客户端已完成整个HDFS的写数据流程。</li></ol><h5 id="如果写数据的过程中某个DataNode发生错误，会采取以下的步骤处理。"><a href="#如果写数据的过程中某个DataNode发生错误，会采取以下的步骤处理。" class="headerlink" title="如果写数据的过程中某个DataNode发生错误，会采取以下的步骤处理。"></a>如果写数据的过程中某个DataNode发生错误，会采取以下的步骤处理。</h5><ol><li>管道关闭</li><li>正常的DataNode上正在写的块会有一个新ID(需要和NameNode通信)，而失败的DataNode上的那个不完整的块会在上报心跳的时候被删除。</li><li>失败的DataNode会被移除出数据管道，块中剩余的数据包继续写入管道中的其他两个DataNode。</li><li>NameNode会标记这个块的副本个数少于指定值，块的副本会稍后在另一个DataNode创建。</li><li>有些时候多个DataNode会失败，只要<code>dfs.replication.min</code>(缺省是1个)属性定义的指定个数的DataNode写入数据成功了，整个写入过程就算成功，缺少的副本会进行异步的恢复。</li></ol><p><code>注意:</code> 只有调用sync()方法，客户端才确保该文件的写操作已经全部完成， 当客户端调用close()方法时，会默认调用sync()方法。</p><h3 id="pid文件"><a href="#pid文件" class="headerlink" title="pid文件"></a>pid文件</h3><p>pid文件具体作用请参考我的另外一个博客:<a href="https://yerias.github.io/2018/10/12/DataWarehouse/hadoop/6/">https://yerias.github.io/2018/10/12/DataWarehouse/hadoop/6/</a></p><p>在这里我们只简单说一下修改pid文件的生成目录的步骤，在修改hadoop文件的时候，hadoop最好是stop状态，否则需要kill进程。</p><ol><li><p>创建/home/hadoop/tmp目录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir /home/hadoop/tmp</span><br></pre></td></tr></table></figure></li><li><p>修改/home/hadoop/tmp的权限</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chmod -R 777 /hadoop/hadoop/tmp</span><br></pre></td></tr></table></figure></li><li><p>修改hadoop-env.sh文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HADOOP_PID_DIR=/home/hadoop/tmp</span><br></pre></td></tr></table></figure></li><li><p>修改yarn-env.sh</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export YARN_PID_DIR=/home/hadoop/tmp</span><br></pre></td></tr></table></figure></li></ol><h3 id="hdfs-dfs-常用命令"><a href="#hdfs-dfs-常用命令" class="headerlink" title="hdfs dfs 常用命令"></a>hdfs dfs 常用命令</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">[-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [&lt;path&gt; ...]]</span><br><span class="line">[-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">[-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span><br></pre></td></tr></table></figure><p>这些命令很点单，和linux的作用一样，这里不做演示。。。</p><h3 id="回收站"><a href="#回收站" class="headerlink" title="回收站"></a>回收站</h3><p>回收站的作用是把hdfs上删除的文件保存一定的时间然后自动删除，Apache是默认关闭的，CDH默认是开启的。</p><p>Apache的参数是由<code>core-default.xml</code>文件控制的<code>fs.trash.interval</code>属性</p><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>fs.trash.interval</td><td>0</td><td>检查点被删除的分钟数。如果为零，则禁用垃圾特性。单位秒</td></tr></tbody></table><p>一般在生产环境下设置保存7天</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun hadoop]# vim core-site.xml </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;10080&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p><code>注意:</code> 切记检查生产环境是否开启回收站,开了回收站，慎用 <code>-skipTrash</code></p><h3 id="安全模式"><a href="#安全模式" class="headerlink" title="安全模式"></a>安全模式</h3><p>安全模式是hadoop的一种保护机制，安全模式下不能进行修改文件的操作，但是可以浏览目录结构、查看文件内容的。</p><p>如果NN的log显示<code>Name node is in safe mode</code> ，正常手动让其离开安全模式，这种操作很少做。</p><p><strong>一般进入safemode情况有:</strong></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1. 启动或者重新启动hdfs时</span><br></pre></td></tr></table></figure><pre><code>2. HDFS维护升级时3. 块文件损坏等。。。</code></pre><p>可以使用<code>fsck</code>检查一下HDFS的健康度，然后进行下一步操作</p><p><code>hdfs fsck / :</code> 用这个命令可以检查整个文件系统的健康状况,但是要注意它不会主动恢复备份缺失的block,这个是由NameNode单独的线程异步处理的</p><p><strong>fsck相关介绍:</strong></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs fsck</span><br><span class="line">　　　　Usage:DFSck &lt;path&gt; [-move | -delete | -openforwrite] [-files [-blocks [-locations | -racks]]]</span><br><span class="line">　　　　&lt;path&gt; 检查这个目录中的文件是否完整</span><br><span class="line">　　　　-move 破损的文件移至/lost+found目录</span><br><span class="line">　　　　-delete 删除破损的文件</span><br><span class="line">　　　　-openforwrite 打印正在打开写操作的文件</span><br><span class="line">　　　　-files 打印正在check的文件名</span><br><span class="line">　　　　-blocks 打印block报告(需要和-files参数一起使用)</span><br><span class="line">　　　　-locations 打印每个block的位置信息(需要和-files参数一起使用)</span><br><span class="line">　　　　-racks 打印位置信息的网络拓扑图(需要和-files参数一起使用)</span><br></pre></td></tr></table></figure><p>一般我们会查看 / 目录下的损坏文件，然后根据损坏文件的路径手动进行<code>hdfs debug</code>修复</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs fsck / -list-corruptfileblocks</span><br></pre></td></tr></table></figure><h3 id="多节点的磁盘均衡"><a href="#多节点的磁盘均衡" class="headerlink" title="多节点的磁盘均衡"></a>多节点的磁盘均衡</h3><p>由于集群中的一些服务器如CPU、磁盘、网络的差异，副本存放并不会一直保持均衡，这就造成某一些服务器的磁盘占用率达到90%，而另外一些服务器的磁盘占用率只有60%或者80%。所以就有必要手动进行均衡操作，事实上hadoop的sbin目录下也有这个命令</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-rwxr-xr-x 1 hadoop hadoop 1128 Jun  3  2019 start-balancer.sh  #开始</span><br><span class="line">-rwxr-xr-x 1 hadoop hadoop 1179 Jun  3  2019 stop-balancer.sh#停止</span><br></pre></td></tr></table></figure><p>那么集群中的磁盘占用率怎么才算正常？这个由参数<code>threshold</code>控制，默认threshold=10，即各个服务器保持所有服务器的磁盘占用空间的平均值上下浮动10%，可能不好理解，我们用上面的占用率90%、60%和80%算一下。</p><p>这三台的平均占用率是:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(90+60+80)/3=76%</span><br></pre></td></tr></table></figure><p>那么<code>threshold</code>参数就控制这三台其中的任意一台的磁盘占用率不得超过86%，不得低于66%。</p><h4 id="那么怎么做呢？"><a href="#那么怎么做呢？" class="headerlink" title="那么怎么做呢？"></a>那么怎么做呢？</h4><p>在进行磁盘均衡之前，我们需要重新设置一下balancer的带宽限制，在<code>hdfs-default.xml</code>文件中的<code>dfs.datanode.balance.bandwidthPerSec</code>属性，默认是10M，生产环境下一般设置为30M</p><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>dfs.datanode.balance.bandwidthPerSec</td><td>10m</td><td>指定每个datanode可用于平衡目的的最大带宽(以每秒字节数为单位)。</td></tr></tbody></table><p>在<code>hadoop</code>的<code>hdfs-site.xml</code>文件中覆盖一下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun hadoop]# vim hdfs-site.xml </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.balance.bandwidthPerSec&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;30m&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>怎么做？</p><p>写个shell脚本，每天凌晨执行<code>./start-balancer.sh</code>调度一次，达到数据平衡，毛刺修正，调度执行完成自动关闭，不需要执行<code>./stop-balancer.sh</code>手段关闭，除非特殊情况。</p><h3 id="单节点的磁盘均衡"><a href="#单节点的磁盘均衡" class="headerlink" title="单节点的磁盘均衡"></a>单节点的磁盘均衡</h3><p>在官网中的描述: </p><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html" target="_blank" rel="noopener">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html</a></p><p>在官网中的描述中有这么一句: <code>dfs.disk.balancer.enabled must be set to true in hdfs-site.xml.</code></p><p>翻译过来就是: 必须在<code>hdfs-site.xml</code>中将<code>dfs.disk.balancer.enabled</code>设置为<code>true</code>。</p><p>这是因为默认情况下，群集上未启用磁盘平衡器</p><p>那么我们先去设置一下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun hadoop]# vim hdfs-site.xml </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.disk.balancer.enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h4 id="场景"><a href="#场景" class="headerlink" title="场景:"></a>场景:</h4><p>假如我们现在有三个数据盘</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/data01   90%</span><br><span class="line">/data02   60%</span><br><span class="line">/data03   80%</span><br></pre></td></tr></table></figure><p>现在磁盘用的差不多了，准备加入一个盘<code>/data04   0%</code></p><p>我们这时候是不是要进行单节点服务器的磁盘均衡？</p><h4 id="怎么做？"><a href="#怎么做？" class="headerlink" title="怎么做？"></a>怎么做？</h4><ol><li><p>生成hadoop001.plan.json</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -plan hadoop001#hadoop001是datanode的主机名</span><br></pre></td></tr></table></figure></li><li><p>执行</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -execute hadoop001.plan.json #hadoop001是datanode的主机名</span><br></pre></td></tr></table></figure></li><li><p>查询状态</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -query ruozedata001 #hadoop001是datanode的主机名</span><br></pre></td></tr></table></figure></li></ol><h4 id="什么时候手动或调度执行？"><a href="#什么时候手动或调度执行？" class="headerlink" title="什么时候手动或调度执行？"></a>什么时候手动或调度执行？</h4><ol><li>新盘加入</li><li>监控服务器的磁盘剩余空间小于阈值10%，发邮件预警 ，手动执行</li></ol><h4 id="怎么在DataNode中挂载磁盘？"><a href="#怎么在DataNode中挂载磁盘？" class="headerlink" title="怎么在DataNode中挂载磁盘？"></a>怎么在DataNode中挂载磁盘？</h4><p>由<code>hdfs-default.xml</code>文件的<code>dfs.datanode.data.dir</code>属性控制</p><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>dfs.datanode.data.dir</td><td>file://${hadoop.tmp.dir}/dfs/data</td><td>确定DFS数据节点应该将其块存储在本地文件系统的何处。多个目录使用逗号分隔。</td></tr></tbody></table><p>假如我们现在有/data01,/data02,/data03,/data04四个目录需要挂载在该DataNode节点中</p><p>修改<code>hdfs-site.xml</code>文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun hadoop]# vim hdfs-site.xml </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/data01,/data02,/data03,/data04&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h5 id="为什么DN的生产上要挂载多个物理的磁盘目录，而不是做一个raid-磁盘阵列"><a href="#为什么DN的生产上要挂载多个物理的磁盘目录，而不是做一个raid-磁盘阵列" class="headerlink" title="为什么DN的生产上要挂载多个物理的磁盘目录，而不是做一个raid(磁盘阵列)"></a>为什么DN的生产上要挂载多个物理的磁盘目录，而不是做一个raid(磁盘阵列)</h5><p>为了高效率写  高效率读</p><p><code>注意:</code> 提前规划好2-3年存储量 ，避免后期加磁盘维护的工作量</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据重刷机制(抛砖引玉)</title>
      <link href="/2018/10/07/mysql/%E6%95%B0%E6%8D%AE%E9%87%8D%E5%88%B7%E6%9C%BA%E5%88%B6/"/>
      <url>/2018/10/07/mysql/%E6%95%B0%E6%8D%AE%E9%87%8D%E5%88%B7%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<h4 id="先抛出几个问题"><a href="#先抛出几个问题" class="headerlink" title="先抛出几个问题"></a>先抛出几个问题</h4><ol><li><p>存储是不是基石？</p></li><li><p>假如存储不挂，数据真的准确吗？</p></li><li><p>存储挂了，数据还准确吗？</p></li><li><p>如何校验是否正确？如何让其正确？机制是不是必须有？</p></li></ol><p>注：<code>sqoop</code>抽数据，无<code>error</code>丢数据的概率很小</p><p>数据质量校验：数据量校验 <code>count</code>相同吗？<code>count</code>相同内容相同吗？</p><p>数据量相同–&gt;数据量不同 重刷机制 补or删 <code>spark</code> 95%–&gt;数据内容不同？ 抽样 5%</p><h4 id="现在重点理解一下重刷机制"><a href="#现在重点理解一下重刷机制" class="headerlink" title="现在重点理解一下重刷机制"></a>现在重点理解一下重刷机制</h4><p>背景：用<code>count</code>校验上下游的数据不准确</p><p>引入重刷机制：通过对上下游的两个表求<code>full outer join</code>来对比字段的<code>null</code>值</p><p>上游表a</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| id   | data  | age  |</span><br><span class="line">| <span class="comment">---- | ----- | ---- |</span></span><br><span class="line">| 1    | data1 | 18   |</span><br><span class="line">| 2    | data2 | 19   |</span><br><span class="line">| 3    | data3 | 20   |</span><br><span class="line">| 7    | data7 | 22   |</span><br></pre></td></tr></table></figure><p>下游表b</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| id   | data  | age  |</span><br><span class="line">| <span class="comment">---- | ----- | ---- |</span></span><br><span class="line">| 1    | data1 | 18   |</span><br><span class="line">| 2    | data2 | 19   |</span><br><span class="line">| 3    | data3 | 20   |</span><br><span class="line">| 7    | data7 | 22   |</span><br></pre></td></tr></table></figure><p>我们发现表 a 和表 b 对比 表 a 少了 5 、6 多了 7 ，表 b 少了 2 、 7 多了 6，我们现在对两个表做 <code>full outer join</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| aid  | data   | age  | bid  |</span><br><span class="line">| <span class="comment">---- | ------ | ---- | ---- |</span></span><br><span class="line">| 1    | ruoze1 | 18   | 1    |</span><br><span class="line">| 2    | ruoze2 | 19   | null |</span><br><span class="line">| 3    | ruoze3 | 20   | 3    |</span><br><span class="line">| 7    | ruoze7 | 22   | null |</span><br><span class="line">| null | null   | null | 5    |</span><br><span class="line">| null | null   | null | 6    |</span><br></pre></td></tr></table></figure><p>以表 a 为标准，对生成后的大表做筛选，分别查找 <code>aid</code> 和 <code>bid</code> 为 <code>null</code> 的记录</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">from</span> t <span class="keyword">where</span> aid=<span class="literal">null</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">from</span> t <span class="keyword">where</span> bid=<span class="literal">null</span></span><br></pre></td></tr></table></figure><p>发现 <code>bid</code>为 5 、 6 的行 <code>aid</code> 为 <code>null</code>，说明 <code>bid</code> 下游数据多了，根据 <code>bid</code> 重新构建</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> b <span class="keyword">where</span> bid=<span class="number">5</span>     </span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> b <span class="keyword">where</span> bid=<span class="number">6</span></span><br></pre></td></tr></table></figure><p>发现 <code>aid</code> 为 2 、 7 的 <code>bid</code> 为<code>null</code>，说明 <code>bid</code> 下游数据少了，根据 <code>aid</code> 重新构建</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="number">2</span> ruoze2 <span class="number">19</span> </span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="number">7</span> ruoze7 <span class="number">22</span></span><br></pre></td></tr></table></figure><p>经过重新构建也就是重刷后的数据是</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| aid  | data   | age  | bid  |</span><br><span class="line">| <span class="comment">---- | ------ | ---- | ---- |</span></span><br><span class="line">| 1    | ruoze1 | 18   | 1    |</span><br><span class="line">| 2    | ruoze2 | 19   | 2    |</span><br><span class="line">| 3    | ruoze3 | 20   | 3    |</span><br><span class="line">| 7    | ruoze7 | 22   | 7    |</span><br></pre></td></tr></table></figure><h4 id="深度思考："><a href="#深度思考：" class="headerlink" title="深度思考："></a>深度思考：</h4><p><code>full outer join</code> 其实就是先 <code>left join</code> 和后 <code>right join</code> 的两个结果，为 <code>null</code> 的刚好是缺少的或者多的，而交集是上下游都有的数据，需要做的是 <code>left join</code> 为 <code>null</code> 做 <code>insert</code> 或者 <code>delete</code>，还是 <code>right join</code> 为 null 做 <code>insert</code> 或者 <code>delete</code>。</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataWarehouse </tag>
            
            <tag> Data </tag>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>理解数据块、副本数、小文件的概念&amp;掌握HDFS架构&amp;掌握NN和SNN交互流程</title>
      <link href="/2018/10/06/hadoop/3/"/>
      <url>/2018/10/06/hadoop/3/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理对 块大小 副本数的理解</li><li>整理对小文件的理解</li><li>整理HDFS架构</li><li>整理SNN流程</li></ol><h3 id="修改hdfs的数据保存文件"><a href="#修改hdfs的数据保存文件" class="headerlink" title="修改hdfs的数据保存文件"></a>修改hdfs的数据保存文件</h3><p>在开始完成今天的目标之前，我们还要做一个事情，那就是修改hdfs的nn、nd、snn文件保存的目录，这个目录默认保存在/tmp目录下，那么为什么会保存在/tmp目录下呢，实际上是由<code>core-default.xml</code>默认参数决定的</p><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>hadoop.tmp.dir</td><td>/tmp/hadoop-${user.name}</td><td>A base for other temporary directories.</td></tr></tbody></table><p>因为/tmp目录具有固定周期清除文件的特性，所以我们这里需要改变hadoop的存储文件路径，防止丢失文件</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ vim core-site.xml </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在这之后我们还要对/home/hadoop/tmp目录做一下调整</p><ul><li><p><code>chmod -R 777 /home/hadoop/tmp</code></p></li><li><p><code>mv /tmp/hadoop-hadoop/dfs /home/hadoop/tmp/</code></p></li><li><p><code>test</code> </p></li></ul><h3 id="对块大小和副本数的理解"><a href="#对块大小和副本数的理解" class="headerlink" title="对块大小和副本数的理解"></a>对块大小和副本数的理解</h3><h4 id="块的理解"><a href="#块的理解" class="headerlink" title="块的理解"></a>块的理解</h4><p>hadoop1.x的块大小是64M，hadoop2.x的块大小是128M，块的大小是<code>hdfs-default.xml</code>文件中的<code>dfs.blocksize</code>属性控制，它是hdfs存储处理数据的最小单元，可以根据实际需求改变块大小，但是一般不建议这么做。</p><table><thead><tr><th>KEY</th><th>VALUE</th></tr></thead><tbody><tr><td>dfs.blocksize</td><td>134217728(128M)</td></tr></tbody></table><p>hadoop1.x的块大小是64M，hadoop2.x的块大小是128M，块的大小是<code>hdfs-default.xml</code>文件中的<code>dfs.blocksize</code>属性控制，它是hdfs存储处理数据的最小单元，可以根据实际需求改变块大小，但是一般不建议这么做。</p><p><code>块大小为什么要设计成128M？</code></p><p>是为了最小化寻址时间，目前磁盘的传输速率普遍是在100M/S左右，所以设计成128M每块。</p><h4 id="副本数的理解"><a href="#副本数的理解" class="headerlink" title="副本数的理解"></a>副本数的理解</h4><p>副本的设置让hadoop具有高可靠性的特点，数据不会轻易丢失。副本是存储在dn中的，由<code>hdfs-default.xml</code>文件的<code>dfs.replication</code>参数控制，伪分布式部署是1份，集群部署是3份，不建议修改。</p><table><thead><tr><th>KEY</th><th>VALUE</th></tr></thead><tbody><tr><td>dfs.replication</td><td>3</td></tr></tbody></table><h3 id="对小文件的理解"><a href="#对小文件的理解" class="headerlink" title="对小文件的理解"></a>对小文件的理解</h3><p>一般来说，小文件是文件大小小于10M的数据，由于hadoop的架构特性，它只能有一台主nn，如果小文件特别多的话，小文件的块也特别多，nn需要维护的块的元数据信息的条数也多，所以我们一般把小文件合并成大文件再放到hdfs上，也有上传hdfs后合并，这样来减少nn维护的块的元数据数量。具体合并的方式，以后再讲。</p><h3 id="整理HDFS架构"><a href="#整理HDFS架构" class="headerlink" title="整理HDFS架构"></a>整理HDFS架构</h3><p>HDFS由NameNode、SecondaryNameNode、DataNode三个组件组成</p><h4 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h4><p>NameNode也被称为名称节点或元数据节点，是HDFS主从架构中的主节点，相当于HDFS的大脑，它管理文件系统的命名空间，维护着整个文件系统的目录树以及目录树中所有子目录和文件。</p><p>这些信息以两个文件的形式持久化保存在本地磁盘上，一个是命令空间镜像FSImage(File System Image)，主要是用来存储HDFS的元数据信息。还有一个是命令空间镜像的编辑日志(Editlog)，该文件保存用户对命令空间镜像的修改信息。</p><h4 id="SecondeayNameNode"><a href="#SecondeayNameNode" class="headerlink" title="SecondeayNameNode"></a>SecondeayNameNode</h4><p>SecondaryNameNode也被称为元数据节点，是HDFS主从架构中的备用节点，主要用于定期合并命名空间镜像(FSImage)和命令空间镜像的操作日志(Editlog)，是一个辅助NameNode的守护进程。</p><p>定期合并FSImage和Editlog的周期时间是由<code>hdfs-default.xml</code>文件的<code>dfs.namenode.checkpoint.period</code>属性决定的，默认一小时合并一次，同时如果Editlog操作日志记录满 1000000条也会触发合并机制，由<code>dfs.namenode.checkpoint.txns</code>属性控制，两者满足一个即可。</p><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>dfs.namenode.checkpoint.period</td><td>3600</td><td>两个周期性检查点之间的秒数。</td></tr><tr><td>dfs.namenode.checkpoint.txns</td><td>1000000</td><td>两个周期性检查点之间的名称空间记录数。</td></tr></tbody></table><p>虽然SecondaryNameNode能够减轻单点故障，但是还会有风险，因为总有一段时间的数据是没有同步的。</p><h5 id="问题-为什么SecondaryNameNode要辅助NameNode定期合并FSImage文件和Editlog文件？"><a href="#问题-为什么SecondaryNameNode要辅助NameNode定期合并FSImage文件和Editlog文件？" class="headerlink" title="问题: 为什么SecondaryNameNode要辅助NameNode定期合并FSImage文件和Editlog文件？"></a>问题: 为什么SecondaryNameNode要辅助NameNode定期合并FSImage文件和Editlog文件？</h5><p>FSImage文件实际上是HDFS文件系统中元数据的一个永久性检查点(checkpoint)，但也并不是每一个写操作都会更新到这个文件中，因为FSImage是一个大型文件，如果频繁地执行写操作，会导致系统运行极其缓慢，那么如何解决呢？</p><p>解决方案就是NameNode将命令空间的改动信息写入命令空间的Editlog，但随着时间的推移，Editlog文件会越来越大，一旦发生故障，那么将需要花费很长的时间进行回滚操作，所以可以像传统的关系型数据库一样，定期地合并FSImage和Editlog，但是如果由NameNode来做合并操作，由于NameNode在为集群提供服务的同时可能无法提供足够的资源，所以为了解决这一问题，SecondaryNameNode就应运而生了。</p><h4 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h4><p>DataNode也被称为数据节点，它是HDFS主从架构在的从节点，它存储数据块和数据块校验和它在NameNode的指导下完成数据的IO操作。</p><p><img src="https://yerias.github.io/hadoop_img/image-20191202110000146.png" alt="数据块和数据库校验和"></p><p>DataNode会不断地向NameNode发送心跳和块报告信息，并执行来自NameNode的指令。</p><p>发送心跳是为了告诉nn我还活着，通过<code>hdfs-default.xml</code>文件的<code>dfs.heartbeat.interval</code>参数可以得知，每3秒发送一次</p><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>dfs.heartbeat.interval</td><td>3</td><td>Determines datanode heartbeat interval in seconds.</td></tr></tbody></table><p>发送块报告信息是为了扫描数据目录并协调内存块和磁盘块之间的差异的，从<code>hdfs-default.xml</code>文件的<code>dfs.datanode.directoryscan.interval</code>属性和<code>dfs.blockreport.intervalMsec</code>可以得知每6小时发送一次块报告，生产环境下建议缩短周期（3小时）</p><table><thead><tr><th>KEY</th><th>VALUE</th><th>DESC</th></tr></thead><tbody><tr><td>dfs.datanode.directoryscan.interval</td><td>21600</td><td>DataNode扫描数据目录并协调内存块和磁盘块之间的差异的时间间隔(以秒为单位)，发现损坏块</td></tr><tr><td>dfs.blockreport.intervalMsec</td><td>21600000</td><td>确定以毫秒为单位的块报告间隔，恢复数据块</td></tr></tbody></table><p>在这里我们需要知道一个hadoop命令，该命令仅适用于高级用户，不正确的使用可能会导致数据丢失。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun subdir0]$ hdfs debug</span><br><span class="line">Usage: hdfs debug &lt;command&gt; [arguments]</span><br><span class="line"></span><br><span class="line">These commands are for advanced users only.</span><br><span class="line"></span><br><span class="line">Incorrect usages may result in data loss. Use at your own risk.</span><br><span class="line"></span><br><span class="line">verifyMeta -meta &lt;metadata-file&gt; [-block &lt;block-file&gt;]</span><br><span class="line">computeMeta -block &lt;block-file&gt; -out &lt;output-metadata-file&gt;</span><br><span class="line">recoverLease -path &lt;path&gt; [-retries &lt;num-retries&gt;]</span><br><span class="line">[hadoop@aliyun subdir0]$</span><br></pre></td></tr></table></figure><h5 id="手动修复"><a href="#手动修复" class="headerlink" title="手动修复"></a>手动修复</h5><p><code>hdfs debug</code>的作用是在多副本的环境下手动修复元数据、块或者副本，我们在这里只说修改副本，这里的xxx是指副本路径，该路径必须驻留在HDFS文件系统上，由<code>hdfs fsck</code>命令查找。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">[hadoop@aluyuncurrent]$</span><span class="bash"> hdfs debug recoverLease -path xxx -retries 10</span></span><br></pre></td></tr></table></figure><h5 id="自动修复"><a href="#自动修复" class="headerlink" title="自动修复"></a><code>自动修复</code></h5><p><a href="https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/" target="_blank" rel="noopener">https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/</a></p><p>但是有可能: 手动修复 + 自动修复都是失败的 </p><p>这就需要保证数据仓库的数据质量和数据重刷机制恢复 </p><h5 id="问题-DataNode是如何存储和管理数据块的？"><a href="#问题-DataNode是如何存储和管理数据块的？" class="headerlink" title="问题: DataNode是如何存储和管理数据块的？"></a>问题: DataNode是如何存储和管理数据块的？</h5><ol><li>DataNode节点是以数据块的形式在本地Linux文件系统上保存HDFS文件的内容，并对外提供文件数据访问功能。</li><li>DataNode节点的一个基本功能就是管理这些保存在Linux文件系统上的数据</li><li>DataNode节点是将数据块以Linux文件的形式保存在本地的存储系统上</li></ol><h3 id="SecondaryNameNode和NameNode的交互流程"><a href="#SecondaryNameNode和NameNode的交互流程" class="headerlink" title="SecondaryNameNode和NameNode的交互流程"></a>SecondaryNameNode和NameNode的交互流程</h3><p><img src="https://yerias.github.io/hadoop_img/IMG_0614(20191202-125538).JPG" alt="SecondaryNameNode和NameNode的交互流程"></p><ol><li><code>SecondaryNameNode</code>引导<code>NameNode</code>滚动更新操作日志，并开始将新的操作日志写进<code>edits.new</code>。</li><li><code>SecondaryNameNode</code>将<code>NameNode</code>的<code>FSImage</code>文件和<code>Edits</code>文件复制到本地的检查点目录。</li><li><code>SecondaryNameNode</code>将<code>FSImage</code>文件导入内存，回放编辑日志<code>Edits</code>文件，将其合并到<code>FSImage.ckpt</code>文件，并将新的<code>FSImage.ckpt</code>文件压缩后写入磁盘。</li><li><code>SecondaryNameNode</code>将新的<code>FSImage.ckpt</code>文件传回<code>NameNode</code>。</li><li><code>NameNode</code>在接收新的<code>FSImage.ckpt</code>文件后，将<code>FSImage.ckpt</code>替换为<code>FSImage</code>，然后直接加载和启用该文件</li><li><code>NameNode</code>将<code>Edits.new</code>更名为<code>Edits</code>。默认情况下，该过程1小时内发生1次，或者当编辑日志达到默认值1000000条也会触发。</li></ol><h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><ol><li><p>NN的fsimage的个数默认是保留2个</p><p><img src="https://yerias.github.io/hadoop_img/57Y5RSNU_6%5DY886.jpg" alt="fsimage文件"></p><p>控制的参数是<code>hdfs-default.xml</code>文件的<code>dfs.namenode.num.checkpoints.retained</code>参数</p><table><thead><tr><th>KEY</th><th>VALUE</th></tr></thead><tbody><tr><td>dfs.namenode.num.checkpoints.retained</td><td>2</td></tr></tbody></table></li><li><p>NN的editlog文件不会保留所有的，至于保留的个数还是周期，解决中。。。</p><p><img src="https://yerias.github.io/hadoop_img/C(J%60DYZC@_%7BEA%5B3(GMDRK%7DI.jpg" alt="editlog文件"></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> HDFS </tag>
            
            <tag> DataWarehouse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>yarn的伪分布式部署&amp;jps的原理&amp;oom-killer&amp;/tmp目录的clean机制</title>
      <link href="/2018/10/05/hadoop/2/"/>
      <url>/2018/10/05/hadoop/2/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>搭建 yarn伪分布式</li><li>跑mr count案例</li><li>整理 jps命令</li><li>Linux两个机制 oom  clean</li></ol><h3 id="Yarn伪分布式部署-amp-单点-amp-主从架构"><a href="#Yarn伪分布式部署-amp-单点-amp-主从架构" class="headerlink" title="Yarn伪分布式部署&amp;单点&amp;主从架构"></a>Yarn伪分布式部署&amp;单点&amp;主从架构</h3><p>由于hadoop集成了MapReduce和Yarn，所以这里我们只要修改相关的配置文件即可使用</p><p>我们需要配置<code>mapred-site.xml</code>和<code>yarn-site.xml</code>这两个文件，由于<code>mapred-site.xml</code>不存在所以需要复制一个出来</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ cp mapred-site.xml.template mapred-site.xml</span><br></pre></td></tr></table></figure><p>然后用<code>vim</code>打开<code>mapred-site.xml</code>并修改</p><p>vim mapred-site.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>然后用<code>vim</code>打开<code>yarn-site.xml</code>并修改文件</p><p>vim yarn-site.xml</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>aliyun:38088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="comment">&lt;!-- 日志聚集功能使能 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 日志保留时间设置7天 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>注意:</code> 这是使用38088替代默认的8088是为了避免云主机被挖矿</p><p>最后启动<code>Yarn</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ start-yarn.sh</span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/yarn-hadoop-resourcemanager-aliyun.out</span><br><span class="line">aliyun: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/yarn-hadoop-nodemanager-aliyun.out</span><br><span class="line"></span><br><span class="line">[hadoop@aliyun hadoop]$ jps</span><br><span class="line">32501 NodeManager</span><br><span class="line">349 Jps</span><br><span class="line">32415 ResourceManager</span><br></pre></td></tr></table></figure><p>最后再用<code>ps -ef</code>命令验证<code>ResourceManager</code>程序是否真的启动完成</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ netstat -nlp|grep 32415</span><br><span class="line">(Not all processes could be identified, non-owned process info</span><br><span class="line"> will not be shown, you would have to be root to see it all.)</span><br><span class="line">tcp        0      0 172.16.39.48:38088      0.0.0.0:*               LISTEN      32415/java          </span><br><span class="line">tcp        0      0 0.0.0.0:8030            0.0.0.0:*               LISTEN      32415/java          </span><br><span class="line">tcp        0      0 0.0.0.0:8031            0.0.0.0:*               LISTEN      32415/java          </span><br><span class="line">tcp        0      0 0.0.0.0:8032            0.0.0.0:*               LISTEN      32415/java          </span><br><span class="line">tcp        0      0 0.0.0.0:8033            0.0.0.0:*               LISTEN      32415/java</span><br></pre></td></tr></table></figure><p>启动完成，可以去玩<code>wordcount</code>案例了</p><h3 id="使用MapReduce运行WordCount案例"><a href="#使用MapReduce运行WordCount案例" class="headerlink" title="使用MapReduce运行WordCount案例"></a>使用MapReduce运行WordCount案例</h3><p>在data目录下创建mapreduce的输入文件</p><p>[hadoop@aliyun data]$ vim name.log</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ruozedata</span><br><span class="line">ruoze</span><br><span class="line">jepson</span><br><span class="line">huhu</span><br><span class="line">ye</span><br><span class="line">tunan</span><br><span class="line">afei</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">3</span><br><span class="line">2</span><br><span class="line">1</span><br><span class="line">a b c a a aaaa bcd</span><br><span class="line">ruoze jepson</span><br></pre></td></tr></table></figure><p>把name.log文件put到hdfs上</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun data]$ hdfs dfs -mkdir -p /wordcount/input</span><br><span class="line">[hadoop@aliyun data]$ hdfs dfs -put name.log /wordcount/input</span><br><span class="line">[hadoop@aliyun data]$ hdfs dfs -ls /wordcount/input/</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         86 2019-12-01 17:46 /wordcount/input/name.log</span><br></pre></td></tr></table></figure><p>使用hadoop jar 运行案例</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar wordcount /wordcount/input /wordcount/output</span><br></pre></td></tr></table></figure><p>查看结果</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ hdfs dfs -cat /wordcount/output/*</span><br><span class="line">12</span><br><span class="line">22</span><br><span class="line">32</span><br><span class="line">a3</span><br><span class="line">aaaa1</span><br><span class="line">afei1</span><br><span class="line">b1</span><br><span class="line">bcd1</span><br><span class="line">c1</span><br><span class="line">huhu1</span><br><span class="line">jepson2</span><br><span class="line">ruoze2</span><br><span class="line">ruozedata1</span><br><span class="line">tunan1</span><br><span class="line">ye1</span><br></pre></td></tr></table></figure><h3 id="jps命令不为人知的地方"><a href="#jps命令不为人知的地方" class="headerlink" title="jps命令不为人知的地方"></a>jps命令不为人知的地方</h3><p>我们想既然jps可以直接运行，肯定在APTH路径下，我们何不which一下看看</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ which jps</span><br><span class="line">/usr/java/jdk/bin/jps</span><br></pre></td></tr></table></figure><p>原来jps是一个java命令，我们使用一下jps看看它有什么作用</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ jps</span><br><span class="line">883 DataNode</span><br><span class="line">2117 Jps</span><br><span class="line">32501 NodeManager</span><br><span class="line">1082 SecondaryNameNode</span><br><span class="line">781 NameNode</span><br><span class="line">32415 ResourceManager</span><br></pre></td></tr></table></figure><p>jps命令打印出来了hadoop组件里面的程序pid和程序名，引申出这些程序都属于java程序，结合jps属于java命令，得出jps打印出来所有运行的java程序</p><p>我们可以从启动程序的脚本中得到pid和程序名存放的文件，这里就不去debug，他们默认存放在/tmp目录下一个叫做hsperfdata_用户名的文件下</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun tmp]$ ll hsperfdata_hadoop/</span><br><span class="line">total 160</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:08 1082</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:08 32415</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:08 32501</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:08 781</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:08 883</span><br></pre></td></tr></table></figure><p>每个pid对应是一些二进制文件，没啥好看的，我们发现这个文件下还有一些以pid结尾的文件，文件保存的其实也是各自对应的pid号码，但是这里的.pid文件是进程自己创建的。用来管理和结束进程的。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun tmp]$ ll grep *.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 4 Dec  1 17:39 hadoop-hadoop-datanode.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 4 Dec  1 17:39 hadoop-hadoop-namenode.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 5 Dec  1 17:39 hadoop-hadoop-secondarynamenode.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 6 Dec  1 17:37 yarn-hadoop-nodemanager.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 6 Dec  1 17:37 yarn-hadoop-resourcemanager.pid</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun tmp]$ cat hadoop-hadoop-datanode.pid </span><br><span class="line">883</span><br></pre></td></tr></table></figure><p>所以jps命令的作用就明白了，在使用jsp命令时，会去/tmp目录下/hsperfdata_username目录下找到程序的pid和程序名并打印出来，/tmp/hsperfdata_username目录会存放该用户所有已经启动的java进程信息。</p><p>这里需要get到一个很关键的知识点，它可能让你在shell脚本中犯错，那就是进程所属的用户去执行 jps命令，只显示自己的相关的进程信息，也就是说，其他用户使用jps命令查看不到本用户启动的程序，root用户可以看所有的，但是显示不可用，我们这里就用root用户尝试一下</p><p>我在这里翻车了，使用root显示出来了所有的进程，原因不明，但是其他普通用户无法显示</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">aliyun:mysqladmin:/usr/local/mysql:&gt;jps</span><br><span class="line">2346 Jps</span><br></pre></td></tr></table></figure><p>这同样能得出我们想要的结果，那就是其他用户使用jps不能查看到本用户启动的java程序。其原理我们看看这个文件夹的权限就知道了</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun tmp]$ ll hsperfdata_hadoop/</span><br><span class="line">total 128</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:26 1082</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:26 32415</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:26 32501</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:26 781</span><br></pre></td></tr></table></figure><p>最后总结一下我们在判断一个程序是否在运行时，切不可使用jps命令，因为jps命令只能查看本用户的java进程，那我们在shell脚本中应该如何判断一个程序是否存在？使用万能的ps -ef。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun tmp]$ ps -ef|grep 781 | grep -v grep | wc -l</span><br><span class="line">1</span><br></pre></td></tr></table></figure><h3 id="Linux的oom-kill和clean机制"><a href="#Linux的oom-kill和clean机制" class="headerlink" title="Linux的oom-kill和clean机制"></a>Linux的oom-kill和clean机制</h3><p>在最后我们说一下在使用hadoop时会遇到的两个坑，而且还都是Linux自带的特性，以及解决的方法。</p><p><code>oom-killer机制:</code> 大数据程序是非常吃内存的，而在Linux内核检测到系统内存不足后，会触发oom-killer，挑选占用最大内存的进程杀掉。</p><p>如果我们的进程突然断了，首先查看日志找<code>EROOR</code>，有<code>ERROR</code>具体分析，没有<code>ERROR</code>但是<code>INFO</code>信息断了，很可能就是触发了<code>oom-killer机制</code>，使用<code>free -h</code>命令查看内存使用情况，再使用<code>cat /var/log/messages | grep oom</code>命令查看有没有类似于<code>Killed process</code> 的命令，如果有，就是触发了<code>oom-killer机制</code></p><p><code>clean机制:</code> Linux的/tmp目录是一个临时目录，它有一个机制，默认清理超过30天的内容，而前面使用<code>jps</code>命令的时候就发现，<code>hadoop</code>的进程<code>pid</code>都存放在<code>/tmp</code>目录中，启动进程的时候去<code>/tmp</code>目录下创建对应的<code>pid</code>文件，结束进程的时候去<code>/tmp</code>目录下找到程序对应的<code>pid</code>用来结束进程并删除<code>pid</code>文件，那么引申出来一个问题，如果我们的<code>hadoop</code>组件进程启动时间超过了30天了呢，<code>pid</code>文件被清理，结束命令找不到<code>pid</code>号，会再重新创建一个<code>pid</code>，结果就是<code>pid</code>号紊乱，进程无法正常结束。</p><p><code>解决的办法</code>就是在家目录下面创建一个tmp目录，然后把hdfs和yarn的pid号管理文件夹设置成家目录下的tmp目录即可。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ vi hadoop-env.sh</span><br><span class="line">export HADOOP_PID_DIR=/home/hadoop/tmp</span><br><span class="line"></span><br><span class="line">[hadoop@aliyun hadoop]$ cat yarn-env.sh</span><br><span class="line">export YARN_PID_DIR=/home/hadoop/tmp</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> Yarn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> MapReduce </tag>
            
            <tag> Yarn </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS的伪分布式部署&amp;HADOOP的常用命令</title>
      <link href="/2018/10/04/hadoop/1/"/>
      <url>/2018/10/04/hadoop/1/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>安装hadoop的hdfs伪分布式部署</li><li>hadoop fs常规命令</li><li>配置文件在官方哪里找</li><li>整理 jdk、ssh、hosts文件</li></ol><h3 id="1-安装hadoop的hdfs伪分布式部署"><a href="#1-安装hadoop的hdfs伪分布式部署" class="headerlink" title="1.安装hadoop的hdfs伪分布式部署"></a>1.安装hadoop的hdfs伪分布式部署</h3><ol><li><p>创建用户和目录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# useradd hadoop</span><br><span class="line">[root@aliyun ~]# su - hadoop</span><br><span class="line">[hadoop@aliyun ~]$ mkdir app software sourcecode log tmp data lib</span><br><span class="line">[hadoop@aliyun ~]$ ll</span><br><span class="line">total 28</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 app　　　　#解压的文件夹  软连接</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 data　　　#数据</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 lib　　　　#第三方的jar</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 log　　　　#日志文件夹</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 software　#压缩包</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 sourcecode　　#源代码编译</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 tmp　　　　#临时文件夹</span><br></pre></td></tr></table></figure></li><li><p>下载/上传压缩包</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ cd software/</span><br><span class="line">[hadoop@aliyun software]$ wget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>解压</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun software]$ tar -xzvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ../app/</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">[hadoop@aliyun software]$ cd ../app/</span><br><span class="line">[hadoop@aliyun app]$ ln -s hadoop-2.6.0-cdh5.16.2/ hadoop</span><br><span class="line">[hadoop@aliyun app]$ ll</span><br><span class="line">total 4</span><br><span class="line">lrwxrwxrwx  1 hadoop hadoop   23 Nov 28 11:36 hadoop -&gt; hadoop-2.6.0-cdh5.16.2/</span><br><span class="line">drwxr-xr-x 14 hadoop hadoop 4096 Jun  3 19:11 hadoop-2.6.0-cdh5.16.2</span><br></pre></td></tr></table></figure></li><li><p>环境要求</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun java]# mkdir /usr/java</span><br><span class="line">[root@aliyun java]# cd /usr/java</span><br><span class="line">[root@aliyun java]# rz -E</span><br><span class="line">[root@aliyun java]# tar -xzvf jdk-8u144-linux-x64.tar.gz</span><br><span class="line">[root@aliyun java]# chown -R  root:root jdk1.8.0_144/</span><br><span class="line">[root@aliyun java]# ln -s jdk1.8.0_144/ jdk</span><br><span class="line">[root@aliyun java]# ll</span><br><span class="line">total 4</span><br><span class="line">lrwxrwxrwx 1 root root   13 Nov 28 12:01 jdk -&gt; jdk1.8.0_144/</span><br><span class="line">drwxr-xr-x 8 root root 4096 Jul 22  2017 jdk1.8.0_144</span><br><span class="line">[root@aliyun java]# vim /etc/profile</span><br><span class="line">    #env</span><br><span class="line">    export JAVA_HOME=/usr/java/jdk</span><br><span class="line">    export PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">[root@aliyun java]# source /etc/profile</span><br><span class="line">[root@aliyun java]# which java</span><br><span class="line">/usr/java/jdk/bin/java</span><br></pre></td></tr></table></figure></li><li><p>JAVA_HOME 显性配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ vi hadoop-env.sh</span><br><span class="line">export JAVA_HOME=/usr/java/jdk</span><br><span class="line">[root@aliyun java]# cat /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">172.16.39.48 aliyun</span><br></pre></td></tr></table></figure></li><li><p>配置文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">etc/hadoop/core-site.xml:</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://aliyun:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line">etc/hadoop/hdfs-site.xml:</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li><li><p>ssh无密码信任关系</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">家目录下输入</span><br><span class="line"><span class="meta">  $</span><span class="bash"> ssh-keygen -t rsa -P <span class="string">''</span> -f ~/.ssh/id_rsa</span></span><br><span class="line"><span class="meta">  $</span><span class="bash"> cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span></span><br><span class="line"><span class="meta">  $</span><span class="bash"> chmod 0600 ~/.ssh/authorized_keys</span></span><br><span class="line">[hadoop@aliyun ~]$ ssh aliyun date</span><br><span class="line">Thu Nov 28 12:15:08 CST 2019</span><br></pre></td></tr></table></figure></li><li><p>环境变量 hadoop</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ vi .bashrc</span><br><span class="line">export HADOOP_HOME=/home/hadoop/app/hadoop</span><br><span class="line">export PATH=$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin:$PATH</span><br><span class="line">[hadoop@aliyun ~]$ source .bashrc </span><br><span class="line">[hadoop@aliyun ~]$ which hadoop</span><br><span class="line">~/app/hadoop/bin/hadoop</span><br></pre></td></tr></table></figure></li><li><p>格式化</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ hdfs namenode -format</span><br><span class="line">has been successfully formatted.</span><br></pre></td></tr></table></figure></li><li><p>第一次启动</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ start-dfs.sh </span><br><span class="line">[hadoop@aliyun ~]$ jps</span><br><span class="line">10804 SecondaryNameNode</span><br><span class="line">10536 NameNode</span><br><span class="line">10907 Jps</span><br><span class="line">10654 DataNode</span><br><span class="line">[hadoop@aliyun ~]$</span><br></pre></td></tr></table></figure><p>坑：第一次启动会输入yes确定信任关系，我们打开./ssh下的known_hosts文件，这个文件中存放信任关系</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun .ssh]$ cat known_hosts</span><br><span class="line">aliyun,172.16.39.48 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBCjHBKn/7LF5sfbae1OLkK5QoWm11Xn8RZs1JTc7K8v4RFum1OKIjArocvRjLOYPsq5ezYo8TlBHTrAgeUcvkBM=</span><br><span class="line">localhost ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBCjHBKn/7LF5sfbae1OLkK5QoWm11Xn8RZs1JTc7K8v4RFum1OKIjArocvRjLOYPsq5ezYo8TlBHTrAgeUcvkBM=</span><br><span class="line">0.0.0.0 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBCjHBKn/7LF5sfbae1OLkK5QoWm11Xn8RZs1JTc7K8v4RFum1OKIjArocvRjLOYPsq5ezYo8TlBHTrAgeUcvkBM=</span><br></pre></td></tr></table></figure><p>将来也许在启动hadoop的时候一直要输入密码，就是这里面已经存在了主机的信任关系，但是密匙对是新的，删除这个文件或者内容即可</p></li><li><p>DN SNN都以 aliyun启动</p></li></ol><ul><li><p>NN：core-site.xml fs.defaultFS控制</p></li><li><p>DN: slaves文件</p></li><li><p>2NN:hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;aliyun:50090&lt;/value&gt;       #注意端口号，新旧版本有区别</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;aliyun:50091&lt;/value&gt;       #注意端口号，新旧版本有区别</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li></ul><h3 id="2-hadoop-fs常规命令"><a href="#2-hadoop-fs常规命令" class="headerlink" title="2.hadoop fs常规命令"></a>2.hadoop fs常规命令</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -mkdir /</span><br><span class="line">hadoop fs -put</span><br><span class="line">hadoop fs -get</span><br><span class="line">hadoop fs -cat</span><br><span class="line">hadoop fs -rm</span><br><span class="line">hadoop fs -ls</span><br></pre></td></tr></table></figure><h3 id="3-配置文件在官方哪里找"><a href="#3-配置文件在官方哪里找" class="headerlink" title="3.配置文件在官方哪里找"></a><strong>3.配置文件在官方哪里找</strong></h3><p><strong><a href="https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation" target="_blank" rel="noopener">https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation</a></strong></p><h3 id="4-整理-jdk、ssh、hosts文件"><a href="#4-整理-jdk、ssh、hosts文件" class="headerlink" title="4.整理 jdk、ssh、hosts文件"></a>4.整理 jdk、ssh、hosts文件</h3><p>jdk和ssh是hadoop运行的先决条件</p><p>hosts文件存放主机名和ip地址的映射</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>掌握where、group、join语句和写SQL</title>
      <link href="/2018/10/03/mysql/3/"/>
      <url>/2018/10/03/mysql/3/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 sql的where各种条件</li><li>整理 sql的group</li><li>整理 sql的join</li><li>04txt文件的案例 9句sql</li><li>整理刚才分享的小知识点</li><li>补充资料文件夹 去看看执行</li><li>彩蛋 视频 sql</li></ol><h3 id="1-整理-sql-的-where-各种条件"><a href="#1-整理-sql-的-where-各种条件" class="headerlink" title="1.整理 sql 的 where 各种条件"></a>1.整理 sql 的 where 各种条件</h3><p><code>where 子句:</code> 如需有条件地从表中选取数据，可将 where 子句添加到 SELECT 语句。</p><p>语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> 列名称 <span class="keyword">FROM</span> 表名称 <span class="keyword">WHERE</span> 列 运算符 值</span><br></pre></td></tr></table></figure><p>下面的运算符可在 where 子句中使用：</p><table><thead><tr><th>操作符</th><th>描述</th></tr></thead><tbody><tr><td>=</td><td>等于</td></tr><tr><td>&lt;&gt;</td><td>不等于</td></tr><tr><td>&gt;</td><td>大于</td></tr><tr><td>&lt;</td><td>小于</td></tr><tr><td>&gt;=</td><td>大于等于</td></tr><tr><td>&lt;=</td><td>小于等于</td></tr><tr><td>BETWEEN</td><td>在某个范围内</td></tr><tr><td>LIKE</td><td>搜索某种模式</td></tr></tbody></table><h3 id="2-整理-sql-的-group"><a href="#2-整理-sql-的-group" class="headerlink" title="2.整理 sql 的 group"></a>2.整理 sql 的 group</h3><p>聚合函数 (比如 SUM) 常常需要添加 group by语句。</p><p><code>group by语句:</code> group by语句用于结合聚合函数，根据一个或多个列对结果集进行分组。</p><p>语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name, aggregate_function(column_name)</span><br><span class="line"><span class="keyword">FROM</span> table_name</span><br><span class="line"><span class="keyword">where</span> column_name <span class="keyword">operator</span> <span class="keyword">value</span></span><br><span class="line"><span class="keyword">group</span> bycolumn_name</span><br></pre></td></tr></table></figure><h3 id="3-整理-sql-的-join"><a href="#3-整理-sql-的-join" class="headerlink" title="3.整理 sql 的 join"></a>3.整理 sql 的 join</h3><p>join分为inner join、left join、right join，分别表示内联结，左联结，右联结</p><p><code>inner join:</code> 在表中存在至少一个匹配时，INNER JOIN 关键字返回行。</p><p>语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name(s)</span><br><span class="line"><span class="keyword">FROM</span> table_name1</span><br><span class="line"><span class="keyword">INNER</span> <span class="keyword">JOIN</span> table_name2 </span><br><span class="line"><span class="keyword">ON</span> table_name1.column_name=table_name2.column_name</span><br></pre></td></tr></table></figure><p><code>注意:</code> inner 与 join是相同的。</p><p><code>left join:</code> left join 关键字会从左表 (table_name1) 那里返回所有的行，即使在右表 (table_name2) 中没有匹配的行。</p><p>语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name(s)</span><br><span class="line"><span class="keyword">FROM</span> table_name1</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> table_name2 </span><br><span class="line"><span class="keyword">ON</span> table_name1.column_name=table_name2.column_name</span><br></pre></td></tr></table></figure><p><code>注意:</code> 在某些数据库中， left join 称为 left outer join。</p><p><code>right join</code> right join 关键字会右表 (table_name2) 那里返回所有的行，即使在左表 (table_name1) 中没有匹配的行。</p><p>语法</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name(s)</span><br><span class="line"><span class="keyword">FROM</span> table_name1</span><br><span class="line"><span class="keyword">right</span> <span class="keyword">join</span> table_name2 </span><br><span class="line"><span class="keyword">ON</span> table_name1.column_name=table_name2.column_name</span><br></pre></td></tr></table></figure><p><code>注意:</code> 在某些数据库中， right join 称为 right outer join。</p><h3 id="4-04txt文件的案例9句sql"><a href="#4-04txt文件的案例9句sql" class="headerlink" title="4. 04txt文件的案例9句sql"></a>4. 04txt文件的案例9句sql</h3><ul><li>查询出部门编号为30的所有员工的编号和姓名</li><li>找出部门编号为10中所有经理，和部门编号为20中所有销售员的详细资料。</li><li>查询所有员工详细信息，用工资降序排序，如果工资相同使用入职日期升序排序</li><li>列出薪金大于1500的各种工作及从事此工作的员工人数。</li><li>列出在销售部工作的员工的姓名，假定不知道销售部的部门编号。</li><li>查询姓名以S开头的\以S结尾\包含S字符\第二个字母为L __</li><li>查询每种工作的最高工资、最低工资、人数</li><li>列出薪金高于公司平均薪金的所有员工号，员工姓名，所在部门名称，上级领导，工资，工资等级</li><li>列出薪金高于在部门30工作的 所有/任何一个员工的薪金的员工姓名和薪金、部门名称。</li></ul><h3 id="5-整理刚才分享的小知识点"><a href="#5-整理刚才分享的小知识点" class="headerlink" title="5.整理刚才分享的小知识点"></a>5.整理刚才分享的小知识点</h3><ol><li><p>关于count()的使用细节</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">使用count(id)替换count(*)的使用，可以提升性能</span><br></pre></td></tr></table></figure></li><li><p>sum()与count()的区别与联想</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sum()计算统计字段的和，count()统计字段的数量，容易混淆，建议使用sum()联想count()，使用count()联想sum()，并区分</span><br></pre></td></tr></table></figure></li><li><p>sql语句的执行顺序</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> yyyyy <span class="keyword">from</span> rzdata</span><br><span class="line"><span class="keyword">where</span> xxx</span><br><span class="line"><span class="keyword">group</span> byxxx <span class="keyword">having</span> xxx </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> xxx</span><br><span class="line"><span class="keyword">limit</span> xxx ;</span><br></pre></td></tr></table></figure></li><li><p>all和any的区别</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">all()的用法是表示要满足字段中所有值即最大值，any()的用法表示的是满足字段值任意一个值即最小值</span><br></pre></td></tr></table></figure></li><li><p>聚合函数中的null值</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">如果在进行数值计算的时候，字段中存在null值，则计算的结果是null值，在进行数值运算的时候使用IFNULL(expression, alt_value)替换null值为0，则能计算出结果</span><br><span class="line">如果第一个参数的表达式 expression 为 NULL，则返回第二个参数的备用值。</span><br></pre></td></tr></table></figure></li><li><p>unoin和union all的区别</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">union的用法是把联合查询中的语句如果整体重复则去重，unoin all不会去重</span><br></pre></td></tr></table></figure></li></ol><h3 id="6-补充资料文件夹-去看看执行"><a href="#6-补充资料文件夹-去看看执行" class="headerlink" title="6.补充资料文件夹 去看看执行"></a>6.补充资料文件夹 去看看执行</h3><p>　　资料文件中</p><h3 id="7-彩蛋-视频-sql"><a href="#7-彩蛋-视频-sql" class="headerlink" title="7.彩蛋 视频 sql"></a>7.彩蛋 视频 sql</h3><p>　　百度云中</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataWarehouse </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>掌握MySQL的建表规范、DDL语句和权限操作</title>
      <link href="/2018/10/02/mysql/2/"/>
      <url>/2018/10/02/mysql/2/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 建表规范</li><li>整理 DDL语句的</li><li>整理 三句话</li></ol><h3 id="首先看一个建表例子，再去研究应该遵循哪些规范"><a href="#首先看一个建表例子，再去研究应该遵循哪些规范" class="headerlink" title="首先看一个建表例子，再去研究应该遵循哪些规范"></a>首先看一个建表例子，再去研究应该遵循哪些规范</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> rzdata(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">not</span> <span class="literal">null</span> auto_increment,</span><br><span class="line"></span><br><span class="line"><span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">200</span>),</span><br><span class="line">age  <span class="built_in">int</span>(<span class="number">3</span>),</span><br><span class="line"></span><br><span class="line">createuser <span class="built_in">varchar</span>(<span class="number">200</span>) ,</span><br><span class="line">createtime <span class="built_in">timestamp</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">default</span> <span class="keyword">current_timestamp</span>,</span><br><span class="line">updateuser <span class="built_in">varchar</span>(<span class="number">200</span>) ,</span><br><span class="line">updatetime <span class="built_in">timestamp</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">default</span> <span class="keyword">current_timestamp</span> <span class="keyword">on</span> <span class="keyword">update</span> <span class="keyword">current_timestamp</span>,</span><br><span class="line"></span><br><span class="line">primary <span class="keyword">key</span> (<span class="keyword">id</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure><ol><li><p>表名</p><p>不能是中文，不能是汉语拼音 ，不然很low</p></li><li><p>风格统一</p><p>统一所有表的风格，可以从已有的表中查看，或者找leader检查，方便后期维护</p></li><li><p>第一个字段</p><p>第一个字段必须是id，并且自增长，是主键，没有意义 –&gt;拓展: 为什么？</p></li><li><p>主键</p><p>一张表只有一个主键，primary key == unique+not null</p></li><li><p>后四个字段</p><p>后四个字段包括：用户、创建时间、修改用户、修改时间</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">createuser varchar(200) ,</span><br><span class="line">createtime timestamp not null default current_timestamp,</span><br><span class="line">updateuser varchar(200) ,</span><br><span class="line">updatetime timestamp not null default current_timestamp on <span class="keyword">update</span> <span class="keyword">current_timestamp</span>,</span><br></pre></td></tr></table></figure></li><li><p>业务字段</p><p>业务字段需要唯一存在，使用unique约束，如订单号</p><p>业务字段都必须加上注释</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">COMMENT</span> <span class="string">'用户名称'</span></span><br></pre></td></tr></table></figure></li><li><p>字符集CHARSET</p><p>查看字符集</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">&gt;&gt; show variables like '%char%';</span><br><span class="line">+<span class="comment">--------------------------+---------------------------------------------------------------+</span></span><br><span class="line">| Variable_name            | Value                                                         |</span><br><span class="line">+<span class="comment">--------------------------+---------------------------------------------------------------+</span></span><br><span class="line">| character_set_client     | utf8                                                          |</span><br><span class="line">| character_set_connection | utf8                                                          |</span><br><span class="line">| character_set_database   | latin1                                                        |</span><br><span class="line">| character_set_filesystem | binary                                                        |</span><br><span class="line">| character_set_results    | utf8                                                          |</span><br><span class="line">| character_set_server     | latin1                                                        |</span><br><span class="line">| character_set_system     | utf8                                                          |</span><br><span class="line">| character_sets_dir       | /usr/local/mysql-5.6.23-linux-glibc2.5-x86_64/share/charsets/ |</span><br><span class="line">+<span class="comment">--------------------------+---------------------------------------------------------------+</span></span><br><span class="line">8 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li></ol><h3 id="DDL语句以及需要注意的点"><a href="#DDL语句以及需要注意的点" class="headerlink" title="DDL语句以及需要注意的点"></a>DDL语句以及需要注意的点</h3><p><code>查询语句：</code>select 查询字段 from 表 ;</p><p>注意：</p><ol><li><p>生产环境下不要用 * 代替所有字段</p><p>错误示范：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from tb_user;</span><br><span class="line">+<span class="comment">----+----------+----------------------------------+-------------+---------------------+----------------------------------+</span></span><br><span class="line">| id | username | password                         | phone       | created             | salt                             |</span><br><span class="line">+<span class="comment">----+----------+----------------------------------+-------------+---------------------+----------------------------------+</span></span><br><span class="line">| 28 | zhangsan | e21d44f200365b57fab2641cd31226d4 | 13600527634 | 2018-05-25 17:52:03 | 05b0f203987e49d2b72b20b95e0e57d9 |</span><br><span class="line">| 30 | leyou    | 4de9a93b3f95d468874a3c1bf3b25a48 | 15855410440 | 2018-09-30 11:37:30 | 4565613d4b0e434cb496d4eb87feb45f |</span><br><span class="line">+<span class="comment">----+----------+----------------------------------+-------------+---------------------+----------------------------------+</span></span><br><span class="line">2 rows in <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>正确示范：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; select username,password from tb_user;</span><br><span class="line">+<span class="comment">----------+----------------------------------+</span></span><br><span class="line">| username | password                         |</span><br><span class="line">+<span class="comment">----------+----------------------------------+</span></span><br><span class="line">| zhangsan | e21d44f200365b57fab2641cd31226d4 |</span><br><span class="line">| leyou    | 4de9a93b3f95d468874a3c1bf3b25a48 |</span><br><span class="line">+<span class="comment">----------+----------------------------------+</span></span><br><span class="line">2 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>查询语句如果数据量特别大必须使用where 或者 limit，否则需要使用大量的资源</p><p>错误示范：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,image,letter <span class="keyword">from</span> tb_brand;</span><br></pre></td></tr></table></figure><p>正确示范：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,image,letter <span class="keyword">from</span> tb_brand <span class="keyword">limit</span> <span class="number">100</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,image,letter <span class="keyword">from</span> tb_brand <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">100</span>;</span><br></pre></td></tr></table></figure></li></ol><p><code>新增语句：</code>insert into 表名（字段1，字段2…） values（数据1，数据2…）;</p><p>注意：在表名后加上对应要添加的字段名</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tb_brand (<span class="keyword">name</span>，letter) <span class="keyword">values</span>(tunan，T);</span><br></pre></td></tr></table></figure><p><code>修改语句：</code>update 表名 set 修改后的字段 where 条件；</p><p>注意：一定要加上条件，否则是全局修改</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> tb_brand <span class="keyword">set</span> <span class="keyword">name</span>=<span class="string">"xiaoqi"</span> <span class="keyword">where</span> <span class="keyword">id</span> = <span class="number">1</span>;</span><br></pre></td></tr></table></figure><p><code>删除语句：</code>delete from 表名 where 条件；</p><p>注意：一定要加上条件，否则是全局删除；</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> tb_brand tb <span class="keyword">where</span> <span class="keyword">id</span> = <span class="number">1</span>;</span><br></pre></td></tr></table></figure><h3 id="当某条SQL验证拖累进程时怎么办？"><a href="#当某条SQL验证拖累进程时怎么办？" class="headerlink" title="当某条SQL验证拖累进程时怎么办？"></a>当某条SQL验证拖累进程时怎么办？</h3><p>使用 show processlist；查看mysql中的 sql 进程</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; show processlist;</span><br><span class="line">+<span class="comment">-----+------+---------------------+--------+---------+------+----------+------------------+</span></span><br><span class="line">| Id  | User | Host                | db     | Command | Time | State    | Info             |</span><br><span class="line">+<span class="comment">-----+------+---------------------+--------+---------+------+----------+------------------+</span></span><br><span class="line">| 272 | root | localhost           | leyou1 | Query   |    0 | starting | <span class="keyword">show</span> <span class="keyword">processlist</span> |</span><br><span class="line">| <span class="number">273</span> | root | <span class="number">121.62</span><span class="number">.184</span><span class="number">.34</span>:<span class="number">56629</span> | leyou1 | <span class="keyword">Sleep</span>   |  <span class="number">448</span> |          | <span class="literal">NULL</span>             |</span><br><span class="line">| <span class="number">274</span> | root | <span class="number">121.62</span><span class="number">.184</span><span class="number">.34</span>:<span class="number">56631</span> | leyou1 | <span class="keyword">Sleep</span>   |  <span class="number">592</span> |          | <span class="literal">NULL</span>             |</span><br><span class="line">+<span class="comment">-----+------+---------------------+--------+---------+------+----------+------------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>然后根据 id 删除即可</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">kill</span> <span class="keyword">id</span>;</span><br></pre></td></tr></table></figure><h3 id="必须要记住的三条命令"><a href="#必须要记住的三条命令" class="headerlink" title="必须要记住的三条命令"></a>必须要记住的三条命令</h3><p><code>修改密码：</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> <span class="keyword">user</span> <span class="keyword">set</span> <span class="keyword">password</span>=<span class="keyword">password</span>(<span class="string">'密码'</span>) <span class="keyword">where</span> <span class="keyword">user</span>=<span class="string">'用户名'</span>;</span><br></pre></td></tr></table></figure><p><code>修改权限:</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> 用户名@<span class="string">'%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'密码'</span>;</span><br></pre></td></tr></table></figure><p><code>刷新权限：</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataWarehouse </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL二进制部署和DBeaver连接MySQL</title>
      <link href="/2018/10/01/mysql/1/"/>
      <url>/2018/10/01/mysql/1/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>二进制部署mysql</li><li>重新部署</li><li>部署dbeaver 打通 mysql</li></ol><h3 id="二进制部署mysql"><a href="#二进制部署mysql" class="headerlink" title="二进制部署mysql"></a>二进制部署mysql</h3><p>[<a href="https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt]" target="_blank" rel="noopener">https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt]</a>(<a href="https://github.com/Hackeruncle/MySQL/blob/master/MySQL" target="_blank" rel="noopener">https://github.com/Hackeruncle/MySQL/blob/master/MySQL</a> 5.6.23 Install.txt)</p><p>京东云下部署代码：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">scripts/mysql_install_db  \</span><br><span class="line"><span class="comment">--user=mysqladmin \</span></span><br><span class="line"><span class="comment">--basedir=/usr/local/mysql \</span></span><br><span class="line"><span class="comment">--datadir=/usr/local/mysql/data</span></span><br></pre></td></tr></table></figure><p>必须要装三个环境：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">yum <span class="keyword">install</span> -y perl</span><br><span class="line">yum <span class="keyword">install</span> -y autoconf</span><br><span class="line">yum <span class="keyword">install</span> -y libaio</span><br></pre></td></tr></table></figure><h3 id="重新部署"><a href="#重新部署" class="headerlink" title="重新部署"></a>重新部署</h3><ol><li><p>删除压缩文件和数据文件</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">rm -rf arch<span class="comment">/* data/*</span></span><br></pre></td></tr></table></figure></li><li><p>重置执行脚本文件</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">scripts/mysql_install_db  \</span><br><span class="line"><span class="comment">--user=mysqladmin \</span></span><br><span class="line"><span class="comment">--basedir=/usr/local/mysql \</span></span><br><span class="line"><span class="comment">--datadir=/usr/local/mysql/data</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="部署dbeaver-打通-mysql"><a href="#部署dbeaver-打通-mysql" class="headerlink" title="部署dbeaver 打通 mysql"></a>部署dbeaver 打通 mysql</h3><p>修改用户密码：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> <span class="keyword">user</span> <span class="keyword">set</span> <span class="keyword">password</span>=<span class="keyword">password</span>(<span class="string">'ruozedata'</span>) <span class="keyword">where</span> <span class="keyword">user</span>=<span class="string">'root'</span>;</span><br></pre></td></tr></table></figure><p>查看用户权限信息：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">user</span>,<span class="keyword">password</span>,host <span class="keyword">from</span> <span class="keyword">user</span>;</span><br></pre></td></tr></table></figure><p>给用户添加权限：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> root@<span class="string">'%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'ruozedata'</span>;</span><br><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;　　<span class="comment">#刷新权限</span></span><br></pre></td></tr></table></figure><p>DBeaver连接Mysql：</p><p><img src="https://img2018.cnblogs.com/i-beta/1657732/201911/1657732-20191121181225170-233024101.png" alt="DBeaver连接设置"> </p><p> <img src="https://img2018.cnblogs.com/i-beta/1657732/201911/1657732-20191121181334218-1241892255.png" alt="DBeaver连接添加jar包"></p><h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><p>为了保证 ip 地址的安全性和可用性</p><p>　　1. 在 linux 的 /etc/hosts 文件中配置 内网ip 地址和主机名的映射环境，这样在shell脚本或者代码中使用主机名替代 ip 使用</p><p>　　2. 如果是云主机，在windows中的hosts文件中配置外网ip地址和主机名的映射</p><p><code>注意：</code>hosts文件中的前两行切记不能删，否则可能带来bug</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataWarehouse </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的test命令</title>
      <link href="/2018/09/30/linux/shell/9/"/>
      <url>/2018/09/30/linux/shell/9/</url>
      
        <content type="html"><![CDATA[<p>Shell中的 test 命令用于检查某个条件是否成立，它可以进行<code>数值</code>、<code>字符</code>和<code>文件</code>三个方面的测试。</p><h3 id="数值测试"><a href="#数值测试" class="headerlink" title="数值测试"></a>数值测试</h3><table><thead><tr><th align="left">参数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">-eq</td><td align="left">等于则为真</td></tr><tr><td align="left">-ne</td><td align="left">不等于则为真</td></tr><tr><td align="left">-gt</td><td align="left">大于则为真</td></tr><tr><td align="left">-ge</td><td align="left">大于等于则为真</td></tr><tr><td align="left">-lt</td><td align="left">小于则为真</td></tr><tr><td align="left">-le</td><td align="left">小于等于则为真</td></tr></tbody></table><p>实例演示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">num1=100</span><br><span class="line">num2=100</span><br><span class="line">if test $[num1] -eq $[num2]</span><br><span class="line">then</span><br><span class="line">    echo '两个数相等！'</span><br><span class="line">else</span><br><span class="line">    echo '两个数不相等！'</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">两个数相等！</span><br></pre></td></tr></table></figure><p>代码中的 [] 执行基本的算数运算，如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">a=5</span><br><span class="line">b=6</span><br><span class="line"></span><br><span class="line">result=$[a+b] # 注意等号两边不能有空格</span><br><span class="line">echo "result 为： $result"</span><br></pre></td></tr></table></figure><p>结果为:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">result 为： 11</span><br></pre></td></tr></table></figure><h3 id="字符串测试"><a href="#字符串测试" class="headerlink" title="字符串测试"></a>字符串测试</h3><table><thead><tr><th align="left">参数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">=</td><td align="left">等于则为真</td></tr><tr><td align="left">!=</td><td align="left">不相等则为真</td></tr><tr><td align="left">-z 字符串</td><td align="left">字符串的长度为零则为真</td></tr><tr><td align="left">-n 字符串</td><td align="left">字符串的长度不为零则为真</td></tr></tbody></table><p>实例演示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">num1="ru1noob"</span><br><span class="line">num2="runoob"</span><br><span class="line">if test $num1 = $num2</span><br><span class="line">then</span><br><span class="line">    echo '两个字符串相等!'</span><br><span class="line">else</span><br><span class="line">    echo '两个字符串不相等!'</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">两个字符串不相等!</span><br></pre></td></tr></table></figure><h3 id="文件测试"><a href="#文件测试" class="headerlink" title="文件测试"></a>文件测试</h3><table><thead><tr><th align="left">参数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">-e 文件名</td><td align="left">如果文件存在则为真</td></tr><tr><td align="left">-r 文件名</td><td align="left">如果文件存在且可读则为真</td></tr><tr><td align="left">-w 文件名</td><td align="left">如果文件存在且可写则为真</td></tr><tr><td align="left">-x 文件名</td><td align="left">如果文件存在且可执行则为真</td></tr><tr><td align="left">-s 文件名</td><td align="left">如果文件存在且至少有一个字符则为真</td></tr><tr><td align="left">-d 文件名</td><td align="left">如果文件存在且为目录则为真</td></tr><tr><td align="left">-f 文件名</td><td align="left">如果文件存在且为普通文件则为真</td></tr><tr><td align="left">-c 文件名</td><td align="left">如果文件存在且为字符型特殊文件则为真</td></tr><tr><td align="left">-b 文件名</td><td align="left">如果文件存在且为块特殊文件则为真</td></tr></tbody></table><p>实例演示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /bin</span><br><span class="line">if test -e ./bash</span><br><span class="line">then</span><br><span class="line">    echo '文件已存在!'</span><br><span class="line">else</span><br><span class="line">    echo '文件不存在!'</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">文件已存在!</span><br></pre></td></tr></table></figure><p>另外，Shell还提供了与( -a )、或( -o )、非( ! )三个逻辑操作符用于将测试条件连接起来，其优先级为：”!”最高，”-a”次之，”-o”最低。例如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /bin</span><br><span class="line">if test -e ./notFile -o -e ./bash</span><br><span class="line">then</span><br><span class="line">    echo '至少有一个文件存在!'</span><br><span class="line">else</span><br><span class="line">    echo '两个文件都不存在'</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">至少有一个文件存在!</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的流程控制</title>
      <link href="/2018/09/29/linux/shell/8/"/>
      <url>/2018/09/29/linux/shell/8/</url>
      
        <content type="html"><![CDATA[<p>和Java、PHP等语言不一样，shell的流程控制不可缺少</p><h3 id="判断语句"><a href="#判断语句" class="headerlink" title="判断语句"></a>判断语句</h3><h4 id="if"><a href="#if" class="headerlink" title="if"></a>if</h4><p>if 语句语法格式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if condition</span><br><span class="line">then</span><br><span class="line">    command1 </span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN </span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>写成一行（适用于终端命令提示符）：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if [ $(ps -ef | grep -c "ssh") -gt 1 ]; then echo "true"; fi</span><br></pre></td></tr></table></figure><p>末尾的fi就是if倒过来拼写，后面还会遇到类似的。</p><h4 id="if-else"><a href="#if-else" class="headerlink" title="if else"></a>if else</h4><p>if else 语法格式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if condition</span><br><span class="line">then</span><br><span class="line">    command1 </span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN</span><br><span class="line">else</span><br><span class="line">    command</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h4 id="if-else-if-else"><a href="#if-else-if-else" class="headerlink" title="if else-if else"></a>if else-if else</h4><p>if else-if else 语法格式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if condition1</span><br><span class="line">then</span><br><span class="line">    command1</span><br><span class="line">elif condition2 </span><br><span class="line">then </span><br><span class="line">    command2</span><br><span class="line">else</span><br><span class="line">    commandN</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>以下实例判断两个变量是否相等：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a=10</span><br><span class="line">b=20</span><br><span class="line">if [ $a == $b ]</span><br><span class="line">then</span><br><span class="line">   echo "a 等于 b"</span><br><span class="line">elif [ $a -gt $b ]</span><br><span class="line">then</span><br><span class="line">   echo "a 大于 b"</span><br><span class="line">elif [ $a -lt $b ]</span><br><span class="line">then</span><br><span class="line">   echo "a 小于 b"</span><br><span class="line">else</span><br><span class="line">   echo "没有符合的条件"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a 小于 b</span><br></pre></td></tr></table></figure><p>if else语句经常与test命令结合使用，如下所示：</p><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">num1=$[<span class="number">2</span>*<span class="number">3</span>]</span><br><span class="line">num2=$[<span class="number">1</span>+<span class="number">5</span>]</span><br><span class="line"><span class="keyword">if</span> test $[num1] <span class="nomarkup">-eq</span> $[num2]</span><br><span class="line">then</span><br><span class="line">    echo <span class="string">'两个数字相等!'</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    echo <span class="string">'两个数字不相等!'</span></span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">两个数字相等!</span><br></pre></td></tr></table></figure><hr><h3 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h3><h4 id="for循环"><a href="#for循环" class="headerlink" title="for循环"></a>for循环</h4><p>与其他编程语言类似，Shell支持for循环。</p><p>for循环一般格式为：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for var in item1 item2 ... itemN</span><br><span class="line">do</span><br><span class="line">    command1</span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>写成一行：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for var in item1 item2 ... itemN; do command1; command2… done;</span><br></pre></td></tr></table></figure><p>当变量值在列表里，for循环即执行一次所有命令，使用变量名获取列表中的当前取值。命令可为任何有效的shell命令和语句。in列表可以包含替换、字符串和文件名。</p><p>in列表是可选的，如果不用它，for循环使用命令行的位置参数。</p><p>例如，顺序输出当前列表中的数字：</p><p><code>方法1:</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for var in 1 2 3 4 5 </span><br><span class="line">do</span><br><span class="line">    echo "The value is: $var"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p><code>方法2:</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for var in &#123;1..5&#125;</span><br><span class="line">do</span><br><span class="line">    echo "The value is: $var"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p><code>方法3:</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for ((var=1;var&lt;=5;var++))</span><br><span class="line">do</span><br><span class="line">    echo "The value is: $var"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">The value is: 1</span><br><span class="line">The value is: 2</span><br><span class="line">The value is: 3</span><br><span class="line">The value is: 4</span><br><span class="line">The value is: 5</span><br></pre></td></tr></table></figure><p>顺序输出字符串中的字符：</p><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> str <span class="keyword">in</span> <span class="string">'This is a string'</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    echo <span class="variable">$str</span></span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">This is a string</span><br></pre></td></tr></table></figure><h4 id="while-语句"><a href="#while-语句" class="headerlink" title="while 语句"></a>while 语句</h4><p>while循环用于不断执行一系列命令，也用于从输入文件中读取数据；命令通常为测试条件。其格式为：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">while condition</span><br><span class="line">do</span><br><span class="line">    command</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>以下是一个基本的while循环，测试条件是：如果int小于等于5，那么条件返回真。int从0开始，每次循环处理时，int加1。运行上述脚本，返回数字1到5，然后终止。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">int=1</span><br><span class="line">while(( $int&lt;=5 ))</span><br><span class="line">do</span><br><span class="line">    echo $int</span><br><span class="line">    let "int++"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>运行脚本，输出：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td></tr></table></figure><p>以上实例使用了 Bash let 命令，let 命令是 BASH 中用于计算的工具，用于执行一个或多个表达式，变量计算中不需要加上 $ 来表示变量。如果表达式中包含了空格或其他特殊字符，则必须引起来。</p><p>while循环可用于读取键盘信息。下面的例子中，输入信息被设置为变量FILM，按<Ctrl-D>结束循环。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo '按下 &lt;CTRL-D&gt; 退出'</span><br><span class="line">echo -n '输入你最喜欢的网站名: '</span><br><span class="line">while read FILM</span><br><span class="line">do</span><br><span class="line">    echo "是的！$FILM 是一个好网站"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>运行脚本，输出类似下面：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">按下 &lt;CTRL-D&gt; 退出</span><br><span class="line">输入你最喜欢的网站名:菜鸟教程</span><br><span class="line">是的！菜鸟教程 是一个好网站</span><br></pre></td></tr></table></figure><h4 id="无限循环"><a href="#无限循环" class="headerlink" title="无限循环"></a>无限循环</h4><p>无限循环语法格式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">while :</span><br><span class="line">do</span><br><span class="line">    command</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">while true</span><br><span class="line">do</span><br><span class="line">    command</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for (( ; ; ))</span><br></pre></td></tr></table></figure><h3 id="开关语句"><a href="#开关语句" class="headerlink" title="开关语句"></a>开关语句</h3><h4 id="case"><a href="#case" class="headerlink" title="case"></a>case</h4><p>Shell case语句为多选择语句。可以用case语句匹配一个值与一个模式，如果匹配成功，执行相匹配的命令。case语句格式如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">case 值 in</span><br><span class="line">模式1)</span><br><span class="line">    command1</span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN</span><br><span class="line">    ;;</span><br><span class="line">模式2）</span><br><span class="line">    command1</span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><p>case工作方式如上所示。取值后面必须为单词in，每一模式必须以右括号结束。取值可以为变量或常数。匹配发现取值符合某一模式后，其间所有命令开始执行直至 ;;。</p><p>取值将检测匹配的每一个模式。一旦模式匹配，则执行完匹配模式相应命令后不再继续其他模式。如果无一匹配模式，使用星号 * 捕获该值，再执行后面的命令。</p><p>下面的脚本提示输入1到4，与每一种模式进行匹配：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo '输入 1 到 4 之间的数字:'</span><br><span class="line">echo '你输入的数字为:'</span><br><span class="line">read aNum</span><br><span class="line">case $aNum in</span><br><span class="line">    1)  echo '你选择了 1'</span><br><span class="line">    ;;</span><br><span class="line">    2)  echo '你选择了 2'</span><br><span class="line">    ;;</span><br><span class="line">    3)  echo '你选择了 3'</span><br><span class="line">    ;;</span><br><span class="line">    4)  echo '你选择了 4'</span><br><span class="line">    ;;</span><br><span class="line">    *)  echo '你没有输入 1 到 4 之间的数字'</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><p>输入不同的内容，会有不同的结果，例如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">输入 1 到 4 之间的数字:</span><br><span class="line">你输入的数字为:</span><br><span class="line">3</span><br><span class="line">你选择了 3</span><br></pre></td></tr></table></figure><h3 id="跳出循环"><a href="#跳出循环" class="headerlink" title="跳出循环"></a>跳出循环</h3><p>在循环过程中，有时候需要在未达到循环结束条件时强制跳出循环，Shell使用两个命令来实现该功能：break和continue。</p><h4 id="break命令"><a href="#break命令" class="headerlink" title="break命令"></a>break命令</h4><p>break命令允许跳出所有循环（终止执行后面的所有循环）。</p><p>下面的例子中，脚本进入死循环直至用户输入数字大于5。要跳出这个循环，返回到shell提示符下，需要使用break命令。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">while :</span><br><span class="line">do</span><br><span class="line">    echo -n "输入 1 到 5 之间的数字:"</span><br><span class="line">    read aNum</span><br><span class="line">    case $aNum in</span><br><span class="line">        1|2|3|4|5) echo "你输入的数字为 $aNum!"</span><br><span class="line">        ;;</span><br><span class="line">        *) echo "你输入的数字不是 1 到 5 之间的! 游戏结束"</span><br><span class="line">            break</span><br><span class="line">        ;;</span><br><span class="line">    esac</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>执行以上代码，输出结果为：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">输入 1 到 5 之间的数字:3</span><br><span class="line">你输入的数字为 3!</span><br><span class="line">输入 1 到 5 之间的数字:7</span><br><span class="line">你输入的数字不是 1 到 5 之间的! 游戏结束</span><br></pre></td></tr></table></figure><h4 id="continue"><a href="#continue" class="headerlink" title="continue"></a>continue</h4><p>continue命令与break命令类似，只有一点差别，它不会跳出所有循环，仅仅跳出当前循环。</p><p>对上面的例子进行修改：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">while :</span><br><span class="line">do</span><br><span class="line">    echo -n "输入 1 到 5 之间的数字: "</span><br><span class="line">    read aNum</span><br><span class="line">    case $aNum in</span><br><span class="line">        1|2|3|4|5) echo "你输入的数字为 $aNum!"</span><br><span class="line">        ;;</span><br><span class="line">        *) echo "你输入的数字不是 1 到 5 之间的!"</span><br><span class="line">            continue</span><br><span class="line">            echo "游戏结束"</span><br><span class="line">        ;;</span><br><span class="line">    esac</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>运行代码发现，当输入大于5的数字时，该例中的循环不会结束，语句 <strong>echo “游戏结束”</strong> 永远不会被执行。</p><hr><h4 id="esac"><a href="#esac" class="headerlink" title="esac"></a>esac</h4><p>case的语法和C family语言差别很大，它需要一个esac（就是case反过来）作为结束标记，每个case分支用右圆括号，用两个分号表示break。</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的printf命令</title>
      <link href="/2018/09/28/linux/shell/7/"/>
      <url>/2018/09/28/linux/shell/7/</url>
      
        <content type="html"><![CDATA[<p>printf 命令模仿 C 程序库（library）里的 printf() 程序。</p><p>printf 由 POSIX 标准所定义，因此使用 printf 的脚本比使用 echo 移植性好。</p><p>printf 使用引用文本或空格分隔的参数，外面可以在 printf 中使用格式化字符串，还可以制定字符串的宽度、左右对齐方式等。默认 printf 不会像 echo 自动添加换行符，我们可以手动添加 \n。</p><p>printf 命令的语法：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">printf  format-string  [arguments...]</span><br></pre></td></tr></table></figure><p><strong>参数说明：</strong></p><ul><li><strong>format-string:</strong> 为格式控制字符串</li><li><strong>arguments:</strong> 为参数列表。</li></ul><p>实例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="string">"Hello, Shell"</span></span></span><br><span class="line">Hello, Shell</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">printf</span> <span class="string">"Hello, Shell\n"</span></span></span><br><span class="line">Hello, Shell</span><br><span class="line"><span class="meta">$</span></span><br></pre></td></tr></table></figure><p>接下来,我来用一个脚本来体现printf的强大功能：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">printf "%-10s %-8s %-4s\n" 姓名 性别 体重kg  </span><br><span class="line">printf "%-10s %-8s %-4.2f\n" 郭靖 男 66.1234 </span><br><span class="line">printf "%-10s %-8s %-4.2f\n" 杨过 男 48.6543 </span><br><span class="line">printf "%-10s %-8s %-4.2f\n" 郭芙 女 47.9876</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">姓名     性别   体重kg</span><br><span class="line">郭靖     男      66.12</span><br><span class="line">杨过     男      48.65</span><br><span class="line">郭芙     女      47.99</span><br></pre></td></tr></table></figure><p>%s %c %d %f都是格式替代符</p><p>%-10s 指一个宽度为10个字符（-表示左对齐，没有则表示右对齐），任何字符都会被显示在10个字符宽的字符内，如果不足则自动以空格填充，超过也会将内容全部显示出来。</p><p>%-4.2f 指格式化为小数，其中.2指保留2位小数。</p><p>更多实例：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> format-string为双引号</span></span><br><span class="line">printf "%d %s\n" 1 "abc"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 单引号与双引号效果一样 </span></span><br><span class="line">printf '%d %s\n' 1 "abc" </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 没有引号也可以输出</span></span><br><span class="line">printf %s abcdef</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 格式只指定了一个参数，但多出的参数仍然会按照该格式输出，format-string 被重用</span></span><br><span class="line">printf %s abc def</span><br><span class="line"></span><br><span class="line">printf "%s\n" abc def</span><br><span class="line"></span><br><span class="line">printf "%s %s %s\n" a b c d e f g h i j</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果没有 arguments，那么 %s 用NULL代替，%d 用 0 代替</span></span><br><span class="line">printf "%s and %d \n"</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1 abc</span><br><span class="line">1 abc</span><br><span class="line">abcdefabcdefabc</span><br><span class="line">def</span><br><span class="line">a b c</span><br><span class="line">d e f</span><br><span class="line">g h i</span><br><span class="line">j  </span><br><span class="line"> and 0</span><br></pre></td></tr></table></figure><h3 id="printf的转义序列"><a href="#printf的转义序列" class="headerlink" title="printf的转义序列"></a>printf的转义序列</h3><table><thead><tr><th align="left">序列</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">\a</td><td align="left">警告字符，通常为ASCII的BEL字符</td></tr><tr><td align="left">\b</td><td align="left">后退</td></tr><tr><td align="left">\c</td><td align="left">抑制（不显示）输出结果中任何结尾的换行字符（只在%b格式指示符控制下的参数字符串中有效），而且，任何留在参数里的字符、任何接下来的参数以及任何留在格式字符串中的字符，都被忽略</td></tr><tr><td align="left">\f</td><td align="left">换页（formfeed）</td></tr><tr><td align="left">\n</td><td align="left">换行</td></tr><tr><td align="left">\r</td><td align="left">回车（Carriage return）</td></tr><tr><td align="left">\t</td><td align="left">水平制表符</td></tr><tr><td align="left">\v</td><td align="left">垂直制表符</td></tr><tr><td align="left">\</td><td align="left">一个字面上的反斜杠字符</td></tr><tr><td align="left">\ddd</td><td align="left">表示1到3位数八进制值的字符。仅在格式字符串中有效</td></tr><tr><td align="left">\0ddd</td><td align="left">表示1到3位的八进制值字符</td></tr></tbody></table><p>实例:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">printf</span> <span class="string">"a string, no processing:&lt;%s&gt;\n"</span> <span class="string">"A\nB"</span></span></span><br><span class="line">a string, no processing:&lt;A\nB&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">printf</span> <span class="string">"a string, no processing:&lt;%b&gt;\n"</span> <span class="string">"A\nB"</span></span></span><br><span class="line">a string, no processing:&lt;A</span><br><span class="line"><span class="meta">B&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">printf</span> <span class="string">"www.runoob.com \a"</span></span></span><br><span class="line">www.runoob.com $                  #不换行</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的echo命令</title>
      <link href="/2018/09/27/linux/shell/6/"/>
      <url>/2018/09/27/linux/shell/6/</url>
      
        <content type="html"><![CDATA[<p>Shell 的 echo 指令与 PHP 的 echo 指令类似，都是用于字符串的输出。命令格式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo string</span><br></pre></td></tr></table></figure><p>您可以使用echo实现更复杂的输出格式控制。</p><h3 id="显示普通字符串"><a href="#显示普通字符串" class="headerlink" title="显示普通字符串:"></a>显示普通字符串:</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "It is a test"</span><br></pre></td></tr></table></figure><p>这里的双引号完全可以省略，以下命令与上面实例效果一致：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo It is a test</span><br></pre></td></tr></table></figure><h3 id="显示转义字符"><a href="#显示转义字符" class="headerlink" title="显示转义字符"></a>显示转义字符</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "\"It is a test\""</span><br></pre></td></tr></table></figure><p>结果将是:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">"It is a test"</span><br></pre></td></tr></table></figure><p>同样，双引号也可以省略</p><h3 id="显示变量"><a href="#显示变量" class="headerlink" title="显示变量"></a>显示变量</h3><p>read 命令从标准输入中读取一行,并把输入行的每个字段的值指定给 shell 变量</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line">read name </span><br><span class="line">echo "$name It is a test"</span><br></pre></td></tr></table></figure><p>以上代码保存为 test.sh，name 接收标准输入的变量，结果将是:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@www ~]# sh test.sh</span><br><span class="line">OK                     #标准输入</span><br><span class="line">OK It is a test        #输出</span><br></pre></td></tr></table></figure><h3 id="显示换行"><a href="#显示换行" class="headerlink" title="显示换行"></a>显示换行</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo -e "OK! \n" # -e 开启转义</span><br><span class="line">echo "It is a test"</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">OK!</span><br><span class="line"></span><br><span class="line">It is a test</span><br></pre></td></tr></table></figure><h3 id="显示不换行"><a href="#显示不换行" class="headerlink" title="显示不换行"></a>显示不换行</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line">echo -e "OK! \c" # -e 开启转义 \c 不换行</span><br><span class="line">echo "It is a test"</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">OK! It is a test</span><br></pre></td></tr></table></figure><h3 id="显示结果定向至文件"><a href="#显示结果定向至文件" class="headerlink" title="显示结果定向至文件"></a>显示结果定向至文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "It is a test" &gt; myfile</span><br></pre></td></tr></table></figure><h3 id="原样输出字符串，不进行转义或取变量-用单引号"><a href="#原样输出字符串，不进行转义或取变量-用单引号" class="headerlink" title="原样输出字符串，不进行转义或取变量(用单引号)"></a>原样输出字符串，不进行转义或取变量(用单引号)</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo '$name\"'</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">name\"</span></span><br></pre></td></tr></table></figure><h3 id="显示命令执行结果"><a href="#显示命令执行结果" class="headerlink" title="显示命令执行结果"></a>显示命令执行结果</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo `date`</span><br></pre></td></tr></table></figure><p><strong>注意：</strong> 这里使用的是反引号 <strong>`</strong>, 而不是单引号 <strong>‘</strong>。</p><p>结果将显示当前日期</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Thu Jul 24 10:08:46 CST 2014</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的运算符</title>
      <link href="/2018/09/26/linux/shell/5/"/>
      <url>/2018/09/26/linux/shell/5/</url>
      
        <content type="html"><![CDATA[<p>Shell 和其他编程语言一样，支持多种运算符，包括：</p><ul><li>算数运算符</li><li>关系运算符</li><li>布尔运算符</li><li>字符串运算符</li><li>文件测试运算符</li></ul><p>原生bash不支持简单的数学运算，但是可以通过其他命令来实现，例如 awk 和 expr，expr 最常用。</p><p>expr 是一款表达式计算工具，使用它能完成表达式的求值操作。</p><p>例如，两个数相加(注意使用的是反引号 ` 而不是单引号 ‘)：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">val=`expr 2 + 2`</span><br><span class="line">echo &quot;两数之和为 : $val&quot;</span><br></pre></td></tr></table></figure><p>运行实例</p><p>执行脚本，输出结果如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">两数之和为 : 4</span><br></pre></td></tr></table></figure><p>两点注意：</p><ul><li>表达式和运算符之间要有空格，例如 2+2 是不对的，必须写成 2 + 2，这与我们熟悉的大多数编程语言不一样。</li><li>完整的表达式要被 <code></code> 包含，注意这个字符不是常用的单引号，在 Esc 键下边。</li></ul><h3 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h3><p>下表列出了常用的算术运算符，假定变量 a 为 10，变量 b 为 20：</p><table><thead><tr><th align="left">运算符</th><th align="left">说明</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">+</td><td align="left">加法</td><td align="left"><code>expr $a + $b</code> 结果为 30。</td></tr><tr><td align="left">-</td><td align="left">减法</td><td align="left"><code>expr $a - $b</code> 结果为 -10。</td></tr><tr><td align="left">*</td><td align="left">乘法</td><td align="left"><code>expr $a * $b</code> 结果为  200。</td></tr><tr><td align="left">/</td><td align="left">除法</td><td align="left"><code>expr $b / $a</code> 结果为 2。</td></tr><tr><td align="left">%</td><td align="left">取余</td><td align="left"><code>expr $b % $a</code> 结果为 0。</td></tr><tr><td align="left">=</td><td align="left">赋值</td><td align="left">a=$b 将把变量 b 的值赋给 a。</td></tr><tr><td align="left">==</td><td align="left">相等。用于比较两个数字，相同则返回 true。</td><td align="left">[ $a == $b ] 返回 false。</td></tr><tr><td align="left">!=</td><td align="left">不相等。用于比较两个数字，不相同则返回 true。</td><td align="left">[ $a != $b ] 返回 true。</td></tr></tbody></table><p>注意：条件表达式要放在方括号之间，并且要有空格，例如: [$a==$b] 是错误的，必须写成 [ $a == $b ]。</p><p>算术运算符实例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a=10</span><br><span class="line">b=20</span><br><span class="line"></span><br><span class="line">val=`expr $a + $b`</span><br><span class="line">echo "a + b : $val"</span><br><span class="line"></span><br><span class="line">val=`expr $a - $b`</span><br><span class="line">echo "a - b : $val"</span><br><span class="line"></span><br><span class="line">val=`expr $a * $b`</span><br><span class="line">echo "a * b : $val"</span><br><span class="line"></span><br><span class="line">val=`expr $b / $a`</span><br><span class="line">echo "b / a : $val"</span><br><span class="line"></span><br><span class="line">val=`expr $b % $a`</span><br><span class="line">echo "b % a : $val"</span><br><span class="line"></span><br><span class="line">if [ $a == $b ]</span><br><span class="line">then</span><br><span class="line">  echo "a 等于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a != $b ]</span><br><span class="line">then</span><br><span class="line">  echo "a 不等于 b"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a + b : 30</span><br><span class="line">a - b : -10</span><br><span class="line">a * b : 200</span><br><span class="line">b / a : 2</span><br><span class="line">b % a : 0</span><br><span class="line">a 不等于 b</span><br></pre></td></tr></table></figure><p>注意：</p><ul><li>乘号()前边必须加反斜杠()才能实现乘法运算；</li><li>if…then…fi 是条件语句，后续将会讲解。</li><li>在 MAC 中 shell 的 expr 语法是：$((表达式))，此处表达式中的 “” 不需要转义符号 “&quot; 。</li></ul><h3 id="关系运算符"><a href="#关系运算符" class="headerlink" title="关系运算符"></a>关系运算符</h3><p>关系运算符只支持数字，不支持字符串，除非字符串的值是数字。</p><p>下表列出了常用的关系运算符，假定变量 a 为 10，变量 b 为 20：</p><table><thead><tr><th align="left">运算符</th><th align="left">说明</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">-eq</td><td align="left">检测两个数是否相等，相等返回 true。</td><td align="left">[ $a -eq $b ] 返回 false。</td></tr><tr><td align="left">-ne</td><td align="left">检测两个数是否不相等，不相等返回 true。</td><td align="left">[ $a -ne $b ] 返回 true。</td></tr><tr><td align="left">-gt</td><td align="left">检测左边的数是否大于右边的，如果是，则返回 true。</td><td align="left">[ $a -gt $b ] 返回 false。</td></tr><tr><td align="left">-lt</td><td align="left">检测左边的数是否小于右边的，如果是，则返回 true。</td><td align="left">[ $a -lt $b ] 返回 true。</td></tr><tr><td align="left">-ge</td><td align="left">检测左边的数是否大于等于右边的，如果是，则返回 true。</td><td align="left">[ $a -ge $b ] 返回 false。</td></tr><tr><td align="left">-le</td><td align="left">检测左边的数是否小于等于右边的，如果是，则返回 true。</td><td align="left">[ $a -le $b ] 返回 true。</td></tr></tbody></table><p>关系运算符实例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a=10</span><br><span class="line">b=20</span><br><span class="line"></span><br><span class="line">if [ $a -eq $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a -eq $b : a 等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a -eq $b: a 不等于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a -ne $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a -ne $b: a 不等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a -ne $b : a 等于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a -gt $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a -gt $b: a 大于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a -gt $b: a 不大于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a -lt $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a -lt $b: a 小于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a -lt $b: a 不小于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a -ge $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a -ge $b: a 大于或等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a -ge $b: a 小于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a -le $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a -le $b: a 小于或等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a -le $b: a 大于 b"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">10 -eq 20: a 不等于 b</span><br><span class="line">10 -ne 20: a 不等于 b</span><br><span class="line">10 -gt 20: a 不大于 b</span><br><span class="line">10 -lt 20: a 小于 b</span><br><span class="line">10 -ge 20: a 小于 b</span><br><span class="line">10 -le 20: a 小于或等于 b</span><br></pre></td></tr></table></figure><h3 id="布尔运算符"><a href="#布尔运算符" class="headerlink" title="布尔运算符"></a>布尔运算符</h3><p>下表列出了常用的布尔运算符，假定变量 a 为 10，变量 b 为 20：</p><table><thead><tr><th align="left">运算符</th><th align="left">说明</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">!</td><td align="left">非运算，表达式为 true 则返回 false，否则返回 true。</td><td align="left">[ ! false ] 返回 true。</td></tr><tr><td align="left">-o</td><td align="left">或运算，有一个表达式为 true 则返回 true。</td><td align="left">[ $a -lt 20 -o $b -gt 100 ] 返回 true。</td></tr><tr><td align="left">-a</td><td align="left">与运算，两个表达式都为 true 才返回 true。</td><td align="left">[ $a -lt 20 -a $b -gt 100 ] 返回 false。</td></tr></tbody></table><p>布尔运算符实例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a=10</span><br><span class="line">b=20</span><br><span class="line"></span><br><span class="line">if [ $a != $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a != $b : a 不等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a == $b: a 等于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a -lt 100 -a $b -gt 15 ]</span><br><span class="line">then</span><br><span class="line">  echo "$a 小于 100 且 $b 大于 15 : 返回 true"</span><br><span class="line">else</span><br><span class="line">  echo "$a 小于 100 且 $b 大于 15 : 返回 false"</span><br><span class="line">fi</span><br><span class="line">if [ $a -lt 100 -o $b -gt 100 ]</span><br><span class="line">then</span><br><span class="line">  echo "$a 小于 100 或 $b 大于 100 : 返回 true"</span><br><span class="line">else</span><br><span class="line">  echo "$a 小于 100 或 $b 大于 100 : 返回 false"</span><br><span class="line">fi</span><br><span class="line">if [ $a -lt 5 -o $b -gt 100 ]</span><br><span class="line">then</span><br><span class="line">  echo "$a 小于 5 或 $b 大于 100 : 返回 true"</span><br><span class="line">else</span><br><span class="line">  echo "$a 小于 5 或 $b 大于 100 : 返回 false"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">10 != 20 : a 不等于 b</span><br><span class="line">10 小于 100 且 20 大于 15 : 返回 true</span><br><span class="line">10 小于 100 或 20 大于 100 : 返回 true</span><br><span class="line">10 小于 5 或 20 大于 100 : 返回 false</span><br></pre></td></tr></table></figure><h3 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h3><p>以下介绍 Shell 的逻辑运算符，假定变量 a 为 10，变量 b 为 20:</p><table><thead><tr><th align="left">运算符</th><th align="left">说明</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">&amp;&amp;</td><td align="left">逻辑的 AND</td><td align="left">[[ $a -lt 100 &amp;&amp; $b -gt 100 ]] 返回 false</td></tr><tr><td align="left">||</td><td align="left">逻辑的 OR</td><td align="left">[[ $a -lt 100 || $b -gt 100 ]] 返回 true</td></tr></tbody></table><p>算符实例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a=10</span><br><span class="line">b=20</span><br><span class="line"></span><br><span class="line">if [[ $a -lt 100 &amp;&amp; $b -gt 100 ]]</span><br><span class="line">then</span><br><span class="line">  echo "返回 true"</span><br><span class="line">else</span><br><span class="line">  echo "返回 false"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [[ $a -lt 100 || $b -gt 100 ]]</span><br><span class="line">then</span><br><span class="line">  echo "返回 true"</span><br><span class="line">else</span><br><span class="line">  echo "返回 false"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">返回 false</span><br><span class="line">返回 true</span><br></pre></td></tr></table></figure><h3 id="字符串运算符"><a href="#字符串运算符" class="headerlink" title="字符串运算符"></a>字符串运算符</h3><p>下表列出了常用的字符串运算符，假定变量 a 为 “abc”，变量 b 为 “efg”：</p><table><thead><tr><th align="left">运算符</th><th align="left">说明</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">=</td><td align="left">检测两个字符串是否相等，相等返回 true。</td><td align="left">[ $a = $b ] 返回 false。</td></tr><tr><td align="left">!=</td><td align="left">检测两个字符串是否相等，不相等返回 true。</td><td align="left">[ $a != $b ] 返回 true。</td></tr><tr><td align="left">-z</td><td align="left">检测字符串长度是否为0，为0返回 true。</td><td align="left">[ -z $a ] 返回 false。</td></tr><tr><td align="left">-n</td><td align="left">检测字符串长度是否为0，不为0返回 true。</td><td align="left">[ -n “$a” ] 返回 true。</td></tr><tr><td align="left">$</td><td align="left">检测字符串是否为空，不为空返回 true。</td><td align="left">[ $a ] 返回 true。</td></tr></tbody></table><p>字符串运算符实例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a="abc"</span><br><span class="line">b="efg"</span><br><span class="line"></span><br><span class="line">if [ $a = $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a = $b : a 等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a = $b: a 不等于 b"</span><br><span class="line">fi</span><br><span class="line">if [ $a != $b ]</span><br><span class="line">then</span><br><span class="line">  echo "$a != $b : a 不等于 b"</span><br><span class="line">else</span><br><span class="line">  echo "$a != $b: a 等于 b"</span><br><span class="line">fi</span><br><span class="line">if [ -z $a ]</span><br><span class="line">then</span><br><span class="line">  echo "-z $a : 字符串长度为 0"</span><br><span class="line">else</span><br><span class="line">  echo "-z $a : 字符串长度不为 0"</span><br><span class="line">fi</span><br><span class="line">if [ -n "$a" ]</span><br><span class="line">then</span><br><span class="line">  echo "-n $a : 字符串长度不为 0"</span><br><span class="line">else</span><br><span class="line">  echo "-n $a : 字符串长度为 0"</span><br><span class="line">fi</span><br><span class="line">if [ $a ]</span><br><span class="line">then</span><br><span class="line">  echo "$a : 字符串不为空"</span><br><span class="line">else</span><br><span class="line">  echo "$a : 字符串为空"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">abc = efg: a 不等于 b</span><br><span class="line">abc != efg : a 不等于 b</span><br><span class="line">-z abc : 字符串长度不为 0</span><br><span class="line">-n abc : 字符串长度不为 0</span><br><span class="line">abc : 字符串不为空</span><br></pre></td></tr></table></figure><h3 id="文件测试运算符"><a href="#文件测试运算符" class="headerlink" title="文件测试运算符"></a>文件测试运算符</h3><p>文件测试运算符用于检测 Unix 文件的各种属性。</p><p>属性检测描述如下：</p><table><thead><tr><th align="left">操作符</th><th align="left">说明</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">-b file</td><td align="left">检测文件是否是块设备文件，如果是，则返回 true。</td><td align="left">[ -b $file ] 返回 false。</td></tr><tr><td align="left">-c file</td><td align="left">检测文件是否是字符设备文件，如果是，则返回 true。</td><td align="left">[ -c $file ] 返回 false。</td></tr><tr><td align="left"><code>-d file</code></td><td align="left">检测文件是否是目录，如果是，则返回 true。</td><td align="left">[ -d $file ] 返回 false。</td></tr><tr><td align="left"><code>-f file</code></td><td align="left">检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回 true。</td><td align="left">[ -f $file ] 返回 true。</td></tr><tr><td align="left">-g file</td><td align="left">检测文件是否设置了 SGID 位，如果是，则返回 true。</td><td align="left">[ -g $file ] 返回 false。</td></tr><tr><td align="left">-k file</td><td align="left">检测文件是否设置了粘着位(Sticky Bit)，如果是，则返回 true。</td><td align="left">[ -k $file ] 返回 false。</td></tr><tr><td align="left">-p file</td><td align="left">检测文件是否是有名管道，如果是，则返回 true。</td><td align="left">[ -p $file ] 返回 false。</td></tr><tr><td align="left">-u file</td><td align="left">检测文件是否设置了 SUID 位，如果是，则返回 true。</td><td align="left">[ -u $file ] 返回 false。</td></tr><tr><td align="left"><code>-r file</code></td><td align="left">检测文件是否可读，如果是，则返回 true。</td><td align="left">[ -r $file ] 返回 true。</td></tr><tr><td align="left"><code>-w file</code></td><td align="left">检测文件是否可写，如果是，则返回 true。</td><td align="left">[ -w $file ] 返回 true。</td></tr><tr><td align="left"><code>-x file</code></td><td align="left">检测文件是否可执行，如果是，则返回 true。</td><td align="left">[ -x $file ] 返回 true。</td></tr><tr><td align="left"><code>-s file</code></td><td align="left">检测文件是否为空（文件大小是否大于0），不为空返回 true。</td><td align="left">[ -s $file ] 返回 true。</td></tr><tr><td align="left"><code>-e file</code></td><td align="left">检测文件（包括目录）是否存在，如果是，则返回 true。</td><td align="left">[ -e $file ] 返回 true。</td></tr></tbody></table><p>其他检查符：</p><ul><li>-S: 判断某文件是否 socket。</li><li>-L: 检测文件是否存在并且是一个符号链接。</li></ul><p>变量 file 表示文件 /var/www/runoob/test.sh，它的大小为 100 字节，具有 rwx 权限。下面的代码，将检测该文件的各种属性：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">file="/var/www/runoob/test.sh"</span><br><span class="line">if [ -r $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件可读"</span><br><span class="line">else</span><br><span class="line">  echo "文件不可读"</span><br><span class="line">fi</span><br><span class="line">if [ -w $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件可写"</span><br><span class="line">else</span><br><span class="line">  echo "文件不可写"</span><br><span class="line">fi</span><br><span class="line">if [ -x $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件可执行"</span><br><span class="line">else</span><br><span class="line">  echo "文件不可执行"</span><br><span class="line">fi</span><br><span class="line">if [ -f $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件为普通文件"</span><br><span class="line">else</span><br><span class="line">  echo "文件为特殊文件"</span><br><span class="line">fi</span><br><span class="line">if [ -d $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件是个目录"</span><br><span class="line">else</span><br><span class="line">  echo "文件不是个目录"</span><br><span class="line">fi</span><br><span class="line">if [ -s $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件不为空"</span><br><span class="line">else</span><br><span class="line">  echo "文件为空"</span><br><span class="line">fi</span><br><span class="line">if [ -e $file ]</span><br><span class="line">then</span><br><span class="line">  echo "文件存在"</span><br><span class="line">else</span><br><span class="line">  echo "文件不存在"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">文件可读</span><br><span class="line">文件可写</span><br><span class="line">文件可执行</span><br><span class="line">文件为普通文件</span><br><span class="line">文件不是个目录</span><br><span class="line">文件不为空</span><br><span class="line">文件存在</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的数组</title>
      <link href="/2018/09/25/linux/shell/4/"/>
      <url>/2018/09/25/linux/shell/4/</url>
      
        <content type="html"><![CDATA[<p>数组中可以存放多个值。Bash Shell 只支持一维数组（不支持多维数组），初始化时不需要定义数组大小（与 PHP 类似）。</p><p>与大部分编程语言类似，数组元素的下标由0开始。</p><p>Shell 数组用括号来表示，元素用”空格”符号分割开，语法格式如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">array_name=(value1 ... valuen)</span><br></pre></td></tr></table></figure><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">my_array=(A B "C" D)</span><br></pre></td></tr></table></figure><p>我们也可以使用下标来定义数组:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">array_name[0]=value0</span><br><span class="line">array_name[1]=value1</span><br><span class="line">array_name[2]=value2</span><br></pre></td></tr></table></figure><h3 id="读取数组"><a href="#读取数组" class="headerlink" title="读取数组"></a>读取数组</h3><p>读取数组元素值的一般格式是：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">&#123;array_name[index]&#125;</span></span><br></pre></td></tr></table></figure><h3 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">my_array=(A B "C" D)</span><br><span class="line"></span><br><span class="line">echo "第一个元素为: $&#123;my_array[0]&#125;"</span><br><span class="line">echo "第二个元素为: $&#123;my_array[1]&#125;"</span><br><span class="line">echo "第三个元素为: $&#123;my_array[2]&#125;"</span><br><span class="line">echo "第四个元素为: $&#123;my_array[3]&#125;"</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod +x test.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./test.sh</span></span><br><span class="line">第一个元素为: A</span><br><span class="line">第二个元素为: B</span><br><span class="line">第三个元素为: C</span><br><span class="line">第四个元素为: D</span><br></pre></td></tr></table></figure><h3 id="获取数组中的所有元素"><a href="#获取数组中的所有元素" class="headerlink" title="获取数组中的所有元素"></a>获取数组中的所有元素</h3><p>使用@ 或 * 可以获取数组中的所有元素，例如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">my_array[0]=A</span><br><span class="line">my_array[1]=B</span><br><span class="line">my_array[2]=C</span><br><span class="line">my_array[3]=D</span><br><span class="line"></span><br><span class="line">echo "数组的元素为: $&#123;my_array[*]&#125;"</span><br><span class="line">echo "数组的元素为: $&#123;my_array[@]&#125;"</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod +x test.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./test.sh</span></span><br><span class="line">数组的元素为: A B C D</span><br><span class="line">数组的元素为: A B C D</span><br></pre></td></tr></table></figure><h3 id="获取数组的长度"><a href="#获取数组的长度" class="headerlink" title="获取数组的长度"></a>获取数组的长度</h3><p>获取数组长度的方法与获取字符串长度的方法相同，例如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">my_array[0]=A</span><br><span class="line">my_array[1]=B</span><br><span class="line">my_array[2]=C</span><br><span class="line">my_array[3]=D</span><br><span class="line"></span><br><span class="line">echo "数组元素个数为: $&#123;#my_array[*]&#125;"</span><br><span class="line">echo "数组元素个数为: $&#123;#my_array[@]&#125;"</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod +x test.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./test.sh</span></span><br><span class="line">数组元素个数为: 4</span><br><span class="line">数组元素个数为: 4</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的参数传递</title>
      <link href="/2018/09/24/linux/shell/3/"/>
      <url>/2018/09/24/linux/shell/3/</url>
      
        <content type="html"><![CDATA[<p>我们可以在执行 Shell 脚本时，向脚本传递参数，脚本内获取参数的格式为：<strong>$n</strong>。<strong>n</strong> 代表一个数字，1 为执行脚本的第一个参数，2 为执行脚本的第二个参数，以此类推……</p><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><p>以下实例我们向脚本传递三个参数，并分别输出，其中 <strong>$0</strong> 为执行的文件名：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "Shell 传递参数实例！";</span><br><span class="line">echo "执行的文件名：$0";</span><br><span class="line">echo "第一个参数为：$1";</span><br><span class="line">echo "第二个参数为：$2";</span><br><span class="line">echo "第三个参数为：$3";</span><br></pre></td></tr></table></figure><p>为脚本设置可执行权限，并执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod +x test.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./test.sh 1 2 3</span></span><br><span class="line">Shell 传递参数实例！</span><br><span class="line">执行的文件名：./test.sh</span><br><span class="line">第一个参数为：1</span><br><span class="line">第二个参数为：2</span><br><span class="line">第三个参数为：3</span><br></pre></td></tr></table></figure><p>另外，还有几个特殊字符用来处理参数：</p><table><thead><tr><th align="left">参数处理</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">$#</td><td align="left">传递到脚本的参数个数</td></tr><tr><td align="left">$*</td><td align="left">以一个单字符串显示所有向脚本传递的参数。 如”$*”用「”」括起来的情况、以”$1 $2 … $n”的形式输出所有参数。</td></tr><tr><td align="left">$$</td><td align="left">脚本运行的当前进程ID号</td></tr><tr><td align="left">$!</td><td align="left">后台运行的最后一个进程的ID号</td></tr><tr><td align="left">$@</td><td align="left">与$*相同，但是使用时加引号，并在引号中返回每个参数。 如”$@”用「”」括起来的情况、以”$1” “$2” … “$n” 的形式输出所有参数。</td></tr><tr><td align="left">$-</td><td align="left">显示Shell使用的当前选项，与set命令功能相同。</td></tr><tr><td align="left">$?</td><td align="left">显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误。</td></tr></tbody></table><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "Shell 传递参数实例！";</span><br><span class="line">echo "第一个参数为：$1";</span><br><span class="line"></span><br><span class="line">echo "参数个数为：$#";</span><br><span class="line">echo "传递的参数作为一个字符串显示：$*";</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod +x test.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./test.sh 1 2 3</span></span><br><span class="line">Shell 传递参数实例！</span><br><span class="line">第一个参数为：1</span><br><span class="line">参数个数为：3</span><br><span class="line">传递的参数作为一个字符串显示：1 2 3</span><br></pre></td></tr></table></figure><p>$* 与 $@ 区别：</p><ul><li>相同点：都是引用所有参数。</li><li>不同点：只有在双引号中体现出来。假设在脚本运行时写了三个参数 1、2、3，，则 “ * “ 等价于 “1 2 3”（传递了一个参数），而 “@” 等价于 “1” “2” “3”（传递了三个参数）。</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "-- \$* 演示 ---"</span><br><span class="line">for i in "$*"; do</span><br><span class="line">    echo $i</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">echo "-- \$@ 演示 ---"</span><br><span class="line">for i in "$@"; do</span><br><span class="line">    echo $i</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>执行脚本，输出结果如下所示：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod +x test.sh </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./test.sh 1 2 3</span></span><br><span class="line">-- $* 演示 ---</span><br><span class="line">1 2 3</span><br><span class="line">-- $@ 演示 ---</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的字符串</title>
      <link href="/2018/09/23/linux/shell/2/"/>
      <url>/2018/09/23/linux/shell/2/</url>
      
        <content type="html"><![CDATA[<p>字符串是shell编程中最常用最有用的数据类型（除了数字和字符串，也没啥其它类型好用了），字符串可以用单引号，也可以用双引号，也可以不用引号。单双引号的区别跟PHP类似。</p><h3 id="单引号"><a href="#单引号" class="headerlink" title="单引号"></a>单引号</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">str='this is a string'</span><br></pre></td></tr></table></figure><p>单引号字符串的限制：</p><ul><li>单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的；</li><li>单引号字串中不能出现单独一个的单引号（对单引号使用转义符后也不行），但可成对出现，作为字符串拼接使用。</li></ul><h3 id="双引号"><a href="#双引号" class="headerlink" title="双引号"></a>双引号</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">your_name='runoob'</span><br><span class="line">str="Hello, I know you are \"$your_name\"! \n"</span><br><span class="line">echo -e $str</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Hello, I know you are "runoob"!</span><br></pre></td></tr></table></figure><p>双引号的优点：</p><ul><li>双引号里可以有变量</li><li>双引号里可以出现转义字符</li></ul><h3 id="拼接字符串"><a href="#拼接字符串" class="headerlink" title="拼接字符串"></a>拼接字符串</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">your_name="runoob"</span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用双引号拼接</span></span><br><span class="line">greeting="hello, "$your_name" !"</span><br><span class="line">greeting_1="hello, $&#123;your_name&#125; !"</span><br><span class="line">echo $greeting  $greeting_1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用单引号拼接</span></span><br><span class="line">greeting_2='hello, '$your_name' !'</span><br><span class="line">greeting_3='hello, $&#123;your_name&#125; !'</span><br><span class="line">echo $greeting_2  $greeting_3</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hello, runoob ! hello, runoob !</span><br><span class="line">hello, runoob ! hello, $&#123;your_name&#125; !</span><br></pre></td></tr></table></figure><h3 id="获取字符串长度"><a href="#获取字符串长度" class="headerlink" title="获取字符串长度"></a>获取字符串长度</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">string="abcd"</span><br><span class="line">echo $&#123;#string&#125; #输出 4</span><br></pre></td></tr></table></figure><h3 id="提取子字符串"><a href="#提取子字符串" class="headerlink" title="提取子字符串"></a>提取子字符串</h3><p>以下实例从字符串第 <strong>2</strong> 个字符开始截取 <strong>4</strong> 个字符：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">string="runoob is a great site"</span><br><span class="line">echo $&#123;string:1:4&#125; # 输出 unoo</span><br></pre></td></tr></table></figure><h3 id="查找子字符串"><a href="#查找子字符串" class="headerlink" title="查找子字符串"></a>查找子字符串</h3><p>查找字符 <strong>i</strong> 或 <strong>o</strong> 的位置(哪个字母先出现就计算哪个)：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">string="runoob is a great site"</span><br><span class="line">echo `expr index "$string" io`  # 输出 4</span><br></pre></td></tr></table></figure><p><strong>注意：</strong> 以上脚本中 <strong>`</strong> 是反引号，而不是单引号 <strong>‘</strong>，不要看错了哦。</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell的变量</title>
      <link href="/2018/09/22/linux/shell/1/"/>
      <url>/2018/09/22/linux/shell/1/</url>
      
        <content type="html"><![CDATA[<h3 id="第一个Shell脚本"><a href="#第一个Shell脚本" class="headerlink" title="第一个Shell脚本"></a>第一个Shell脚本</h3><p>打开文本编辑器(可以使用 vi/vim 命令来创建文件)，新建一个文件 test.sh，扩展名为 sh（sh代表shell），扩展名并不影响脚本执行，见名知意就好，如果你用 php 写 shell 脚本，扩展名就用 php 好了。</p><p>输入一些代码，第一行一般是这样：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">echo "Hello World !"</span><br></pre></td></tr></table></figure><p><code>#! 是一个约定的标记，它告诉系统这个脚本需要什么解释器来执行，即使用哪一种 Shell。</code></p><p>echo 命令用于向窗口输出文本。</p><h3 id="运行-Shell-脚本的两种方法"><a href="#运行-Shell-脚本的两种方法" class="headerlink" title="运行 Shell 脚本的两种方法"></a>运行 Shell 脚本的两种方法</h3><ol><li><p>作为可执行程序</p><p>将上面的代码保存为 test.sh，并 cd 到相应目录：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chmod +x ./test.sh  #使脚本具有执行权限</span><br><span class="line">./test.sh  #执行脚本</span><br></pre></td></tr></table></figure><p>注意，一定要写成 <strong>./test.sh</strong>，而不是 <strong>test.sh</strong>，运行其它二进制的程序也一样，直接写 test.sh，linux 系统会去 PATH 里寻找有没有叫 test.sh 的，而只有 /bin, /sbin, /usr/bin，/usr/sbin 等在 PATH 里，你的当前目录通常不在 PATH 里，所以写成 test.sh 是会找不到命令的，要用 ./test.sh 告诉系统说，就在当前目录找。</p></li><li><p>作为解释器参数</p><p>这种运行方式是，直接运行解释器，其参数就是 shell 脚本的文件名，如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/bin/sh test.sh</span><br><span class="line">/bin/php test.php</span><br></pre></td></tr></table></figure><p>这种方式运行的脚本，可以省略第一行指定解释器信息。</p></li></ol><h3 id="Shell-变量"><a href="#Shell-变量" class="headerlink" title="Shell 变量"></a>Shell 变量</h3><p>定义变量时，变量名不加美元符号（$，PHP语言中变量需要），如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">your_name="runoob.com"</span><br></pre></td></tr></table></figure><p>注意，<code>变量名和等号之间不能有空格</code>，这可能和你熟悉的所有编程语言都不一样。同时，变量名的命名须遵循如下规则：</p><ul><li>命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。</li><li>中间不能有空格，可以使用下划线（_）。</li><li>不能使用标点符号。</li><li>不能使用bash里的关键字（可用help命令查看保留关键字）。</li></ul><p>有效的 Shell 变量名示例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">RUNOOB</span><br><span class="line">LD_LIBRARY_PATH</span><br><span class="line">_var</span><br><span class="line">var2</span><br></pre></td></tr></table></figure><p>无效的变量命名：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">?var=123</span><br><span class="line">user*name=runoob</span><br></pre></td></tr></table></figure><p>除了显式地直接赋值，还可以用语句给变量赋值，如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for file in `ls /etc`</span><br><span class="line">或</span><br><span class="line">for file in $(ls /etc)</span><br></pre></td></tr></table></figure><p>以上语句将 /etc 下目录的文件名循环出来。</p><h4 id="使用变量"><a href="#使用变量" class="headerlink" title="使用变量"></a>使用变量</h4><p>使用一个定义过的变量，只要在变量名前面加美元符号即可，如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">your_name="qinjx"</span><br><span class="line">echo $your_name</span><br><span class="line">echo $&#123;your_name&#125;</span><br></pre></td></tr></table></figure><p>变量名外面的花括号是可选的，加不加都行，加花括号是为了帮助解释器识别变量的边界，比如下面这种情况：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for skill in Ada Coffe Action Java; do</span><br><span class="line">    echo "I am good at $&#123;skill&#125;Script"</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>如果不给skill变量加花括号，写成echo “I am good at $skillScript”，解释器就会把$skillScript当成一个变量（其值为空），代码执行结果就不是我们期望的样子了。</p><p>推荐给所有变量加上花括号，这是个好的编程习惯。</p><p>已定义的变量，可以被重新定义，如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">your_name="tom"</span><br><span class="line">echo $your_name</span><br><span class="line">your_name="alibaba"</span><br><span class="line">echo $your_name</span><br></pre></td></tr></table></figure><p>这样写是合法的，但注意，第二次赋值的时候不能写$your_name=”alibaba”，使用变量的时候才加美元符（$）。</p><h4 id="只读变量"><a href="#只读变量" class="headerlink" title="只读变量"></a>只读变量</h4><p>使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。</p><p>下面的例子尝试更改只读变量，结果报错：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">myUrl="http://www.google.com"</span><br><span class="line">readonly myUrl</span><br><span class="line">myUrl="http://www.runoob.com"</span><br></pre></td></tr></table></figure><p>运行脚本，结果如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/bin/sh: NAME: This variable is read only.</span><br></pre></td></tr></table></figure><h4 id="删除变量"><a href="#删除变量" class="headerlink" title="删除变量"></a>删除变量</h4><p>使用 unset 命令可以删除变量。语法：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">unset variable_name</span><br></pre></td></tr></table></figure><p>变量被删除后不能再次使用。unset 命令不能删除只读变量。</p><p>实例: </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line">myUrl="http://www.runoob.com"</span><br><span class="line">unset myUrl</span><br><span class="line">echo $myUrl</span><br></pre></td></tr></table></figure><p>以上实例执行将没有任何输出。</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>后台执行、crontab调度和软连接的使用场景</title>
      <link href="/2018/09/21/linux/5/"/>
      <url>/2018/09/21/linux/5/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 后台执行脚本</li><li>整理 rundeck 视频 部署</li><li>整理 crontab 每隔10s</li><li>整理 软连接 场景 坑</li></ol><h3 id="整理后台执行脚本"><a href="#整理后台执行脚本" class="headerlink" title="整理后台执行脚本"></a>整理后台执行脚本</h3><p>后台执行后命令有三个，分别是：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">./test.sh &amp;</span><br><span class="line">nohup ./test.sh &amp; </span><br><span class="line">nohup ./test.sh &gt; /root/test.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>一般使用第三条</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# nohup ./show.sh &gt;&gt; ./show.log 2&gt;&amp;1 &amp;　　　　#输出重定向</span><br><span class="line">[4] 18637</span><br><span class="line">[root@aliyun ~]# </span><br><span class="line">[root@aliyun ~]# tail -F show.log 　　　　#实时接收输出内容</span><br><span class="line">nohup: ignoring input</span><br><span class="line">Thu Nov 21 17:07:58 CST 2019</span><br><span class="line">Thu Nov 21 17:08:08 CST 2019</span><br><span class="line">Thu Nov 21 17:08:18 CST 2019</span><br><span class="line">Thu Nov 21 17:08:28 CST 2019</span><br><span class="line">Thu Nov 21 17:08:48 CST 2019</span><br></pre></td></tr></table></figure><h3 id="整理-rundeck-视频-部署"><a href="#整理-rundeck-视频-部署" class="headerlink" title="整理 rundeck 视频 部署"></a>整理 rundeck 视频 部署</h3><p><a href="https://www.bilibili.com/video/av35466584?from=search&amp;seid=1197620829255678947" target="_blank" rel="noopener">https://www.bilibili.com/video/av35466584?from=search&amp;seid=1197620829255678947</a></p><h3 id="整理-crontab-每隔10s"><a href="#整理-crontab-每隔10s" class="headerlink" title="整理 crontab 每隔10s"></a>整理 crontab 每隔10s</h3><p>Linux自带的任务调度工具 crontab 的调度单位分别是 分、时、日、周、月 最小的划分粒度是分钟，因此不能解决秒级别的调度问题，</p><p>* 代表每次，如 * / 6 代表每6分钟执行一次</p><p>但是换一种思路，我可以把调度代码包在循环体中，这个循环体执行6次，每次sleep 10s ，加起来就是分钟，即每分钟执行6次，每次间隔10秒</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">for((i=0;i&lt;6;i++));</span><br><span class="line">do</span><br><span class="line">        date</span><br><span class="line">        sleep 10s</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>这样打印出来的结果是间隔10秒</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ./show.sh </span><br><span class="line">Thu Nov 21 17:19:23 CST 2019</span><br><span class="line">Thu Nov 21 17:19:33 CST 2019</span><br><span class="line">Thu Nov 21 17:19:43 CST 2019</span><br><span class="line">Thu Nov 21 17:19:53 CST 2019</span><br><span class="line">Thu Nov 21 17:20:03 CST 2019</span><br><span class="line">Thu Nov 21 17:20:13 CST 2019</span><br></pre></td></tr></table></figure><h3 id="4-整理-软连接-场景-坑"><a href="#4-整理-软连接-场景-坑" class="headerlink" title="4.整理 软连接 场景 坑"></a>4.整理 软连接 场景 坑</h3><p>软连接的使用: ln -s 源文件路径 目标文件路径</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ln -s test.txt test</span><br><span class="line">[root@aliyun ~]# ll</span><br><span class="line">total 24</span><br><span class="line">-rw------- 1 root root       5 Nov 21 17:03 nohup.out</span><br><span class="line">-rw-r--r-- 1 root root      45 Nov 12 23:21 print.sh</span><br><span class="line">-rw-r--r-- 1 root root     196 Nov 21 17:08 show.log</span><br><span class="line">-rwxr--r-- 1 root root      58 Nov 21 17:07 show.sh</span><br><span class="line">drwxr-xr-x 2 root root    4096 Nov 17 10:24 size.log</span><br><span class="line">lrwxrwxrwx 1 root root       8 Nov 21 17:23 test -&gt; test.txt</span><br><span class="line">-rwxr-xr-- 1 root bigdata  198 Nov 18 11:47 test.txt</span><br><span class="line">[root@aliyun ~]#</span><br></pre></td></tr></table></figure><h3 id="使用场景："><a href="#使用场景：" class="headerlink" title="使用场景："></a>使用场景：</h3><ol><li><p>作为源文件的快捷方式存在，好处：升级源文件的时候只需要重新创建软连接，注意：环境变量中不能写源文件的路径，必须写软连接文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun~]# ll</span><br><span class="line">total 5</span><br><span class="line">lrwxrwxrwx 1 root root   8 Nov 21 17:23 mysql -&gt; mysql5.6</span><br><span class="line">drwxr-xr-x 2 root root   6 Nov 20 21:33 mysql5.6  #低版本部署</span><br><span class="line">drwxr-xr-x 2 root root   6 Nov 20 21:33 mysql5.7  #通过软连接来切换升级文件</span><br><span class="line">drwxr-xr-x 3 root root  44 Nov 17 23:13 ruozedata</span><br><span class="line">-rw-r--r-- 1 root root 846 Nov 17 23:12 ruozedata.zip</span><br></pre></td></tr></table></figure></li><li><p>作为数据盘在系统盘中日志写入目录的软连接，好处：日志写入和存储多个文件需要占用大量的磁盘，把日志的存储位置换到了数据盘中</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir /data01/log/  #创建数据盘下的日志目录</span><br><span class="line">mv  /var/log/hbase /data01/log/　　#移动系统盘的日志文件到数据盘</span><br><span class="line">ln -s /data01/log/hbase /var/log/hbase　　#数据盘的日志文件再软连接到系统盘</span><br></pre></td></tr></table></figure></li></ol><p><code>坑</code>：软连接文件创建后的文件和源为文件权限不同，必须注意和修改软连接文件和目标文件的权限</p><p><code>建议</code>：在创建软连接的时候，源文件路径和目标文件路径推介使用绝对路径</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> DataWarehouse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>熟练使用vim、系统命令和程序管理工具</title>
      <link href="/2018/09/20/linux/4/"/>
      <url>/2018/09/20/linux/4/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 vi</li><li>整理 进程 端口号</li><li>整理 连接拒绝 (权限受限)</li><li>整理 高危命令</li><li>常用的 wget yum rpm 压缩</li></ol><h3 id="vim中的常见用法-部分"><a href="#vim中的常见用法-部分" class="headerlink" title="vim中的常见用法(部分)"></a>vim中的常见用法(部分)</h3><table><thead><tr><th>复制</th><th>yy</th></tr></thead><tbody><tr><td>复制多行</td><td>nyy</td></tr><tr><td>当前行向下粘贴</td><td>p</td></tr><tr><td>当前行下上粘贴</td><td>P</td></tr><tr><td>当前位置插入</td><td>i(I)</td></tr><tr><td>当下位置的下一个位置插入</td><td>a(A)</td></tr><tr><td>当前行的下一行插入</td><td>o(O)</td></tr><tr><td>删除当前字符</td><td>x</td></tr><tr><td>删除当前位置到行尾</td><td>D</td></tr><tr><td>删除当前行</td><td>dd</td></tr><tr><td>删除当前行到未行</td><td>dG</td></tr><tr><td>删除n行</td><td>ndd</td></tr><tr><td>删除全部</td><td>gg + dG</td></tr><tr><td>跳转行尾</td><td>Shift + $</td></tr><tr><td>跳转行首</td><td>Shift + ^</td></tr><tr><td>跳转首行</td><td>gg</td></tr><tr><td>跳转未行</td><td>G</td></tr><tr><td>跳转到n行</td><td>:n或者是nG或者是ngg</td></tr><tr><td>撤回一次</td><td>u</td></tr><tr><td>撤回多次</td><td>U</td></tr></tbody></table><p><code>vim编辑中的坑：</code>编辑或者调优配置文件前，一定要备份</p><h3 id="系统命令"><a href="#系统命令" class="headerlink" title="系统命令"></a>系统命令</h3><h4 id="查看磁盘-（df-h）"><a href="#查看磁盘-（df-h）" class="headerlink" title="查看磁盘 （df -h）"></a>查看磁盘 （df -h）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# df -h</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/vda1        40G   11G   27G  29% /</span><br><span class="line">devtmpfs        911M     0  911M   0% /dev</span><br><span class="line">tmpfs           920M     0  920M   0% /dev/shm</span><br><span class="line">tmpfs           920M  332K  920M   1% /run</span><br><span class="line">tmpfs           920M     0  920M   0% /sys/fs/cgroup</span><br><span class="line">tmpfs           184M     0  184M   0% /run/user/0</span><br></pre></td></tr></table></figure><p>了解数据盘的格式：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/dev/vdb1        1T   10G    XXX   X% /data01 数据盘</span><br></pre></td></tr></table></figure><h4 id="查看内存（free-h）"><a href="#查看内存（free-h）" class="headerlink" title="查看内存（free -h）"></a>查看内存（free -h）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# free -h</span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           1.8G        853M         88M        336K        897M        792M</span><br><span class="line">Swap:            0B          0B          0B</span><br></pre></td></tr></table></figure><p>延伸：<a href="http://blog.itpub.net/30089851/viewspace-2131678/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-2131678/</a></p><h4 id="查看负载均衡（top）"><a href="#查看负载均衡（top）" class="headerlink" title="查看负载均衡（top）"></a>查看负载均衡（top）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# top</span><br><span class="line">top - 11:51:16 up 23 days, 12:56,  1 user,  load average: 0.00, 0.01, 0.05</span><br><span class="line">Tasks:  65 total,   1 running,  64 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu(s):  0.0 us,  0.3 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">KiB Mem :  1883724 total,    90012 free,   874596 used,   919116 buff/cache</span><br><span class="line">KiB Swap:        0 total,        0 free,        0 used.   810836 avail Mem </span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND                                       </span><br><span class="line">19516 root       0 -20  126596   9340   6508 S  0.3  0.5  98:38.33 AliYunDun                                     </span><br><span class="line">    1 root      20   0  125124   3344   2112 S  0.0  0.2   0:10.82 systemd                                       </span><br><span class="line">    2 root      20   0       0      0      0 S  0.0  0.0   0:00.00 kthreadd                                      </span><br><span class="line">    3 root      20   0       0      0      0 S  0.0  0.0   0:02.21 ksoftirqd/0                                   </span><br><span class="line">    5 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 kworker/0:0H</span><br></pre></td></tr></table></figure><p>注意：</p><ol><li>负载均衡的数值不能超过10</li><li>如果某服务长期占用cpu或者men，去检查这个进程是在做什么</li><li>如果cpu飙升3000%以上，夯住 ，代码级别，如果不是自己编写的代码，大概率硬件级别–&gt;内存条坏了</li></ol><h4 id="查看进程（ps-ef）"><a href="#查看进程（ps-ef）" class="headerlink" title="查看进程（ps -ef）"></a>查看进程（ps -ef）</h4><p>查看进程常常和 grep 配合使用</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ps -ef|grep ssh</span><br><span class="line">root      4295     1  0 Oct25 ?        00:00:00 /usr/sbin/sshd -D</span><br><span class="line">root     20282  4295  0 11:34 ?        00:00:00 sshd: root@pts/0</span><br><span class="line">root     20318 20284  0 11:56 pts/0    00:00:00 grep --color=auto ssh</span><br></pre></td></tr></table></figure><p>最后一条是自己的进程，可以加上 grep -v grep 去掉这条记录</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ps -ef|grep ssh | grep -v grep进程用户　进程的pid　父id　　　　　　　　　　　　　　　　进程用户的内容(进程所属的目录)</span><br><span class="line">root      4295     1  0 Oct25 ?        00:00:00 /usr/sbin/sshd -D</span><br><span class="line">root     20282  4295  0 11:34 ?        00:00:00 sshd: root@pts/0</span><br></pre></td></tr></table></figure><h4 id="查看端口号（netstat-nlp）"><a href="#查看端口号（netstat-nlp）" class="headerlink" title="查看端口号（netstat -nlp）"></a>查看端口号（netstat -nlp）</h4><p>最常配合查看进程得到的pid号查看端口号</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# netstat -nlp | grep 4295</span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      4295/sshd</span><br></pre></td></tr></table></figure><p><code>注意：</code> 如果查询出来的端口号前面的ip是127.0.0.1或者localhost，属于本地回环ip地址，需要修改相应的配置文件</p><p><code>场景：</code> 在centos部署大数据组件，发现一个错误 Connection refused</p><p>解决思路：</p><ol><li>ping ip 测试ip</li><li>telnet ip port 测试ip和端口号 </li><li>防火墙</li></ol><h4 id="telnet命令安装"><a href="#telnet命令安装" class="headerlink" title="telnet命令安装"></a>telnet命令安装</h4><h5 id="window："><a href="#window：" class="headerlink" title="window："></a>window：</h5><p><img src="https://img2018.cnblogs.com/i-beta/1657732/201911/1657732-20191118121808051-1147850773.png" alt="win10开启telnet命令"></p><h5 id="linux："><a href="#linux：" class="headerlink" title="linux："></a>linux：</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# yum install -y telnet</span><br><span class="line">[root@aliyun ~]# which telnet</span><br><span class="line">/usr/bin/telnet</span><br><span class="line">[root@aliyun ~]# telnet 121.196.220.143 22</span><br><span class="line">Trying 121.196.220.143...</span><br><span class="line">Connected to 121.196.220.143.</span><br><span class="line">Escape character is &apos;^]&apos;.</span><br><span class="line">SSH-2.0-OpenSSH_6.6.1</span><br></pre></td></tr></table></figure><h3 id="三个高危命令"><a href="#三个高危命令" class="headerlink" title="三个高危命令"></a>三个高危命令</h3><table><thead><tr><th>命令</th><th>作用</th></tr></thead><tbody><tr><td><code>rm -rf /</code></td><td>强制无提示删除</td></tr><tr><td><code>vim</code></td><td>编辑生产环境的配置文件不备份</td></tr><tr><td><code>kill -9 $(pgrep -f 匹配关键词)</code></td><td>杀死全部的进程</td></tr></tbody></table><p><code>杀进程之前，先ps 找到相关的进程，搞清楚，哪些是你要杀的，不然造成生产事故</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ps -ef | grep ssh</span><br><span class="line">root      4295     1  0 Oct25 ?        00:00:00 /usr/sbin/sshd -D</span><br><span class="line">root     20282  4295  0 11:34 ?        00:00:00 sshd: root@pts/0</span><br><span class="line">root     20329  4295  0 12:00 ?        00:00:00 sshd: root@pts/1</span><br><span class="line">root     20360  4295  0 12:01 ?        00:00:00 sshd: root@pts/2</span><br><span class="line">root     20380  4295  0 12:02 ?        00:00:00 sshd: root@pts/3</span><br><span class="line">root     20407  4295  0 12:12 ?        00:00:00 sshd: root@pts/4</span><br><span class="line">root     20474  4295  0 12:22 ?        00:00:00 sshd: root@pts/6</span><br><span class="line">root     20502 20476  0 12:30 pts/6    00:00:00 grep --color=auto ssh</span><br><span class="line">[root@aliyun ~]# kill -9 $(pgrep -f ssh)</span><br></pre></td></tr></table></figure><h3 id="常用的程序管理工具"><a href="#常用的程序管理工具" class="headerlink" title="常用的程序管理工具"></a>常用的程序管理工具</h3><h4 id="wget下载安装包"><a href="#wget下载安装包" class="headerlink" title="wget下载安装包"></a>wget下载安装包</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz</span><br></pre></td></tr></table></figure><h4 id="yum包管理"><a href="#yum包管理" class="headerlink" title="yum包管理"></a>yum包管理</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum search xxx</span><br><span class="line">yum install -y xxx</span><br><span class="line">yum remove xxx</span><br></pre></td></tr></table></figure><h4 id="rpm包管理"><a href="#rpm包管理" class="headerlink" title="rpm包管理"></a>rpm包管理</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@aliyun conf]# rpm -qa | grep http　　#查看</span><br><span class="line">httpd-2.4.6-90.el7.centos.x86_64</span><br><span class="line">httpd-tools-2.4.6-90.el7.centos.x86_64</span><br><span class="line">[root@aliyun conf]# rpm -e httpd-tools-2.4.6-90.el7.centos.x86_64　　#卸载失败，有依赖文件</span><br><span class="line">error: Failed dependencies:</span><br><span class="line">        httpd-tools = 2.4.6-90.el7.centos is needed by (installed) httpd-2.4.6-90.el7.centos.x86_64</span><br><span class="line">[root@aliyun conf]# rpm -e  --nodeps     httpd-tools-2.4.6-90.el7.centos.x86_64　　#强制跳过依赖检查</span><br></pre></td></tr></table></figure><h4 id="zip压缩解压"><a href="#zip压缩解压" class="headerlink" title="zip压缩解压"></a>zip压缩解压</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zip -r xxx.zip ./*  　　　　#在文件夹里面压缩文件 　　　　</span><br><span class="line">zip -r test.zip test/* 　　#在文件夹外卖压缩文件夹里面的文件unzip test.zip</span><br></pre></td></tr></table></figure><h4 id="tar压缩解压"><a href="#tar压缩解压" class="headerlink" title="tar压缩解压"></a>tar压缩解压</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -xzvf hadoop-2.6.0-cdh5.16.2.tar.gz　　　　#解压缩.ge的tar包</span><br><span class="line">tar -czvf hadoop-2.6.0-cdh5.16.2.tar.gz  hadoop-2.6.0-cdh5.16.2/*　　#压缩.ge的tar包</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> DataWarehouse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>熟悉Linux权限相关命令</title>
      <link href="/2018/09/20/linux/3/"/>
      <url>/2018/09/20/linux/3/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 用户用户组</li><li>整理 <code>sudo</code>命令</li><li>整理 用户无法登录 <code>passwd</code>文件</li><li>权限 <code>rwx------ chmod chown</code> 案例</li><li>其他命令 <code>- su find du</code>等</li></ol><h3 id="用户和用户组"><a href="#用户和用户组" class="headerlink" title="用户和用户组"></a>用户和用户组</h3><p>针对用户的相关文件在：<code>/usr/sbin/user*</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ll /usr/sbin/user*</span><br><span class="line">-rwxr-x---. 1 root root 118192 Nov  6  2016 /usr/sbin/useradd</span><br><span class="line">-rwxr-x---. 1 root root  80360 Nov  6  2016 /usr/sbin/userdel</span><br><span class="line">-rwxr-x---. 1 root root 113840 Nov  6  2016 /usr/sbin/usermod</span><br><span class="line">-rwsr-xr-x  1 root root  11296 Apr 13  2017 /usr/sbin/usernetctl</span><br></pre></td></tr></table></figure><p>针对用户组的相关文件在：<code>/usr/sbin/group*</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ll /usr/sbin/group*</span><br><span class="line">-rwxr-x---. 1 root root 65480 Nov  6  2016 /usr/sbin/groupadd</span><br><span class="line">-rwxr-x---. 1 root root 57016 Nov  6  2016 /usr/sbin/groupdel</span><br><span class="line">-rwxr-x---. 1 root root 57064 Nov  6  2016 /usr/sbin/groupmems</span><br><span class="line">-rwxr-x---. 1 root root 76424 Nov  6  2016 /usr/sbin/groupmod</span><br></pre></td></tr></table></figure><p>可以打印出<code>PATH</code>路径，就会发现<code>/user/sbin</code>已经被添加在了<code>PATH</code>环境中了，可以从主机的任意位置使用这些命令</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# echo $PATH</span><br><span class="line">/opt/module/jdk1.8.0_144/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin</span><br></pre></td></tr></table></figure><p>需求：</p><p>​        1. 添加<code>hadoop</code>用户</p><p>​        2. 删除<code>hadoop</code>用户</p><p>​        3. 重新创建<code>hadoop</code>用户，模拟用户丢失样式，并修正样式</p><p>​        4. 创建<code>bigdata</code>用户组，并把<code>hadoop</code>用户添加进这个用户组</p><p>​        5. 修改<code>bigdata</code>为<code>hadoop</code>的主组</p><ol><li><p>添加<code>hadoop</code>用户</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# useradd hadoop</span><br><span class="line">[root@aliyun ~]# id hadoop</span><br><span class="line">uid=1000(hadoop) gid=1000(hadoop) groups=1000(hadoop)</span><br></pre></td></tr></table></figure></li><li><p>删除<code>hadoop</code>用户</p><p>使用命令帮助查看 <code>userdel</code> 命令</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# userdel --help</span><br><span class="line">Usage: userdel [options] LOGIN</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  -f, --force                   force some actions that would fail otherwise</span><br><span class="line">                                e.g. removal of user still logged in</span><br><span class="line">                                or files, even if not owned by the user</span><br><span class="line">  -h, --help                    display this help message and exit</span><br><span class="line">  -r, --remove                  remove home directory and mail spool</span><br><span class="line">  -R, --root CHROOT_DIR         directory to chroot into</span><br><span class="line">  -Z, --selinux-user            remove any SELinux user mapping for the user</span><br></pre></td></tr></table></figure><p>会发现 <code>-r</code> 选项是删除家目录</p><p>在这里我们选择删除用户的时候不删除家目录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# userdel hadoop</span><br><span class="line">[root@aliyun ~]# id hadoop</span><br><span class="line">id: hadoop: no such user</span><br><span class="line">[root@aliyun home]# cat /etc/passwd | grep ruoze</span><br><span class="line">[root@aliyun home]# cat /etc/group | grep ruoze</span><br></pre></td></tr></table></figure><p>因为<code>hadoop</code>该组只有<code>hadoop</code>用户，当这个用户删除时，组会校验就他自己，会自动删除</p></li><li><p>重新创建<code>hadoop</code>用户，模拟用户丢失样式，并修正样式</p><p>创建<code>hadoop</code>用户</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# useradd hadoop</span><br><span class="line">useradd: warning: the home directory already exists.</span><br><span class="line">Not copying any file from skel directory into it.</span><br><span class="line">Creating mailbox file: File exists</span><br><span class="line">[root@aliyun ~]# id hadoop</span><br><span class="line">uid=1000(hadoop) gid=1000(hadoop) groups=1000(hadoop)</span><br></pre></td></tr></table></figure><p>模拟用户丢失样式</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ ll -a .bash*</span><br><span class="line">-rw-r--r-- 1 hadoop hadoop  18 Dec  7  2016 .bash_logout</span><br><span class="line">-rw-r--r-- 1 hadoop hadoop 193 Dec  7  2016 .bash_profile</span><br><span class="line">-rw-r--r-- 1 hadoop hadoop 231 Dec  7  2016 .bashrc</span><br><span class="line">[hadoop@aliyun ~]$ rm -rf .bash*</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop　　　　#切换用户</span><br><span class="line">Last login: Sun Nov 17 09:29:10 CST 2019 on pts/0</span><br><span class="line">-bash-4.2$ 　　　#用户样式丢失</span><br></pre></td></tr></table></figure><p>修正样式 (这里只有root权限才可以拷贝)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ll  -a /etc/skel/</span><br><span class="line">total 20</span><br><span class="line">drwxr-xr-x.  2 root root 4096 Aug 18  2017 .</span><br><span class="line">drwxr-xr-x. 81 root root 4096 Nov 17 09:27 ..</span><br><span class="line">-rw-r--r--   1 root root   18 Dec  7  2016 .bash_logout</span><br><span class="line">-rw-r--r--   1 root root  193 Dec  7  2016 .bash_profile</span><br><span class="line">-rw-r--r--   1 root root  231 Dec  7  2016 .bashrc</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cp /etc/skel/ .bash* /home/hadoop/</span><br><span class="line">cp: omitting directory ‘/etc/skel/’</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop　　#样式回来了</span><br><span class="line">Last login: Sun Nov 17 09:33:39 CST 2019 on pts/2</span><br><span class="line">[hadoop@aliyun ~]$</span><br></pre></td></tr></table></figure><p>创建<code>bigdata</code>用户组，并把<code>hadoop</code>用户添加进这个用户组</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# groupadd bigdata</span><br><span class="line">[root@aliyun ~]# usermod -a -G bigdata hadoop</span><br><span class="line">[root@aliyun ~]# id hadoop</span><br><span class="line">uid=1000(hadoop) gid=1000(hadoop) groups=1000(hadoop),1001(bigdata)</span><br></pre></td></tr></table></figure><p><em>20200309更新</em>：<code>mysqladmin</code>的属组一里加入<code>hadoop</code>用户</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">usermod -a -G hadoop mysqladmin</span><br></pre></td></tr></table></figure><p>修改<code>bigdata</code>为<code>hadoop</code>的属组</p><p>查看命令帮助发现有一条命令是改变用户的属组的</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-g, --gid GROUP               force use GROUP as new primary group</span><br><span class="line">[root@aliyun ~]# usermod -g bigdata hadoop　　#强制改变属组</span><br><span class="line">[root@aliyun ~]# id hadoop</span><br><span class="line">uid=1000(hadoop) gid=1001(bigdata) groups=1001(bigdata)</span><br></pre></td></tr></table></figure></li></ol><h3 id="sudo命令"><a href="#sudo命令" class="headerlink" title="sudo命令"></a>sudo命令</h3><p><code>sudo</code>命令是让普通用户具备<code>root</code>用户的权限</p><p>添加普通用户具备<code>root</code>权限的文件是：<code>/etc/sudoers</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">90 ## Allow root to run any commands anywhere </span><br><span class="line">91 root    ALL=(ALL)       ALL</span><br><span class="line">92 hadoop  ALL=(root)      NOPASSWD:ALL　　#新添加的内容</span><br></pre></td></tr></table></figure><h3 id="用户无法登录-修改passwd文件"><a href="#用户无法登录-修改passwd文件" class="headerlink" title="用户无法登录 修改passwd文件"></a>用户无法登录 修改<code>passwd</code>文件</h3><p>在模拟用户无法登陆之前，先说明管理用户信息的文件是：<code>/etc/passwd</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# tail -3  /etc/passwd</span><br><span class="line">redis:x:996:994:Redis Database Server:/var/lib/redis:/sbin/nologin</span><br><span class="line">mysqladmin:x:514:101::/usr/local/mysql:/bin/bash</span><br><span class="line">hadoop:x:1000:1001::/home/hadoop:/bin/bash</span><br></pre></td></tr></table></figure><p>需要注意的是最后一个冒号后面是用户的登陆权限</p><p>需求：</p><p>　　1. 模拟用户的登录权限是<code>/bin/false</code>，修改，并登录</p><p>　　2. 模拟用户的登录权限是<code>/sbin/nologin</code>，修改，并登录</p><p><strong>1. 模拟用户的登录权限是<code>/bin/false</code>，修改，并登录</strong></p><ol><li><p>模拟用户的登录权限是<code>/bin/false</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cat /etc/passwd | grep hadoop</span><br><span class="line">hadoop:x:1000:1001::/home/hadoop:/bin/false</span><br></pre></td></tr></table></figure></li><li><p>尝试登录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop</span><br><span class="line">Last login: Sun Nov 17 09:37:25 CST 2019 on pts/2</span><br><span class="line">[root@aliyun ~]# 　　#登录失败</span><br></pre></td></tr></table></figure></li><li><p>查看用户文件权限，并修改</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cat /etc/passwd | grep 'hadoop'</span><br><span class="line">hadoop:x:1000:1001::/home/hadoop:/bin/bash</span><br></pre></td></tr></table></figure></li><li><p>再次登录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop</span><br><span class="line">Last login: Sun Nov 17 09:56:43 CST 2019 on pts/1</span><br><span class="line">[hadoop@aliyun ~]$     #登录成功</span><br></pre></td></tr></table></figure></li></ol><p><strong>2. 模拟用户的登录权限是<code>/sbin/nologin</code>，修改，并登录</strong></p><ol><li><p>模拟用户的登录权限是<code>/sbin/nologin</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cat /etc/passwd | grep hadoop</span><br><span class="line">hadoop:x:1000:1001::/home/hadoop:/sbin/nologin</span><br></pre></td></tr></table></figure></li><li><p>尝试登录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop</span><br><span class="line">Last login: Sun Nov 17 09:59:35 CST 2019 on pts/1</span><br><span class="line">This account is currently not available.</span><br><span class="line">[root@aliyun ~]# 　　#登录失败</span><br></pre></td></tr></table></figure></li><li><p>查看用户文件权限，并修改</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cat /etc/passwd | grep 'hadoop'</span><br><span class="line">hadoop:x:1000:1001::/home/hadoop:/bin/bash</span><br></pre></td></tr></table></figure></li><li><p>再次登录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop</span><br><span class="line">Last login: Sun Nov 17 09:56:43 CST 2019 on pts/1</span><br><span class="line">[hadoop@aliyun ~]$     #登录成功</span><br></pre></td></tr></table></figure></li></ol><h3 id="rwx-chmod-chown-案例"><a href="#rwx-chmod-chown-案例" class="headerlink" title="rwx------ chmod chown 案例"></a><code>rwx------</code> <code>chmod</code> <code>chown</code> 案例</h3><p>查看文件或者目录的读写执行权限</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ll test.txt</span><br><span class="line">-rw-r--r-- 1 root root 12 Nov 12 23:36 test.txt</span><br><span class="line">r: read  4</span><br><span class="line">w: write 2 </span><br><span class="line">x: 执行  1</span><br><span class="line">-: 没权限 0</span><br></pre></td></tr></table></figure><p>　<code>rw-</code>第一组 6 代表文件或文件夹的用户<code>root</code>，读写<br>　<code>r--</code> 第二组 4 代表文件或文件夹的用户组<code>root</code>，读<br>　<code>r--</code> 第三组 4 代表其他组的所属用户对这个文件或文件夹的权限: 读</p><p><code>chmod</code> 命令用来修改文件或者目录的读写执行权限，加 <code>-R</code> 表示递归修改</p><p><code>chown</code> 命令用来修改文件或者目录的属主和属组，加 <code>-R</code> 表示递归修改</p><p>需求：</p><p>​        1. 修改 <code>test.tx</code>t 文件的属组为<code>bigdata</code></p><p>​        2.  <code>test.txt</code> 文件的权限为属主读写执行，属组读执行</p><ol><li><p>修改 <code>test.txt</code> 文件的属组为<code>bigdata</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# chown -R :bigdata test.txt </span><br><span class="line">[root@aliyun ~]# ll test.txt </span><br><span class="line">-rw-r--r-- 1 root bigdata 12 Nov 12 23:36 test.txt</span><br></pre></td></tr></table></figure></li><li><p>修改 <code>test.txt</code> 文件的权限为属主可读写执行，属组可读执行,其他可读</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# chmod -R 754 test.txt </span><br><span class="line">[root@aliyun ~]# ll test.txt </span><br><span class="line">-rwxr-xr-- 1 root bigdata 12 Nov 12 23:36 test.txt</span><br></pre></td></tr></table></figure><p>注意： <code>-R</code> 参数，目前可认为只有<code>chown</code>和<code>chmod</code>命令有，其他都为 <code>-r</code></p></li></ol><h3 id="其他命令-su-find-du"><a href="#其他命令-su-find-du" class="headerlink" title="其他命令 - su find du"></a>其他命令 - <code>su</code> <code>find</code> <code>du</code></h3><p><u><code>su</code>命令用来切换用户，使用<code>su -</code> 用户名的方式，切换的时候把环境也切换了</u></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# su - hadoop　　#su - 用户名</span><br><span class="line">Last login: Sun Nov 17 10:16:08 CST 2019 on pts/2</span><br><span class="line">[hadoop@aliyun ~]$ pwd</span><br><span class="line">/home/hadoop</span><br><span class="line">[root@aliyun ~]# su hadoop　　#su 用户名</span><br><span class="line">[hadoop@aliyun root]$ pwd</span><br><span class="line">/root</span><br></pre></td></tr></table></figure><p>需要注意 <code>.bash_profile</code> 和 <code>.bashrc</code> 两个文件中的环境生效的区别</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">.bash_profile文件 su ruoze不会执行，su - ruoze 都执行</span><br><span class="line">.bashrc文件       su ruoze执行   ，su - ruoze 都执行</span><br></pre></td></tr></table></figure><p><u><code>find</code>命令用来查找文件，在不确定文件名的情况下使用模糊匹配</u></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun root]$ find /home -name '*hadoop*'</span><br><span class="line">/home/hadoop</span><br></pre></td></tr></table></figure><p><u><code>du</code>命令用来查看文件或者目录大小</u></p><p>虽然 ls -l 也可以查看文件或者目录的大小，但是 <code>ls -l</code> 显示的目录大小并不准确</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# ll -h  size.log/</span><br><span class="line">total 12K</span><br><span class="line">-rw-r--r-- 1 root root 286K Nov 17 10:24 lastlog</span><br></pre></td></tr></table></figure><p>再使用<code>du -sh</code> 查看一次</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# du -sh size.log/</span><br><span class="line">12K    size.log/</span><br></pre></td></tr></table></figure><p>最后进入<code>size.log</code>文件夹查看文件的大小</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun log]# du -sh lastlog</span><br><span class="line">12K    lastlog</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> DataWarehouse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux对环境变量的理解以及alias、rm、hostory的使用</title>
      <link href="/2018/09/19/linux/2/"/>
      <url>/2018/09/19/linux/2/</url>
      
        <content type="html"><![CDATA[<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol><li>整理 全局环境变量 个人环境变量 <code>which</code>的理解</li><li>整理 别名</li><li>整理 删除</li><li>整理 <code>history</code></li></ol><h3 id="全局环境变量"><a href="#全局环境变量" class="headerlink" title="全局环境变量"></a>全局环境变量</h3><p>全局环境变量的配置文件是：<code>/etc/profile</code></p><p>全局环境变量中一般配置的是共用的程序环境 比如<code>java</code></p><p>下面以<code>java</code>为例子配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# vim /etc/profile</span><br></pre></td></tr></table></figure><p>java的安装路径在 <code>/usr/java</code>下，所以文件中如下配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_144</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br></pre></td></tr></table></figure><p>需要注意的是 <code>$PATH</code>接在<code>$JAVA_HOME</code>的后面，即把<code>$JAVA_HOME</code>放在<code>$PATH</code>的最前面</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/opt/module/jdk1.8.0_144/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/module/jdk1.8.0_144/bin:/root/bin</span><br></pre></td></tr></table></figure><h3 id="个人环境变量"><a href="#个人环境变量" class="headerlink" title="个人环境变量"></a>个人环境变量</h3><p>个人环境变量配置在 <code>~/.bashrc</code> 文件中，这里需要注意的是如果配置在 <code>~/.bash_profile</code>文件中，使用<code>ssh</code>远程连接的时候不会加载 <code>~/.bash_profile</code>，造成一些无法排查的<code>bug</code></p><p>个人环境配置一些独自使用的程序变量，如果配置在用户的个人环境中，其他用户无法访问，比如在<code>hadoop</code>用户下配置 <code>hadoop</code>的环境变量，只有<code>hadoop</code>一个用户能使用</p><h3 id="which的理解"><a href="#which的理解" class="headerlink" title="which的理解"></a>which的理解</h3><p>安装完程序或者配置完变量后，最好的习惯是使用<code>which</code>看一下，检查一下环境是否配置正确，否则可能遇到自以为正确的<code>bug</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# which java</span><br><span class="line">/opt/module/jdk1.8.0_144/bin/java</span><br></pre></td></tr></table></figure><h3 id="别名"><a href="#别名" class="headerlink" title="别名"></a>别名</h3><p>别名的使用可以简写冗长且难以记忆或者难以书写的命令</p><p>格式：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alias [-p] [name[=value] ... ]</span><br></pre></td></tr></table></figure><p>例子：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# alias a='ll -a'</span><br><span class="line">[root@aliyun ~]# a</span><br><span class="line">total 80</span><br><span class="line">dr-xr-x---.  6 root root 4096 Nov 15 19:46 .</span><br><span class="line">dr-xr-xr-x. 19 root root 4096 Nov  3 17:29 ..</span><br><span class="line">-rw-------   1 root root 6453 Nov 14 11:16 .bash_history</span><br><span class="line">-rw-r--r--.  1 root root   18 Dec 29  2013 .bash_logout</span><br><span class="line">-rw-r--r--.  1 root root  176 Dec 29  2013 .bash_profile</span><br><span class="line">-rw-r--r--.  1 root root  176 Dec 29  2013 .bashrc</span><br><span class="line">drwx------   3 root root 4096 Aug 18  2017 .cache</span><br></pre></td></tr></table></figure><p>查询主机中已经存在的别名：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# alias</span><br><span class="line">alias cp='cp -i'</span><br><span class="line">alias egrep='egrep --color=auto'</span><br><span class="line">alias fgrep='fgrep --color=auto'</span><br><span class="line">alias grep='grep --color=auto'</span><br><span class="line">alias l.='ls -d .* --color=auto'</span><br><span class="line">alias ll='ls -l --color=auto'</span><br><span class="line">alias ls='ls --color=auto'</span><br><span class="line">alias mv='mv -i'</span><br><span class="line">alias rm='rm -i'</span><br><span class="line">alias which='alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde'</span><br></pre></td></tr></table></figure><p>永久配置别名：</p><p>配置在环境变量中 <code>/etc/profile</code> ，<code>~/.bashrc</code>，<code>~/.bash_profile</code>中，即永久配置别名</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> User specific aliases and <span class="built_in">functions</span></span></span><br><span class="line"></span><br><span class="line">alias rm='rm -i'</span><br><span class="line">alias cp='cp -i'</span><br><span class="line">alias mv='mv -i'</span><br></pre></td></tr></table></figure><h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><table><thead><tr><th>命令</th><th>作用</th></tr></thead><tbody><tr><td><code>mkdir</code></td><td>删除一个空文件</td></tr><tr><td><code>rm -f</code></td><td>直接删除文件</td></tr><tr><td><code>rm -rf</code></td><td>直接删除文件夹</td></tr><tr><td><code>rm -rf</code></td><td>是一个高危的命令</td></tr></tbody></table><p>场景：</p><p><code>shell</code>脚本中，定义变量<code>k = &quot;&quot;</code> 然后<code>rm -rf $k</code> 会默认指定根目录下</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">K=&quot;&quot;rm -rf $K 实际上是rm -rf /</span><br></pre></td></tr></table></figure><p>解决办法是先判断k是否为空</p><h3 id="history"><a href="#history" class="headerlink" title="history"></a>history</h3><p><code>history</code>命令用来查询历史记录</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# history | head -10</span><br><span class="line">    1  passwd </span><br><span class="line">    2  ls</span><br><span class="line">    3  cat /etc/hosts</span><br><span class="line">    4  yum install redis</span><br><span class="line">    5  yum install epel-release</span><br><span class="line">    6  yum install redis</span><br><span class="line">    7  df -hT</span><br><span class="line">    8  service redis start</span><br><span class="line">    9  service redis stop</span><br><span class="line">   10  service redis status</span><br></pre></td></tr></table></figure><p>场景：</p><p>莫名其妙的发现主机中的数据没了，可以查看一下历史记录用了哪些命令</p><p>使用 <code>!n</code> 来快速使用一条历史命令</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# !7</span><br><span class="line">df -hT</span><br><span class="line">Filesystem     Type      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/vda1      ext4       40G   11G   27G  29% /</span><br><span class="line">devtmpfs       devtmpfs  911M     0  911M   0% /dev</span><br><span class="line">tmpfs          tmpfs     920M     0  920M   0% /dev/shm</span><br><span class="line">tmpfs          tmpfs     920M  332K  920M   1% /run</span><br><span class="line">tmpfs          tmpfs     920M     0  920M   0% /sys/fs/cgroup</span><br><span class="line">tmpfs          tmpfs     184M     0  184M   0% /run/user/0</span><br></pre></td></tr></table></figure><p><code>history -c</code> 命令可以清空当前窗口的历史输出命令。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# history -c</span><br><span class="line">[root@aliyun ~]# history</span><br><span class="line">1  history</span><br></pre></td></tr></table></figure><p>但是历史记录实际上保存在 <code>~/.bash_history</code>中的</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cat .bash_history | head -10</span><br><span class="line">passwd </span><br><span class="line">ls</span><br><span class="line">cat /etc/hosts</span><br><span class="line">yum install redis</span><br><span class="line">yum install epel-release</span><br><span class="line">yum install redis</span><br><span class="line">df -hT</span><br><span class="line">service redis start</span><br><span class="line">service redis stop</span><br><span class="line">service redis status</span><br></pre></td></tr></table></figure><p>彻底清空该文件夹的方式为：<code>cat /dell/null &gt; .bash_history</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# cat /dev/null &gt; .bash_history </span><br><span class="line">[root@aliyun ~]# cat .bash_history</span><br><span class="line">[root@aliyun ~]#</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> DataWarehouse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux操作文件和定位错误</title>
      <link href="/2018/09/18/linux/1/"/>
      <url>/2018/09/18/linux/1/</url>
      
        <content type="html"><![CDATA[<h3 id="置空文件的一些坑"><a href="#置空文件的一些坑" class="headerlink" title="置空文件的一些坑"></a>置空文件的一些坑</h3><ol><li><p>最简单的是直接创建一个空文件:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# touch test.txt | ll test.txt</span><br><span class="line">-rw-r--r-- 1 root root 0 Nov 11 22:29 test.txt</span><br><span class="line">[root@aliyun var]#</span><br></pre></td></tr></table></figure></li><li><p>慎用 <code>echo &quot;&quot; &gt; test.txt</code> 这种方式置空文件</p><p>如果我们使用这种方法置空文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# echo "" &gt; test.txt</span><br><span class="line">[root@aliyun var]#  ll -h test.txt</span><br><span class="line">-rw-r--r-- 1 root root 1 Nov 11 22:31 test.txt</span><br></pre></td></tr></table></figure><p>它不是绝对意义上的为空，文件占有一个字节的大小</p></li><li><p>可以更换为<code>cat /dev/null &gt; test.txt</code> 这种方式置空文件</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# cat /dev/null &gt; test.txt | ll -h test.txt</span><br><span class="line">-rw-r--r-- 1 root root 0 Nov 11 22:34 test.txt</span><br></pre></td></tr></table></figure><p>真正意义上把文件置空为0个字节</p></li></ol><h3 id="如何定位ERROR"><a href="#如何定位ERROR" class="headerlink" title="如何定位ERROR"></a>如何定位ERROR</h3><ul><li><p>文件内容很小 几十兆<br>上传给<code>windows</code>，用<code>editplus</code>工具打开，在<code>editplus</code>中搜索，定位</p><p>上传下载  <code>yum install -y lrzsz</code></p></li><li><p>文件内容很大 至少几百兆</p><p> 直接定位到<code>ERROR</code>行</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# cat test.log | grep ERROR</span><br><span class="line">10 ERROR : 模拟错误</span><br></pre></td></tr></table></figure></li></ul><h3 id="使用grep定位REEOR上下文"><a href="#使用grep定位REEOR上下文" class="headerlink" title="使用grep定位REEOR上下文"></a>使用<code>grep</code>定位<code>REEOR</code>上下文</h3><ol><li><p>查看<code>ERROR</code>行的前十行（<code>before</code>）</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# cat test.log | grep -B 10 ERROR</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">9</span><br><span class="line">10 ERROR : 模拟错误</span><br></pre></td></tr></table></figure></li><li><p>查看<code>ERROR</code>行的后十行（<code>after</code>）</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# cat test.log | grep -A 10 ERROR</span><br><span class="line">10 ERROR : 模拟错误</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td></tr></table></figure></li><li><p>查看<code>ERROR</code>行的前后二十行</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun var]# cat test.log | grep -C 10 ERROR</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">9</span><br><span class="line">10 ERROR : 模拟错误</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> DataWarehouse </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
