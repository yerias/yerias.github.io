<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="BigData Developer"><meta name="keywords" content="yerias,TUNANのBlog,BigData"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017 | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">133</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">30</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">26</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">TUNANのBlog</div><div id="site-sub-title">感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017</div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/26/jvm/4/">JVM之内存模型</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-26</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Java/">Java</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Java/">Java</a></span><div class="content"><p>Java内存模型其实就是围绕着在并发过程中如果解决原子性、有序性和可见性的通信规则</p>
<p><img src="https://yerias.github.io/java_img/1.jpg" alt="线程、主内存、工作内存三者之间的关系"></p>
<h2 id="主内存与工作内存"><a href="#主内存与工作内存" class="headerlink" title="主内存与工作内存"></a>主内存与工作内存</h2><p>Java内存模型的主要目的就是定义程序中各种变量的访问规则，即关注在虚拟机中把变量存储到内存和从内存中取出变量这样的底层细节。</p>
<p>此处的变量指的是包括了实例字段，静态字段和构成数组对象的元素。但不包括局部变量和方法参数，因为后者是线程私有的。</p>
<ol>
<li><p>主内存和工作内存的关系？</p>
<p>Java内存模型规定了所有的变量都存储在主内存中，每条线程还有自己的工作内存，线程的工作内存保存了被该线程使用的变量的主内存副本。线程对变量的所有操作(读取、赋值等)都必须在工作内存中进行，而不能直接读写主内存中的数据。不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递需要通过主内存来完成。</p>
</li>
<li><p>主内存和工作内存如何交互？</p>
<p>主内存和工作内存如何交互？即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步回主内存。Java内存模型定义了8种操作来完成。分别是</p>
<ul>
<li>lock(锁定)</li>
<li>unlock(解锁)</li>
<li>read(读取)</li>
<li>load(载入)</li>
<li>use(使用)</li>
<li>assign(赋值)</li>
<li>store(存储)</li>
<li>write(写入)</li>
</ul>
<p>他们每一种操作都是原子的、不可再分的(double和long类型，32位主机会拆分成两次执行)</p>
<p>他们规定了八种执行规则，但这不是我们关心的重点，作为开发者更需要了解的是先行发生原则–用来确定一个操作在并发环境下是否安全</p>
</li>
</ol>
<h2 id="Volatile变量的特殊规则"><a href="#Volatile变量的特殊规则" class="headerlink" title="Volatile变量的特殊规则"></a>Volatile变量的特殊规则</h2><p>Volatile是Java虚拟机提供的最轻量级的<strong>同步机制</strong></p>
<p>为什么说他是最轻量级的同步机制？因为它只能保证可见性、有序性，而不能保证原子性。虽然应用场景有限，但是快，能保证其他线程的立即可见性。</p>
<p>当一个变量被定义成volatile之后，它具备两项特性：</p>
<ol>
<li><p><strong>保证此变量对所有线程的可见性，</strong>这里的可见性是指一条线程修改了这个变量的值，新值对于其他线程来说是立即得知的。由于volatile不能保证原子性(比如运算是分两步来做的)，只能在以下两条规则的场景中进行运算。</p>
<ul>
<li>运算结果不依赖变量的当前值。</li>
<li>变量不需要与其他的状态变量共同参与不变约束。</li>
</ul>
<p>简单点说，就是自己玩自己的，典型的引用场景就是作位状态标志的修饰符。</p>
</li>
<li><p><strong>通过加入内存屏障禁止指令重排序</strong>，指令重排序优化是编辑器做的一种代码执行顺序的优化，只关注结果，不关注过程，但是这么做可能让程序逻辑混乱，比如本来应该在后面执行的代码，跑到前面来执行了，这时候就要使用volatile禁止指令重排序，从而保证代码的执行顺序和程序的顺序相同。</p>
</li>
</ol>
<h2 id="可见性、有序性、原子性"><a href="#可见性、有序性、原子性" class="headerlink" title="可见性、有序性、原子性"></a>可见性、有序性、原子性</h2><ol>
<li><p>原子性</p>
<p>Java内存模型直接保证的原子性变量操作包括read、load、assign、use、store、write这六个，我们大致可以认为，对基本数据类型的访问、读写都是具备原子性的。</p>
<p>对于其他场景的原子性保证，Java内存模型提供了lock和unlock操作满足这种需求，反应到代码中就是synchronized关键字，因此<strong>synchronized是具备原子性的</strong>。</p>
</li>
<li><p>可见性</p>
<p>可见性就是指当一个线程修改了共享变量的值时，其他线程能够立即得知这个修改。</p>
<p><strong>volatile</strong>的特殊规则保证了新值能够立即从工作内存同步到主内存，以及每次使用前从主内存刷新。</p>
<p><strong>synchronized</strong>是通过”对一个变量执行unlock操作之前，必须先把此变量同步回主内存中”实现的。</p>
<p><strong>final</strong>关键字修饰的字段在构造器中初始化完成，并且构造器没有把”this”的引用逃逸出去，那么在其他线程中就能看见final修饰字段的值。</p>
</li>
<li><p>有序性</p>
<p>java程序中天然的有序性可以总结为一句话: 如果在本线程内观察，所有的操作都是有序的；如果在一个线程中观察另一个线程，所有的操作都是无序的。前半句是指 “线程内变量为串行语义”，后半句是指 “指令重排序” 现象和 “工作内存和主内存同步延迟” 现象。</p>
<p><strong>volatile</strong>关键字本身就包含了禁止指令重排序的语义。</p>
<p><strong>synchronized</strong>是由 “一个变量在同一时刻只允许一条线程对其进行lock操作” 这条规则获得的。 </p>
</li>
</ol>
<h2 id="先行发生原则"><a href="#先行发生原则" class="headerlink" title="先行发生原则"></a>先行发生原则</h2><p>先行发生是Java内存模型定义的两项操作之间的顺序关系，如果说操作A先行发生于操作B，其实就是说在发生操作B之前，操作A产生的影响被操作B观察到，”影响” 包括修改了内存中共享变量的值、发生了消息、调用了方法等。</p>
<p>Java内存模型中 “天然的” 先行发生关系包括以下八种，如果两个操作之间的关系不在此列，并且无法从下列规则推导出来，则他们就没有<strong>顺序型保障</strong>，虚拟机可以对它们进行任意的<strong>重排序</strong>。 </p>
<ol>
<li>程序次序规则</li>
<li>管程锁定规则</li>
<li>volatile变量规则</li>
<li>线程启动规则</li>
<li>线程终止规则</li>
<li>线程中断规则</li>
<li>对象终结规则</li>
<li>传递性</li>
</ol>
<ul>
<li>“时间上的先后顺序” 与 “先行发生” 之间有什么不同?</li>
</ul>
<p>时间先后顺序与先行发生原则之间基本没有因果关系，所以我们衡量并发安全问题的时候不要受时间顺序的干扰，一切必须以先行发生原则为准。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/25/spark/4/">Spark之排序模块</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-25</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><p>目录</p>
<ol>
<li>算子排序</li>
<li>面向对象排序</li>
<li>隐式转换排序</li>
<li>Ordering.on排序</li>
</ol>
<p>Spark中的排序模块，顾名思义，这篇文章都是说如何排序</p>
<h2 id="算子排序"><a href="#算子排序" class="headerlink" title="算子排序"></a>算子排序</h2><p>的确，在Spark中有很多算子可以排序，可以给数组排序，可以给键值对排序，我们会使用算子引入排序，然后再重点介绍如何使用隐式转换达到排序的效果。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">(<span class="string">"西瓜 20 100"</span>, <span class="string">"苹果 10 500"</span>, <span class="string">"香蕉 10 30"</span>, <span class="string">"菠萝 30 200"</span>)</span><br></pre></td></tr></table></figure>

<p>现在我们有一行数据，我们如何使它按价格降序排序？</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//TODO List中每个东西的含义：名称name、价格price、库存amount</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="string">"西瓜 20 100"</span>, <span class="string">"苹果 10 500"</span>, <span class="string">"香蕉 10 30"</span>, <span class="string">"菠萝 30 200"</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//拆分map拿出来 ==&gt; 返回tuple(productData) ==&gt; 排序sortBy</span></span><br><span class="line"><span class="keyword">val</span> mapRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    (name, price, amount)</span><br><span class="line">&#125;)</span><br><span class="line">mapRDD.sortBy(-_._2).collect().foreach(println);</span><br></pre></td></tr></table></figure>

<p>也许你会认为很简单，那么现在要求按价格降序排序，价格相同库存降序排序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//拆分 ==&gt; 返回tuple() ==&gt; 排序sortBy(x =&gt; (-x._2,-x._3))</span></span><br><span class="line"><span class="keyword">val</span> mapRDD2 = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    (name, price, amount)</span><br><span class="line">&#125;)</span><br><span class="line">mapRDD2.sortBy(x=&gt;(-x._2,-x._3)).collect().foreach(println);</span><br></pre></td></tr></table></figure>

<p>的确，使用算子排序很简单，但是简洁的代码，能让你回想起几个月前的这几行是什么意思吗？</p>
<h2 id="面向对象排序"><a href="#面向对象排序" class="headerlink" title="面向对象排序"></a>面向对象排序</h2><p>现在我们引入面向对象的方式来排序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//TODO 面向对象的方式实现</span></span><br><span class="line"><span class="keyword">val</span> productRDD: <span class="type">RDD</span>[<span class="type">ProductCaseClass</span>] = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Products</span>(name, price, amount)</span><br><span class="line">&#125;)</span><br><span class="line">productRDD.sortBy(x=&gt;x).collect().foreach(println)</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Products</span>(<span class="params">val name: <span class="type">String</span>, val price: <span class="type">Double</span>, val amount: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">Ordered</span>[<span class="type">Products</span>]</span></span><br><span class="line"><span class="class">    <span class="keyword">with</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = <span class="string">s"<span class="subst">$name</span> | <span class="subst">$price</span> | <span class="subst">$amount</span>"</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">Products</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        that.price.toInt-<span class="keyword">this</span>.price.toInt</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>你是不是傻？case class 自己实现了序列化，实现了toString、equals、hashCode，用起来还不需要new，创建class干啥？</p>
<p>于是再次改造</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> productRDD: <span class="type">RDD</span>[<span class="type">ProductCaseClass</span>] = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    <span class="type">ProductCaseClass</span>(name, price, amount)</span><br><span class="line">&#125;)</span><br><span class="line">productRDD.sortBy(x=&gt;x).collect().foreach(println)</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ProductCaseClass</span>(<span class="params">name:<span class="type">String</span>,price:<span class="type">Double</span>,amount:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Ordered</span>[<span class="type">ProductCaseClass</span>]</span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> =  <span class="string">s"case class:  <span class="subst">$name</span> | <span class="subst">$price</span> | <span class="subst">$amount</span>"</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">ProductCaseClass</span>): <span class="type">Int</span> = that.price.toInt - <span class="keyword">this</span>.price.toInt</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="隐式转换排序"><a href="#隐式转换排序" class="headerlink" title="隐式转换排序"></a>隐式转换排序</h2><p>于是呼~ 不爽，现在只给你一个最普通的类，给我增强出带排序功能的类。这不是隐式转换吗？</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//一个最普通的类,不实现ordered排序的功能</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductsInfo</span>(<span class="params">val name: <span class="type">String</span>, val price: <span class="type">Double</span>, val amount: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = <span class="string">s"ProductsInfo:  <span class="subst">$name</span> | <span class="subst">$price</span> | <span class="subst">$amount</span>"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>那么我们该如何改造它？</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> productRDD: <span class="type">RDD</span>[<span class="type">ProductsInfo</span>] = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ProductsInfo</span>(name, price, amount)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p>我就把数据切分出来，返回一个<code>ProductsInfo</code>，其他啥都不干</p>
<p>现在最重要的就是实现隐式转换，将ProductsInfo增加排序的功能，排序的规则还要自定义</p>
<ol>
<li><p>隐式方法/转换 </p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">ProductsInfo2Orderding</span></span>(productsInfo:<span class="type">ProductsInfo</span>):<span class="type">Ordered</span>[<span class="type">ProductsInfo</span>] = <span class="keyword">new</span> <span class="type">Ordered</span>[<span class="type">ProductsInfo</span>] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">ProductsInfo</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span> (that.price-productsInfo.price&gt;<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="number">1</span></span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (that.price-productsInfo.price==<span class="number">0</span> &amp;&amp; that.amount-productsInfo.amount &gt;<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="number">1</span></span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="number">-1</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>隐式变量</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> <span class="type">ProductsInfo2Orderding</span>:<span class="type">Ordering</span>[<span class="type">ProductsInfo</span>] = <span class="keyword">new</span> <span class="type">Ordering</span>[<span class="type">ProductsInfo</span>] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">ProductsInfo</span>, y: <span class="type">ProductsInfo</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        y.amount - x.amount</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>隐式对象</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">ProductsInfo22Orderding</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[<span class="type">ProductsInfo</span>]</span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">ProductsInfo</span>, y: <span class="type">ProductsInfo</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        y.amount - x.amount</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>这三种方式都可以达到增强<code>ProductsInfo</code>的功能，他们都离不开一个公式：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">x2y</span></span>(普通的x):牛逼的y = <span class="keyword">new</span> 牛逼的y(普通的x)</span><br></pre></td></tr></table></figure>

<p>上面的隐式转换就是这个公式的直接应用</p>
<h2 id="Ordering-on排序"><a href="#Ordering-on排序" class="headerlink" title="Ordering.on排序"></a>Ordering.on排序</h2><p>你也许会说，够了吧，这么多种方式排序了，累都累死人了，那么我告诉有一种方法，不需要case class、不需要class、不需要隐式转换，只有一行代码就能排序，学不学？</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> productRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    (name, price, amount)	<span class="comment">//请注意，我们这里productRDD返回的是一个tuple</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p>实现排序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * (Double,Int) : 定义排序规则的返回值类型,可以是class</span></span><br><span class="line"><span class="comment"> * (String,Double,Int) : 进来数据的类型</span></span><br><span class="line"><span class="comment"> * (x =&gt; (-x._2,-x._3)) : 定义排序的规则</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> order = <span class="type">Ordering</span>[(<span class="type">Double</span>,<span class="type">Int</span>)].on[(<span class="type">String</span>,<span class="type">Double</span>,<span class="type">Int</span>)](x =&gt; (-x._2,-x._3))</span><br></pre></td></tr></table></figure>

<p>是不是看懵逼了？使用<code>productRDD</code>调用<code>sortBy()</code>就输出了排序后的结果</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">productRDD.sortBy(x =&gt; x).print()</span><br></pre></td></tr></table></figure>

<p>结果:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">(菠萝,<span class="number">30.0</span>,<span class="number">200</span>)</span><br><span class="line">(西瓜,<span class="number">20.0</span>,<span class="number">100</span>)</span><br><span class="line">(苹果,<span class="number">10.0</span>,<span class="number">500</span>)</span><br><span class="line">(香蕉,<span class="number">10.0</span>,<span class="number">30</span>)</span><br></pre></td></tr></table></figure>

</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/25/error/3/">执行Hive SQL/MR 报错：Current usage: 77.8mb of 512.0mb physical memory used; 1.1gb of 1.0gb virtual memory used. Killing container.</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-25</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Error/">Error</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Error/">Error</a></span><div class="content"><p>从错误消息中，可以看到使用的虚拟内存超过了当前1.0gb的限制。这可以通过两种方式解决：</p>
<p><strong>禁用虚拟内存限制检查</strong></p>
<p>YARN只会忽略该限制；为此，请将其添加到您的<code>yarn-site.xml</code>：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether virtual memory limits will be enforced for containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>此设置的默认值为<code>true</code>。</p>
<p><strong>增加虚拟内存与物理内存的比率</strong></p>
<p>在<code>yarn-site.xml</code>更改中，此值将高于当前设置</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Ratio between virtual memory to physical memory when setting memory limits for containers. Container allocations are expressed in terms of physical memory, and virtual memory usage is allowed to exceed this allocation by this ratio.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>默认是 <code>2.1</code></p>
<p>还可以增加分配给容器的物理内存量。</p>
<p>注意：更改配置后不要忘记重新启动yarn。</p>
<hr>
<p>如果不能修改集群配置，我们可以参考这么做：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">-Dmapreduce.map.memory.mb=<span class="number">4096</span></span><br></pre></td></tr></table></figure>

<hr>
<p>在运行MR的作业中我们需要关心一下几个参数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">yarn.nodemanager.resource.memory-mb</span><br><span class="line">yarn.scheduler.minimum-allocation-mb</span><br><span class="line">yarn.scheduler.maximum-allocation-mb</span><br><span class="line">yarn.nodemanager.vmem-check-enabled</span><br><span class="line">yarn.nodemanager.vmem-pmem-ratio</span><br></pre></td></tr></table></figure></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/25/error/2/">MapJoin，文件在HDFS上Idea报错：File does not exist: /xxx/yyy.txt#yyy.txt</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-25</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Error/">Error</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Error/">Error</a></span><div class="content"><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.io.FileNotFoundException: File does not exist: /data/dept.txt#dept.txt</span><br></pre></td></tr></table></figure>

<p>先去HDFS上确定文件是否存在，文件不存在，put文件上去，再次运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://192.168.91.10:9000/data/emp.txt</span><br></pre></td></tr></table></figure>

<p>有是Path找不到，再去HDFS上检查这个文件是否存在，文件不存在，再次put上去，然后运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INFO DFSClient: Could not obtain BP-1292531802-192.168.181.10-1583457649867:blk_1073746058_5236 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.91.10:50010,DS-f8088840-5065-4320-8b51-4563ccec125a,DISK] Dead nodes:  DatanodeInfoWithStorage[192.168.91.10:50010,DS-f8088840-5065-4320-8b51-4563ccec125a,DISK]. Will get new block locations from namenode and retry...</span><br><span class="line">WARN DFSClient: DFS chooseDataNode: got # 2 IOException, will wait for 3263.5374223592803 msec.</span><br><span class="line">WARN BlockReaderFactory: I/O error constructing remote block reader.</span><br><span class="line">java.net.ConnectException: Connection timed out: no further information</span><br></pre></td></tr></table></figure>

<p>继续报错，连接不到DataNode的块，去命令行输入jps发现进程没挂，然后我怀疑是因为我的host没有配置hadoop的ip映射关系，于是配上后，继续运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java.lang.Exception: java.io.FileNotFoundException: dept.txt (系统找不到指定的文件。)</span><br><span class="line">Caused by: java.io.FileNotFoundException: dept.txt (系统找不到指定的文件。)</span><br></pre></td></tr></table></figure>

<p>不出所望的再次报错，这次的日志多，上下文分析一下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">WARN FileUtil: Command &apos;G:\hadoop-2.7.2\bin\winutils.exe symlink E:\Java\hadoop\hadoop-client\dept.txt \tmp\hadoop-Tunan\mapred\local\1585106809398\dept.txt&apos; failed 1 with: CreateSymbolicLink error (1314): ???????????</span><br><span class="line">WARN LocalDistributedCacheManager: Failed to create symlink: \tmp\hadoop-Tunan\mapred\local\1585106809398\dept.txt &lt;- E:\Java\hadoop\hadoop-client/dept.txt</span><br></pre></td></tr></table></figure>

<p>由于我们给hdfs上面文件的路径创建了一个链接，也就是symlink，我们发现这个两个警告是symlink创建失败，于是使用管理员的身份启动Idea，解决</p>
<hr>
<p>需要注意是除了管理员身份启动Idea，还需要在在程序的main方法下开启symlink的功能，才能在<code>new FileInputStream(new File(&quot;dept.txt&quot;))</code>中使用</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">FileSystem.enableSymlinks();</span><br></pre></td></tr></table></figure>

<p>还需要注意的是本地操作HDFS的一些配置</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"hadoop"</span>);</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://192.168.91.10:9000"</span>);</span><br><span class="line">conf.set(<span class="string">"dfs.client.use.datanode.hostname"</span>, <span class="string">"true"</span>);</span><br></pre></td></tr></table></figure>

</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/24/spark/2/">Spark之Transformations&amp;Action</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-24</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>Transformations</li>
<li>Action</li>
</ol>
<h2 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h2><p>Transformations的特点是lazy的，和Scala中的lazy该念一致：延迟/懒加载，也就是不会立刻执行，只有等待遇到第一个action才会去提交作业到Spark上</p>
<h3 id="转换算子"><a href="#转换算子" class="headerlink" title="转换算子"></a>转换算子</h3><p><strong>map</strong> 作用到每一个元素</p>
<p>输入:任意类型的函数，输出:泛型U类型的函数，返回RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure>

<p><strong>mapPartitions</strong> 作用到每一个分区</p>
<p>输入:一个可迭代的类型T，输出:一个可迭代的类型U，返回RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure>

<p><strong>mapPartitionsWithIndex</strong> 作用到每一个分区并打印分区数</p>
<p>输入:分区索引，可迭代的类型T，输出:可迭代的类型U，返回RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    f: (<span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure>

<p><strong>glom()</strong> 按分区返回数组</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">glom</span></span>(): <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">T</span>]]</span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD.glom().collect().foreach(f=&gt;f.foreach(x =&gt; println(_)))</span><br></pre></td></tr></table></figure>

<p><strong>filter()</strong> 过滤</p>
<p>输入:输入一个函数T，输出:一个布尔值，返回一个RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Boolean</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<p><strong>sample()</strong> 取样</p>
<p>输入:是否放回的布尔值，抽出来的概率，返回一个RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(</span><br><span class="line">      withReplacement: <span class="type">Boolean</span>,</span><br><span class="line">      fraction: <span class="type">Double</span>,</span><br><span class="line">      seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<p><strong>distinct(x)</strong> 去重 ==&gt; numPartitions可指定分区</p>
<p>输入的必须是RDD，返回的也是一个RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dist: <span class="type">RDD</span>[<span class="type">Int</span>] = listRDD3.distinct()</span><br></pre></td></tr></table></figure>

<p><strong>coalesce(x)</strong> 重点(小文件相关场景大量使用):   ==&gt; reduce数量决定最终输出的文件数，coalesce的作用是减少到指定分区数(x)，减少分区是窄依赖<br>==&gt; Spark作业遇到shuffle 会切分stage<br>输入一个分区数，返回一个重分区后的RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coalesce</span></span>(numPartitions: <span class="type">Int</span>, shuffle: <span class="type">Boolean</span> = <span class="literal">false</span>,</span><br><span class="line">               partitionCoalescer: <span class="type">Option</span>[<span class="type">PartitionCoalescer</span>] = <span class="type">Option</span>.empty)</span><br><span class="line">              (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>)</span><br><span class="line">      : <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD2.coalesce(<span class="number">1</span>).getNumPartitions</span><br></pre></td></tr></table></figure>

<h3 id="双value算子"><a href="#双value算子" class="headerlink" title="双value算子"></a>双value算子</h3><p><strong>zip()</strong> 拉链 ==&gt; 不同分区和不同元素都不能用</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zip</span></span>[<span class="type">A1</span> &gt;: <span class="type">A</span>, <span class="type">B</span>, <span class="type">That</span>](that: <span class="type">GenIterable</span>[<span class="type">B</span>])(<span class="keyword">implicit</span> bf: <span class="type">CanBuildFrom</span>[<span class="type">Repr</span>, (<span class="type">A1</span>, <span class="type">B</span>), <span class="type">That</span>]): <span class="type">That</span></span><br></pre></td></tr></table></figure>

<p><strong>zipWithIndex()</strong> 打印拉链所在的分区</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zipWithIndex</span></span>[<span class="type">A1</span> &gt;: <span class="type">A</span>, <span class="type">That</span>](<span class="keyword">implicit</span> bf: <span class="type">CanBuildFrom</span>[<span class="type">Repr</span>, (<span class="type">A1</span>, <span class="type">Int</span>), <span class="type">That</span>]): <span class="type">That</span></span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> name = <span class="type">List</span>(<span class="string">"张三"</span>, <span class="string">"李四"</span>, <span class="string">"王五"</span>)</span><br><span class="line"><span class="keyword">val</span> age = <span class="type">List</span>(<span class="number">19</span>, <span class="number">26</span>, <span class="number">38</span>)</span><br><span class="line"><span class="keyword">val</span> zipRDD: <span class="type">List</span>[((<span class="type">String</span>, <span class="type">Int</span>), <span class="type">Int</span>)] = name.zip(age).zipWithIndex</span><br></pre></td></tr></table></figure>

<p><strong>union()</strong> 并集 ==&gt; 分区数相加</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">union</span></span>[<span class="type">B</span> &gt;: <span class="type">A</span>, <span class="type">That</span>](that: <span class="type">GenSeq</span>[<span class="type">B</span>])(<span class="keyword">implicit</span> bf: <span class="type">CanBuildFrom</span>[<span class="type">Repr</span>, <span class="type">B</span>, <span class="type">That</span>]): <span class="type">That</span></span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line"><span class="keyword">val</span> list2 = <span class="type">List</span>(<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">8</span>)</span><br><span class="line"><span class="keyword">val</span> ints: <span class="type">List</span>[<span class="type">Int</span>] = list1.union(list2)</span><br></pre></td></tr></table></figure>

<p><strong>intersection()</strong> 交集</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">intersect</span></span>[<span class="type">B</span> &gt;: <span class="type">A</span>](that: <span class="type">GenSeq</span>[<span class="type">B</span>]): <span class="type">Repr</span></span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> inter: <span class="type">List</span>[<span class="type">Int</span>] = list1.intersect(list2)</span><br></pre></td></tr></table></figure>

<p><strong>subtract()</strong> 差集</p>
<p>输入的必须是RDD，返回的也是一个RDD</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def subtract(other: RDD[T]): RDD[T]</span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sub: <span class="type">RDD</span>[<span class="type">Int</span>] = listRDD2.subtract(listRDD3)</span><br></pre></td></tr></table></figure>

<p><strong>cartesian()</strong> 笛卡尔积</p>
<p>输入的必须是RDD，返回的也是一个RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cartesian</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](other: <span class="type">RDD</span>[<span class="type">U</span>]): <span class="type">RDD</span>[(<span class="type">T</span>, <span class="type">U</span>)]</span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> car = listRDD2.cartesian(listRDD3)</span><br></pre></td></tr></table></figure>

<h3 id="kv算子"><a href="#kv算子" class="headerlink" title="kv算子"></a>kv算子</h3><p><strong>mapValues</strong> 得到所有ky的函数</p>
<p>输入:一个函数V，输出:一个值U，返回key为K，value为U的键函数对RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapValues</span></span>[<span class="type">U</span>](f: <span class="type">V</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">U</span>)]</span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapRDD.groupByKey().mapValues(_.sum).print()</span><br></pre></td></tr></table></figure>

<p><strong>sortBy(x)</strong> 降序指定-x，指定任意参数</p>
<p>输入键值对，指定排序的值，默认升序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortBy</span></span>[<span class="type">K</span>](</span><br><span class="line">      f: (<span class="type">T</span>) =&gt; <span class="type">K</span>,</span><br><span class="line">      ascending: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">      numPartitions: <span class="type">Int</span> = <span class="keyword">this</span>.partitions.length)</span><br><span class="line">      (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">K</span>], ctag: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<p><strong>sortByKey(true|false)</strong> 只能根据key排序</p>
<p>默认升序为true，可指定降序为false</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortByKey</span></span>(ascending: <span class="type">Boolean</span> = <span class="literal">true</span>, numPartitions: <span class="type">Int</span> = self.partitions.length)</span><br><span class="line">      : <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapRDD.map(x=&gt;(x._2,x._1)).sortByKey(<span class="literal">false</span>).map(x=&gt;(x._2,x._1)).print()</span><br></pre></td></tr></table></figure>

<p><strong>groupByKey</strong> 返回的kv对中的函数可迭代<br>    ==&gt;每个数据都经过shuffle，到reduce聚合，数据量大</p>
<p>可指定分区数，返回一个PariRDD，包含一个Key和一个可迭代的Value</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapRDD.groupByKey().mapValues(_.sum).print()</span><br></pre></td></tr></table></figure>

<p><strong>reduceByKey()</strong> 对value做指定的操作，直接返回函数<br>    ==&gt;map端有Combiner先进行了一次预聚合操作，减少了网络IO传输的数据量，所以比groupByKey快<br>    ==&gt;groupByKey的shuffle数据量明显多于reduceByKey，所以建议使用reduceByKey</p>
<p>输入两个值，输出一个值，返回一个PariRDD，包含一个明确的Key和一个明确的Value</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure>

<p><strong>join()</strong></p>
<p>两个RDDjoin，返回一个PariRDD包含一个key，两个Value</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> mapRDD2 = sc.parallelize(<span class="type">List</span>((<span class="string">"zhaoliu"</span>, <span class="number">18</span>), (<span class="string">"zhangsan"</span>, <span class="number">22</span>), (<span class="string">"list"</span>, <span class="number">21</span>), (<span class="string">"wangwu"</span>, <span class="number">26</span>)))</span><br><span class="line"><span class="keyword">val</span> mapRDD3 = sc.parallelize(<span class="type">List</span>((<span class="string">"hongqi"</span>, <span class="string">"男"</span>), (<span class="string">"zhangsan"</span>, <span class="string">"男"</span>), (<span class="string">"list"</span>, <span class="string">"女"</span>), (<span class="string">"wangwu"</span>, <span class="string">"男"</span>)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))]</span><br></pre></td></tr></table></figure>

<p><strong>leftOuterJoin</strong></p>
<p>两个RDDjoin，返回一个PariRDD包含一个key，一个确定的左表Value值，一个Option类型的右表Value值，即可能为空</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leftOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">Option</span>[<span class="type">W</span>]))]</span><br></pre></td></tr></table></figure>

<p><strong>rightOuterJoin</strong></p>
<p>两个RDDjoin，返回一个PariRDD包含一个key，一个确定的右表Value值，一个Option类型的左表Value值，即可能为空</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rightOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Option</span>[<span class="type">V</span>], <span class="type">W</span>))]</span><br></pre></td></tr></table></figure>

<p><strong>fullOuterJoin</strong></p>
<p>两个RDDjoin，返回一个PariRDD包含一个key，一个Option类型的右表Value值，一个Option类型的左表Value值，即都可能为空</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fullOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Option</span>[<span class="type">V</span>], <span class="type">Option</span>[<span class="type">W</span>]))]</span><br></pre></td></tr></table></figure>

<p><strong>cogroup</strong> </p>
<p>作用和join类似，不同的是返回的结果是可迭代的，而join返回的是值，原因是join底层调用了cogroup</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W</span>]))]</span><br></pre></td></tr></table></figure>

<p>面试题：Spark Core 不使用distinct去重</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> listRDD3 = sc.parallelize(<span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">listRDD3.map(x=&gt;(x,<span class="literal">null</span>)).reduceByKey((x,y)=&gt;x).map(_._1).print()</span><br></pre></td></tr></table></figure>

<h2 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h2><p><strong>first()</strong></p>
<p>返回第一个元素，等于take(1)</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">first</span></span>(): <span class="type">T</span></span><br></pre></td></tr></table></figure>

<p><strong>take()</strong>    </p>
<p>拿出指定的前N个元素,返回一个数组，结果为原始顺序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">take</span></span>(num: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<p><strong>count()</strong></p>
<p>返回元素数量，是个Long型</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span> = sc.runJob(<span class="keyword">this</span>, <span class="type">Utils</span>.getIteratorSize _).sum</span><br></pre></td></tr></table></figure>

<p><strong>sum</strong> </p>
<p>求和，返回一个Double型</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(): <span class="type">Double</span></span><br></pre></td></tr></table></figure>

<p><strong>max</strong> </p>
<p>返回最大值，结果通过隐式转换排序过</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max</span></span>()(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">T</span></span><br></pre></td></tr></table></figure>

<p><strong>min</strong></p>
<p>返回最小值，结果通过隐式转换排序过</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">min</span></span>()(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">T</span></span><br></pre></td></tr></table></figure>

<p><strong>top()</strong></p>
<p>先排降序再返回前N个元素组成的<strong>数组</strong>，字典序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<p>案例：升序排序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD.top(<span class="number">3</span>)(<span class="type">Ordering</span>.by(x =&gt; -x)).foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>takeOrdered</strong></p>
<p>先排降序再返回N个元素组成的<strong>数组</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeOrdered</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<p>案例：升序排序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD.takeOrdered(<span class="number">3</span>)(<span class="type">Ordering</span>.by(x =&gt; x)).foreach(println)</span><br></pre></td></tr></table></figure>

<p><strong>reduce</strong></p>
<p>聚合，输入两个元素输出一个元素，类型相同</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(f: (<span class="type">T</span>, <span class="type">T</span>) =&gt; <span class="type">T</span>): <span class="type">T</span></span><br></pre></td></tr></table></figure>

<p><strong>foreach</strong></p>
<p>循环输出</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def foreach(f: T =&gt; Unit): Unit</span><br></pre></td></tr></table></figure>

<p><strong>foreachPartition</strong></p>
<p>分区循环输出</p>
<p>输入的是一个可迭代的类型T，输出Unit</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreachPartition</span></span>(f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Unit</span>): <span class="type">Unit</span></span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">listRDD.foreachPartition(x=&gt;x.foreach(println))</span><br></pre></td></tr></table></figure>

<p><strong>countByKey</strong></p>
<p>根据key统计个数，用作<strong>检测数据倾斜</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countByKey</span></span>(): <span class="type">Map</span>[<span class="type">K</span>, <span class="type">Long</span>]</span><br></pre></td></tr></table></figure>

<p><strong>lookup</strong></p>
<p>根据map中的键来取出相应的值的，</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lookup</span></span>(key: <span class="type">K</span>): <span class="type">Seq</span>[<span class="type">V</span>]</span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapRDD.lookup(<span class="string">"zhangsan"</span>).foreach(println)</span><br></pre></td></tr></table></figure>

</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/24/spark/1/">编译Spark&amp;Idea配置Spark环境&amp;RDD五大特点&amp;Spark参数管理&amp;数据的读写</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-24</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>编译Spark</li>
<li>Idea配置Spark环境</li>
<li>RDD五大特点</li>
<li>Spark参数管理</li>
<li>数据的读写</li>
</ol>
<h2 id="编译Spark"><a href="#编译Spark" class="headerlink" title="编译Spark"></a>编译Spark</h2><p>作为一个Spark玩的6的攻城狮，第一步就是要学会如何编译Spark</p>
<ol>
<li><p>下载spark源码: <a href="http://spark.apache.org/" target="_blank" rel="noopener">官网</a>或者<a href="https://github.com/apache/spark" target="_blank" rel="noopener">github</a></p>
</li>
<li><p>查看官网<a href="看官方文档http://spark.apache.org/docs/latest/building-spark.html">编译文档</a>，切记注意版本号，不同版本号编译方式区别很大</p>
</li>
<li><p>修改相关配置</p>
<ol>
<li><p>注释掉make-distribution.sh脚本中的128行左右一下，使用固定的版本替代</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">VERSION=2.4.5</span><br><span class="line">SCALA_VERSION=2.12.10</span><br><span class="line">SPARK_HADOOP_VERSION=2.6.0-cdh5.16.2</span><br><span class="line">SPARK_HIVE=1</span><br></pre></td></tr></table></figure>

<p>替代==&gt;</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">#VERSION=$("$MVN" help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v "INFO"\</span><br><span class="line">#    | grep -v "WARNING"\</span><br><span class="line">#    | tail -n 1)</span><br><span class="line">#SCALA_VERSION=$("$MVN" help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v "INFO"\</span><br><span class="line">#    | grep -v "WARNING"\</span><br><span class="line">#    | tail -n 1)</span><br><span class="line">#SPARK_HADOOP_VERSION=$("$MVN" help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v "INFO"\</span><br><span class="line">#    | grep -v "WARNING"\</span><br><span class="line">#    | tail -n 1)</span><br><span class="line">#140 #SPARK_HIVE=$("$MVN" help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\</span><br><span class="line">#    | grep -v "INFO"\</span><br><span class="line">#    | grep -v "WARNING"\</span><br><span class="line">#    | fgrep --count "<span class="tag">&lt;<span class="name">id</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">id</span>&gt;</span>";\</span><br><span class="line">#    # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\</span><br><span class="line">#    # because we use "set -o pipefail"</span><br><span class="line">#    echo -n)</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改maven仓库地址，在253行左右，pom.xml文件</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">Maven Repository</span><br><span class="line"><span class="comment">&lt;!--&lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt; --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"></span><br><span class="line">加上cdh的下载地址</span><br><span class="line"><span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">releases</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">releases</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>false<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>开始编译</p>
<p>maven编译前执行</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">export MAVEN_OPTS=<span class="string">"-Xmx2g -XX:ReservedCodeCacheSize=1g"</span></span><br></pre></td></tr></table></figure>

<p>make-distribution.sh编译不需要执行，我们这里使用make-distribution.sh编译，它的脚本自动执行了这句</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">./dev/change-scala-version.sh <span class="number">2.12</span></span><br><span class="line">./dev/make-distribution.sh --name <span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.16</span><span class="number">.2</span> --tgz -Pyarn -Phive -Phive-thriftserver -Pscala-<span class="number">2.12</span> -Phadoop-<span class="number">2.6</span> -Dhadoop.version=<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.16</span><span class="number">.2</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>查看生成的jar包</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">/home/hadoop/app/spark-<span class="number">2.4</span><span class="number">.5</span>/spark-<span class="number">2.4</span><span class="number">.5</span>-bin-<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.16</span><span class="number">.2</span>.tgz</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="Idea配置Spark环境"><a href="#Idea配置Spark环境" class="headerlink" title="Idea配置Spark环境"></a>Idea配置Spark环境</h2><ol>
<li><p>idae引入依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.source</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.target</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>2.6.0-cdh5.16.2<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.12.10<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.tools.version</span>&gt;</span>2.12<span class="tag">&lt;/<span class="name">scala.tools.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">spark.version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_$&#123;scala.tools.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>配置阿里云和cdh的仓库地址</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>false<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>central<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>aliyun maven<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">layout</span>&gt;</span>default<span class="tag">&lt;/<span class="name">layout</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 是否开启发布版构件下载 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">releases</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">releases</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 是否开启快照版构件下载 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>false<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="RDD五大特点"><a href="#RDD五大特点" class="headerlink" title="RDD五大特点"></a>RDD五大特点</h2><p>先看官方介绍:</p>
<p>弹性分布式数据集(RDD)，是Spark的基本抽象。表示可以并行操作的不可变的、分区的元素集合。这个类包含所有RDDS上可用的基本操作，如<code>map</code>、<code>filter</code>和<code>filter</code>。</p>
<p><code>[[org.apache.spark.rdd.PairRDDFunctions]]</code>，包含仅在键值对的RDDs上可用的操作，如<code>groupByKey</code> 和<code>join</code> ; </p>
<p><code>[[org.apache.spark.rdd.DoubleRDDFunctions]]</code> ，包含仅在双精度的RDDs上可用的操作;</p>
<p><code>[[org.apache.spark.rdd.SequenceFileRDDFunctions]]</code>，包含可在RDDs上使用的操作，这些操作可以保存为序列文件。</p>
<p>所有的操作都可以通过隐式转换的方式在任何正确类型的RDD上自动使用(例如RDD[(Int, Int)]);</p>
<p>RDD: resilient distributed dataset(弹性分布式数据集)，</p>
<ol>
<li><p>弹性表示容错</p>
</li>
<li><p>分布式表示分区</p>
</li>
<li><p>数据集表示集</p>
</li>
</ol>
<p><strong>每个RDD有五个主要特征:</strong></p>
<ol>
<li>一个RDD由很多partition构成(block块对应partition)，在spark中，有多少partition就对应有多少个task来执行。</li>
<li>对RDD做计算，相当于对RDD的每个partition或split做计算</li>
<li>RDD之间有依赖关系，可溯源，容错机制</li>
<li>如果RDD里面存的数据是key-value形式，则可以进行重新分区</li>
<li>最优位置计算，也就是数据的本地性，移动计算而不是移动数据。</li>
</ol>
<p>RDD是一个顶级的抽象类，它有五个抽象方法，分别实现了这五个特性</p>
<ol>
<li><p>分区</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Implemented by subclasses to return the set of partitions in this RDD. This method will only</span></span><br><span class="line"><span class="comment"> * be called once, so it is safe to implement a time-consuming computation in it.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * The partitions in this array must satisfy the following property:</span></span><br><span class="line"><span class="comment"> *   `rdd.partitions.zipWithIndex.forall &#123; case (partition, index) =&gt; partition.index == index &#125;`</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>计算</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * :: DeveloperApi ::</span></span><br><span class="line"><span class="comment">  * Implemented by subclasses to compute a given partition.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>依赖</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only</span></span><br><span class="line"><span class="comment">  * be called once, so it is safe to implement a time-consuming computation in it.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = deps</span><br></pre></td></tr></table></figure>
</li>
<li><p>重新分区</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/** Optionally overridden by subclasses to specify how they are partitioned. */</span></span><br><span class="line"><span class="meta">@transient</span> <span class="keyword">val</span> partitioner: <span class="type">Option</span>[<span class="type">Partitioner</span>] = <span class="type">None</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>最优位置</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Optionally overridden by subclasses to specify placement preferences.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPreferredLocations</span></span>(split: <span class="type">Partition</span>): <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Nil</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="Spark参数管理"><a href="#Spark参数管理" class="headerlink" title="Spark参数管理"></a>Spark参数管理</h2><ol>
<li><p>如果想要定义自己的参数传递到spark中去，一定要以<code>spark.</code>开头</p>
</li>
<li><p>如果想要获取spark中的参数的值，使用<code>sc.getConf.get(key)</code></p>
</li>
</ol>
<h2 id="数据的读写"><a href="#数据的读写" class="headerlink" title="数据的读写"></a>数据的读写</h2><ol>
<li><p>读本地数据：<br><code>sc.textFile(&quot;file://&quot;)</code> 需要添加<code>file://</code></p>
</li>
<li><p>读hdfs数据：<br><code>sc.textFile(&quot;&quot;)</code>  默认读hdfs 不需要加前缀<br>注意：textFile() 可以使用通配符匹配目录、指定文件、指定文件夹</p>
</li>
<li><p>wholeTextFiles()</p>
<p>如果使用的是wholeTextFiles() ，它会返回路径+内容</p>
</li>
<li><p>写本地</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">rdd.saveAsTestFile(<span class="string">"path"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>写HDFS需要配置</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"hadoop"</span>);</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://192.168.91.10:9000"</span>);</span><br><span class="line">conf.set(<span class="string">"dfs.client.use.datanode.hostname"</span>, <span class="string">"true"</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>压缩写</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">saveAsTestFile(out,classOf[<span class="type">BZip2Codec</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p>对象写</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span>(<span class="params">name:<span class="type">String</span>,age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">p1</span> </span>= (<span class="string">"张三"</span>,<span class="number">18</span>)</span><br><span class="line"><span class="keyword">val</span> p2 = (<span class="string">"李四"</span>,<span class="number">21</span>)</span><br><span class="line">parallelize(<span class="type">List</span>(p1,p2)).saveAsObjectFile(<span class="string">"out"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>对象读</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc.objectFile[<span class="type">Persion</span>](<span class="string">"out"</span>).collect().foreach(println)</span><br></pre></td></tr></table></figure>

</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/23/scala/4/">Scala之闭包&amp;柯里化</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-23</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Scala/">Scala</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Scala/">Scala</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>闭包</li>
<li>方法与函数的区别</li>
<li>柯里化</li>
</ol>
<h2 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h2><p>说到柯里化必先说起闭包，我们先不关心闭包和柯里化是什么，而是看一个transformation</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> init:<span class="type">Int</span> = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> i = list.foldLeft[<span class="type">Int</span>](init)((x,y) =&gt; &#123;</span><br><span class="line">    println(<span class="string">s"init = <span class="subst">$init</span> | x = <span class="subst">$x</span> | y = <span class="subst">$y</span>"</span> )</span><br><span class="line">    x+y</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">println(i)</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">init = <span class="number">10</span> | x = <span class="number">10</span> | y = <span class="number">1</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">11</span> | y = <span class="number">2</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">13</span> | y = <span class="number">3</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">16</span> | y = <span class="number">4</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">20</span> | y = <span class="number">5</span></span><br><span class="line">init = <span class="number">10</span> | x = <span class="number">25</span> | y = <span class="number">6</span></span><br><span class="line"><span class="number">31</span></span><br></pre></td></tr></table></figure>

<p>从结果来看 <code>foldLeft</code> 需要三个参数，<code>init</code>初始值，变量x，变量y，然后将他们累加(不考虑其他运算规则)</p>
<p>现在我们来看看闭包的解释：</p>
<ol>
<li><p>闭包是一个函数，返回值依赖于声明在函数外部的一个或多个变量。</p>
</li>
<li><p>闭包通常来讲可以简单的认为是可以访问一个函数里面局部变量的另外一个函数。</p>
</li>
</ol>
<p><code>val i = list.foldLeft[Int](init)((x,y))</code> 是一个闭包，返回值依赖声明在函数外的<code>init</code></p>
<p>如果我们把<code>foldLeft[Int](init)((x,y))</code> 拆分成两个函数的话，一个是有明确参数引用的 <code>foldLeft[Int](init)</code>， 另外一个是匿名函数<code>((x,y))</code>，如果我们观察仔细的话，会发现上面的代码中，第一次输出的结果 <code>init = x</code>  我们就可以简单的认为匿名函数<code>((x,y))</code>访问了<code>flodLeft</code>中的局部变量<code>init</code> ,事实上，<code>x</code>第一次的值就是拿的<code>init</code> 。</p>
<p>我们现在可以总结闭包就是在函数外面声明了一个变量，在函数中引用了这个变量，就称之为闭包，其他函数可以依赖闭包(函数)中的这个变量。由于闭包是把外部变量包了进来，所以这个变量的生命周期和闭包的生命周期一致。</p>
<p>最后在看一个案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> factor = <span class="number">3</span></span><br><span class="line"><span class="keyword">val</span> multiplier = (i:<span class="type">Int</span>) =&gt; i * factor</span><br><span class="line">println(multiplier(<span class="number">3</span>))</span><br></pre></td></tr></table></figure>

<p>没错，<strong>函数的返回值依赖于声明在函数外部的一个或多个变量就是闭包。</strong></p>
<h2 id="方法与函数的区别"><a href="#方法与函数的区别" class="headerlink" title="方法与函数的区别"></a>方法与函数的区别</h2><p>还需要讲一下方法与函数区别，因为柯里化指的是将原来接受两个参数的方法变成新的接受一个参数的函数的过程。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">method</span> </span>= (x:<span class="type">Int</span>,y:<span class="type">Int</span>) =&gt; x+y</span><br><span class="line">method: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span>	<span class="comment">//方法</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> func = (x:<span class="type">Int</span>,y:<span class="type">Int</span>) =&gt; x+y</span><br><span class="line">func: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1307</span>/<span class="number">1309775952</span>@<span class="number">32</span>a1aabf	<span class="comment">//函数</span></span><br></pre></td></tr></table></figure>

<ol>
<li>首先应该要知道=号右边的内容 <code>(x: Int, y: Int) =&gt; x + y</code>是一个函数体。</li>
<li>方法只能用def修饰，函数可以用def修饰，也可以用val修饰。</li>
<li>当函数用def来接收之后，不再显示为function，转换为方法。</li>
<li>方法可以省略参数，函数不可以。</li>
<li>函数可以作为方法的参数。</li>
</ol>
<h2 id="柯里化"><a href="#柯里化" class="headerlink" title="柯里化"></a>柯里化</h2><p>理解闭包后，我们看一个复杂的闭包案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">addBase</span></span>(x:<span class="type">Int</span>) = (y:<span class="type">Int</span>) =&gt; x+y</span><br><span class="line">addBase: (x: <span class="type">Int</span>)<span class="type">Int</span> =&gt; <span class="type">Int</span></span><br></pre></td></tr></table></figure>

<p>首先我们看到<code>addBase</code>是一个方法，<code>(y:Int) =&gt; x+y</code>是一个函数体，下面我们试试执行<code>addBase(x:Int)</code></p>
<p>会返回什么?</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; addBase(<span class="number">3</span>)</span><br><span class="line">res27: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1308</span>/<span class="number">761477414</span>@<span class="number">553</span>f7b1e</span><br></pre></td></tr></table></figure>

<p>返回了一个函数，现在我们明白了，<code>addBase(x:Int)</code>是一个<strong>方法</strong>，在传入具体的值后，返回了一个具体的<strong>函数</strong></p>
<p>到现在 “=” 号后面的 <code>(y:Int) =&gt; x+y</code> 这段还没有使用到，但是我们知道 “=” 号后面是一个函数体，给函数中传入y 返回 x+y? 我们试试。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> addThree = addBase(<span class="number">3</span>)	<span class="comment">//使用个变量接收函数</span></span><br><span class="line">addThree: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1308</span>/<span class="number">761477414</span>@<span class="number">65e1</span>c98c</span><br><span class="line">scala&gt; addThree(<span class="number">4</span>)	<span class="comment">//传入4</span></span><br><span class="line">res29: <span class="type">Int</span> = <span class="number">7</span>	<span class="comment">//输出7</span></span><br></pre></td></tr></table></figure>

<p>等等。。。这就是一个闭包啊，x=3是外部传入的给函数<code>addBase</code>的，这时的<code>addBase</code>就是函数，它的内部维护了一个变量x=3，这时另外一个函数<code>addThree</code> 在输出x+y前，访问了<code>addBase</code>中的x=3。</p>
<p>完全符合闭包的定义规则，<strong>函数的返回值依赖于声明在函数外部的一个或多个变量就是闭包</strong>。</p>
<p>上面的<code>addBase</code>方法使我们一次传入一个3一次传入一个4，那么有没有办法一次传入呢？</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; addBase(<span class="number">3</span>)(<span class="number">4</span>)</span><br><span class="line">res34: <span class="type">Int</span> = <span class="number">7</span></span><br></pre></td></tr></table></figure>

<p>没错，这就是柯里化，<strong>柯里化指的是将原来接受两个参数的方法变成新的接受一个参数的函数的过程。并且新的函数返回一个以原有第二个参数作为参数的函数。</strong></p>
<p>我们发现了上面闭包的代码其实就是柯里化的过程</p>
<p>现在我们总结下柯里化是什么?</p>
<ol>
<li><p>柯里化指的是将原来接受两个参数的方法变成新的接受一个参数的函数的过程。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">sum1</span></span>(x:<span class="type">Int</span>): <span class="type">Int</span> =&gt; <span class="type">Int</span> = (y:<span class="type">Int</span>)=&gt;x+y</span><br><span class="line">sum1: (x: <span class="type">Int</span>)<span class="type">Int</span> =&gt; <span class="type">Int</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> res1 = sum1(<span class="number">3</span>)</span><br><span class="line">res1: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1321</span>/<span class="number">962341885</span>@<span class="number">41</span>d283de</span><br><span class="line"></span><br><span class="line">scala&gt; res1(<span class="number">4</span>)</span><br><span class="line">res38: <span class="type">Int</span> = <span class="number">7</span></span><br></pre></td></tr></table></figure>

<p>==&gt;上面是接受两个参数，下面是接受一个参数</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">sum2</span></span>(x:<span class="type">Int</span>)(y:<span class="type">Int</span>): <span class="type">Int</span> = x+y</span><br><span class="line">sum2: (x: <span class="type">Int</span>)(y: <span class="type">Int</span>)<span class="type">Int</span></span><br><span class="line"></span><br><span class="line">scala&gt; sum2(<span class="number">2</span>)(<span class="number">3</span>)</span><br><span class="line">res39: <span class="type">Int</span> = <span class="number">5</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>新的函数返回一个以原有第二个参数作为参数的函数。</p>
<p>意思就是原来要分两步做的事情，现在分一步就做好了，底层自动调用和返回。在这个过程中，使用了闭包。</p>
</li>
</ol>
<p>懵逼之余。。。返回最开始的<code>foldLeft</code>案例，看<code>foldLeft</code> 方法源码：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foldLeft</span></span>[<span class="type">B</span>](z: <span class="type">B</span>)(<span class="meta">@deprecatedName</span>(<span class="symbol">'f</span>) op: (<span class="type">B</span>, <span class="type">A</span>) =&gt; <span class="type">B</span>): <span class="type">B</span></span><br></pre></td></tr></table></figure>

<p>这其实就是一个柯里化应用</p>
<ol>
<li><p>折叠操作是一个递归的过程，将上一次的计算结果代入到函数中 </p>
</li>
<li><p>作为结果的参数在<code>foldLeft</code>是第一个参数，下个参数在<code>foldRight</code>是第二个参数</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> i = list.foldLeft[<span class="type">Int</span>](init)(_+_)</span><br></pre></td></tr></table></figure>

<p>==&gt;<code>init</code> = 10 作为初始值只用一次，然后(_+_)累加，累加的结果放在第一个参数位置</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">x = <span class="number">10</span> | y = <span class="number">1</span></span><br><span class="line">x = <span class="number">11</span> | y = <span class="number">2</span></span><br><span class="line">x = <span class="number">13</span> | y = <span class="number">3</span></span><br><span class="line">x = <span class="number">16</span> | y = <span class="number">4</span></span><br><span class="line">x = <span class="number">20</span> | y = <span class="number">5</span></span><br><span class="line">x = <span class="number">25</span> | y = <span class="number">6</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<p><strong>总结</strong></p>
<ol>
<li><p>柯里化技术在提高适用性还是在延迟执行或者固定易变因素等方面有着重要重要的作用，加上scala语言本身就是推崇简洁编码，使得同样功能的函数在定义与转换的时候会更加灵活多样。另外在Spark的源码中有大量运用scala柯里化技术的情况，需要掌握好该技术才能看得懂相关的源代码。</p>
</li>
<li><p>在scala柯里化中，闭包也发挥着重要的作用。所谓的闭包就是变量出了函数的定义域外在其他代码块还能其作用，这样的情况称之为闭包。就上述讨论的案例而言，如果没有闭包作用，那么转换后函数其实返回的匿名函数是无法在与第一个参数x相关结合的，自然也就无法保证其所实现的功能是跟原来一致的。</p>
</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/20/spark/3/">Spark之WC产生多少个RDD</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-20</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>WC产生多少个RDD</li>
</ol>
<h2 id="WC产生多少个RDD"><a href="#WC产生多少个RDD" class="headerlink" title="WC产生多少个RDD"></a>WC产生多少个RDD</h2><p>一句标准的WC产生了多少个RDD？</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> result = sc.textFile(<span class="string">"E:\\Java\\spark\\tunan-spark\\tunan-spark-core\\data\\wc.txt"</span>).flatMap(_.split(<span class="string">"\t"</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">result.saveAsTextFile(<span class="string">"out"</span>)</span><br></pre></td></tr></table></figure>

<ol>
<li><p>使用<code>toDebugString</code>方法查看RDD的数量</p>
<p>result.toDebugString(不包括<code>saveAsTextFile</code>方法)</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">(<span class="number">2</span>) <span class="type">ShuffledRDD</span>[<span class="number">4</span>] at reduceByKey at wordcount.scala:<span class="number">11</span> []</span><br><span class="line"> +-(<span class="number">2</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at wordcount.scala:<span class="number">11</span> []</span><br><span class="line">    |  <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at flatMap at wordcount.scala:<span class="number">11</span> []</span><br><span class="line">    |  <span class="type">E</span>:\<span class="type">Java</span>\spark\tunan-spark\tunan-spark-core\data\wc.txt <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at wordcount.scala:<span class="number">11</span> []</span><br><span class="line">    |  <span class="type">E</span>:\<span class="type">Java</span>\spark\tunan-spark\tunan-spark-core\data\wc.txt <span class="type">HadoopRDD</span>[<span class="number">0</span>] at textFile at wordcount.scala:<span class="number">11</span> []</span><br></pre></td></tr></table></figure>

<p>上面的方法中：textFile算子中有HadoopRDD和MapPartitionsRDD；flatMap方法有MapPartitionsRDD；map方法有MapPartitionsRDD；reduceByKey方法有ShuffledRDD。</p>
<p>这里一共是5个RDD，如果加上saveAsTextFile方法中的一个MapPartitionsRDD，则一共是6个RDD，如果加上sort方法也是一样的算法。</p>
</li>
<li><p>查看源码的方式计算RDD的数量</p>
<p><code>sc.textFile(&quot;...&quot;)</code></p>
<p>textFile的作用是从HDFS、本地文件系统(在所有节点上可用)或任何hadoop支持的文件系统URI读取文本文件，并将其作为字符串的RDD返回。</p>
<p>path：受支持的文件系统上的文本文件的路径</p>
<p>minPartitions：建议的结果RDD的最小分区数，默认值是2</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(</span><br><span class="line">    path: <span class="type">String</span>,</span><br><span class="line">    minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[<span class="type">String</span>] = withScope &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    hadoopFile(path, classOf[<span class="type">TextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>],</span><br><span class="line">               minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在textFile值调用了hadoopFile方法，该方法传入了<code>path</code>，<code>TextInputFormat</code>(也就是mapreduce中的FileInputFormat方法，特点是按行读取)，<code>LongWritable</code>(mapreduce计算中的key，记录的是offset)，<code>Text</code>(mapreduce计算中的key，记录的是每行的内容)，<code>minPartitions</code>(分区数)，然后返回一个tuple，tuple记录的是key和value，我们这里做了一个处理，<code>.map(pair =&gt; pair._2.toString)</code>方法让结果只有内容，而忽略掉了offset。</p>
<p>继续看hadoopFile的源码</p>
<p>使用任意的InputFormat获取Hadoop文件的RDD，返回一个RDD类型的包含(offset，value)的元组</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hadoopFile</span></span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">      path: <span class="type">String</span>,		<span class="comment">//目录下的输入数据文件，路径可以用逗号分隔路径作为输入列表</span></span><br><span class="line">      inputFormatClass: <span class="type">Class</span>[_ &lt;: <span class="type">InputFormat</span>[<span class="type">K</span>, <span class="type">V</span>]],	<span class="comment">//要读取的数据的存储格式</span></span><br><span class="line">      keyClass: <span class="type">Class</span>[<span class="type">K</span>],	<span class="comment">//key的类型</span></span><br><span class="line">      valueClass: <span class="type">Class</span>[<span class="type">V</span>],	<span class="comment">//value的类型</span></span><br><span class="line">      minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]	<span class="comment">//分区数，默认值是2</span></span><br><span class="line">		= withScope &#123;assertNotStopped()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这是一种强制加载hdfs-site.xml的方法</span></span><br><span class="line">    <span class="type">FileSystem</span>.getLocal(hadoopConfiguration)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//一个Hadoop的配置文件大概10 KB,这是相当大的,所以广播它</span></span><br><span class="line">    <span class="keyword">val</span> confBroadcast = broadcast(<span class="keyword">new</span> <span class="type">SerializableConfiguration</span>(hadoopConfiguration))</span><br><span class="line">    <span class="keyword">val</span> setInputPathsFunc = (jobConf: <span class="type">JobConf</span>) =&gt; 						<span class="type">FileInputFormat</span>.setInputPaths(jobConf, path)	<span class="comment">//设置作业环境</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">HadoopRDD</span>(	<span class="comment">//创建一个HadoopRDD</span></span><br><span class="line">      <span class="keyword">this</span>,</span><br><span class="line">      confBroadcast,	<span class="comment">//广播配置</span></span><br><span class="line">      <span class="type">Some</span>(setInputPathsFunc),	<span class="comment">//作业环境也许可能出错，所以使用Some()</span></span><br><span class="line">      inputFormatClass,	<span class="comment">//读取文件的格式化类</span></span><br><span class="line">      keyClass,	<span class="comment">//key类型</span></span><br><span class="line">      valueClass,	<span class="comment">//value类型</span></span><br><span class="line">      minPartitions).setName(path)	<span class="comment">//分片数</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>因为Hadoop的RecordReader类为每条记录使用相同的Writable对象，直接保存或者直接使用aggregation或者shuffle将会产生很多对同一个对象的引用，所以我们保存、排序或者聚合操作writable对象前，要使用map方法做一个映射。</p>
<p>回到上一步的textFile方法中，Hadoop的返回值是一个包含offset和value的元组，我们只需要内容，所以使用map方法做一个映射，只拿元祖中的value即可</p>
<p><code>.map(pair =&gt; pair._2.toString)</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)	<span class="comment">//初始化检查</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.map(cleanF))	==&gt;(context, pid, iter) ==&gt; hadoopRDD, 分区<span class="type">ID</span>, 迭代器</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该map方法又创建一个MapPartitionsRDD，将map算子应用于每个分区的子RDD。这应用到了RDD五大特性之一的，对每个RDD做计算，实际上是对每个RDD的partition或者split做计算。由于MapPartitionsRDD较为复杂，暂不解析。</p>
<p><strong><em>到此，textFile产生了两个RDD，分别是HadoopRDD和MapPartitionsRDD。共两个RDD</em></strong></p>
</li>
</ol>
<p>   <code>.flatMap(_.split(&quot;\t&quot;))</code></p>
<p>   flatMap首先作用在每一个元素上，然后将结果扁平化，最后返回一个新的RDD</p>
   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.flatMap(cleanF))	<span class="comment">//对RDD的所有分区做计算</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>   <strong><em>到此，flatMap产生了一个RDD，是MapPartitionsRDD。共三个RDD</em></strong></p>
<p>   <code>.map((_, 1))</code></p>
<p>   将map作用在每一个元素上，然后返回一个新的RDD</p>
   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.map(cleanF))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>   <strong><em>到此，map产生了一个RDD，是MapPartitionsRDD。共四个RDD</em></strong></p>
<p>   <code>.reduceByKey(_ + _)</code></p>
<p>   使用联合和交换reduce函数合并每个键的值。<strong>在将结果发送到reduce之前，这也将在每个mapper上本地执行合并，类似于MapReduce中的“combiner”。</strong>输出将使用现有分区器/并行度级别进行哈希分区。</p>
   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">    reduceByKey(defaultPartitioner(self), func)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>   继续查看<code>reduceByKey(defaultPartitioner(self), func)</code>方法</p>
   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(partitioner: <span class="type">Partitioner</span>, func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">    combineByKeyWithClassTag[<span class="type">V</span>]((v: <span class="type">V</span>) =&gt; v, func, func, partitioner)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>   继续查看<code>combineByKeyWithClassTag[V]((v: V) =&gt; v, func, func, partitioner)</code>方法</p>
<p>   这是一个具体干活的方法，它使用一组自定义聚合函数组合每个键的元素。将RDD[(K, V)]转换为RDD[(K, C)]类型的结果，用于“组合类型”C。这是一个复杂的方法，暂不做解析。</p>
   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKeyWithClassTag</span></span>[<span class="type">C</span>](</span><br><span class="line">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    partitioner: <span class="type">Partitioner</span>,</span><br><span class="line">    mapSideCombine: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">    serializer: <span class="type">Serializer</span> = <span class="literal">null</span>)(<span class="keyword">implicit</span> ct: <span class="type">ClassTag</span>[<span class="type">C</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = self.withScope &#123;</span><br><span class="line">    require(mergeCombiners != <span class="literal">null</span>, <span class="string">"mergeCombiners must be defined"</span>) <span class="comment">// required as of Spark 0.9.0</span></span><br><span class="line">    <span class="keyword">if</span> (keyClass.isArray) &#123;</span><br><span class="line">        <span class="keyword">if</span> (mapSideCombine) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Cannot use map-side combining with array keys."</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (partitioner.isInstanceOf[<span class="type">HashPartitioner</span>]) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"HashPartitioner cannot partition array keys."</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> aggregator = <span class="keyword">new</span> <span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">        self.context.clean(createCombiner),</span><br><span class="line">        self.context.clean(mergeValue),</span><br><span class="line">        self.context.clean(mergeCombiners))</span><br><span class="line">    <span class="keyword">if</span> (self.partitioner == <span class="type">Some</span>(partitioner)) &#123;</span><br><span class="line">        self.mapPartitions(iter =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> context = <span class="type">TaskContext</span>.get()</span><br><span class="line">            <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context, aggregator.combineValuesByKey(iter, context))</span><br><span class="line">        &#125;, preservesPartitioning = <span class="literal">true</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](self, partitioner)	<span class="comment">//创建ShuffledRDD</span></span><br><span class="line">        .setSerializer(serializer)</span><br><span class="line">        .setAggregator(aggregator)</span><br><span class="line">        .setMapSideCombine(mapSideCombine)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>   <strong><em>到此，reduceByKey产生了一个RDD，是ShuffledRDD。共五个RDD</em></strong></p>
<p>   <code>.saveAsTextFile(&quot;...&quot;)</code></p>
<p>   将每个元素使用字符串表示形式，将此RDD保存为文本文件。</p>
   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsTextFile</span></span>(path: <span class="type">String</span>): <span class="type">Unit</span> = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> nullWritableClassTag = implicitly[<span class="type">ClassTag</span>[<span class="type">NullWritable</span>]]	</span><br><span class="line">    <span class="keyword">val</span> textClassTag = implicitly[<span class="type">ClassTag</span>[<span class="type">Text</span>]]</span><br><span class="line">    <span class="keyword">val</span> r = <span class="keyword">this</span>.mapPartitions &#123; iter =&gt;	<span class="comment">//注意这里</span></span><br><span class="line">        <span class="keyword">val</span> text = <span class="keyword">new</span> <span class="type">Text</span>()</span><br><span class="line">        iter.map &#123; x =&gt;</span><br><span class="line">            text.set(x.toString)</span><br><span class="line">            (<span class="type">NullWritable</span>.get(), text)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">RDD</span>.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, <span class="literal">null</span>)</span><br><span class="line">    .saveAsHadoopFile[<span class="type">TextOutputFormat</span>[<span class="type">NullWritable</span>, <span class="type">Text</span>]](path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>   通过每次将一个分区的数据以流的方式传入到HDFS中再关闭流</p>
   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanedF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>(	<span class="comment">//创建MapPartitionsRDD</span></span><br><span class="line">    <span class="keyword">this</span>,</span><br><span class="line">    (context: <span class="type">TaskContext</span>, index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedF(iter),</span><br><span class="line">    preservesPartitioning)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>   <del><strong><em>到此，saveAsTextFile产生了一个RDD，是MapPartitionsRDD。共六个RDD</em></strong></del></p>
<p>   <em>20200325更新：</em></p>
<p>   在最后的saveAsTextFile()算子中，我们忽略了一个RDD，它就是是<code>PairRDDFunctions</code>，这个RDD是通过RDD隐式转换过来的</p>
   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">rddToPairRDDFunctions</span></span>[<span class="type">K</span>, <span class="type">V</span>](rdd: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)])</span><br><span class="line">(<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>], vt: <span class="type">ClassTag</span>[<span class="type">V</span>], ord: <span class="type">Ordering</span>[<span class="type">K</span>] = <span class="literal">null</span>): <span class="type">PairRDDFunctions</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">PairRDDFunctions</span>(rdd)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>   <strong><em>修正后的 RDD数量是 7个</em></strong></p>
<ol start="3">
<li>总结：使用toDebugString方法，简单的看到了生成了多少个RDD，通过阅读源码的方式，详细了解到了生成了多少个RDD，他们分别做了什么事情。我们这个流程生成了<del>6</del>7个RDD，如果对结果进行排序，也是相同的方法可以看到答案。</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/19/scala/3/">Scala之var和val的比较&amp;lazy懒加载</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-19</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Scala/">Scala</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Scala/">Scala</a></span><div class="content"><h3 id="1：内容是否可变：val修饰的是不可变的，var修饰是可变的"><a href="#1：内容是否可变：val修饰的是不可变的，var修饰是可变的" class="headerlink" title="1：内容是否可变：val修饰的是不可变的，var修饰是可变的"></a>1：内容是否可变：val修饰的是不可变的，var修饰是可变的</h3><p>下面看一段代码，你猜是否有错误</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ValAndVar</span> </span>&#123;</span><br><span class="line">    <span class="comment">//val 修饰由于不可变性必须初始化</span></span><br><span class="line">    <span class="keyword">val</span> <span class="type">LOVE</span>:<span class="type">String</span> = _</span><br><span class="line">    <span class="keyword">var</span> <span class="type">SEX</span>:<span class="type">String</span>  = _</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> name = <span class="string">"tunan"</span></span><br><span class="line">        <span class="keyword">var</span> age = <span class="number">18</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//val 修饰由于不可变性不能重新赋值</span></span><br><span class="line">        name = <span class="string">"zhangsan"</span></span><br><span class="line">        age = <span class="number">19</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>真实的结果:</p>
<p><img src="https://yerias.github.io/scala_img/val%E5%92%8Cvar%E7%9A%84%E5%8F%AF%E5%8F%98%E6%80%A7%E6%AF%94%E8%BE%83.jpg" alt="val和var的可变性比较"></p>
<ol>
<li>val是不可变的，所以修饰的变量必须初始化</li>
<li>val是不可变的，所以修饰的变量不能重新赋值</li>
<li>val是不可变的，所以是多线程安全的</li>
<li>val是不可变的，不用担心会改变它修饰的对象的状态</li>
<li>val是不可变的，增强了代码的可读性，不用担心它的内容发生变化</li>
<li>var是可变的，可以增强代码的灵活性，和val互补</li>
</ol>
<h3 id="2：val修饰的变量在编译后类似于java中的中的变量被final修饰"><a href="#2：val修饰的变量在编译后类似于java中的中的变量被final修饰" class="headerlink" title="2：val修饰的变量在编译后类似于java中的中的变量被final修饰"></a>2：val修饰的变量在编译后类似于java中的中的变量被final修饰</h3><ol>
<li><p>先看源代码</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ValAndVar</span> </span>&#123;</span><br><span class="line">    <span class="keyword">val</span> <span class="type">LOVE</span>:<span class="type">String</span> = <span class="string">"篮球"</span></span><br><span class="line">    <span class="keyword">var</span> <span class="type">SEX</span>:<span class="type">String</span>  = _</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> name:<span class="type">String</span> = <span class="string">"tunan"</span></span><br><span class="line">        <span class="keyword">var</span> age:<span class="type">Int</span> = <span class="number">18</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>再看反编译后的代码(只保留了我们想要的部分)</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ValAndVar$</span> </span>&#123;</span><br><span class="line">  public static <span class="type">ValAndVar</span>$ <span class="type">MODULE</span>$;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">String</span> <span class="type">LOVE</span>;</span><br><span class="line">    </span><br><span class="line">  public void main(<span class="type">String</span>[] args) &#123;</span><br><span class="line">    <span class="type">String</span> name = <span class="string">"tunan"</span>;</span><br><span class="line">    int age = <span class="number">18</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们发现这段代码很诡异，scala中的类变量，在字节码层面转换成了 parivate final ，而main方法中的变量却没有添加final修饰，这是否证明编译器有问题？</p>
<p>答案是否定的，对于val或者final都只是给编译器用的，编译器如果发现你给此变量重新赋值会抛出错误。同时字节码(bytecode)不具备表达一个局部变量是不可变(immutable)的能力。</p>
<p>所以就有了现在结果。</p>
</li>
</ol>
<h3 id="3：lazy修饰符可以修饰变量，但是这个变量必须是val修饰的"><a href="#3：lazy修饰符可以修饰变量，但是这个变量必须是val修饰的" class="headerlink" title="3：lazy修饰符可以修饰变量，但是这个变量必须是val修饰的"></a>3：lazy修饰符可以修饰变量，但是这个变量必须是val修饰的</h3><ol>
<li><p>在证明lazy修饰的变量必须是val之前，我们先看看lazy是什么？</p>
<p>Scala中使用关键字lazy来定义惰性变量，实现延迟加载(懒加载)。<br>惰性变量只能是不可变变量，并且只有在调用惰性变量时，才会去实例化这个变量。</p>
<p>在Java中，一般使用get和set实现延迟加载(懒加载)，而在Scala中对延迟加载这一特性提供了语法级别的支持:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> name = initName()</span><br></pre></td></tr></table></figure>

<p>使用lazy关键字修饰变量后，只有在使用该变量时，才会调用其实例化方法。也就是说在定义name=initName()时并不会调用initName()方法，只有在后面的代码中使用变量name时才会调用initName()方法。</p>
<p>如果<strong>不使用lazy关键字对变量修饰</strong>，那么变量name是立即实例化的，下面将通过一组案例对比认识：</p>
<p><code>不使用lazy修饰的方法：</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LazyDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initName</span></span>:<span class="type">String</span>=&#123;</span><br><span class="line">        println(<span class="string">"初始化initName"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"返回intName"</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="comment">//        lazy val name = initName</span></span><br><span class="line">        <span class="keyword">val</span> name = initName	<span class="comment">//程序走到这里，就打印了initName的输出语句</span></span><br><span class="line">        println(<span class="string">"hello，欢迎来到图南之家"</span>)</span><br><span class="line">        println(name)	<span class="comment">//程序走到这里，打印initName的返回值</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面的name没有使用lazy关键字进行修饰，所以name是立即实例化的。</p>
<p>结果：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">初始化initName</span><br><span class="line">hello，欢迎来到图南之家</span><br><span class="line">返回intName</span><br></pre></td></tr></table></figure>

<p><code>使用lazy修饰后的方法：</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LazyDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initName</span></span>:<span class="type">String</span>=&#123;</span><br><span class="line">        println(<span class="string">"初始化initName"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"返回intName"</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">lazy</span> <span class="keyword">val</span> name = initName	<span class="comment">//不调用initName方法，即不打印initName中的输出语句</span></span><br><span class="line"><span class="comment">//        val name = initName</span></span><br><span class="line">        println(<span class="string">"hello，欢迎来到图南之家"</span>)	<span class="comment">//打印main方法中的输出语句</span></span><br><span class="line">        println(name)	<span class="comment">//打印initName的输出语句，打印返回值</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在声明name时，并没有立即调用实例化方法initName(),而是在使用name时，才会调用实例化方法,并且无论调用多少次，实例化方法只会执行一次。</p>
<p>结果：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">hello，欢迎来到图南之家</span><br><span class="line">初始化initName</span><br><span class="line">返回intName</span><br></pre></td></tr></table></figure>
</li>
<li><p>证明lazy只能修饰的变量只能使用val</p>
<p>我们发现name都是使用val修饰的，如果我们使用var修饰会怎么样呢？</p>
<p><img src="https://yerias.github.io/scala_img/lazy%E6%87%92%E5%8A%A0%E8%BD%BD.jpg" alt="lazy懒加载"></p>
<p>我们发现报错：<code>&#39;lazy&#39; modifier allowed only with value definitions</code></p>
<p>实际上就是认为<code>lazy</code>修饰的变量只能<code>val</code>修饰</p>
</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/19/error/1/">执行Saprk or Scala程序:找不到或者无法加载主类 xxx</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-19</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Error/">Error</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Error/">Error</a></span><div class="content"><p>使用百度和谷歌，测试了广大程序员给出的各种解决办法，包括更换jdk版本(之前也是jdk8，小版本不同)，更换scala 的版本(2.12大版本内更换小版本，因为我的spark2.4.5需要的scala版本是2.12)，更换idea的输出路径，重构代码，清理idea的缓存，删除依赖重新下载，重建项目，导入其他同学的项目，皆出现<code>找不到或者无法加载主类 xxx</code>异常</p>
<p>最后重装idea，解决！</p>
</div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/4/">&lt;&lt;</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/6/">&gt;&gt;</a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>