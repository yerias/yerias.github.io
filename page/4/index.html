<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="BigData Developer"><meta name="keywords" content="yerias,TUNANのBlog,BigData"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017 | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">129</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">30</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">26</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">TUNANのBlog</div><div id="site-sub-title">感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017</div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/31/spark/8/">Spark之短信告警</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-31</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><p>下面的案例继续延续Spark监控中的邮件监控，在监控中检测到数据异常，需要发送邮件告警</p>
<p>发送邮件工具类</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">MsgUtils</span> </span>&#123;</span><br><span class="line">    public static void send(<span class="type">String</span> recivers, <span class="type">String</span> title, <span class="type">String</span> content) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">Properties</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>();</span><br><span class="line">        properties.setProperty(<span class="string">"mail.host"</span>,<span class="string">"smtp.qq.com"</span>);</span><br><span class="line">        properties.setProperty(<span class="string">"mail.transport.protocol"</span>,<span class="string">"smtp"</span>);</span><br><span class="line">        properties.setProperty(<span class="string">"mail.smtp.auth"</span>, <span class="string">"true"</span>);</span><br><span class="line">        properties.setProperty(<span class="string">"mail.smtp.ssl.enable"</span>,<span class="string">"false"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">MailSSLSocketFactory</span> factory = <span class="keyword">new</span> <span class="type">MailSSLSocketFactory</span>();</span><br><span class="line">        factory.setTrustAllHosts(<span class="literal">true</span>);</span><br><span class="line">        properties.put(<span class="string">"mail.smtp.ssl.socketFactory"</span>, factory);</span><br><span class="line"></span><br><span class="line">        <span class="type">Authenticator</span> authenticator = <span class="keyword">new</span> <span class="type">Authenticator</span>() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">protected</span> <span class="type">PasswordAuthentication</span> getPasswordAuthentication() &#123;</span><br><span class="line">                <span class="type">String</span> username = <span class="string">"发送者qq邮箱"</span>;</span><br><span class="line">                <span class="type">String</span> password = <span class="string">"发送者qq授权码"</span>;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">PasswordAuthentication</span>(username, password);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="type">Session</span> session = <span class="type">Session</span>.getInstance(properties, authenticator);</span><br><span class="line"></span><br><span class="line">        <span class="type">MimeMessage</span> message = <span class="keyword">new</span> <span class="type">MimeMessage</span>(session);</span><br><span class="line">        <span class="type">InternetAddress</span> from = <span class="keyword">new</span> <span class="type">InternetAddress</span>(<span class="string">"发送者qq邮箱"</span>);</span><br><span class="line">        message.setFrom(from);</span><br><span class="line">        <span class="type">InternetAddress</span>[] tos = <span class="type">InternetAddress</span>.parse(recivers);</span><br><span class="line">        message.setRecipients(<span class="type">Message</span>.<span class="type">RecipientType</span>.<span class="type">TO</span>, tos);</span><br><span class="line">        message.setSubject(title);</span><br><span class="line">        message.setContent(content, <span class="string">"text/html;charset=UTF-8"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">Transport</span>.send(message);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(<span class="type">String</span>[] args) <span class="keyword">throws</span> <span class="type">Exception</span>&#123;</span><br><span class="line">        send(<span class="string">"接收邮箱"</span>, <span class="string">"测试"</span>, <span class="string">"测试内容"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Spark监控中调用</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">MsgUtils</span>.send(<span class="string">"接收者邮箱,接收者邮箱"</span>, <span class="string">"ERROR：数据异常"</span>, <span class="string">s"jobID: <span class="subst">$jobId</span> 数据异常，请马上检查: <span class="subst">$&#123;listener.toString&#125;</span>s"</span>)</span><br></pre></td></tr></table></figure>

</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/31/spark/7/">Spark之监控模块</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-31</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>Spark自带监控</li>
<li>Spark接口监控</li>
<li>Spark自定义监控</li>
</ol>
<h2 id="Spark自带监控"><a href="#Spark自带监控" class="headerlink" title="Spark自带监控"></a><a href="http://spark.apache.org/docs/latest/monitoring.html#spark-history-server-configuration-options" target="_blank" rel="noopener">Spark自带监控</a></h2><p>第一种监控方式是Spark自带的，由于Spark Web UI界面只在sc的生命周期内有效，所以我们需要存储日志，在Spark sc 生命周期结束后重构UI界面。</p>
<p>首先看官方文档配置，这里只是简单配置</p>
<ol>
<li><p>修改spark.default.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">#开启日志存储</span><br><span class="line">spark.eventLog.enabled true</span><br><span class="line">#指定日志存储的HDFS目录</span><br><span class="line">spark.eventLog.dir hdfs://hadoop:9000/spark-logs</span><br><span class="line">#开启日志存储7天自动删除</span><br><span class="line">spark.history.fs.cleaner.enabled true</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改spark.env.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">#指定日志恢复目录，就是上面的日志存储目录</span><br><span class="line">SPARK_HISTORY_OPTS = "-Dspark.history.fs.logDirectory=hdfs://hadoop:9000/spark-logs"</span><br></pre></td></tr></table></figure>
</li>
<li><p>在 sc 的生命周期外打开历史UI界面</p>
</li>
</ol>
<h2 id="Spark接口监控"><a href="#Spark接口监控" class="headerlink" title="Spark接口监控"></a><a href="http://spark.apache.org/docs/latest/monitoring.html#rest-api" target="_blank" rel="noopener">Spark接口监控</a></h2><p>首先看官方文档配置，这里只是简单介绍</p>
<p>查看application列表：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">http:<span class="comment">//hadoop:18080/api/v1/applications</span></span><br></pre></td></tr></table></figure>

<p>查看application的所有job</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">http:<span class="comment">//hadoop:18080/history/application_1585632916452_0002/jobs/</span></span><br></pre></td></tr></table></figure>

<h2 id="Spark自定义监控"><a href="#Spark自定义监控" class="headerlink" title="Spark自定义监控"></a><a href="http://spark.apache.org/docs/latest/monitoring.html#executor-task-metrics" target="_blank" rel="noopener">Spark自定义监控</a></h2><p>metrics: 数据信息</p>
<p>spark 提供了一系列整个任务生命周期中各个阶段变化的事件监听机制，通过这一机制可以在任务的各个阶段做一些自定义的各种动作。SparkListener便是这些阶段的事件监听接口类 通过实现这个类中的各种方法便可实现自定义的事件处理动作。</p>
<p>自定义监听sparListener后的注册方式有两种：</p>
<p><strong>方法1：</strong>conf 配置中指定</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">.setAppName(getClass.getSimpleName)</span><br><span class="line">.setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line"><span class="comment">//监听类注册</span></span><br><span class="line">.set(<span class="string">"spark.extraListeners"</span>, <span class="string">"com.tunan.spark.listener.MySparkListener"</span>)</span><br><span class="line">.getOrCreate()</span><br></pre></td></tr></table></figure>

<p><strong>方法2：</strong>sparkContext 类中指定</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc.addSparkListener(<span class="keyword">new</span> <span class="type">MySparkAppListener</span>)</span><br></pre></td></tr></table></figure>

<h3 id="SparkListerner"><a href="#SparkListerner" class="headerlink" title="SparkListerner"></a>SparkListerner</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//SparkListener 下各个事件对应的函数名非常直白，即如字面所表达意思。</span></span><br><span class="line"><span class="comment">//想对哪个阶段的事件做一些自定义的动作，变继承SparkListener实现对应的函数即可</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkListener</span> <span class="keyword">extends</span> <span class="title">SparkListenerInterface</span> </span>&#123;</span><br><span class="line">  <span class="comment">//阶段完成时触发的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStageCompleted</span></span>(stageCompleted: <span class="type">SparkListenerStageCompleted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//阶段提交时触发的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStageSubmitted</span></span>(stageSubmitted: <span class="type">SparkListenerStageSubmitted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//任务启动时触发的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskStart</span></span>(taskStart: <span class="type">SparkListenerTaskStart</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//下载任务结果的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskGettingResult</span></span>(taskGettingResult: <span class="type">SparkListenerTaskGettingResult</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//任务结束的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskEnd</span></span>(taskEnd: <span class="type">SparkListenerTaskEnd</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//job启动的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onJobStart</span></span>(jobStart: <span class="type">SparkListenerJobStart</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//job结束的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onJobEnd</span></span>(jobEnd: <span class="type">SparkListenerJobEnd</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//环境变量被更新的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onEnvironmentUpdate</span></span>(environmentUpdate: <span class="type">SparkListenerEnvironmentUpdate</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//块管理被添加的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBlockManagerAdded</span></span>(blockManagerAdded: <span class="type">SparkListenerBlockManagerAdded</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBlockManagerRemoved</span></span>(</span><br><span class="line">      blockManagerRemoved: <span class="type">SparkListenerBlockManagerRemoved</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//取消rdd缓存的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onUnpersistRDD</span></span>(unpersistRDD: <span class="type">SparkListenerUnpersistRDD</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//app启动的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onApplicationStart</span></span>(applicationStart: <span class="type">SparkListenerApplicationStart</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//app结束的事件 [以下各事件也如同函数名所表达各个阶段被触发的事件不在一一标注]</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onApplicationEnd</span></span>(applicationEnd: <span class="type">SparkListenerApplicationEnd</span>): <span class="type">Unit</span> = &#123; &#125; </span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorMetricsUpdate</span></span>(</span><br><span class="line">      executorMetricsUpdate: <span class="type">SparkListenerExecutorMetricsUpdate</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorAdded</span></span>(executorAdded: <span class="type">SparkListenerExecutorAdded</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorRemoved</span></span>(executorRemoved: <span class="type">SparkListenerExecutorRemoved</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorBlacklisted</span></span>(</span><br><span class="line">      executorBlacklisted: <span class="type">SparkListenerExecutorBlacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorUnblacklisted</span></span>(</span><br><span class="line">      executorUnblacklisted: <span class="type">SparkListenerExecutorUnblacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onNodeBlacklisted</span></span>(</span><br><span class="line">      nodeBlacklisted: <span class="type">SparkListenerNodeBlacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onNodeUnblacklisted</span></span>(</span><br><span class="line">      nodeUnblacklisted: <span class="type">SparkListenerNodeUnblacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBlockUpdated</span></span>(blockUpdated: <span class="type">SparkListenerBlockUpdated</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onOtherEvent</span></span>(event: <span class="type">SparkListenerEvent</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h3><p>首先看官方文档配置，这里只是简单案例</p>
<ol>
<li>自定义监控类，继承SparkListener</li>
<li>重写onTaskEnd方法，拿到taskMetrics</li>
<li>从taskMetrics获取各种数据信息</li>
<li>注册到被监听的类</li>
</ol>
<p><strong>第1-3步代码</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySparkListener</span>(<span class="params">conf:<span class="type">SparkConf</span></span>) <span class="keyword">extends</span> <span class="title">SparkListener</span> <span class="keyword">with</span> <span class="title">Logging</span></span>&#123;</span><br><span class="line">    <span class="comment">//监听每个Task结束</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskEnd</span></span>(taskEnd: <span class="type">SparkListenerTaskEnd</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//获取应用程序名称</span></span><br><span class="line">        <span class="keyword">val</span> appName = conf.get(<span class="string">"spark.app.name"</span>)</span><br><span class="line">        <span class="comment">//获取作业的taskMetrics</span></span><br><span class="line">        <span class="keyword">val</span> metrics = taskEnd.taskMetrics</span><br><span class="line">        <span class="comment">//使用对象接收参数</span></span><br><span class="line">        <span class="keyword">val</span> metricsObject = <span class="type">Metrics</span>(appName,taskEnd.stageId, taskEnd.taskInfo.taskId, metrics.inputMetrics.bytesRead, metrics.shuffleReadMetrics.totalBytesRead, metrics.outputMetrics.bytesWritten, metrics.shuffleWriteMetrics.bytesWritten)</span><br><span class="line">        <span class="comment">//输出字符串类型的metricsObject</span></span><br><span class="line">        logError(metricsObject.toString)</span><br><span class="line">        <span class="comment">//输出Json类型的metricsObject</span></span><br><span class="line">        logError(<span class="type">Json</span>(<span class="type">DefaultFormats</span>).write(metricsObject))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//定义case class对象</span></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Metrics</span>(<span class="params">appName:<span class="type">String</span>,stageId:<span class="type">Long</span>,taskId:<span class="type">Long</span>,bytesRead:<span class="type">Long</span>,bytesWritten:<span class="type">Long</span>,shuffleReadMetrics:<span class="type">Long</span>,shuffleWriteMetrics:<span class="type">Long</span></span>)</span>&#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = <span class="string">s"appName:<span class="subst">$appName</span>,stageId:<span class="subst">$stageId</span>,taskId:<span class="subst">$taskId</span>,bytesRead:<span class="subst">$bytesRead</span>,bytesWritten:<span class="subst">$bytesWritten</span>,shuffleReadMetrics:<span class="subst">$shuffleReadMetrics</span>,shuffleWriteMetrics:<span class="subst">$shuffleWriteMetrics</span>"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>第四步代码</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//输入、输出路径</span></span><br><span class="line">        <span class="keyword">val</span> (in,out) = (args(<span class="number">0</span>),args(<span class="number">1</span>))</span><br><span class="line">        <span class="comment">//配置conf</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">            .setAppName(getClass.getSimpleName)</span><br><span class="line">            .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">            <span class="comment">//监听类注册</span></span><br><span class="line">            .set(<span class="string">"spark.extraListeners"</span>, <span class="string">"com.tunan.spark.listener.MySparkListener"</span>)</span><br><span class="line">        <span class="comment">//拿到sc</span></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="comment">//删除输出目录</span></span><br><span class="line">        <span class="type">CheckHDFSOutPath</span>.ifExistsDeletePath(<span class="keyword">new</span> <span class="type">Configuration</span>(),out)</span><br><span class="line">        <span class="comment">//操作算子</span></span><br><span class="line">        <span class="keyword">val</span> result = sc.textFile(in).flatMap(_.split(<span class="string">"\t"</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">        <span class="comment">//保存文件</span></span><br><span class="line">        result.saveAsTextFile(out)</span><br><span class="line">        <span class="comment">//关闭sc</span></span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Spark自定义监控案例"><a href="#Spark自定义监控案例" class="headerlink" title="Spark自定义监控案例"></a>Spark自定义监控案例</h2><p>把监控参数写入到MySQL</p>
<p>需求：应用程序名字、jobID号、stageID号、taskID号、读取数据量、写入数据量、shuffle读取数据量、shuffle写入数据量。</p>
<ol>
<li><p>建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> wc2mysql(</span><br><span class="line">app_name <span class="built_in">varchar</span>(<span class="number">32</span>),</span><br><span class="line">job_id <span class="built_in">bigint</span>,</span><br><span class="line">stage_id <span class="built_in">bigInt</span>,</span><br><span class="line">task_id <span class="built_in">bigint</span>,</span><br><span class="line">file_read_byte <span class="built_in">bigint</span>,</span><br><span class="line">file_write_byte <span class="built_in">bigint</span>,</span><br><span class="line">shuffle_read_byte <span class="built_in">bigint</span>,</span><br><span class="line">shuffle_write_byte <span class="built_in">bigint</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li><p>实现监控类</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySparkListenerV2</span>(<span class="params">conf: <span class="type">SparkConf</span></span>) <span class="keyword">extends</span> <span class="title">SparkListener</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义JobID</span></span><br><span class="line">    <span class="keyword">var</span> jobId:<span class="type">Long</span> = _</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onJobStart</span></span>(jobStart: <span class="type">SparkListenerJobStart</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//初始化</span></span><br><span class="line">        jobId = jobStart.jobId</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"============准备插入数据============"</span>)</span><br><span class="line">    <span class="comment">//监听每个Task结束</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskEnd</span></span>(taskEnd: <span class="type">SparkListenerTaskEnd</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//获取应用程序名称</span></span><br><span class="line">        <span class="keyword">val</span> appName = conf.get(<span class="string">"spark.app.name"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取作业的taskMetrics</span></span><br><span class="line">        <span class="keyword">val</span> metrics = taskEnd.taskMetrics</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用对象接收参数</span></span><br><span class="line">        <span class="keyword">val</span> listener = <span class="type">Listener</span>(appName, jobId, taskEnd.stageId, taskEnd.taskInfo.taskId, metrics.inputMetrics.bytesRead, metrics.outputMetrics.bytesWritten, metrics.shuffleReadMetrics.totalBytesRead, metrics.shuffleWriteMetrics.bytesWritten)</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//结果插入到MySQL</span></span><br><span class="line">        <span class="type">ListenerCURD</span>.insert(listener)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//发送监控邮件</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"true"</span> == conf.get(<span class="string">"spark.send.mail.enabled"</span>))&#123;</span><br><span class="line">            <span class="type">MsgUtils</span>.send(<span class="string">"971118017@qq.com"</span>, <span class="string">"ERROR：数据异常"</span>, <span class="string">s"jobID: <span class="subst">$jobId</span> 数据异常，请马上检查: <span class="subst">$&#123;listener.toString&#125;</span>"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    println(<span class="string">"============成功插入数据============"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>实现监控数据写入MySQL</p>
<p>使用的是scalikejdbc框架实现的，使用具体方法在我的<a href="http://yerias.github.io/2020/03/16/scala/2/">博客</a></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Listener</span>(<span class="params">app_name: <span class="type">String</span>, job_id: <span class="type">Long</span>, stage_id: <span class="type">Long</span>, task_id: <span class="type">Long</span>, file_read_byte: <span class="type">Long</span>, file_write_byte: <span class="type">Long</span>, shuffle_read_byte: <span class="type">Long</span>, shuffle_write_byte: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">ListenerCURD</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Before</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//初始化配置</span></span><br><span class="line">        <span class="type">DBs</span>.setupAll()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(listener: <span class="type">Listener</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="type">Before</span>()</span><br><span class="line">        <span class="comment">//事物插入</span></span><br><span class="line">        <span class="type">DB</span>.localTx &#123;</span><br><span class="line">            <span class="keyword">implicit</span> session =&gt; &#123;</span><br><span class="line">                <span class="type">SQL</span>(<span class="string">"insert into wc2mysql(app_name,job_id,stage_id,task_id,file_read_byte,file_write_byte,shuffle_read_byte,shuffle_write_byte) values(?,?,?,?,?,?,?,?)"</span>)</span><br><span class="line">                    .bind(listener.app_name,listener.job_id, listener.stage_id, listener.task_id, listener.file_read_byte, listener.file_write_byte, listener.shuffle_read_byte, listener.shuffle_write_byte)</span><br><span class="line">                    .update()</span><br><span class="line">                    .apply()</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">After</span>()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">After</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//关闭资源</span></span><br><span class="line">        <span class="type">DBs</span>.closeAll()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动被监控类</p>
<p>被监控的类还是我们上面的WordCount的类，关键在于在SparkConf()中注册</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//配置conf</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">.setAppName(getClass.getSimpleName)</span><br><span class="line">.setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line"><span class="comment">//监听类注册</span></span><br><span class="line">.set(<span class="string">"spark.extraListeners"</span>, <span class="string">"com.tunan.spark.listener.MySparkListenerV2"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>在数据库中查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> wc2mysql;</span><br></pre></td></tr></table></figure>

</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/30/spark/6/">Spark术语&amp;Spark提交&amp;YARN上的提交模式&amp;窄依赖&amp;宽依赖</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-30</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>Spark术语</li>
<li>Spark提交</li>
<li>YARN上提交模式</li>
<li>宽依赖</li>
<li>窄依赖</li>
</ol>
<h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><p>下表总结了关于集群概念的术语:</p>
<table>
<thead>
<tr>
<th><strong>Term</strong></th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td>Application</td>
<td>Spark上的应用程序。由一个<strong>driver program</strong>和集群上的<strong>executors</strong>组成。</td>
</tr>
<tr>
<td>Application jar</td>
<td>一个包含用户的Spark应用程序的jar包</td>
</tr>
<tr>
<td>Driver program</td>
<td>运行应用程序main()函数并创建SparkContext的进程</td>
</tr>
<tr>
<td>Cluster manager</td>
<td>用于获取集群资源的外部服务(例如，standalone manager、Mesos、YARN)</td>
</tr>
<tr>
<td>Deploy mode</td>
<td>区别driver process在何处运行。在“cluster”模式下，框架启动集群内部的驱动程序。在“client”模式下，提交者启动集群外部的驱动程序。</td>
</tr>
<tr>
<td>Worker node</td>
<td>可以在集群中运行application的任何节点</td>
</tr>
<tr>
<td>Executor</td>
<td>在Worker node上被application启动的进程，它运行任务并将数据保存在内存或磁盘存储器中。每个application都有自己的Executor。</td>
</tr>
<tr>
<td>Task</td>
<td>将被发送给一个执行者的工作单元</td>
</tr>
<tr>
<td>Job</td>
<td>由多个任务组成的并行计算，这些任务在响应一个Spark操作时产生(如保存、收集)</td>
</tr>
<tr>
<td>Stage</td>
<td>每个作业被分成更小的任务集，称为阶段，这些阶段相互依赖(类似于MapReduce中的map和reduce阶段)</td>
</tr>
</tbody></table>
<h2 id="Spark提交"><a href="#Spark提交" class="headerlink" title="Spark提交"></a><a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">Spark提交</a></h2><p>注意：在使用Spark提交之前，一定要在环境变量中配置<code>HADOOP_CONF_DIR</code>，否则hadoop的环境引不进来</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HADOOP_CONF_DIR=XXX</span><br></pre></td></tr></table></figure>

<p>Spark支持的部署模式：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class &lt;main-class&gt; \</span><br><span class="line">  --master &lt;master-url&gt; \</span><br><span class="line">  --deploy-mode &lt;deploy-mode&gt; \</span><br><span class="line">  --conf &lt;key&gt;=&lt;value&gt; \</span><br><span class="line">  ... # other options</span><br><span class="line">  &lt;application-jar&gt; \</span><br><span class="line">  [application-arguments]</span><br></pre></td></tr></table></figure>

<p>一些常用的选项是：</p>
<ul>
<li><code>--class</code>：您的应用程序的入口点（例如<code>org.apache.spark.examples.SparkPi</code>）</li>
<li><code>--master</code>：集群的<a href="http://spark.apache.org/docs/latest/submitting-applications.html#master-urls" target="_blank" rel="noopener">主URL</a>（例如<code>spark://23.195.26.187:7077</code>）</li>
<li><code>--deploy-mode</code>：将驱动程序部署在工作节点（<code>cluster</code>）上还是作为外部客户端（<code>client</code>）本地部署（默认值：<code>client</code>）</li>
<li><code>--conf</code>：键值格式的任意Spark配置属性。对于包含空格的值，将“ key = value”用引号引起来（如图所示）。</li>
<li><code>application-jar</code>：jar包的路径，包括您的应用程序和所有依赖项。URL必须在群集内部全局可见，例如，<code>hdfs://</code>路径或<code>file://</code>所有节点上都存在的路径。</li>
<li><code>application-arguments</code>：参数传递给您的主类的main方法（如果有）</li>
</ul>
<p>其他常用的选项：</p>
<ul>
<li><p><code>--num-executors</code>：executors的数量</p>
</li>
<li><p><code>--executor-memory</code>：每个executor的内存数量</p>
</li>
<li><p><code>--total-executor-cores 100</code>：executor的总的core数</p>
</li>
<li><p><code>--jars</code>：指定需要依赖的jar包，多个jar包逗号分隔，application中直接引用</p>
</li>
<li><p><code>--files</code>：需要依赖的文件，在application中使用SparkFiles.get(“file”)取出，同时需要放在resources目录下</p>
</li>
</ul>
<p>注意：local模式默认读写HDFS数据 读本地要加<code>file://</code></p>
<h2 id="提交模式"><a href="#提交模式" class="headerlink" title="提交模式"></a>提交模式</h2><h3 id="cliet模式"><a href="#cliet模式" class="headerlink" title="cliet模式"></a>cliet模式</h3><ol>
<li>Driver运行在Client</li>
<li>AM职责就是去YARN上申请资源</li>
<li>Driver会和请求到的container/executor进行通信</li>
<li>Driver是不能退出的</li>
</ol>
<p><img src="https://yerias.github.io/spark_img/Client%E6%A8%A1%E5%BC%8F.jpg" alt="Client模式"></p>
<p><strong>Client模式控制台能看到日志</strong></p>
<h3 id="cluster模式"><a href="#cluster模式" class="headerlink" title="cluster模式"></a>cluster模式</h3><ol>
<li><p>Driver运行位置在AM</p>
</li>
<li><p>Client提交上去了  它退出对整个作业没影响</p>
</li>
<li><p>AM(申请资源)+Driver（调度DAG，分发任务）</p>
<p><img src="https://yerias.github.io/spark_img/Cluster%E6%A8%A1%E5%BC%8F.jpg" alt="Cluster模式"></p>
</li>
</ol>
<p><strong>控制台不能看到日志，不支持Spark-shell(Spark-SQL) ，交互性操作的都不能</strong></p>
<h2 id="窄依赖"><a href="#窄依赖" class="headerlink" title="窄依赖"></a>窄依赖</h2><ol>
<li>一个父RDD的分区至多被一个子RDD的某个分区使用一次</li>
<li>一个父RDD的分区和一个子RDD的分区是唯一映射 典型的map</li>
<li>多个父RDD的分区和一个子RDD的分区是唯一映射 典型的union</li>
</ol>
<p><img src="https://yerias.github.io/spark_img/%E7%AA%84%E4%BE%9D%E8%B5%96.jpg" alt="窄依赖"></p>
<p>在窄依赖中有个特殊的join是不经过shuffle 的</p>
<p>这个特殊的join的存在有三个条件：</p>
<ol>
<li>RDD1的分区数 = RDD2的分区数</li>
<li>RDD1的分区数 = Join的分区数</li>
<li>RDD2的分区数 = Join的分区数 </li>
</ol>
<p>我们看一个案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * rdd1、rdd2、join 三者的分区数相同，不经过shuffle</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>((<span class="string">"香蕉"</span>,<span class="number">20</span>), (<span class="string">"苹果"</span>,<span class="number">50</span>), (<span class="string">"菠萝"</span>,<span class="number">30</span>), (<span class="string">"猕猴桃"</span>, <span class="number">50</span>)),<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">List</span>((<span class="string">"草莓"</span>,<span class="number">90</span>), (<span class="string">"苹果"</span>,<span class="number">25</span>), (<span class="string">"菠萝"</span>,<span class="number">25</span>), (<span class="string">"猕猴桃"</span>, <span class="number">30</span>), (<span class="string">"西瓜"</span>, <span class="number">45</span>)),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.reduceByKey(_ + _)</span><br><span class="line"><span class="keyword">val</span> rdd4 = rdd2.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> joinRDD = rdd3.join(rdd4,<span class="number">2</span>)</span><br><span class="line">joinRDD.collect()</span><br></pre></td></tr></table></figure>

<p>再看Application的DAG图，从两个 reduceByKey 到 join 是一个 stage 中的，说明没有产生 shuffle</p>
<p><img src="https://yerias.github.io/spark_img/%E7%89%B9%E6%AE%8A%E7%9A%84Jion.jpg" alt="特殊的Jion"></p>
<h2 id="宽依赖"><a href="#宽依赖" class="headerlink" title="宽依赖"></a>宽依赖</h2><ol>
<li>一个父RDD的分区会被子RDD的分区使用多次</li>
</ol>
<p><img src="https://yerias.github.io/spark_img/%E5%AE%BD%E4%BE%9D%E8%B5%96.jpg" alt="宽依赖"></p>
<p>除了前面那种是三个条件满足的，其他的 join 都是宽依赖</p>
<p>我们使RDD1的分区数和RDD2的分区数相等，但是 join的分区数不相等</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * rdd1、rdd2的分区数不同，但是和join的分区数不同，会经过shuffle</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>((<span class="string">"香蕉"</span>,<span class="number">20</span>), (<span class="string">"苹果"</span>,<span class="number">50</span>), (<span class="string">"菠萝"</span>,<span class="number">30</span>), (<span class="string">"猕猴桃"</span>, <span class="number">50</span>)),<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">List</span>((<span class="string">"草莓"</span>,<span class="number">90</span>), (<span class="string">"苹果"</span>,<span class="number">25</span>), (<span class="string">"菠萝"</span>,<span class="number">25</span>), (<span class="string">"猕猴桃"</span>, <span class="number">30</span>), (<span class="string">"西瓜"</span>, <span class="number">45</span>)),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.reduceByKey(_ + _)</span><br><span class="line"><span class="keyword">val</span> rdd4 = rdd2.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> joinRDD = rdd3.join(rdd4,<span class="number">4</span>)</span><br><span class="line">joinRDD.collect()</span><br></pre></td></tr></table></figure>

<p>我们看DAG图，产生了stage，也就是经过了shuffle</p>
<p><img src="https://yerias.github.io/spark_img/%E7%89%B9%E6%AE%8A%E7%9A%84Jion%E5%AE%BD%E4%BE%9D%E8%B5%96.jpg" alt="特殊的Jion宽依赖"></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/30/error/5/">Spark疯狂踩坑系列</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-30</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Error/">Error</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Error/">Error</a></span><div class="content"><p>如果WEB UI界面或者程序日志里面看不到错误，使用以下方式查看日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn logs -applicationId application_1585536649766_xxxx</span><br></pre></td></tr></table></figure>



<p>错误1</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Error: Could not find or load main class org.apache.spark.deploy.yarn.ApplicationMaster</span><br></pre></td></tr></table></figure>

<p>解决办法：</p>
<p>检查spark-defaults.conf中的配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark.yarn.jars=hdfs://hadoop:9000/spark-yarn/jars/*.jar</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark.yarn.archive  hdfs://hadoop000:8020/tmp/spark-archive/spark2.4.5.zip</span><br></pre></td></tr></table></figure>

<p>以上两种配置方式不可以错乱</p>
<p>错误2</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">java.lang.NoClassDefFoundError: org/lionsoul/ip2region/DbConfig</span><br></pre></td></tr></table></figure>

<p>解决办法</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--jars /home/hadoop/lib/ip2region-1.7.2.jar</span><br></pre></td></tr></table></figure>



<p>错误3</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">java.lang.NullPointerException</span><br><span class="line">	at com.tunan.spark.utils.IpParseUtil.IpParse(IpParseUtil.java:19)</span><br></pre></td></tr></table></figure>

<p>解决办法</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--files /home/hadoop/lib/ip2region.db</span><br></pre></td></tr></table></figure>

<hr>
<p>代码中拿出文件有两种方式</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String dbPath = GetIPRegion.class.getResource("/ip2region.db").getPath();</span><br><span class="line">String dbPath = SparkFiles.get(<span class="string">"/ip2region.db"</span>);</span><br></pre></td></tr></table></figure>

</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/29/spark/5/">Spark使用Yarn模式解决Jar乱飞情况</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-29</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><ol>
<li><p>在本地创建zip文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在spark的jars目录下</span></span><br><span class="line">zip spark.zip ./*</span><br></pre></td></tr></table></figure>
</li>
<li><p>HDFS上创建存放spark jar的目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -mkdir -p  /spark-yarn/jars</span><br></pre></td></tr></table></figure>
</li>
<li><p>将$SPARK_HOME/jars下的spark.zip包上传至刚建的HDFS路径</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop jars]$ hadoop fs -put ./spark.zip /spark-yarn/jars/</span><br></pre></td></tr></table></figure>
</li>
<li><p>在 spark-defaults.conf中添加(也可以在启动的时候–conf指定)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark.yarn.archive=hdfs://hadoop:9000/spark-yarn/jars/spark.zip</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看Spark log</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn log -applicationID xxx</span><br></pre></td></tr></table></figure>

</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/26/error/4/">MR编程时，Driver传递的参数Mapper显示为NULL</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-26</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Error/">Error</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Error/">Error</a></span><div class="content"><p>在进行MR编程时，除了需要拿到HDFS上面的数据，有时候还需要Driver和Mapper或者Reducer之间进行参数传递</p>
<p>先看看我碰到的问题</p>
<p><img src="https://yerias.github.io/java_img/14.png" alt=""></p>
<p><img src="https://yerias.github.io/java_img/15.png" alt=""></p>
<p>在Driver中配置向Conf中配置了参数，在Mapper中从Context中拿出来的却是null值，<strong>问题出现在Job.getInstance() 中没有把Conf传递进去。</strong></p>
<p>所以以后要注意，在创建Job的时候要把Conf也放进去。</p>
<p>还需要注意的是Conf中的设值必须在Job.getInstance()上面完成</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/26/jvm/4/">JVM之内存模型</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-26</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Java/">Java</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Java/">Java</a></span><div class="content"><p>Java内存模型其实就是围绕着在并发过程中如果解决原子性、有序性和可见性的通信规则</p>
<p><img src="https://yerias.github.io/java_img/1.jpg" alt="线程、主内存、工作内存三者之间的关系"></p>
<h2 id="主内存与工作内存"><a href="#主内存与工作内存" class="headerlink" title="主内存与工作内存"></a>主内存与工作内存</h2><p>Java内存模型的主要目的就是定义程序中各种变量的访问规则，即关注在虚拟机中把变量存储到内存和从内存中取出变量这样的底层细节。</p>
<p>此处的变量指的是包括了实例字段，静态字段和构成数组对象的元素。但不包括局部变量和方法参数，因为后者是线程私有的。</p>
<ol>
<li><p>主内存和工作内存的关系？</p>
<p>Java内存模型规定了所有的变量都存储在主内存中，每条线程还有自己的工作内存，线程的工作内存保存了被该线程使用的变量的主内存副本。线程对变量的所有操作(读取、赋值等)都必须在工作内存中进行，而不能直接读写主内存中的数据。不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递需要通过主内存来完成。</p>
</li>
<li><p>主内存和工作内存如何交互？</p>
<p>主内存和工作内存如何交互？即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步回主内存。Java内存模型定义了8种操作来完成。分别是</p>
<ul>
<li>lock(锁定)</li>
<li>unlock(解锁)</li>
<li>read(读取)</li>
<li>load(载入)</li>
<li>use(使用)</li>
<li>assign(赋值)</li>
<li>store(存储)</li>
<li>write(写入)</li>
</ul>
<p>他们每一种操作都是原子的、不可再分的(double和long类型，32位主机会拆分成两次执行)</p>
<p>他们规定了八种执行规则，但这不是我们关心的重点，作为开发者更需要了解的是先行发生原则–用来确定一个操作在并发环境下是否安全</p>
</li>
</ol>
<h2 id="Volatile变量的特殊规则"><a href="#Volatile变量的特殊规则" class="headerlink" title="Volatile变量的特殊规则"></a>Volatile变量的特殊规则</h2><p>Volatile是Java虚拟机提供的最轻量级的<strong>同步机制</strong></p>
<p>为什么说他是最轻量级的同步机制？因为它只能保证可见性、有序性，而不能保证原子性。虽然应用场景有限，但是快，能保证其他线程的立即可见性。</p>
<p>当一个变量被定义成volatile之后，它具备两项特性：</p>
<ol>
<li><p><strong>保证此变量对所有线程的可见性，</strong>这里的可见性是指一条线程修改了这个变量的值，新值对于其他线程来说是立即得知的。由于volatile不能保证原子性(比如运算是分两步来做的)，只能在以下两条规则的场景中进行运算。</p>
<ul>
<li>运算结果不依赖变量的当前值。</li>
<li>变量不需要与其他的状态变量共同参与不变约束。</li>
</ul>
<p>简单点说，就是自己玩自己的，典型的引用场景就是作位状态标志的修饰符。</p>
</li>
<li><p><strong>通过加入内存屏障禁止指令重排序</strong>，指令重排序优化是编辑器做的一种代码执行顺序的优化，只关注结果，不关注过程，但是这么做可能让程序逻辑混乱，比如本来应该在后面执行的代码，跑到前面来执行了，这时候就要使用volatile禁止指令重排序，从而保证代码的执行顺序和程序的顺序相同。</p>
</li>
</ol>
<h2 id="可见性、有序性、原子性"><a href="#可见性、有序性、原子性" class="headerlink" title="可见性、有序性、原子性"></a>可见性、有序性、原子性</h2><ol>
<li><p>原子性</p>
<p>Java内存模型直接保证的原子性变量操作包括read、load、assign、use、store、write这六个，我们大致可以认为，对基本数据类型的访问、读写都是具备原子性的。</p>
<p>对于其他场景的原子性保证，Java内存模型提供了lock和unlock操作满足这种需求，反应到代码中就是synchronized关键字，因此<strong>synchronized是具备原子性的</strong>。</p>
</li>
<li><p>可见性</p>
<p>可见性就是指当一个线程修改了共享变量的值时，其他线程能够立即得知这个修改。</p>
<p><strong>volatile</strong>的特殊规则保证了新值能够立即从工作内存同步到主内存，以及每次使用前从主内存刷新。</p>
<p><strong>synchronized</strong>是通过”对一个变量执行unlock操作之前，必须先把此变量同步回主内存中”实现的。</p>
<p><strong>final</strong>关键字修饰的字段在构造器中初始化完成，并且构造器没有把”this”的引用逃逸出去，那么在其他线程中就能看见final修饰字段的值。</p>
</li>
<li><p>有序性</p>
<p>java程序中天然的有序性可以总结为一句话: 如果在本线程内观察，所有的操作都是有序的；如果在一个线程中观察另一个线程，所有的操作都是无序的。前半句是指 “线程内变量为串行语义”，后半句是指 “指令重排序” 现象和 “工作内存和主内存同步延迟” 现象。</p>
<p><strong>volatile</strong>关键字本身就包含了禁止指令重排序的语义。</p>
<p><strong>synchronized</strong>是由 “一个变量在同一时刻只允许一条线程对其进行lock操作” 这条规则获得的。 </p>
</li>
</ol>
<h2 id="先行发生原则"><a href="#先行发生原则" class="headerlink" title="先行发生原则"></a>先行发生原则</h2><p>先行发生是Java内存模型定义的两项操作之间的顺序关系，如果说操作A先行发生于操作B，其实就是说在发生操作B之前，操作A产生的影响被操作B观察到，”影响” 包括修改了内存中共享变量的值、发生了消息、调用了方法等。</p>
<p>Java内存模型中 “天然的” 先行发生关系包括以下八种，如果两个操作之间的关系不在此列，并且无法从下列规则推导出来，则他们就没有<strong>顺序型保障</strong>，虚拟机可以对它们进行任意的<strong>重排序</strong>。 </p>
<ol>
<li>程序次序规则</li>
<li>管程锁定规则</li>
<li>volatile变量规则</li>
<li>线程启动规则</li>
<li>线程终止规则</li>
<li>线程中断规则</li>
<li>对象终结规则</li>
<li>传递性</li>
</ol>
<ul>
<li>“时间上的先后顺序” 与 “先行发生” 之间有什么不同?</li>
</ul>
<p>时间先后顺序与先行发生原则之间基本没有因果关系，所以我们衡量并发安全问题的时候不要受时间顺序的干扰，一切必须以先行发生原则为准。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/25/spark/4/">Spark之排序模块</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-25</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><p>目录</p>
<ol>
<li>算子排序</li>
<li>面向对象排序</li>
<li>隐式转换排序</li>
<li>Ordering.on排序</li>
</ol>
<p>Spark中的排序模块，顾名思义，这篇文章都是说如何排序</p>
<h2 id="算子排序"><a href="#算子排序" class="headerlink" title="算子排序"></a>算子排序</h2><p>的确，在Spark中有很多算子可以排序，可以给数组排序，可以给键值对排序，我们会使用算子引入排序，然后再重点介绍如何使用隐式转换达到排序的效果。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">(<span class="string">"西瓜 20 100"</span>, <span class="string">"苹果 10 500"</span>, <span class="string">"香蕉 10 30"</span>, <span class="string">"菠萝 30 200"</span>)</span><br></pre></td></tr></table></figure>

<p>现在我们有一行数据，我们如何使它按价格降序排序？</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//TODO List中每个东西的含义：名称name、价格price、库存amount</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="string">"西瓜 20 100"</span>, <span class="string">"苹果 10 500"</span>, <span class="string">"香蕉 10 30"</span>, <span class="string">"菠萝 30 200"</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//拆分map拿出来 ==&gt; 返回tuple(productData) ==&gt; 排序sortBy</span></span><br><span class="line"><span class="keyword">val</span> mapRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    (name, price, amount)</span><br><span class="line">&#125;)</span><br><span class="line">mapRDD.sortBy(-_._2).collect().foreach(println);</span><br></pre></td></tr></table></figure>

<p>也许你会认为很简单，那么现在要求按价格降序排序，价格相同库存降序排序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//拆分 ==&gt; 返回tuple() ==&gt; 排序sortBy(x =&gt; (-x._2,-x._3))</span></span><br><span class="line"><span class="keyword">val</span> mapRDD2 = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    (name, price, amount)</span><br><span class="line">&#125;)</span><br><span class="line">mapRDD2.sortBy(x=&gt;(-x._2,-x._3)).collect().foreach(println);</span><br></pre></td></tr></table></figure>

<p>的确，使用算子排序很简单，但是简洁的代码，能让你回想起几个月前的这几行是什么意思吗？</p>
<h2 id="面向对象排序"><a href="#面向对象排序" class="headerlink" title="面向对象排序"></a>面向对象排序</h2><p>现在我们引入面向对象的方式来排序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//TODO 面向对象的方式实现</span></span><br><span class="line"><span class="keyword">val</span> productRDD: <span class="type">RDD</span>[<span class="type">ProductCaseClass</span>] = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Products</span>(name, price, amount)</span><br><span class="line">&#125;)</span><br><span class="line">productRDD.sortBy(x=&gt;x).collect().foreach(println)</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Products</span>(<span class="params">val name: <span class="type">String</span>, val price: <span class="type">Double</span>, val amount: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">Ordered</span>[<span class="type">Products</span>]</span></span><br><span class="line"><span class="class">    <span class="keyword">with</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = <span class="string">s"<span class="subst">$name</span> | <span class="subst">$price</span> | <span class="subst">$amount</span>"</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">Products</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        that.price.toInt-<span class="keyword">this</span>.price.toInt</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>你是不是傻？case class 自己实现了序列化，实现了toString、equals、hashCode，用起来还不需要new，创建class干啥？</p>
<p>于是再次改造</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> productRDD: <span class="type">RDD</span>[<span class="type">ProductCaseClass</span>] = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    <span class="type">ProductCaseClass</span>(name, price, amount)</span><br><span class="line">&#125;)</span><br><span class="line">productRDD.sortBy(x=&gt;x).collect().foreach(println)</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ProductCaseClass</span>(<span class="params">name:<span class="type">String</span>,price:<span class="type">Double</span>,amount:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Ordered</span>[<span class="type">ProductCaseClass</span>]</span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> =  <span class="string">s"case class:  <span class="subst">$name</span> | <span class="subst">$price</span> | <span class="subst">$amount</span>"</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">ProductCaseClass</span>): <span class="type">Int</span> = that.price.toInt - <span class="keyword">this</span>.price.toInt</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="隐式转换排序"><a href="#隐式转换排序" class="headerlink" title="隐式转换排序"></a>隐式转换排序</h2><p>于是呼~ 不爽，现在只给你一个最普通的类，给我增强出带排序功能的类。这不是隐式转换吗？</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//一个最普通的类,不实现ordered排序的功能</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductsInfo</span>(<span class="params">val name: <span class="type">String</span>, val price: <span class="type">Double</span>, val amount: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = <span class="string">s"ProductsInfo:  <span class="subst">$name</span> | <span class="subst">$price</span> | <span class="subst">$amount</span>"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>那么我们该如何改造它？</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> productRDD: <span class="type">RDD</span>[<span class="type">ProductsInfo</span>] = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ProductsInfo</span>(name, price, amount)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p>我就把数据切分出来，返回一个<code>ProductsInfo</code>，其他啥都不干</p>
<p>现在最重要的就是实现隐式转换，将ProductsInfo增加排序的功能，排序的规则还要自定义</p>
<ol>
<li><p>隐式方法/转换 </p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">ProductsInfo2Orderding</span></span>(productsInfo:<span class="type">ProductsInfo</span>):<span class="type">Ordered</span>[<span class="type">ProductsInfo</span>] = <span class="keyword">new</span> <span class="type">Ordered</span>[<span class="type">ProductsInfo</span>] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">ProductsInfo</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span> (that.price-productsInfo.price&gt;<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="number">1</span></span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (that.price-productsInfo.price==<span class="number">0</span> &amp;&amp; that.amount-productsInfo.amount &gt;<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="number">1</span></span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="number">-1</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>隐式变量</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> <span class="type">ProductsInfo2Orderding</span>:<span class="type">Ordering</span>[<span class="type">ProductsInfo</span>] = <span class="keyword">new</span> <span class="type">Ordering</span>[<span class="type">ProductsInfo</span>] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">ProductsInfo</span>, y: <span class="type">ProductsInfo</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        y.amount - x.amount</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>隐式对象</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">ProductsInfo22Orderding</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[<span class="type">ProductsInfo</span>]</span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">ProductsInfo</span>, y: <span class="type">ProductsInfo</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        y.amount - x.amount</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>这三种方式都可以达到增强<code>ProductsInfo</code>的功能，他们都离不开一个公式：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">x2y</span></span>(普通的x):牛逼的y = <span class="keyword">new</span> 牛逼的y(普通的x)</span><br></pre></td></tr></table></figure>

<p>上面的隐式转换就是这个公式的直接应用</p>
<h2 id="Ordering-on排序"><a href="#Ordering-on排序" class="headerlink" title="Ordering.on排序"></a>Ordering.on排序</h2><p>你也许会说，够了吧，这么多种方式排序了，累都累死人了，那么我告诉有一种方法，不需要case class、不需要class、不需要隐式转换，只有一行代码就能排序，学不学？</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> productRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = split(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> price = split(<span class="number">1</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> amount = split(<span class="number">2</span>).toInt</span><br><span class="line">    (name, price, amount)	<span class="comment">//请注意，我们这里productRDD返回的是一个tuple</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p>实现排序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * (Double,Int) : 定义排序规则的返回值类型,可以是class</span></span><br><span class="line"><span class="comment"> * (String,Double,Int) : 进来数据的类型</span></span><br><span class="line"><span class="comment"> * (x =&gt; (-x._2,-x._3)) : 定义排序的规则</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> order = <span class="type">Ordering</span>[(<span class="type">Double</span>,<span class="type">Int</span>)].on[(<span class="type">String</span>,<span class="type">Double</span>,<span class="type">Int</span>)](x =&gt; (-x._2,-x._3))</span><br></pre></td></tr></table></figure>

<p>是不是看懵逼了？使用<code>productRDD</code>调用<code>sortBy()</code>就输出了排序后的结果</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">productRDD.sortBy(x =&gt; x).print()</span><br></pre></td></tr></table></figure>

<p>结果:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">(菠萝,<span class="number">30.0</span>,<span class="number">200</span>)</span><br><span class="line">(西瓜,<span class="number">20.0</span>,<span class="number">100</span>)</span><br><span class="line">(苹果,<span class="number">10.0</span>,<span class="number">500</span>)</span><br><span class="line">(香蕉,<span class="number">10.0</span>,<span class="number">30</span>)</span><br></pre></td></tr></table></figure>

</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/25/error/3/">执行Hive SQL/MR 报错：Current usage: 77.8mb of 512.0mb physical memory used; 1.1gb of 1.0gb virtual memory used. Killing container.</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-25</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Error/">Error</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Error/">Error</a></span><div class="content"><p>从错误消息中，可以看到使用的虚拟内存超过了当前1.0gb的限制。这可以通过两种方式解决：</p>
<p><strong>禁用虚拟内存限制检查</strong></p>
<p>YARN只会忽略该限制；为此，请将其添加到您的<code>yarn-site.xml</code>：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether virtual memory limits will be enforced for containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>此设置的默认值为<code>true</code>。</p>
<p><strong>增加虚拟内存与物理内存的比率</strong></p>
<p>在<code>yarn-site.xml</code>更改中，此值将高于当前设置</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Ratio between virtual memory to physical memory when setting memory limits for containers. Container allocations are expressed in terms of physical memory, and virtual memory usage is allowed to exceed this allocation by this ratio.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>默认是 <code>2.1</code></p>
<p>还可以增加分配给容器的物理内存量。</p>
<p>注意：更改配置后不要忘记重新启动yarn。</p>
<hr>
<p>如果不能修改集群配置，我们可以参考这么做：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">-Dmapreduce.map.memory.mb=<span class="number">4096</span></span><br></pre></td></tr></table></figure>

<hr>
<p>在运行MR的作业中我们需要关心一下几个参数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">yarn.nodemanager.resource.memory-mb</span><br><span class="line">yarn.scheduler.minimum-allocation-mb</span><br><span class="line">yarn.scheduler.maximum-allocation-mb</span><br><span class="line">yarn.nodemanager.vmem-check-enabled</span><br><span class="line">yarn.nodemanager.vmem-pmem-ratio</span><br></pre></td></tr></table></figure></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/25/error/2/">MapJoin，文件在HDFS上Idea报错：File does not exist: /xxx/yyy.txt#yyy.txt</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-25</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Error/">Error</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Error/">Error</a></span><div class="content"><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.io.FileNotFoundException: File does not exist: /data/dept.txt#dept.txt</span><br></pre></td></tr></table></figure>

<p>先去HDFS上确定文件是否存在，文件不存在，put文件上去，再次运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://192.168.91.10:9000/data/emp.txt</span><br></pre></td></tr></table></figure>

<p>有是Path找不到，再去HDFS上检查这个文件是否存在，文件不存在，再次put上去，然后运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INFO DFSClient: Could not obtain BP-1292531802-192.168.181.10-1583457649867:blk_1073746058_5236 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[192.168.91.10:50010,DS-f8088840-5065-4320-8b51-4563ccec125a,DISK] Dead nodes:  DatanodeInfoWithStorage[192.168.91.10:50010,DS-f8088840-5065-4320-8b51-4563ccec125a,DISK]. Will get new block locations from namenode and retry...</span><br><span class="line">WARN DFSClient: DFS chooseDataNode: got # 2 IOException, will wait for 3263.5374223592803 msec.</span><br><span class="line">WARN BlockReaderFactory: I/O error constructing remote block reader.</span><br><span class="line">java.net.ConnectException: Connection timed out: no further information</span><br></pre></td></tr></table></figure>

<p>继续报错，连接不到DataNode的块，去命令行输入jps发现进程没挂，然后我怀疑是因为我的host没有配置hadoop的ip映射关系，于是配上后，继续运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java.lang.Exception: java.io.FileNotFoundException: dept.txt (系统找不到指定的文件。)</span><br><span class="line">Caused by: java.io.FileNotFoundException: dept.txt (系统找不到指定的文件。)</span><br></pre></td></tr></table></figure>

<p>不出所望的再次报错，这次的日志多，上下文分析一下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">WARN FileUtil: Command &apos;G:\hadoop-2.7.2\bin\winutils.exe symlink E:\Java\hadoop\hadoop-client\dept.txt \tmp\hadoop-Tunan\mapred\local\1585106809398\dept.txt&apos; failed 1 with: CreateSymbolicLink error (1314): ???????????</span><br><span class="line">WARN LocalDistributedCacheManager: Failed to create symlink: \tmp\hadoop-Tunan\mapred\local\1585106809398\dept.txt &lt;- E:\Java\hadoop\hadoop-client/dept.txt</span><br></pre></td></tr></table></figure>

<p>由于我们给hdfs上面文件的路径创建了一个链接，也就是symlink，我们发现这个两个警告是symlink创建失败，于是使用管理员的身份启动Idea，解决</p>
<hr>
<p>需要注意是除了管理员身份启动Idea，还需要在在程序的main方法下开启symlink的功能，才能在<code>new FileInputStream(new File(&quot;dept.txt&quot;))</code>中使用</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">FileSystem.enableSymlinks();</span><br></pre></td></tr></table></figure>

<p>还需要注意的是本地操作HDFS的一些配置</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"hadoop"</span>);</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://192.168.91.10:9000"</span>);</span><br><span class="line">conf.set(<span class="string">"dfs.client.use.datanode.hostname"</span>, <span class="string">"true"</span>);</span><br></pre></td></tr></table></figure>

</div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/3/">&lt;&lt;</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/5/">&gt;&gt;</a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>