<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="BigData Developer"><meta name="keywords" content="yerias,TUNANのBlog,BigData"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017 | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">133</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">30</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">26</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">TUNANのBlog</div><div id="site-sub-title">感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017</div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2020/04/10/spark/12/">Spark中的序列化</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-10</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><p>在写Spark应用时，常常会碰到序列化的问题。例如，在Driver端的程序中创建了一个对象，而在各个Executor端会用到这个对象——由于Driver端的代码和Executor端的代码在不同的JVM中，甚至在不同的节点上，因此必然要有相应</p>
<h2 id="Java框架进行序列化"><a href="#Java框架进行序列化" class="headerlink" title="Java框架进行序列化"></a>Java框架进行序列化</h2><p>在默认情况下，Spark会使用Java的ObjectOutputStream框架对对象进行序列化，并且可以与任何实现java.io.Serializable的类一起工作。您还可以通过扩展java.io.Externalizable来更紧密地控制序列化的性能。Java序列化是灵活的，但通常相当慢，并且会导致许多类的大型序列化格式。</p>
<p>测试代码：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">String</span>, name: <span class="type">String</span>, age: <span class="type">Int</span>, gender: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">SerializationDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"SerializationDemo"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> stduentArr = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Student</span>]()</span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">1000000</span>) &#123;</span><br><span class="line">            stduentArr += (<span class="type">Student</span>(i + <span class="string">""</span>, i + <span class="string">"a"</span>, <span class="number">10</span>, <span class="string">"male"</span>))</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> <span class="type">JavaSerialization</span> = sc.parallelize(stduentArr)</span><br><span class="line">        <span class="type">JavaSerialization</span>.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>).count()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="type">Thread</span>.sleep(<span class="number">10000</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在Web界面查看：33.2M</p>
<h2 id="Kryo框架进行序列化"><a href="#Kryo框架进行序列化" class="headerlink" title="Kryo框架进行序列化"></a>Kryo框架进行序列化</h2><p>Spark还可以使用Kryo库（Spark 2.x）来更快地序列化对象。Kryo比Java（通常多达10倍）要快得多，也更紧凑，但是不支持所有可串行化类型，并且要求您提前注册您将在程序中使用的类，以获得最佳性能。</p>
<ol>
<li><p>不注册使用的类测试</p>
<p>在conf中配置spark.serializer = org.apache.spark.serializer.KryoSerializer来使用kryo序列化</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">String</span>, name: <span class="type">String</span>, age: <span class="type">Int</span>, gender: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">SerializationDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">        .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">        .setAppName(<span class="string">"SerializationDemo"</span>)</span><br><span class="line">        .set(<span class="string">"spark.serializer"</span>,<span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> stduentArr = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Student</span>]()</span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">1000000</span>) &#123;</span><br><span class="line">            stduentArr += (<span class="type">Student</span>(i + <span class="string">""</span>, i + <span class="string">"a"</span>, <span class="number">10</span>, <span class="string">"male"</span>))</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> <span class="type">JavaSerialization</span> = sc.parallelize(stduentArr)</span><br><span class="line">        <span class="type">JavaSerialization</span>.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>).count()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="type">Thread</span>.sleep(<span class="number">10000</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在Web界面查看：53.2M</p>
<p>这是因为使用Kryo时，不将使用的类注册，往往会得到比java序列化占用更大的内存</p>
</li>
<li><p>注册使用的类测试</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">String</span>, name: <span class="type">String</span>, age: <span class="type">Int</span>, gender: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">SerializationDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">        .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">        .setAppName(<span class="string">"SerializationDemo"</span>)</span><br><span class="line">        .set(<span class="string">"spark.serializer"</span>,<span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line">        .registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">Student</span>])) <span class="comment">// 将自定义的类注册到Kryo</span></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> stduentArr = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Student</span>]()</span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">1000000</span>) &#123;</span><br><span class="line">            stduentArr += (<span class="type">Student</span>(i + <span class="string">""</span>, i + <span class="string">"a"</span>, <span class="number">10</span>, <span class="string">"male"</span>))</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> <span class="type">JavaSerialization</span> = sc.parallelize(stduentArr)</span><br><span class="line">        <span class="type">JavaSerialization</span>.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>).count()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="type">Thread</span>.sleep(<span class="number">10000</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在Web界面查看：21.7 M</p>
<p>在conf中注册</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="总结及拓展"><a href="#总结及拓展" class="headerlink" title="总结及拓展"></a>总结及拓展</h2><p>Kryo serialization 性能和序列化大小都比默认提供的 Java serialization 要好，但是<strong>使用Kryo需要将自定义的类先注册进去</strong>，使用起来比 Java serialization麻烦。自从Spark 2.x 以来，我们在使用简单类型、简单类型数组或字符串类型的简单类型来调整RDDs时，在内部使用Kryo序列化器。</p>
<p>通过查找sparkcontext初始化的源码，可以发现某些类型已经在sparkcontext初始化的时候被注册进去。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Component which configures serialization, compression and encryption for various Spark</span></span><br><span class="line"><span class="comment"> * components, including automatic selection of which [[Serializer]] to use for shuffles.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">SerializerManager</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    defaultSerializer: <span class="type">Serializer</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    conf: <span class="type">SparkConf</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    encryptionKey: <span class="type">Option</span>[<span class="type">Array</span>[<span class="type">Byte</span>]]</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(defaultSerializer: <span class="type">Serializer</span>, conf: <span class="type">SparkConf</span>) = <span class="keyword">this</span>(defaultSerializer, conf, <span class="type">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> kryoSerializer = <span class="keyword">new</span> <span class="type">KryoSerializer</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> stringClassTag: <span class="type">ClassTag</span>[<span class="type">String</span>] = implicitly[<span class="type">ClassTag</span>[<span class="type">String</span>]]</span><br><span class="line">    <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> primitiveAndPrimitiveArrayClassTags: <span class="type">Set</span>[<span class="type">ClassTag</span>[_]] = &#123;</span><br><span class="line">        <span class="keyword">val</span> primitiveClassTags = <span class="type">Set</span>[<span class="type">ClassTag</span>[_]](</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Boolean</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Byte</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Char</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Double</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Float</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Int</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Long</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Null</span>,</span><br><span class="line">            <span class="type">ClassTag</span>.<span class="type">Short</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">val</span> arrayClassTags = primitiveClassTags.map(_.wrap)</span><br><span class="line">        primitiveClassTags ++ arrayClassTags</span><br></pre></td></tr></table></figure>

<p>也就是说，Boolean、Byte、Char、Double、Float、Int、Long、Null、Short这些类型修饰的属性，自动使用kryo序列化。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/04/10/spark/10/">Spark之分组TopN模块</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-10</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><p>在Spark中，分组TopN好写，但是如果想写出性能好的代码却也很难。下面我们将通过写TopN的方式，找出问题，解决问题。</p>
<ol>
<li><p>直接reduceByKey完成分组求和排序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> in = <span class="string">"file:///home/hadoop/data/site.log"</span></span><br><span class="line">    <span class="comment">//连接SparkMaster</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">        ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)	<span class="comment">//((domain,url),1)</span></span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = mapRDD.reduceByKey(_ + _).groupBy(x =&gt; x._1._1).mapValues( x=&gt; x.toList.sortBy(x =&gt; -x._2).map(x =&gt; (x._1._1,x._1._2,x._2)).take(<span class="number">2</span>))</span><br><span class="line">    result.foreach(println)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该方法虽然直接，但是在reduceByKey和groupBy分别进过了shuffle，而且x.toList是一个非常吃内存的操作，如果数据量大，直接OOM</p>
</li>
<li><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> in = <span class="string">"tunan-spark-core/data/site.log"</span></span><br><span class="line">    <span class="comment">//连接SparkMaster</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">        ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> domains = <span class="type">Array</span>(<span class="string">"www.google.com"</span>, <span class="string">"www.ruozedata.com"</span>, <span class="string">"www.baidu.com"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (domain &lt;- domains)&#123;</span><br><span class="line">        mapRDD.filter(x =&gt; x._1._1.equals(domain)).reduceByKey(_+_).sortBy(x =&gt; -x._2).take(<span class="number">2</span>).foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>核心思想：把需要分组分类的数据提前拿出来，在filter中过滤，每次执行一个分组，虽然减少了一次shuffle，但是我们不可能每次都把需要的数据都能提前拿到数据</p>
</li>
<li><p>使用ditinct.collect返回的数组替换人为创建的数组</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> in = <span class="string">"tunan-spark-core/data/site.log"</span></span><br><span class="line">    <span class="comment">//连接SparkMaster</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">        ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> domains = mapRDD.map(x =&gt; x._1._1).distinct().collect()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (domain &lt;- domains)&#123;</span><br><span class="line">        mapRDD.filter( x =&gt; domain.equals(x._1._1)).reduceByKey(_+_).sortBy(x =&gt; -x._2).take(<span class="number">2</span>).foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>有人说distinct性能不好，但是我们这里使用去重的是domain，这个数据量并不是很大，可以勉强接受，现在每次都使用for循环来处理数据，能不能更加优化一下呢</p>
</li>
<li><p>使用分区执行替换for循环</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> in = <span class="string">"tunan-spark-core/data/site.log"</span></span><br><span class="line">    <span class="comment">//连接SparkMaster</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">        ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> domains = mapRDD.map(x =&gt; x._1._1).distinct().collect()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapPartRDD = mapRDD.reduceByKey(<span class="keyword">new</span> <span class="type">MyPartitioner</span>(domains), _ + _).mapPartitions(partition =&gt; &#123;</span><br><span class="line">        partition.toList.sortBy(x =&gt; -x._2).take(<span class="number">2</span>).iterator</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    mapPartRDD.foreach(println)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>自定义的分区类</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span>(<span class="params">domains:<span class="type">Array</span>[<span class="type">String</span>]</span>) <span class="keyword">extends</span> <span class="title">Partitioner</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> map = mutable.<span class="type">HashMap</span>[<span class="type">String</span>,<span class="type">Int</span>]()</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until (domains.length))&#123;</span><br><span class="line">        map(domains(i)) = i</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = domains.length</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> domain = key.asInstanceOf[(<span class="type">String</span>, <span class="type">String</span>)]._1</span><br><span class="line">        map(domain)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这么做的好处是原本一起计算的RDD，现在每个分区里面去计算了，虽然toList内存占用大，但是还凑合，最终的版本就是把toList替换掉。</p>
</li>
<li><p>使用TreeSet替换toList实现最终的排序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-core/data/site.log"</span></span><br><span class="line">        <span class="comment">//连接SparkMaster</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="keyword">this</span>.getClass.getSimpleName).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> fileRDD = sc.textFile(in)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> mapRDD = fileRDD.map(lines =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = lines.split(<span class="string">"\t"</span>)</span><br><span class="line">            ((words(<span class="number">0</span>), words(<span class="number">1</span>)), <span class="number">1</span>)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> domains = mapRDD.map(x =&gt; x._1._1).distinct().collect()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> ord: <span class="type">Ordering</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = <span class="keyword">new</span> <span class="type">Ordering</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)]() &#123;</span><br><span class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: ((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>), y: ((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)): <span class="type">Int</span> = &#123;</span><br><span class="line">                <span class="keyword">if</span> (!x._1.equals(y._1) &amp;&amp; x._2 == y._2) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//  降序排</span></span><br><span class="line">                y._2 - x._2</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> treeSort = mapRDD.reduceByKey(<span class="keyword">new</span> <span class="type">MyPartitioner</span>(domains), _ + _).mapPartitions(partition =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> set = mutable.<span class="type">TreeSet</span>.empty(ord)</span><br><span class="line">            partition.foreach(x =&gt; &#123;</span><br><span class="line">                set.add(x)</span><br><span class="line">                <span class="keyword">if</span> (set.size &gt; <span class="number">2</span>) &#123;</span><br><span class="line">                    set.remove(set.lastKey) <span class="comment">//移除最后一个</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            set.toIterator</span><br><span class="line">        &#125;).collect()</span><br><span class="line">        treeSort.foreach(println)</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>使用TreeSet实现自定义排序器，使之每次维护的只有需要的极少量数据，这样占用内存少，效率最高。</p>
</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/04/03/spark/11/">SparkSQL&amp;DataFrame的read和write&amp;SparkSQL做统计分析&amp;UDF函数&amp;存储格式的转换</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-03</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>SparkSQL</li>
<li>DataFrame的read和write</li>
<li>SparkSQL做统计分析</li>
<li>UDF函数</li>
<li>存储格式的转换</li>
</ol>
<h2 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h2><h3 id="认识SparkSQL"><a href="#认识SparkSQL" class="headerlink" title="认识SparkSQL"></a>认识SparkSQL</h3><ol>
<li><p>SparkSQL的进化之路</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.0以前：</span><br><span class="line">   Shark</span><br><span class="line">1.1.x开始：</span><br><span class="line">   SparkSQL(只是测试性的) SQL</span><br><span class="line">1.3.x:</span><br><span class="line">   SparkSQL(正式版本)+Dataframe</span><br><span class="line">1.5.x:</span><br><span class="line">SparkSQL 钨丝计划</span><br><span class="line">1.6.x：</span><br><span class="line">   SparkSQL+DataFrame+DataSet(测试版本)</span><br><span class="line">2.x.x:</span><br><span class="line">   SparkSQL+DataFrame+DataSet(正式版本)</span><br><span class="line">   SparkSQL:还有其他的优化</span><br><span class="line">   StructuredStreaming(DataSet)</span><br></pre></td></tr></table></figure>
</li>
<li><p>什么是SparkSQL?</p>
<p>spark SQL是spark的一个模块，主要用于进行结构化数据的处理。它提供的最核心的编程抽象就是DataFrame。</p>
</li>
<li><p>SparkSQL的作用</p>
<p>提供一个编程抽象（DataFrame） 并且作为分布式 SQL 查询引擎</p>
<p>DataFrame：它可以根据很多源进行构建，包括：<strong>结构化的数据文件，hive中的表，外部的关系型数据库，以及RDD</strong></p>
</li>
<li><p>运行原理</p>
<p>将 Spark SQL 转化为 RDD， 然后提交到集群执行</p>
</li>
<li><p>特点</p>
<ol>
<li>容易整合</li>
<li>统一的数据访问方式</li>
<li>兼容 Hive</li>
<li>标准的数据连接</li>
</ol>
</li>
<li><p>spark-sql</p>
<p>spark-sql是一个Spark专属的SQL命令行交互工具，在使用spark-sql之前要把hive-site.xml 拷贝到Spark/Conf下，spark-sql和spark-shell用法一样，但是在引入外部依赖的时候，spark-sql需要用–jars和–driver-class-path同时引入依赖才不会报错</p>
</li>
<li><p>持久化</p>
<p>在spark-sql中的持久化Table命令是: cache table xxx，清除持久化 uncache table xxx</p>
<p>spark-SQL中的cache和uncache都是eager的，立即执行的</p>
<p><strong>考点：RDD和SparkSQL的cache有什么区别？</strong></p>
<ul>
<li>RDD中的cache是lazy的 spark-SQL中的cache是eager的</li>
</ul>
</li>
<li><p>遗留问题</p>
<p>–files/–jars    传进去的东西清不掉</p>
</li>
</ol>
<h3 id="SparkSession"><a href="#SparkSession" class="headerlink" title="SparkSession"></a>SparkSession</h3><p>SparkSession是Spark 2.0引如的新概念。SparkSession为用户提供了统一的切入点，来让用户学习spark的各项功能。 </p>
<p>在spark的早期版本中，SparkContext是spark的主要切入点，由于RDD是主要的API，我们通过sparkcontext来创建和操作RDD。对于每个其他的API，我们需要使用不同的context。例如，对于Streming，我们需要使用StreamingContext；对于sql，使用sqlContext；对于Hive，使用hiveContext。但是随着DataSet和DataFrame的API逐渐成为标准的API，就需要为他们建立接入点。所以在spark2.0中，引入SparkSession作为DataSet和DataFrame API的切入点，SparkSession封装了SparkConf、SparkContext和SQLContext。为了向后兼容，SQLContext和HiveContext也被保存下来。 </p>
<p>SparkSession实质上是SQLContext和HiveContext的组合（未来可能还会加上StreamingContext），所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。</p>
<p>特点：</p>
<ol>
<li><p><strong>为用户提供一个统一的切入点使用Spark 各项功能</strong></p>
</li>
<li><p><strong>允许用户通过它调用 DataFrame 和 Dataset 相关 API 来编写程序</strong></p>
</li>
<li><p><strong>减少了用户需要了解的一些概念，可以很容易的与 Spark 进行交互</strong></p>
</li>
<li><p><strong>与 Spark 交互之时不需要显示的创建 SparkConf, SparkContext 以及 SQlContext，这些对象已经封闭在 SparkSession 中</strong></p>
</li>
</ol>
<h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><p>在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame与RDD的主要区别在于，前者带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得Spark SQL得以洞察更多的结构信息，从而对藏于DataFrame背后的数据源以及作用于DataFrame之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在stage层面进行简单、通用的流水线优化。</p>
<p><img src="https://yerias.github.io/spark_img/RDD%E5%92%8CDataFrame%E7%9A%84%E5%AD%98%E5%82%A8%E5%86%85%E5%AE%B9%E6%AF%94%E8%BE%83.png" alt="RDD和DataFrame的存储内容比较"></p>
<h2 id="DataFrame的read和write"><a href="#DataFrame的read和write" class="headerlink" title="DataFrame的read和write"></a>DataFrame的read和write</h2><h3 id="json"><a href="#json" class="headerlink" title="json"></a>json</h3><ol>
<li><p>数据的读取[DataFrameReader]</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">rdd2df</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-sql/data/people.json"</span></span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line">		<span class="comment">//  读取json数据</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.format(<span class="string">"json"</span>).load(in)</span><br><span class="line">        <span class="comment">//  使用$"" 导入隐式转换</span></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        <span class="comment">//  可以使用UDF</span></span><br><span class="line">        df.select($<span class="string">"name"</span>,$<span class="string">"age"</span>).show(<span class="number">2</span>,<span class="literal">false</span>)</span><br><span class="line">        <span class="comment">//  不可以使用UDF 适合大部分场景</span></span><br><span class="line">        df.select(<span class="string">"name"</span>,<span class="string">"age"</span>).show()</span><br><span class="line">        <span class="comment">//  不推介，写着复杂</span></span><br><span class="line">        df.select(df(<span class="string">"name"</span>),df(<span class="string">"age"</span>)).show(<span class="number">2</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>select方法用于选择要输出的列，推介使用 $”col” 和 “col” 的方法</p>
<ol>
<li>使用select可以选取打印的列，空值为null</li>
<li>show()默认打印20条数据，可以指定条数</li>
<li>truncate默认为true，截取长度，可以设置为false</li>
</ol>
<p>select方法有三种不同的写法，fliter也有</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.select(df(<span class="string">"name"</span>),df(<span class="string">"age"</span>)).filter(<span class="symbol">'name</span> === <span class="string">"Andy"</span>).show()	<span class="comment">//推介使用</span></span><br><span class="line">df.select(df(<span class="string">"name"</span>),df(<span class="string">"age"</span>)).filter(df(<span class="string">"name"</span>) === <span class="string">"Andy"</span>).show()</span><br><span class="line">df.select(df(<span class="string">"name"</span>),df(<span class="string">"age"</span>)).filter(<span class="string">"name = 'Andy'"</span>).show()</span><br></pre></td></tr></table></figure>

<p>printSchema()方法可以查看数据的Schema信息</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.printSchema()</span><br><span class="line">------------------------------------------------</span><br><span class="line">root</span><br><span class="line"> |-- age: long (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- name: string (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>数据的存储[DataFrameWriter]</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> selectDf: <span class="type">DataFrame</span> = df.select($<span class="string">"name"</span>, $<span class="string">"age"</span>)</span><br><span class="line"><span class="comment">//  写出json数据</span></span><br><span class="line">selectDf.write.format(<span class="string">"json"</span>).mode(<span class="string">"overwrite"</span>).save(out)</span><br></pre></td></tr></table></figure>

<p>这里需要知道的一个概念是Save Modes</p>
<p>Save操作可以选择使用SaveMode，它指定目标如果存在，如何处理现有数据。重要的是要认识到，这些保存模式不利用任何锁定，也不是原子性的。此外，在执行覆盖时，在写入新数据之前将删除数据。</p>
<table>
<thead>
<tr>
<th align="left">Scala/Java</th>
<th align="left">Any Language</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>SaveMode.ErrorIfExists</code> (default)</td>
<td align="left"><code>&quot;error&quot; or &quot;errorifexists&quot;</code> (default)</td>
<td align="left">在将DataFrame保存到数据源时，如果数据已经存在，则会抛出error。</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Append</code></td>
<td align="left"><code>&quot;append&quot;</code></td>
<td align="left">在将DataFrame保存到数据源时，如果数据/表已经存在，则DataFrame的内容将被append到现有数据中。</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Overwrite</code></td>
<td align="left"><code>&quot;overwrite&quot;</code></td>
<td align="left">overwrite模式意味着在将DataFrame保存到数据源时，如果数据/表已经存在，则现有数据将被DataFrame的内容覆盖。</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Ignore</code></td>
<td align="left"><code>&quot;ignore&quot;</code></td>
<td align="left">ignore模式意味着在将DataFrame保存到数据源时，如果数据已经存在，则save操作不保存DataFrame的内容，也不更改现有数据。这类似于SQL中的<code>CREATE TABLE IF NOT EXISTS</code>。</td>
</tr>
</tbody></table>
</li>
</ol>
<h3 id="text"><a href="#text" class="headerlink" title="text"></a>text</h3><ol>
<li><p>数据的读取</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">text2df</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-sql/data/people.txt"</span></span><br><span class="line">        <span class="keyword">val</span> out = <span class="string">"tunan-spark-sql/out"</span></span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">        <span class="comment">//DataFrame不能直接split，且调用map返回的是一个Dataset</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.format(<span class="string">"text"</span>).load(in)</span><br><span class="line">        <span class="keyword">val</span> mapDF: <span class="type">Dataset</span>[(<span class="type">String</span>, <span class="type">String</span>)] = df.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.toString().split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">        mapDF.show()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//DataFrame转换为RDD后，再toDF，返回的是一个DataFrame</span></span><br><span class="line">        <span class="keyword">val</span> mapRDD2DF: <span class="type">DataFrame</span> = df.rdd.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.toString().split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;).toDF()</span><br><span class="line">        mapRDD2DF.show()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用textFile方法读取文本文件直接返回的是一个Dataset</span></span><br><span class="line">        <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(in)</span><br><span class="line">        <span class="keyword">val</span> mapDs: <span class="type">Dataset</span>[(<span class="type">String</span>, <span class="type">String</span>)] = ds.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">        mapDs.show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>文本数据读进来的一行在一个字段里面，所以要使用map算子，在map中split</p>
<ol>
<li>直接read.format()读进来的是DataFrame，map中不能直接split</li>
<li>DataFrame通过.rdd的方式转换成RDD，map中也不能直接split</li>
<li>通过read.textFile()的方式读进来的是Dataset，map中可以split</li>
</ol>
</li>
<li><p>数据的存储</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.format(<span class="string">"text"</span>).load(in)</span><br><span class="line"><span class="keyword">val</span> mapDF = df.map(row =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> words = row.toString().split(<span class="string">","</span>)</span><br><span class="line">    <span class="comment">// 拼接成一列</span></span><br><span class="line">    words(<span class="number">0</span>) +<span class="string">","</span>+words(<span class="number">1</span>)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">mapDF.write.format(<span class="string">"text"</span>).mode(<span class="string">"overwrite"</span>).save(out)</span><br></pre></td></tr></table></figure>

<p>文本数据写出去的时候</p>
<ol>
<li>不支持int类型，如果存在int类型，会报错，解决办法是toString，转换成字符串</li>
<li>只能作为一列输出，如果是多列，会报错，解决办法是拼接起来，组成一列</li>
</ol>
<p><strong>文本数据压缩输出，只要是Spark支持的压缩的格式，都可以指定</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mapDF.write</span><br><span class="line">    .format(<span class="string">"text"</span>)</span><br><span class="line">    <span class="comment">// 添加压缩操作</span></span><br><span class="line">    .option(<span class="string">"compression"</span>,<span class="string">"gzip"</span>)</span><br><span class="line">    .mode(<span class="string">"overwrite"</span>)</span><br><span class="line">    .save(out)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="csv"><a href="#csv" class="headerlink" title="csv"></a>csv</h3><ol>
<li><p>数据的读取</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">csv2df</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-sql/data/people.csv"</span></span><br><span class="line">        <span class="keyword">val</span> out = <span class="string">"tunan-spark-sql/out"</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read</span><br><span class="line">            .format(<span class="string">"csv"</span>)</span><br><span class="line">            .option(<span class="string">"header"</span>, <span class="string">"true"</span>)</span><br><span class="line">            .option(<span class="string">"sep"</span>, <span class="string">";"</span>)</span><br><span class="line">            .option(<span class="string">"interSchema"</span>,<span class="string">"true"</span>)</span><br><span class="line">            .load(in)</span><br><span class="line">        df.show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>csv读取数据注意使用几个参数</p>
<ol>
<li>指定表头：<code>option(&quot;header&quot;, &quot;true&quot;)</code></li>
<li>指定分隔符：<code>option(&quot;sep&quot;, &quot;;&quot;)</code></li>
<li>类型自动推测：<code>option(&quot;interSchema&quot;,&quot;true&quot;)</code></li>
</ol>
</li>
</ol>
<h3 id="jdbc"><a href="#jdbc" class="headerlink" title="jdbc"></a>jdbc</h3><p>在操作jdbc之前要导入两个依赖，一个是mysql-jdbc，用来连接mysql，一个是config，用来解决硬编码的问题</p>
<p>依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.typesafe<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>config<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.38<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>application.conf文件</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">db.<span class="keyword">default</span>.driver=<span class="string">"com.mysql.jdbc.Driver"</span></span><br><span class="line">db.<span class="keyword">default</span>.url=<span class="string">"jdbc:mysql://hadoop/listener?characterEncoding=utf-8&amp;useSSL=false"</span></span><br><span class="line">db.<span class="keyword">default</span>.user=<span class="string">"root"</span></span><br><span class="line">db.<span class="keyword">default</span>.password=<span class="string">"root"</span></span><br><span class="line">db.<span class="keyword">default</span>.source=<span class="string">"dws_ad_phone_type_dist"</span></span><br><span class="line">db.<span class="keyword">default</span>.target=<span class="string">"dws_ad_phone_type_dist_1"</span></span><br><span class="line">db.<span class="keyword">default</span>.db=<span class="string">"access_dw"</span></span><br><span class="line"></span><br><span class="line"># Connection Pool settings</span><br><span class="line">db.<span class="keyword">default</span>.poolInitialSize=<span class="number">10</span></span><br><span class="line">db.<span class="keyword">default</span>.poolMaxSize=<span class="number">20</span></span><br><span class="line">db.<span class="keyword">default</span>.connectionTimeoutMillis=<span class="number">1000</span></span><br></pre></td></tr></table></figure>

<ol>
<li><p>数据的读取</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">mysql2df</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">		<span class="comment">//获取配置文件中的值，db.default开头</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="type">ConfigFactory</span>.load()</span><br><span class="line">        <span class="keyword">val</span> driver = conf.getString(<span class="string">"db.default.driver"</span>)</span><br><span class="line">        <span class="keyword">val</span> url = conf.getString(<span class="string">"db.default.url"</span>)</span><br><span class="line">        <span class="keyword">val</span> user = conf.getString(<span class="string">"db.default.user"</span>)</span><br><span class="line">        <span class="keyword">val</span> password = conf.getString(<span class="string">"db.default.password"</span>)</span><br><span class="line">        <span class="keyword">val</span> source = conf.getString(<span class="string">"db.default.source"</span>)</span><br><span class="line">        <span class="keyword">val</span> target = conf.getString(<span class="string">"db.default.target"</span>)</span><br><span class="line">        <span class="keyword">val</span> db = conf.getString(<span class="string">"db.default.db"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取数据库的内容</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read</span><br><span class="line">            .format(<span class="string">"jdbc"</span>)</span><br><span class="line">            .option(<span class="string">"url"</span>, url)</span><br><span class="line">            .option(<span class="string">"dbtable"</span>, <span class="string">s"<span class="subst">$db</span>.<span class="subst">$source</span>"</span>)	<span class="comment">//库名.源表</span></span><br><span class="line">            .option(<span class="string">"user"</span>, user)</span><br><span class="line">            .option(<span class="string">"password"</span>, password)</span><br><span class="line">            .option(<span class="string">"driver"</span>, driver)</span><br><span class="line">            .load()</span><br><span class="line">        <span class="comment">//使用DataFrame创建临时表提供spark.sql查询</span></span><br><span class="line">        df.createOrReplaceTempView(<span class="string">"phone_type_dist"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//spark.sql写SQL返回一个DataFrame</span></span><br><span class="line">        <span class="keyword">val</span> sqlDF: <span class="type">DataFrame</span> = spark.sql(<span class="string">"select * from phone_type_dist where phoneSystemType = 'IOS'"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>使用df.createOrReplaceTempView()方法创建一个DataFrame数据生成的临时表，提供spark.sql()使用SQL操作数据，返回的也是一个DataFrame</li>
</ul>
</li>
<li><p>数据的存储</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//接着上面返回的sqlDF: DataFrame</span></span><br><span class="line">sqlDF.write</span><br><span class="line">    .format(<span class="string">"jdbc"</span>)</span><br><span class="line">    .option(<span class="string">"url"</span>, url)</span><br><span class="line">    .option(<span class="string">"dbtable"</span>, <span class="string">s"<span class="subst">$db</span>.<span class="subst">$target</span>"</span>)	<span class="comment">//库名.目标表</span></span><br><span class="line">    .option(<span class="string">"user"</span>, user)</span><br><span class="line">    .option(<span class="string">"password"</span>, password)</span><br><span class="line">    .option(<span class="string">"driver"</span>,driver)</span><br><span class="line">    .save()</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="SparkSQL做统计分析"><a href="#SparkSQL做统计分析" class="headerlink" title="SparkSQL做统计分析"></a>SparkSQL做统计分析</h2><ol>
<li><p>数据</p>
</li>
<li><p>需求：求每个国家的每个域名的访问流量排名前2</p>
</li>
<li><p>SQL实现</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GroupTopN</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">		<span class="comment">//读取数据</span></span><br><span class="line">        <span class="keyword">val</span> ds = spark.read.textFile(in)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        <span class="comment">//为生成需要的表格做准备</span></span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = ds.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">3</span>), words(<span class="number">12</span>), words(<span class="number">15</span>).toLong)</span><br><span class="line">        &#125;).toDF(<span class="string">"country"</span>, <span class="string">"domain"</span>, <span class="string">"traffic"</span>)</span><br><span class="line"></span><br><span class="line">        df.createOrReplaceTempView(<span class="string">"access"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 每个国家的域名流量前2</span></span><br><span class="line">        <span class="keyword">val</span> topNSQL=<span class="string">""</span><span class="string">"select</span></span><br><span class="line"><span class="string">                      |	*</span></span><br><span class="line"><span class="string">                      |from (</span></span><br><span class="line"><span class="string">                      |		select</span></span><br><span class="line"><span class="string">                      |			t.*,row_number() over(partition by country order by sum_traffic desc) r</span></span><br><span class="line"><span class="string">                      |		from</span></span><br><span class="line"><span class="string">                      |			(</span></span><br><span class="line"><span class="string">                      |				select country,domain,sum(traffic) as sum_traffic from access group by country,domain</span></span><br><span class="line"><span class="string">                      |			) t</span></span><br><span class="line"><span class="string">                      |	) rt</span></span><br><span class="line"><span class="string">                      |where rt.r &lt;=2 "</span><span class="string">""</span>.stripMargin</span><br><span class="line"></span><br><span class="line">        spark.sql(topNSQL).show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果只要求traffic的降序，可以使用API直接写出来</p>
<p>分组，求和，别名，降序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//traffic降序排序</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">df.groupBy(<span class="string">"country"</span>,<span class="string">"domain"</span>).agg(sum(<span class="string">"traffic"</span>).as(<span class="string">"sum_traffic"</span>)).sort($<span class="string">"sum_traffic"</span>.desc).show()</span><br></pre></td></tr></table></figure>

<p><strong>注意看源码中案例仿写</strong></p>
</li>
<li><p>结果展示</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+----------------+-----------------+-----------+---+</span><br><span class="line">|         country|           domain|sum_traffic|  r|</span><br><span class="line">+----------------+-----------------+-----------+---+</span><br><span class="line">|            中国| www.bilibili.com|   <span class="number">24265886</span>|  <span class="number">1</span>|</span><br><span class="line">|            中国|www.ruozedata.com|    <span class="number">4187637</span>|  <span class="number">2</span>|</span><br><span class="line">|          利比亚| www.bilibili.com|      <span class="number">22816</span>|  <span class="number">1</span>|</span><br><span class="line">|          利比亚|  ruoze.ke.qq.com|      <span class="number">15970</span>|  <span class="number">2</span>|</span><br><span class="line">|            加纳| www.bilibili.com|     <span class="number">138659</span>|  <span class="number">1</span>|</span><br><span class="line">|            加纳|www.ruozedata.com|      <span class="number">17988</span>|  <span class="number">2</span>|</span><br><span class="line">|        利比里亚| www.bilibili.com|      <span class="number">20593</span>|  <span class="number">1</span>|</span><br><span class="line">|        利比里亚|  ruoze.ke.qq.com|       <span class="number">7466</span>|  <span class="number">2</span>|</span><br><span class="line">+----------------+-----------------+-----------+---+</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="UDF函数"><a href="#UDF函数" class="headerlink" title="UDF函数"></a>UDF函数</h2><ol>
<li><p>数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">大狗	小破车,渣团,热刺,我纯</span><br><span class="line">桶子	利物浦</span><br><span class="line">二娃	南大王,西班牙人</span><br></pre></td></tr></table></figure>
</li>
<li><p>需求：求出每个人的爱好的个数</p>
</li>
<li><p>SQLs实现</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LoveLength</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取文本内容</span></span><br><span class="line">        <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(in)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//文本转换成DF</span></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        <span class="keyword">val</span> df: <span class="type">DataFrame</span> = ds.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.split(<span class="string">"\t"</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;).toDF(<span class="string">"name"</span>, <span class="string">"love"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建UDF</span></span><br><span class="line">        spark.udf.register(<span class="string">"length"</span>, (love: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">            love.split(<span class="string">","</span>).length</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//DF创建临时表</span></span><br><span class="line">        df.createOrReplaceTempView(<span class="string">"udf_love"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//在sql中使用UDF函数</span></span><br><span class="line">        spark.sql(<span class="string">"select name,love,length(love) as love_length from udf_love"</span>).show()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>上面是使用SQL的解决方案，还可以使用API的方法</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//自定义的udf需要返回值</span></span><br><span class="line"><span class="keyword">val</span> loveLengthUDF: <span class="type">UserDefinedFunction</span> = spark.udf.register(<span class="string">"length"</span>, (love: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">    love.split(<span class="string">","</span>).length</span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment">//df.select中传入UDF函数</span></span><br><span class="line">df.select($<span class="string">"name"</span>,$<span class="string">"love"</span>,loveLengthUDF($<span class="string">"love"</span>)).show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>结果展示</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+----+---------------------+----------------+</span><br><span class="line">|大狗|小破车,渣团,热刺,我纯   |               <span class="number">4</span>|</span><br><span class="line">|桶子|               利物浦 |               <span class="number">1</span>|</span><br><span class="line">|二娃|      南大王,西班牙人  |               <span class="number">2</span>|</span><br><span class="line">+----+---------------------+----------------+</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="存储格式的转换"><a href="#存储格式的转换" class="headerlink" title="存储格式的转换"></a>存储格式的转换</h2><p>Spark读text文件进行清洗，清洗完以后直接以我们想要的列式存储格式输出，如果按以前的方式要经过很多复杂的步骤</p>
<p>用Spark的时候只需要在<code>df.write.format(&quot;orc&quot;).mode().save()</code>中指定格式即可，如orc，现在就很方便了，想转成什么格式，只要format支持就ok</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">text2orc</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .master(<span class="string">"local[2]"</span>)</span><br><span class="line">            .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">            .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">val</span> in = <span class="string">"tunan-spark-sql/data/people.txt"</span></span><br><span class="line">        <span class="keyword">val</span> out = <span class="string">"tunan-spark-sql/out"</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> df = spark.read.textFile(in)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//对文本文件做处理</span></span><br><span class="line">        df.map(row =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = row.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>),words(<span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">            .toDF(<span class="string">"name"</span>,<span class="string">"age"</span>)	<span class="comment">//这一步解决了数据没有表头的问题</span></span><br><span class="line">            .write</span><br><span class="line">            .mode(<span class="string">"overwrite"</span>)	<span class="comment">//save mode</span></span><br><span class="line">            .format(<span class="string">"orc"</span>)	<span class="comment">//save format</span></span><br><span class="line">            .save(out)	<span class="comment">//save path</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><em>20200416更新：</em> df.write.format(“…”).option(“compression”,”…”)   ==&gt; 存储格式+压缩格式</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/04/02/spark/9/">经典案例&amp;多目录输出&amp;计数器&amp;持久化&amp;广播变量</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-02</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>经典案例</li>
<li>多目录输出</li>
<li>计数器</li>
<li>持久化</li>
<li>广播变量</li>
</ol>
<h2 id="经典案例"><a href="#经典案例" class="headerlink" title="经典案例"></a>经典案例</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 用户     节目            展示 点击</span></span><br><span class="line"><span class="comment"> * 001,一起看|电视剧|军旅|亮剑,1,1</span></span><br><span class="line"><span class="comment"> * 001,一起看|电视剧|军旅|亮剑,1,0</span></span><br><span class="line"><span class="comment"> * 002,一起看|电视剧|军旅|士兵突击,1,1</span></span><br><span class="line"><span class="comment"> * ==&gt;</span></span><br><span class="line"><span class="comment"> * 001,一起看,2,1</span></span><br><span class="line"><span class="comment"> * 001,电视剧,2,1</span></span><br><span class="line"><span class="comment"> * 001,军旅,2,1</span></span><br><span class="line"><span class="comment"> * 001,亮剑,2,1</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">exercise02</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> linesRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"tunan-spark-core/data/test2.txt"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用map返回的是一个数组，我不要数组，就使用flatMap</span></span><br><span class="line">        <span class="keyword">import</span> com.tunan.spark.utils.<span class="type">ImplicitAspect</span>.rdd2RichRDD</span><br><span class="line">        <span class="keyword">val</span> map2RDD: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), (<span class="type">Int</span>, <span class="type">Int</span>))] = linesRDD.flatMap(line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">","</span>)</span><br><span class="line">            <span class="keyword">val</span> programs: <span class="type">Array</span>[<span class="type">String</span>] = words(<span class="number">1</span>).split(<span class="string">"\\|"</span>)</span><br><span class="line">            <span class="keyword">val</span> mapRDD: <span class="type">Array</span>[((<span class="type">String</span>, <span class="type">String</span>), (<span class="type">Int</span>, <span class="type">Int</span>))] = programs.map(program =&gt; ((words(<span class="number">0</span>), program), (words(<span class="number">2</span>).toInt, words(<span class="number">3</span>).toInt)))</span><br><span class="line">            mapRDD</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="keyword">val</span> groupRDD: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Iterable</span>[(<span class="type">Int</span>, <span class="type">Int</span>)])] = map2RDD.groupByKey()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//这里是mapValues很好的一个使用案例</span></span><br><span class="line">        <span class="keyword">val</span> mapVRDD: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), (<span class="type">Int</span>, <span class="type">Int</span>))] = groupRDD.mapValues(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> imps: <span class="type">Int</span> = x.map(_._1).sum</span><br><span class="line">            <span class="keyword">val</span> check: <span class="type">Int</span> = x.map(_._2).sum</span><br><span class="line">            (imps, check)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//格式化输出</span></span><br><span class="line">        mapVRDD.map(x =&gt; &#123;</span><br><span class="line">            (x._1._1,x._1._2,x._2._1,x._2._1)</span><br><span class="line">        &#125;).print()</span><br><span class="line"></span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="多目录输出"><a href="#多目录输出" class="headerlink" title="多目录输出"></a>多目录输出</h2><ol>
<li><p>实现多目录输出自定义类</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.<span class="type">NullWritable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.lib.<span class="type">MultipleTextOutputFormat</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyMultipleTextOutputFormat</span> <span class="keyword">extends</span> <span class="title">MultipleTextOutputFormat</span>[<span class="type">Any</span>,<span class="type">Any</span>] </span>&#123;</span><br><span class="line">    <span class="comment">//生成最终生成的key的类型，这里不要，给Null</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">generateActualKey</span></span>(key: <span class="type">Any</span>, value: <span class="type">Any</span>): <span class="type">Any</span> = <span class="type">NullWritable</span>.get()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//生成最终生成的value的类型，这里是String</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">generateActualValue</span></span>(key: <span class="type">Any</span>, value: <span class="type">Any</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">        value.asInstanceOf[<span class="type">String</span>]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//生成文件名</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">generateFileNameForKeyValue</span></span>(key: <span class="type">Any</span>, value: <span class="type">Any</span>, name: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">        <span class="string">s"<span class="subst">$key</span>/<span class="subst">$name</span>"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>主类，使用<code>saveAsHadoopFile(path, keyClass, valueClass, fm.runtimeClass.asInstanceOf[Class[F]])</code>方法保存数据，指定参数</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MultipleDirectory</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> out = <span class="string">"tunan-spark-core/out"</span></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">        <span class="type">CheckHDFSOutPath</span>.ifExistsDeletePath(sc.hadoopConfiguration,out)</span><br><span class="line">        <span class="comment">//读取数组，转换成键值对的格式</span></span><br><span class="line">        <span class="keyword">val</span> lines = sc.textFile(<span class="string">"tunan-spark-core/ip/access-result/*"</span>)</span><br><span class="line">        <span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = lines.map(line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = line.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">12</span>), line)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">//多目录保存文件</span></span><br><span class="line">        mapRDD.saveAsHadoopFile(out,classOf[<span class="type">String</span>],classOf[<span class="type">String</span>],classOf[<span class="type">MyMultipleTextOutputFormat</span>])</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>结果</p>
<p><img src="https://yerias.github.io/spark_img/%E5%A4%9A%E7%9B%AE%E5%BD%95%E8%BE%93%E5%87%BA.jpg" alt="多目录输出"></p>
</li>
</ol>
<p>在spark程序中，当一个传递给Spark操作(例如map和reduce)的函数在远程节点上面运行时，Spark操作实际上操作的是这个函数所用变量的一个<strong>独立副本</strong>。这些变量会被复制到每台机器上，并且这些变量在远程机器上的所有更新都不会传递回驱动程序。通常跨任务的读写变量是低效的，但是，Spark还是为两种常见的使用模式提供了两种有限的共享变量：广播变量（broadcast variable）和累加器（accumulator）</p>
<h2 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h2><h3 id="为什么要定义计数器？"><a href="#为什么要定义计数器？" class="headerlink" title="为什么要定义计数器？"></a>为什么要定义计数器？</h3><p>在spark应用程序中，我们经常会有这样的需求，如<em>异常监控</em>，<em>调试</em>，<em>记录符合某特性的数据的数目</em>，这种需求都需要用到计数器，如果一个变量不被声明为一个累加器，那么它将在被改变时不会再driver端进行全局汇总，即在分布式运行时每个task运行的只是原始变量的一个副本，并不能改变原始变量的值，但是当这个变量被声明为累加器后，该变量就会有分布式计数的功能。</p>
<h3 id="图解计数器"><a href="#图解计数器" class="headerlink" title="图解计数器"></a>图解计数器</h3><p>错误的图解</p>
<p><img src="https://yerias.github.io/spark_img/%E7%B4%AF%E5%8A%A0%E5%99%A8%E9%94%99%E8%AF%AF%E7%9A%84%E5%9B%BE%E8%A7%A3.png" alt="累加器错误的图解"></p>
<p>正确的图解</p>
<p><img src="https://yerias.github.io/spark_img/%E7%B4%AF%E5%8A%A0%E5%99%A8%E6%AD%A3%E7%A1%AE%E7%9A%84%E5%9B%BE%E8%A7%A3.png" alt="累加器正确的图解"></p>
<p>计数器种类很多，但是经常用的就是两种，<code>longAccumulator</code>和<code>collectionAccumulator</code></p>
<p><strong>需要注意的是计数器是lazy的，只有触发action才会进行计数，在不持久化的情况下重复触发action，计数器会重复累加</strong></p>
<h3 id="LongAccumulator"><a href="#LongAccumulator" class="headerlink" title="LongAccumulator"></a>LongAccumulator</h3><p>Accumulators 是只能通过associative和commutative操作“added”的变量，因此可以有效地并行支持。它们可用于实现计数器(如MapReduce)和Spark本身支持数字类型的累加器，程序员还<strong>可以添加对新类型的支持</strong>。</p>
<p><code>longAccumulator</code>通过累加的方式计数</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyLongAccumulator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">        <span class="keyword">var</span> acc = sc.longAccumulator(<span class="string">"计数"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> forRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">            <span class="comment">// 计数器做累加</span></span><br><span class="line">            acc.add(<span class="number">1</span>L)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">// action操作 </span></span><br><span class="line">        forRDD.count()</span><br><span class="line">       </span><br><span class="line">        println(acc.value)	<span class="comment">// 9</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用<code>longAccumulator</code>做计数的时候要小心重复执行action导致的acc.value的变化</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyLongAccumulatorV2</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">        <span class="comment">//生成计数器</span></span><br><span class="line">        <span class="keyword">val</span> acc = sc.longAccumulator(<span class="string">"计数"</span>)</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">8</span>)</span><br><span class="line">        <span class="keyword">val</span> forRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">            <span class="comment">//计数器做累加</span></span><br><span class="line">            acc.add(<span class="number">1</span>L)</span><br><span class="line">        &#125;)</span><br><span class="line">        forRDD.count()</span><br><span class="line">        println(acc.value)	<span class="comment">//8</span></span><br><span class="line">        forRDD.count()</span><br><span class="line">        println(acc.value)	<span class="comment">//16</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>由于重复执行了count()，累加器的数量成倍增长，解决这种错误累加也很简单，就是在count之前调用forRDD的cache方法(或persist)，这样在count后数据集就会被缓存下来，reduce操作就会读取缓存的数据集，而无需从头开始计算。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyLongAccumulator</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">        <span class="comment">//生成计数器</span></span><br><span class="line">        <span class="keyword">val</span> acc = sc.longAccumulator(<span class="string">"计数"</span>)</span><br><span class="line">        <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">8</span>)</span><br><span class="line">        <span class="keyword">val</span> forRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">            <span class="comment">//计数器做累加</span></span><br><span class="line">            acc.add(<span class="number">1</span>L)</span><br><span class="line">        &#125;)</span><br><span class="line">        forRDD.cache().count()</span><br><span class="line">        println(acc.value)	<span class="comment">//8</span></span><br><span class="line">        forRDD.count()</span><br><span class="line">        println(acc.value)	<span class="comment">//8</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="CollectionAccumulator"><a href="#CollectionAccumulator" class="headerlink" title="CollectionAccumulator"></a>CollectionAccumulator</h3><p><code>collectionAccumulator</code>，集合计数器，计数器中保存的是集合元素，通过泛型指定。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：id后三位相同的加入计数器</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyCollectionAccumulator</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sc  = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//生成集合计数器</span></span><br><span class="line">        <span class="keyword">val</span> acc = sc.collectionAccumulator[<span class="type">People</span>](<span class="string">"集合计数器"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//生成RDD</span></span><br><span class="line">        <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">People</span>] = sc.parallelize(<span class="type">Array</span>(<span class="type">People</span>(<span class="string">"tunan"</span>, <span class="number">100000</span>), <span class="type">People</span>(<span class="string">"xiaoqi"</span>, <span class="number">100001</span>), <span class="type">People</span>(<span class="string">"张三"</span>, <span class="number">100222</span>), <span class="type">People</span>(<span class="string">"李四"</span>, <span class="number">100003</span>)))</span><br><span class="line"></span><br><span class="line"> 		<span class="comment">//map操作</span></span><br><span class="line">        rdd.map(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> id2 = x.id.toString.reverse</span><br><span class="line">            <span class="comment">//满足条件就加入计数器，</span></span><br><span class="line">            <span class="keyword">if</span> (id2(<span class="number">0</span>) == id2(<span class="number">1</span>) &amp;&amp; id2(<span class="number">0</span>) ==id2(<span class="number">2</span>))&#123;</span><br><span class="line">                acc.add(x)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).count()	<span class="comment">//触发action</span></span><br><span class="line"></span><br><span class="line">        println(acc.value)	<span class="comment">//[People(张三,100222), People(tunan,100000)]</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">People</span>(<span class="params">name:<span class="type">String</span>,id:<span class="type">Long</span></span>)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>注意事项：</strong></p>
<ol>
<li><p>计数器在Driver端定义赋初始值，计数器只能在Driver端读取最后的值，在Excutor端更新。</p>
</li>
<li><p>计数器不是一个调优的操作，因为如果不这样做，结果是错的</p>
</li>
</ol>
<h2 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h2><p>Spark中最重要的功能之一是跨操作在内存中持久化数据集。持久化一个RDD时，每个节点在内存中存储它计算的任何分区，并在该数据集(或从中派生的数据集)的其他操作中重构它们。这使得将来的操作要快得多(通常超过10倍)。缓存是迭代算法和快速交互使用的关键工具。</p>
<p>可以使用其上的persist()或cache()方法将RDD标记为持久的。第一次在操作中计算它时，它将保存在节点的内存中。Spark的缓存是容错的——如果一个RDD的任何分区丢失了，它将使用最初创建它的转换自动重新计算。</p>
<p>持久化的存储级别很多，常用的是MEMORY_ONLY、MEMORY_ONLY_SER、MEMORY_AND_DISK</p>
<table>
<thead>
<tr>
<th align="left">Storage Level</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left">MEMORY_ONLY</td>
<td align="left">将RDD作为不序列化的Java对象存储在JVM中。如果RDD不适合内存，那么一些分区将不会被缓存，而是在需要它们时动态地重新计算。这是默认级别。</td>
</tr>
<tr>
<td align="left">MEMORY_AND_DISK</td>
<td align="left">将RDD作为不序列化的Java对象存储在JVM中。如果RDD不适合内存，那么将不适合的分区存储在磁盘上，并在需要时从那里读取它们。</td>
</tr>
<tr>
<td align="left">MEMORY_ONLY_SER (Java and Scala)</td>
<td align="left">将RDD存储为序列化的Java对象(每个分区一个字节数组)。这通常比反序列化对象更节省空间，特别是在使用快速序列化器时，但读取时需要更多cpu。</td>
</tr>
</tbody></table>
<h3 id="如何选择它们？"><a href="#如何选择它们？" class="headerlink" title="如何选择它们？"></a><strong>如何选择它们？</strong></h3><p>Storage Level的选择是内存和CPU的权衡</p>
<ol>
<li>内存多：MEMORY_ONLY (不进行序列化)</li>
<li>CPU跟的上：MEMORY_ONLY_SER (进行了序列化，推介)</li>
<li>不建议写Disk</li>
</ol>
<p>使用cache()和persist()进行持久化操作，它们都是<strong>lazy</strong>的，需要action才能触发，默认使用MEMORY_ONLY</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> forRDD.cache</span></span><br><span class="line">res18: forRDD.type = MapPartitionsRDD[9] at map at &lt;console&gt;:27</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> forRDD.count</span></span><br><span class="line">res19: Long = 8</span><br></pre></td></tr></table></figure>

<p>结果可以在Web UI的<strong>Storage</strong>中查看</p>
<p>如果需要<strong>清除缓存</strong>，使用unpersist()，清除缓存数据是立即执行的</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> forRDD.unpersist()</span></span><br><span class="line">res8: forRDD.type = MapPartitionsRDD[3] at map at &lt;console&gt;:28</span><br></pre></td></tr></table></figure>

<p><strong>怎么修改存储级别？</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> forRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">    <span class="comment">//计数器做累加</span></span><br><span class="line">    acc.add(<span class="number">1</span>L)</span><br><span class="line">&#125;).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>).count()</span><br></pre></td></tr></table></figure>

<p>StorageLevel是个object，需要的级别都可以从里面拿出来</p>
<h4 id="考点：cache和persist有什么区别？"><a href="#考点：cache和persist有什么区别？" class="headerlink" title="考点：cache和persist有什么区别？"></a><strong>考点：cache和persist有什么区别？</strong></h4><ul>
<li>cache调用的persist，persist调用的persist(storage level)</li>
</ul>
<h4 id="考点：序列化和非序列化有什么区别？"><a href="#考点：序列化和非序列化有什么区别？" class="headerlink" title="考点：序列化和非序列化有什么区别？"></a><strong>考点：序列化和非序列化有什么区别？</strong></h4><ul>
<li>序列化将对象转换成字节数组了，节省空间，占CPU</li>
</ul>
<h3 id="Removing-Data"><a href="#Removing-Data" class="headerlink" title="Removing Data"></a>Removing Data</h3><p>Spark自动监视每个节点上的缓存使用情况，并以最近最少使用(LRU)的方式删除旧的数据分区。如果想要手动删除一个RDD，而不是等待它从缓存中消失，那么可以使用RDD.unpersist()方法。</p>
<p>伪代码以及画图表示出什么是LRU？</p>
<h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><h3 id="为什么要将变量定义成广播变量？"><a href="#为什么要将变量定义成广播变量？" class="headerlink" title="为什么要将变量定义成广播变量？"></a>为什么要将变量定义成广播变量？</h3><p>如果我们要在分布式计算里面分发大对象，例如：<em>字典</em>，<em>集合</em>，<em>黑白名单</em>等，这个都会由Driver端进行分发，一般来讲，如果这个变量不是广播变量，那么每个task就会分发一份，这在<strong>task数目十分多的情况下Driver的带宽会成为系统的瓶颈，而且会大量消耗task服务器上的资源</strong>，如果将这个变量声明为广播变量，那么知识每个executor拥有一份，这个executor启动的task会共享这个变量，节省了通信的成本和服务器的资源。</p>
<h3 id="广播变量图解"><a href="#广播变量图解" class="headerlink" title="广播变量图解"></a>广播变量图解</h3><p>错误的，不使用广播变量</p>
<p><img src="https://yerias.github.io/spark_img/%E4%B8%8D%E4%BD%BF%E7%94%A8%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F.png" alt="不使用广播变量"></p>
<p>正确的，使用广播变量的情况</p>
<p><img src="https://yerias.github.io/spark_img/%E4%BD%BF%E7%94%A8%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F.png" alt="使用广播变量"></p>
<h3 id="小表广播案例"><a href="#小表广播案例" class="headerlink" title="小表广播案例"></a>小表广播案例</h3><p>使用广播变量的场景很多， 我们都知道spark 一种常见的优化方式就是小表广播， 使用 map join 来代替 reduce join， 我们通过把小的数据集广播到各个节点上，节省了一次特别 expensive 的 shuffle 操作。</p>
<p>比如driver 上有一张数据量很小的表， 其他节点上的task 都需要 lookup 这张表， 那么 driver 可以先把这张表 copy 到这些节点，这样 task 就可以在本地查表了。</p>
<ol>
<li><p>Fact table  航线(起点机场, 终点机场, 航空公司, 起飞时间)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SEA,JFK,DL,7:00</span><br><span class="line">SFO,LAX,AA,7:05</span><br><span class="line">SFO,JFK,VX,7:05</span><br><span class="line">JFK,LAX,DL,7:10</span><br><span class="line">LAX,SEA,DL,7:10</span><br></pre></td></tr></table></figure>
</li>
<li><p>Dimension table 机场(简称, 全称, 城市, 所处城市简称)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">JFK,John F. Kennedy International Airport,New York,NY</span><br><span class="line">LAX,Los Angeles International Airport,Los Angeles,CA</span><br><span class="line">SEA,Seattle-Tacoma International Airport,Seattle,WA</span><br><span class="line">SFO,San Francisco International Airport,San Francisco,CA</span><br></pre></td></tr></table></figure>
</li>
<li><p>Dimension table  航空公司(简称,全称)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">AA,American Airlines</span><br><span class="line">DL,Delta Airlines</span><br><span class="line">VX,Virgin America</span><br></pre></td></tr></table></figure>
</li>
<li><p>思路：将机场维度表和航空公司维度表进行广播，生成Map，航线事实表从广播变量中通过key拿到value(计算在每个executor上)</p>
</li>
<li><p>代码</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BroadcastApp</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="type">ContextUtils</span>.getSparkContext(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Fact table  航线(起点机场, 终点机场, 航空公司, 起飞时间)</span></span><br><span class="line">        <span class="keyword">val</span> flights = sc.textFile(<span class="string">"tunan-spark-core/broadcast/flights.txt"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Dimension table 机场(简称, 全称, 城市, 所处城市简称)</span></span><br><span class="line">        <span class="keyword">val</span> airports: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"tunan-spark-core/broadcast/airports.txt"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Dimension table  航空公司(简称,全称)</span></span><br><span class="line">        <span class="keyword">val</span> airlines = sc.textFile(<span class="string">"tunan-spark-core/broadcast/airlines.txt"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 最终统计结果：</span></span><br><span class="line"><span class="comment">         * 出发城市           终点城市           航空公司名称         起飞时间</span></span><br><span class="line"><span class="comment">         * Seattle           New York       Delta Airlines          7:00</span></span><br><span class="line"><span class="comment">         * San Francisco     Los Angeles    American Airlines       7:05</span></span><br><span class="line"><span class="comment">         * San Francisco     New York       Virgin America          7:05</span></span><br><span class="line"><span class="comment">         * New York          Los Angeles    Delta Airlines          7:10</span></span><br><span class="line"><span class="comment">         * Los Angeles       Seattle        Delta Airlines          7:10</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//广播Dimension Table airport，生成Map</span></span><br><span class="line">        <span class="keyword">val</span> airportsBC = sc.broadcast(airports.map(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = x.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">2</span>))</span><br><span class="line">        &#125;).collectAsMap())</span><br><span class="line"></span><br><span class="line">        <span class="comment">//广播Dimension Table airlines，生成Map</span></span><br><span class="line">        <span class="keyword">val</span> airlinesBC = sc.broadcast(airlines.map(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = x.split(<span class="string">","</span>)</span><br><span class="line">            (words(<span class="number">0</span>), words(<span class="number">1</span>))</span><br><span class="line">        &#125;).collectAsMap())</span><br><span class="line"></span><br><span class="line">        <span class="comment">//通过key获取value</span></span><br><span class="line">        flights.map(lines =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words = lines.split(<span class="string">","</span>)</span><br><span class="line">            <span class="keyword">val</span> a = airportsBC.value.get(words(<span class="number">0</span>)).get</span><br><span class="line">            <span class="keyword">val</span> b = airportsBC.value.get(words(<span class="number">1</span>)).get</span><br><span class="line">            <span class="keyword">val</span> c = airlinesBC.value.get(words(<span class="number">2</span>)).get</span><br><span class="line">            a+<span class="string">"    "</span>+b+<span class="string">"    "</span>+c+<span class="string">"    "</span>+words(<span class="number">3</span>)</span><br><span class="line">        &#125;).foreach(println)</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">New York    	Los Angeles     Delta Airlines    7:10</span><br><span class="line">Los Angeles     Seattle    		Delta Airlines    7:10</span><br><span class="line">Seattle    		New York    	Delta Airlines    7:00</span><br><span class="line">San Francisco   Los Angeles     American Airlines 7:05</span><br><span class="line">San Francisco   New York    	Virgin America    7:05</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="为什么只能-broadcast-只读的变量"><a href="#为什么只能-broadcast-只读的变量" class="headerlink" title="为什么只能 broadcast 只读的变量"></a>为什么只能 broadcast 只读的变量</h3><p>这就涉及一致性的问题，如果变量可以被更新，那么一旦变量被某个节点更新，其他节点要不要一块更新？如果多个节点同时在更新，更新顺序是什么？怎么做同步？ 仔细想一下， 每个都很头疼， spark 目前就索性搞成了只读的。  因为分布式强一致性真的很蛋疼</p>
<h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ol>
<li><p>变量一旦被定义为一个广播变量，那么这个变量只能读，不能修改</p>
</li>
<li><p>能不能将一个RDD使用广播变量广播出去？因为RDD是不存储数据的。<strong>可以将RDD的结果广播出去。</strong></p>
</li>
<li><p>广播变量只能在Driver端定义，<strong>不能在Executor端定义。</strong></p>
</li>
<li><p>在Driver端可以修改广播变量的值，<strong>在Executor端无法修改广播变量的值。</strong></p>
</li>
<li><p>如果executor端用到了Driver的变量，<strong>不使用广播变量在Executor有多少task就有多少Driver端的变量副本。</strong></p>
</li>
<li><p>如果Executor端用到了Driver的变量，<strong>使用广播变量在每个Executor中只有一份Driver端的变量副本。</strong></p>
</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/31/spark/8/">Spark之短信告警</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-31</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><p>下面的案例继续延续Spark监控中的邮件监控，在监控中检测到数据异常，需要发送邮件告警</p>
<p>发送邮件工具类</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">MsgUtils</span> </span>&#123;</span><br><span class="line">    public static void send(<span class="type">String</span> recivers, <span class="type">String</span> title, <span class="type">String</span> content) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">Properties</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>();</span><br><span class="line">        properties.setProperty(<span class="string">"mail.host"</span>,<span class="string">"smtp.qq.com"</span>);</span><br><span class="line">        properties.setProperty(<span class="string">"mail.transport.protocol"</span>,<span class="string">"smtp"</span>);</span><br><span class="line">        properties.setProperty(<span class="string">"mail.smtp.auth"</span>, <span class="string">"true"</span>);</span><br><span class="line">        properties.setProperty(<span class="string">"mail.smtp.ssl.enable"</span>,<span class="string">"false"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">MailSSLSocketFactory</span> factory = <span class="keyword">new</span> <span class="type">MailSSLSocketFactory</span>();</span><br><span class="line">        factory.setTrustAllHosts(<span class="literal">true</span>);</span><br><span class="line">        properties.put(<span class="string">"mail.smtp.ssl.socketFactory"</span>, factory);</span><br><span class="line"></span><br><span class="line">        <span class="type">Authenticator</span> authenticator = <span class="keyword">new</span> <span class="type">Authenticator</span>() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">protected</span> <span class="type">PasswordAuthentication</span> getPasswordAuthentication() &#123;</span><br><span class="line">                <span class="type">String</span> username = <span class="string">"发送者qq邮箱"</span>;</span><br><span class="line">                <span class="type">String</span> password = <span class="string">"发送者qq授权码"</span>;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">PasswordAuthentication</span>(username, password);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="type">Session</span> session = <span class="type">Session</span>.getInstance(properties, authenticator);</span><br><span class="line"></span><br><span class="line">        <span class="type">MimeMessage</span> message = <span class="keyword">new</span> <span class="type">MimeMessage</span>(session);</span><br><span class="line">        <span class="type">InternetAddress</span> from = <span class="keyword">new</span> <span class="type">InternetAddress</span>(<span class="string">"发送者qq邮箱"</span>);</span><br><span class="line">        message.setFrom(from);</span><br><span class="line">        <span class="type">InternetAddress</span>[] tos = <span class="type">InternetAddress</span>.parse(recivers);</span><br><span class="line">        message.setRecipients(<span class="type">Message</span>.<span class="type">RecipientType</span>.<span class="type">TO</span>, tos);</span><br><span class="line">        message.setSubject(title);</span><br><span class="line">        message.setContent(content, <span class="string">"text/html;charset=UTF-8"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">Transport</span>.send(message);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(<span class="type">String</span>[] args) <span class="keyword">throws</span> <span class="type">Exception</span>&#123;</span><br><span class="line">        send(<span class="string">"接收邮箱"</span>, <span class="string">"测试"</span>, <span class="string">"测试内容"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Spark监控中调用</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">MsgUtils</span>.send(<span class="string">"接收者邮箱,接收者邮箱"</span>, <span class="string">"ERROR：数据异常"</span>, <span class="string">s"jobID: <span class="subst">$jobId</span> 数据异常，请马上检查: <span class="subst">$&#123;listener.toString&#125;</span>s"</span>)</span><br></pre></td></tr></table></figure>

</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/31/spark/7/">Spark之监控模块</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-31</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>Spark自带监控</li>
<li>Spark接口监控</li>
<li>Spark自定义监控</li>
</ol>
<h2 id="Spark自带监控"><a href="#Spark自带监控" class="headerlink" title="Spark自带监控"></a><a href="http://spark.apache.org/docs/latest/monitoring.html#spark-history-server-configuration-options" target="_blank" rel="noopener">Spark自带监控</a></h2><p>第一种监控方式是Spark自带的，由于Spark Web UI界面只在sc的生命周期内有效，所以我们需要存储日志，在Spark sc 生命周期结束后重构UI界面。</p>
<p>首先看官方文档配置，这里只是简单配置</p>
<ol>
<li><p>修改spark.default.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">#开启日志存储</span><br><span class="line">spark.eventLog.enabled true</span><br><span class="line">#指定日志存储的HDFS目录</span><br><span class="line">spark.eventLog.dir hdfs://hadoop:9000/spark-logs</span><br><span class="line">#开启日志存储7天自动删除</span><br><span class="line">spark.history.fs.cleaner.enabled true</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改spark.env.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">#指定日志恢复目录，就是上面的日志存储目录</span><br><span class="line">SPARK_HISTORY_OPTS = "-Dspark.history.fs.logDirectory=hdfs://hadoop:9000/spark-logs"</span><br></pre></td></tr></table></figure>
</li>
<li><p>在 sc 的生命周期外打开历史UI界面</p>
</li>
</ol>
<h2 id="Spark接口监控"><a href="#Spark接口监控" class="headerlink" title="Spark接口监控"></a><a href="http://spark.apache.org/docs/latest/monitoring.html#rest-api" target="_blank" rel="noopener">Spark接口监控</a></h2><p>首先看官方文档配置，这里只是简单介绍</p>
<p>查看application列表：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">http:<span class="comment">//hadoop:18080/api/v1/applications</span></span><br></pre></td></tr></table></figure>

<p>查看application的所有job</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">http:<span class="comment">//hadoop:18080/history/application_1585632916452_0002/jobs/</span></span><br></pre></td></tr></table></figure>

<h2 id="Spark自定义监控"><a href="#Spark自定义监控" class="headerlink" title="Spark自定义监控"></a><a href="http://spark.apache.org/docs/latest/monitoring.html#executor-task-metrics" target="_blank" rel="noopener">Spark自定义监控</a></h2><p>metrics: 数据信息</p>
<p>spark 提供了一系列整个任务生命周期中各个阶段变化的事件监听机制，通过这一机制可以在任务的各个阶段做一些自定义的各种动作。SparkListener便是这些阶段的事件监听接口类 通过实现这个类中的各种方法便可实现自定义的事件处理动作。</p>
<p>自定义监听sparListener后的注册方式有两种：</p>
<p><strong>方法1：</strong>conf 配置中指定</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">.setAppName(getClass.getSimpleName)</span><br><span class="line">.setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line"><span class="comment">//监听类注册</span></span><br><span class="line">.set(<span class="string">"spark.extraListeners"</span>, <span class="string">"com.tunan.spark.listener.MySparkListener"</span>)</span><br><span class="line">.getOrCreate()</span><br></pre></td></tr></table></figure>

<p><strong>方法2：</strong>sparkContext 类中指定</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc.addSparkListener(<span class="keyword">new</span> <span class="type">MySparkAppListener</span>)</span><br></pre></td></tr></table></figure>

<h3 id="SparkListerner"><a href="#SparkListerner" class="headerlink" title="SparkListerner"></a>SparkListerner</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//SparkListener 下各个事件对应的函数名非常直白，即如字面所表达意思。</span></span><br><span class="line"><span class="comment">//想对哪个阶段的事件做一些自定义的动作，变继承SparkListener实现对应的函数即可</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkListener</span> <span class="keyword">extends</span> <span class="title">SparkListenerInterface</span> </span>&#123;</span><br><span class="line">  <span class="comment">//阶段完成时触发的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStageCompleted</span></span>(stageCompleted: <span class="type">SparkListenerStageCompleted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//阶段提交时触发的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStageSubmitted</span></span>(stageSubmitted: <span class="type">SparkListenerStageSubmitted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//任务启动时触发的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskStart</span></span>(taskStart: <span class="type">SparkListenerTaskStart</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//下载任务结果的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskGettingResult</span></span>(taskGettingResult: <span class="type">SparkListenerTaskGettingResult</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//任务结束的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskEnd</span></span>(taskEnd: <span class="type">SparkListenerTaskEnd</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//job启动的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onJobStart</span></span>(jobStart: <span class="type">SparkListenerJobStart</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//job结束的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onJobEnd</span></span>(jobEnd: <span class="type">SparkListenerJobEnd</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//环境变量被更新的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onEnvironmentUpdate</span></span>(environmentUpdate: <span class="type">SparkListenerEnvironmentUpdate</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//块管理被添加的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBlockManagerAdded</span></span>(blockManagerAdded: <span class="type">SparkListenerBlockManagerAdded</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBlockManagerRemoved</span></span>(</span><br><span class="line">      blockManagerRemoved: <span class="type">SparkListenerBlockManagerRemoved</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//取消rdd缓存的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onUnpersistRDD</span></span>(unpersistRDD: <span class="type">SparkListenerUnpersistRDD</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//app启动的事件</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onApplicationStart</span></span>(applicationStart: <span class="type">SparkListenerApplicationStart</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//app结束的事件 [以下各事件也如同函数名所表达各个阶段被触发的事件不在一一标注]</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onApplicationEnd</span></span>(applicationEnd: <span class="type">SparkListenerApplicationEnd</span>): <span class="type">Unit</span> = &#123; &#125; </span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorMetricsUpdate</span></span>(</span><br><span class="line">      executorMetricsUpdate: <span class="type">SparkListenerExecutorMetricsUpdate</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorAdded</span></span>(executorAdded: <span class="type">SparkListenerExecutorAdded</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorRemoved</span></span>(executorRemoved: <span class="type">SparkListenerExecutorRemoved</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorBlacklisted</span></span>(</span><br><span class="line">      executorBlacklisted: <span class="type">SparkListenerExecutorBlacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onExecutorUnblacklisted</span></span>(</span><br><span class="line">      executorUnblacklisted: <span class="type">SparkListenerExecutorUnblacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onNodeBlacklisted</span></span>(</span><br><span class="line">      nodeBlacklisted: <span class="type">SparkListenerNodeBlacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onNodeUnblacklisted</span></span>(</span><br><span class="line">      nodeUnblacklisted: <span class="type">SparkListenerNodeUnblacklisted</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBlockUpdated</span></span>(blockUpdated: <span class="type">SparkListenerBlockUpdated</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onOtherEvent</span></span>(event: <span class="type">SparkListenerEvent</span>): <span class="type">Unit</span> = &#123; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h3><p>首先看官方文档配置，这里只是简单案例</p>
<ol>
<li>自定义监控类，继承SparkListener</li>
<li>重写onTaskEnd方法，拿到taskMetrics</li>
<li>从taskMetrics获取各种数据信息</li>
<li>注册到被监听的类</li>
</ol>
<p><strong>第1-3步代码</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySparkListener</span>(<span class="params">conf:<span class="type">SparkConf</span></span>) <span class="keyword">extends</span> <span class="title">SparkListener</span> <span class="keyword">with</span> <span class="title">Logging</span></span>&#123;</span><br><span class="line">    <span class="comment">//监听每个Task结束</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskEnd</span></span>(taskEnd: <span class="type">SparkListenerTaskEnd</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//获取应用程序名称</span></span><br><span class="line">        <span class="keyword">val</span> appName = conf.get(<span class="string">"spark.app.name"</span>)</span><br><span class="line">        <span class="comment">//获取作业的taskMetrics</span></span><br><span class="line">        <span class="keyword">val</span> metrics = taskEnd.taskMetrics</span><br><span class="line">        <span class="comment">//使用对象接收参数</span></span><br><span class="line">        <span class="keyword">val</span> metricsObject = <span class="type">Metrics</span>(appName,taskEnd.stageId, taskEnd.taskInfo.taskId, metrics.inputMetrics.bytesRead, metrics.shuffleReadMetrics.totalBytesRead, metrics.outputMetrics.bytesWritten, metrics.shuffleWriteMetrics.bytesWritten)</span><br><span class="line">        <span class="comment">//输出字符串类型的metricsObject</span></span><br><span class="line">        logError(metricsObject.toString)</span><br><span class="line">        <span class="comment">//输出Json类型的metricsObject</span></span><br><span class="line">        logError(<span class="type">Json</span>(<span class="type">DefaultFormats</span>).write(metricsObject))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//定义case class对象</span></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Metrics</span>(<span class="params">appName:<span class="type">String</span>,stageId:<span class="type">Long</span>,taskId:<span class="type">Long</span>,bytesRead:<span class="type">Long</span>,bytesWritten:<span class="type">Long</span>,shuffleReadMetrics:<span class="type">Long</span>,shuffleWriteMetrics:<span class="type">Long</span></span>)</span>&#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = <span class="string">s"appName:<span class="subst">$appName</span>,stageId:<span class="subst">$stageId</span>,taskId:<span class="subst">$taskId</span>,bytesRead:<span class="subst">$bytesRead</span>,bytesWritten:<span class="subst">$bytesWritten</span>,shuffleReadMetrics:<span class="subst">$shuffleReadMetrics</span>,shuffleWriteMetrics:<span class="subst">$shuffleWriteMetrics</span>"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>第四步代码</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//输入、输出路径</span></span><br><span class="line">        <span class="keyword">val</span> (in,out) = (args(<span class="number">0</span>),args(<span class="number">1</span>))</span><br><span class="line">        <span class="comment">//配置conf</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">            .setAppName(getClass.getSimpleName)</span><br><span class="line">            .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">            <span class="comment">//监听类注册</span></span><br><span class="line">            .set(<span class="string">"spark.extraListeners"</span>, <span class="string">"com.tunan.spark.listener.MySparkListener"</span>)</span><br><span class="line">        <span class="comment">//拿到sc</span></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="comment">//删除输出目录</span></span><br><span class="line">        <span class="type">CheckHDFSOutPath</span>.ifExistsDeletePath(<span class="keyword">new</span> <span class="type">Configuration</span>(),out)</span><br><span class="line">        <span class="comment">//操作算子</span></span><br><span class="line">        <span class="keyword">val</span> result = sc.textFile(in).flatMap(_.split(<span class="string">"\t"</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">        <span class="comment">//保存文件</span></span><br><span class="line">        result.saveAsTextFile(out)</span><br><span class="line">        <span class="comment">//关闭sc</span></span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Spark自定义监控案例"><a href="#Spark自定义监控案例" class="headerlink" title="Spark自定义监控案例"></a>Spark自定义监控案例</h2><p>把监控参数写入到MySQL</p>
<p>需求：应用程序名字、jobID号、stageID号、taskID号、读取数据量、写入数据量、shuffle读取数据量、shuffle写入数据量。</p>
<ol>
<li><p>建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> wc2mysql(</span><br><span class="line">app_name <span class="built_in">varchar</span>(<span class="number">32</span>),</span><br><span class="line">job_id <span class="built_in">bigint</span>,</span><br><span class="line">stage_id <span class="built_in">bigInt</span>,</span><br><span class="line">task_id <span class="built_in">bigint</span>,</span><br><span class="line">file_read_byte <span class="built_in">bigint</span>,</span><br><span class="line">file_write_byte <span class="built_in">bigint</span>,</span><br><span class="line">shuffle_read_byte <span class="built_in">bigint</span>,</span><br><span class="line">shuffle_write_byte <span class="built_in">bigint</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li><p>实现监控类</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySparkListenerV2</span>(<span class="params">conf: <span class="type">SparkConf</span></span>) <span class="keyword">extends</span> <span class="title">SparkListener</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义JobID</span></span><br><span class="line">    <span class="keyword">var</span> jobId:<span class="type">Long</span> = _</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onJobStart</span></span>(jobStart: <span class="type">SparkListenerJobStart</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//初始化</span></span><br><span class="line">        jobId = jobStart.jobId</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"============准备插入数据============"</span>)</span><br><span class="line">    <span class="comment">//监听每个Task结束</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTaskEnd</span></span>(taskEnd: <span class="type">SparkListenerTaskEnd</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//获取应用程序名称</span></span><br><span class="line">        <span class="keyword">val</span> appName = conf.get(<span class="string">"spark.app.name"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取作业的taskMetrics</span></span><br><span class="line">        <span class="keyword">val</span> metrics = taskEnd.taskMetrics</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用对象接收参数</span></span><br><span class="line">        <span class="keyword">val</span> listener = <span class="type">Listener</span>(appName, jobId, taskEnd.stageId, taskEnd.taskInfo.taskId, metrics.inputMetrics.bytesRead, metrics.outputMetrics.bytesWritten, metrics.shuffleReadMetrics.totalBytesRead, metrics.shuffleWriteMetrics.bytesWritten)</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//结果插入到MySQL</span></span><br><span class="line">        <span class="type">ListenerCURD</span>.insert(listener)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//发送监控邮件</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"true"</span> == conf.get(<span class="string">"spark.send.mail.enabled"</span>))&#123;</span><br><span class="line">            <span class="type">MsgUtils</span>.send(<span class="string">"971118017@qq.com"</span>, <span class="string">"ERROR：数据异常"</span>, <span class="string">s"jobID: <span class="subst">$jobId</span> 数据异常，请马上检查: <span class="subst">$&#123;listener.toString&#125;</span>"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    println(<span class="string">"============成功插入数据============"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>实现监控数据写入MySQL</p>
<p>使用的是scalikejdbc框架实现的，使用具体方法在我的<a href="http://yerias.github.io/2020/03/16/scala/2/">博客</a></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Listener</span>(<span class="params">app_name: <span class="type">String</span>, job_id: <span class="type">Long</span>, stage_id: <span class="type">Long</span>, task_id: <span class="type">Long</span>, file_read_byte: <span class="type">Long</span>, file_write_byte: <span class="type">Long</span>, shuffle_read_byte: <span class="type">Long</span>, shuffle_write_byte: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">ListenerCURD</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Before</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//初始化配置</span></span><br><span class="line">        <span class="type">DBs</span>.setupAll()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(listener: <span class="type">Listener</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="type">Before</span>()</span><br><span class="line">        <span class="comment">//事物插入</span></span><br><span class="line">        <span class="type">DB</span>.localTx &#123;</span><br><span class="line">            <span class="keyword">implicit</span> session =&gt; &#123;</span><br><span class="line">                <span class="type">SQL</span>(<span class="string">"insert into wc2mysql(app_name,job_id,stage_id,task_id,file_read_byte,file_write_byte,shuffle_read_byte,shuffle_write_byte) values(?,?,?,?,?,?,?,?)"</span>)</span><br><span class="line">                    .bind(listener.app_name,listener.job_id, listener.stage_id, listener.task_id, listener.file_read_byte, listener.file_write_byte, listener.shuffle_read_byte, listener.shuffle_write_byte)</span><br><span class="line">                    .update()</span><br><span class="line">                    .apply()</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">After</span>()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">After</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//关闭资源</span></span><br><span class="line">        <span class="type">DBs</span>.closeAll()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动被监控类</p>
<p>被监控的类还是我们上面的WordCount的类，关键在于在SparkConf()中注册</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//配置conf</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">.setAppName(getClass.getSimpleName)</span><br><span class="line">.setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line"><span class="comment">//监听类注册</span></span><br><span class="line">.set(<span class="string">"spark.extraListeners"</span>, <span class="string">"com.tunan.spark.listener.MySparkListenerV2"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>在数据库中查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> wc2mysql;</span><br></pre></td></tr></table></figure>

</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/30/spark/6/">Spark术语&amp;Spark提交&amp;YARN上的提交模式&amp;窄依赖&amp;宽依赖</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-30</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>Spark术语</li>
<li>Spark提交</li>
<li>YARN上提交模式</li>
<li>宽依赖</li>
<li>窄依赖</li>
</ol>
<h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><p>下表总结了关于集群概念的术语:</p>
<table>
<thead>
<tr>
<th><strong>Term</strong></th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td>Application</td>
<td>Spark上的应用程序。由一个<strong>driver program</strong>和集群上的<strong>executors</strong>组成。</td>
</tr>
<tr>
<td>Application jar</td>
<td>一个包含用户的Spark应用程序的jar包</td>
</tr>
<tr>
<td>Driver program</td>
<td>运行应用程序main()函数并创建SparkContext的进程</td>
</tr>
<tr>
<td>Cluster manager</td>
<td>用于获取集群资源的外部服务(例如，standalone manager、Mesos、YARN)</td>
</tr>
<tr>
<td>Deploy mode</td>
<td>区别driver process在何处运行。在“cluster”模式下，框架启动集群内部的驱动程序。在“client”模式下，提交者启动集群外部的驱动程序。</td>
</tr>
<tr>
<td>Worker node</td>
<td>可以在集群中运行application的任何节点</td>
</tr>
<tr>
<td>Executor</td>
<td>在Worker node上被application启动的进程，它运行任务并将数据保存在内存或磁盘存储器中。每个application都有自己的Executor。</td>
</tr>
<tr>
<td>Task</td>
<td>将被发送给一个执行者的工作单元</td>
</tr>
<tr>
<td>Job</td>
<td>由多个任务组成的并行计算，这些任务在响应一个Spark操作时产生(如保存、收集)</td>
</tr>
<tr>
<td>Stage</td>
<td>每个作业被分成更小的任务集，称为阶段，这些阶段相互依赖(类似于MapReduce中的map和reduce阶段)</td>
</tr>
</tbody></table>
<h2 id="Spark提交"><a href="#Spark提交" class="headerlink" title="Spark提交"></a><a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">Spark提交</a></h2><p>注意：在使用Spark提交之前，一定要在环境变量中配置<code>HADOOP_CONF_DIR</code>，否则hadoop的环境引不进来</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HADOOP_CONF_DIR=XXX</span><br></pre></td></tr></table></figure>

<p>Spark支持的部署模式：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class &lt;main-class&gt; \</span><br><span class="line">  --master &lt;master-url&gt; \</span><br><span class="line">  --deploy-mode &lt;deploy-mode&gt; \</span><br><span class="line">  --conf &lt;key&gt;=&lt;value&gt; \</span><br><span class="line">  ... # other options</span><br><span class="line">  &lt;application-jar&gt; \</span><br><span class="line">  [application-arguments]</span><br></pre></td></tr></table></figure>

<p>一些常用的选项是：</p>
<ul>
<li><code>--class</code>：您的应用程序的入口点（例如<code>org.apache.spark.examples.SparkPi</code>）</li>
<li><code>--master</code>：集群的<a href="http://spark.apache.org/docs/latest/submitting-applications.html#master-urls" target="_blank" rel="noopener">主URL</a>（例如<code>spark://23.195.26.187:7077</code>）</li>
<li><code>--deploy-mode</code>：将驱动程序部署在工作节点（<code>cluster</code>）上还是作为外部客户端（<code>client</code>）本地部署（默认值：<code>client</code>）</li>
<li><code>--conf</code>：键值格式的任意Spark配置属性。对于包含空格的值，将“ key = value”用引号引起来（如图所示）。</li>
<li><code>application-jar</code>：jar包的路径，包括您的应用程序和所有依赖项。URL必须在群集内部全局可见，例如，<code>hdfs://</code>路径或<code>file://</code>所有节点上都存在的路径。</li>
<li><code>application-arguments</code>：参数传递给您的主类的main方法（如果有）</li>
</ul>
<p>其他常用的选项：</p>
<ul>
<li><p><code>--num-executors</code>：executors的数量</p>
</li>
<li><p><code>--executor-memory</code>：每个executor的内存数量</p>
</li>
<li><p><code>--total-executor-cores 100</code>：executor的总的core数</p>
</li>
<li><p><code>--jars</code>：指定需要依赖的jar包，多个jar包逗号分隔，application中直接引用</p>
</li>
<li><p><code>--files</code>：需要依赖的文件，在application中使用SparkFiles.get(“file”)取出，同时需要放在resources目录下</p>
</li>
</ul>
<p>注意：local模式默认读写HDFS数据 读本地要加<code>file://</code></p>
<h2 id="提交模式"><a href="#提交模式" class="headerlink" title="提交模式"></a>提交模式</h2><h3 id="cliet模式"><a href="#cliet模式" class="headerlink" title="cliet模式"></a>cliet模式</h3><ol>
<li>Driver运行在Client</li>
<li>AM职责就是去YARN上申请资源</li>
<li>Driver会和请求到的container/executor进行通信</li>
<li>Driver是不能退出的</li>
</ol>
<p><img src="https://yerias.github.io/spark_img/Client%E6%A8%A1%E5%BC%8F.jpg" alt="Client模式"></p>
<p><strong>Client模式控制台能看到日志</strong></p>
<h3 id="cluster模式"><a href="#cluster模式" class="headerlink" title="cluster模式"></a>cluster模式</h3><ol>
<li><p>Driver运行位置在AM</p>
</li>
<li><p>Client提交上去了  它退出对整个作业没影响</p>
</li>
<li><p>AM(申请资源)+Driver（调度DAG，分发任务）</p>
<p><img src="https://yerias.github.io/spark_img/Cluster%E6%A8%A1%E5%BC%8F.jpg" alt="Cluster模式"></p>
</li>
</ol>
<p><strong>控制台不能看到日志，不支持Spark-shell(Spark-SQL) ，交互性操作的都不能</strong></p>
<h2 id="窄依赖"><a href="#窄依赖" class="headerlink" title="窄依赖"></a>窄依赖</h2><ol>
<li>一个父RDD的分区至多被一个子RDD的某个分区使用一次</li>
<li>一个父RDD的分区和一个子RDD的分区是唯一映射 典型的map</li>
<li>多个父RDD的分区和一个子RDD的分区是唯一映射 典型的union</li>
</ol>
<p><img src="https://yerias.github.io/spark_img/%E7%AA%84%E4%BE%9D%E8%B5%96.jpg" alt="窄依赖"></p>
<p>在窄依赖中有个特殊的join是不经过shuffle 的</p>
<p>这个特殊的join的存在有三个条件：</p>
<ol>
<li>RDD1的分区数 = RDD2的分区数</li>
<li>RDD1的分区数 = Join的分区数</li>
<li>RDD2的分区数 = Join的分区数 </li>
</ol>
<p>我们看一个案例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * rdd1、rdd2、join 三者的分区数相同，不经过shuffle</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>((<span class="string">"香蕉"</span>,<span class="number">20</span>), (<span class="string">"苹果"</span>,<span class="number">50</span>), (<span class="string">"菠萝"</span>,<span class="number">30</span>), (<span class="string">"猕猴桃"</span>, <span class="number">50</span>)),<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">List</span>((<span class="string">"草莓"</span>,<span class="number">90</span>), (<span class="string">"苹果"</span>,<span class="number">25</span>), (<span class="string">"菠萝"</span>,<span class="number">25</span>), (<span class="string">"猕猴桃"</span>, <span class="number">30</span>), (<span class="string">"西瓜"</span>, <span class="number">45</span>)),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.reduceByKey(_ + _)</span><br><span class="line"><span class="keyword">val</span> rdd4 = rdd2.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> joinRDD = rdd3.join(rdd4,<span class="number">2</span>)</span><br><span class="line">joinRDD.collect()</span><br></pre></td></tr></table></figure>

<p>再看Application的DAG图，从两个 reduceByKey 到 join 是一个 stage 中的，说明没有产生 shuffle</p>
<p><img src="https://yerias.github.io/spark_img/%E7%89%B9%E6%AE%8A%E7%9A%84Jion.jpg" alt="特殊的Jion"></p>
<h2 id="宽依赖"><a href="#宽依赖" class="headerlink" title="宽依赖"></a>宽依赖</h2><ol>
<li>一个父RDD的分区会被子RDD的分区使用多次</li>
</ol>
<p><img src="https://yerias.github.io/spark_img/%E5%AE%BD%E4%BE%9D%E8%B5%96.jpg" alt="宽依赖"></p>
<p>除了前面那种是三个条件满足的，其他的 join 都是宽依赖</p>
<p>我们使RDD1的分区数和RDD2的分区数相等，但是 join的分区数不相等</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * rdd1、rdd2的分区数不同，但是和join的分区数不同，会经过shuffle</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>((<span class="string">"香蕉"</span>,<span class="number">20</span>), (<span class="string">"苹果"</span>,<span class="number">50</span>), (<span class="string">"菠萝"</span>,<span class="number">30</span>), (<span class="string">"猕猴桃"</span>, <span class="number">50</span>)),<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">List</span>((<span class="string">"草莓"</span>,<span class="number">90</span>), (<span class="string">"苹果"</span>,<span class="number">25</span>), (<span class="string">"菠萝"</span>,<span class="number">25</span>), (<span class="string">"猕猴桃"</span>, <span class="number">30</span>), (<span class="string">"西瓜"</span>, <span class="number">45</span>)),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.reduceByKey(_ + _)</span><br><span class="line"><span class="keyword">val</span> rdd4 = rdd2.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> joinRDD = rdd3.join(rdd4,<span class="number">4</span>)</span><br><span class="line">joinRDD.collect()</span><br></pre></td></tr></table></figure>

<p>我们看DAG图，产生了stage，也就是经过了shuffle</p>
<p><img src="https://yerias.github.io/spark_img/%E7%89%B9%E6%AE%8A%E7%9A%84Jion%E5%AE%BD%E4%BE%9D%E8%B5%96.jpg" alt="特殊的Jion宽依赖"></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/30/error/5/">Spark疯狂踩坑系列</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-30</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Error/">Error</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Error/">Error</a></span><div class="content"><p>如果WEB UI界面或者程序日志里面看不到错误，使用以下方式查看日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn logs -applicationId application_1585536649766_xxxx</span><br></pre></td></tr></table></figure>



<p>错误1</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Error: Could not find or load main class org.apache.spark.deploy.yarn.ApplicationMaster</span><br></pre></td></tr></table></figure>

<p>解决办法：</p>
<p>检查spark-defaults.conf中的配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark.yarn.jars=hdfs://hadoop:9000/spark-yarn/jars/*.jar</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark.yarn.archive  hdfs://hadoop000:8020/tmp/spark-archive/spark2.4.5.zip</span><br></pre></td></tr></table></figure>

<p>以上两种配置方式不可以错乱</p>
<p>错误2</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">java.lang.NoClassDefFoundError: org/lionsoul/ip2region/DbConfig</span><br></pre></td></tr></table></figure>

<p>解决办法</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--jars /home/hadoop/lib/ip2region-1.7.2.jar</span><br></pre></td></tr></table></figure>



<p>错误3</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">java.lang.NullPointerException</span><br><span class="line">	at com.tunan.spark.utils.IpParseUtil.IpParse(IpParseUtil.java:19)</span><br></pre></td></tr></table></figure>

<p>解决办法</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--files /home/hadoop/lib/ip2region.db</span><br></pre></td></tr></table></figure>

<hr>
<p>代码中拿出文件有两种方式</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String dbPath = GetIPRegion.class.getResource("/ip2region.db").getPath();</span><br><span class="line">String dbPath = SparkFiles.get(<span class="string">"/ip2region.db"</span>);</span><br></pre></td></tr></table></figure>

</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/29/spark/5/">Spark使用Yarn模式解决Jar乱飞情况</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-29</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Spark/">Spark</a></span><div class="content"><ol>
<li><p>在本地创建zip文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在spark的jars目录下</span></span><br><span class="line">zip spark.zip ./*</span><br></pre></td></tr></table></figure>
</li>
<li><p>HDFS上创建存放spark jar的目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -mkdir -p  /spark-yarn/jars</span><br></pre></td></tr></table></figure>
</li>
<li><p>将$SPARK_HOME/jars下的spark.zip包上传至刚建的HDFS路径</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop jars]$ hadoop fs -put ./spark.zip /spark-yarn/jars/</span><br></pre></td></tr></table></figure>
</li>
<li><p>在 spark-defaults.conf中添加(也可以在启动的时候–conf指定)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark.yarn.archive=hdfs://hadoop:9000/spark-yarn/jars/spark.zip</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看Spark log</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn log -applicationID xxx</span><br></pre></td></tr></table></figure>

</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/26/error/4/">MR编程时，Driver传递的参数Mapper显示为NULL</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-26</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Error/">Error</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Error/">Error</a></span><div class="content"><p>在进行MR编程时，除了需要拿到HDFS上面的数据，有时候还需要Driver和Mapper或者Reducer之间进行参数传递</p>
<p>先看看我碰到的问题</p>
<p><img src="https://yerias.github.io/java_img/14.png" alt=""></p>
<p><img src="https://yerias.github.io/java_img/15.png" alt=""></p>
<p>在Driver中配置向Conf中配置了参数，在Mapper中从Context中拿出来的却是null值，<strong>问题出现在Job.getInstance() 中没有把Conf传递进去。</strong></p>
<p>所以以后要注意，在创建Job的时候要把Conf也放进去。</p>
<p>还需要注意的是Conf中的设值必须在Job.getInstance()上面完成</p>
</div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/3/">&lt;&lt;</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/5/">&gt;&gt;</a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>