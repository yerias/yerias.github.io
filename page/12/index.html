<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="BigData Developer"><meta name="keywords" content="yerias,TUNANのBlog,BigData"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017 | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">133</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">30</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">26</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">TUNANのBlog</div><div id="site-sub-title">感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017</div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/08/mysql/5/">Linux下MySQL进程死掉的可能解决方案</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-08</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/MySQL/">MySQL</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/MySQL/">MySQL</a></span><div class="content"><p>linux下mysql进程死掉，且无法启动mysql服务，查看myql日志，发现如下日志：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">2019-10-10 18:11:03 9772 [Note] InnoDB: Initializing buffer pool, size = 128.0M</span><br><span class="line">InnoDB: mmap(136019968 bytes) failed; errno 12</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] InnoDB: Cannot allocate memory for the buffer pool</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Plugin 'InnoDB' init function returned error.</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Unknown/unsupported storage engine: InnoDB</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Aborting</span><br></pre></td></tr></table></figure>

<p>其中InnoDB: mmap(136019968 bytes) failed; errno 12是关键的错误信息。<br>从网上查资料，有人说修改innodb_buffer_pool_size，经过测试无效。<br>有人说是swap分区为0导致的此错误，使用free -m命令查看系统内存，发现swap确实为0。使用如下命令建立一个临时的swap分区：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">d if=/dev/zero of=/swap bs=1M count=512  //创建一个swap文件，大小为512M</span><br><span class="line">mkswap /swap                              //将swap文件变为swap分区文件</span><br><span class="line">swapon /swap                              //将其映射为swap分区</span><br></pre></td></tr></table></figure>

<p>此时使用<code>free -m</code>命令即可看到swap分区已存在了，然后启动mysql服务即可。<br>为了保证下次系统启动后，此swap分区被自动加载，需要修改系统的fstab文件，操作如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">vi /etc/fstab</span><br><span class="line">//在其中添加如下一行</span><br><span class="line">/swap swap swap defaults 0 0</span><br></pre></td></tr></table></figure>

</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/07/hadoop/4/">HDFS的副本存放策略&amp;HDFS的读写流程&amp;Pid文件详解&amp;HDFS常用命令&amp;HDFS的回收站机制&amp;安全模式详解&amp;单、多节点的磁盘均衡策略</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-07</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/HDFS/">HDFS</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/HDFS/">HDFS</a></span><div class="content"><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol>
<li>整理副本放置策略</li>
<li>整理读写流程</li>
<li>整理pid文件</li>
<li>整理hdfs dfs 常用命令</li>
<li>整理多节点，单节点的磁盘均衡</li>
<li>整理安全模式</li>
</ol>
<h3 id="副本放置策略"><a href="#副本放置策略" class="headerlink" title="副本放置策略"></a>副本放置策略</h3><p>副本存放策略存在新旧两个版本</p>
<p>具体可参考我的另一个博客:<a href="https://yerias.github.io/2018/10/11/DataWarehouse/hadoop/5/">https://yerias.github.io/2018/10/11/DataWarehouse/hadoop/5/</a></p>
<h4 id="hadoop2-7-6之前的副本存放策略"><a href="#hadoop2-7-6之前的副本存放策略" class="headerlink" title="hadoop2.7.6之前的副本存放策略"></a>hadoop2.7.6之前的副本存放策略</h4><ul>
<li>副本一：同机架的不同节点 </li>
<li>副本二：同机架的另一节点 </li>
<li>副本三：不同机架的另一节点 </li>
<li>其他副本：随机挑选</li>
</ul>
<h4 id="hadoop2-8-4之后的副本存放策略"><a href="#hadoop2-8-4之后的副本存放策略" class="headerlink" title="hadoop2.8.4之后的副本存放策略"></a>hadoop2.8.4之后的副本存放策略</h4><ul>
<li>副本一：同Client的节点上 </li>
<li>副本二：不同机架中的节点上 </li>
<li>副本三：同第二个副本的机架中的另一个节点上</li>
<li>其他副本：随机挑选</li>
</ul>
<h4 id="副本存放策略优点"><a href="#副本存放策略优点" class="headerlink" title="副本存放策略优点"></a>副本存放策略优点</h4><ul>
<li>提高系统的可靠性</li>
<li>提供负载均衡</li>
<li>提高访问效率</li>
</ul>
<h3 id="读写流程"><a href="#读写流程" class="headerlink" title="读写流程"></a>读写流程</h3><h4 id="读取流程-FSData-InputStream"><a href="#读取流程-FSData-InputStream" class="headerlink" title="读取流程(FSData InputStream)"></a>读取流程(FSData InputStream)</h4><p><img src="http://yerias.github.io/hadoop_img/read.jpg" alt="hdfs读取数据流程"></p>
<ol>
<li>首先调用FileSystem对象的open()方法，获得一个分布式文件系统(DistributedFileSystem)的实例。</li>
<li>分布式文件系统(DistributedFileSystem)通过RPC获得文件的第一批块(Block)的位置信息，<a href="">同一个块按照副本数会返回多个位置信息</a>，这些位置信息按照Hadoop拓扑结构排序，距离客户端近的排在前面。</li>
<li>前两步会返回一个文件系统数据输入流(FSDataInputStream)对象，该对象会被封装为分布式文件系统输入流(DFSInputStream)对象，DFSInputStream可以方便地管理DataNode和NameNode的数据流。客户端调用read方法，DFSInputStream会找出离客户端最近的DataNode并连接。</li>
<li>数据从DataNode源源不断地流向客户端</li>
<li>如果第一个块的数据读完了，就会管理指向第一个块的DataNode的连接，接着读取下一个块。这些操作对客户端来说是透明的，从客户端的角度看来只是在读一个持续不断的数据流。</li>
<li>如果第一批块都读取完了，DFSInputStream就会去NameNode拿下一批块的位置信息，然后继续读，如果所有的块都读完了，这时就会关闭掉所有的流。</li>
</ol>
<p><code>注意:</code> 如果在读数据的时候，DFSInputStream和DataNode的通信发生异常，就会尝试连接正在读的块的排序第二近的DataNode，并且会记录哪个DataNode发生错误，剩余的块读的时候就会直接跳过该DataNode。DFSInputStream也会检查块的校验和，如果发现一个坏的块，就会先报告到NameNode，然后DFSIputStream在其它的DataNode上读取该块的数据。</p>
<h4 id="写入流程-FSData-OutputStream"><a href="#写入流程-FSData-OutputStream" class="headerlink" title="写入流程(FSData OutputStream)"></a>写入流程(FSData OutputStream)</h4><p><img src="http://yerias.github.io/hadoop_img/write.jpg" alt="hdfs写入数据流程"></p>
<ol>
<li>客户端在同过调用分布式文件系统(DistributedFileSystem)的create()方法创建新文件</li>
<li>DistributedFileSystem通过RPC调用NameNode去创建一个没有块关联的新文件，创建前NameNode会做各种校验，比如文件是否存在，客户端有没有权限等。如果通过校验，NameNode就会记录下新文件，否则就会抛出I/O异常。</li>
<li>前两步结合，会返回文件系统数据输出流(FSDataOutputStream)的对象，与读文件的时候相似，DistributedFileSystem被封装成分布式文件系统的输出流(DFSOutputStream)。DFSOutputStream可以协调NameNode和DataNode的通信。客户端开始写数据到DFSOutputStream，DFSOutputStream会把数据切分成一个个的数据包(packet)，然后排成数据队列(data quenc)</li>
<li>接下来，数据队列中的数据包首先输出到数据管道(多个datanode节点组成数据管道)中的第一个DataNode(写数据包)，第一个DataNode又把数据包输出到第二个DataNode中，依次类推。</li>
<li>DFSOutputStream还维护着一个队列叫做确认队列(ack quenc)，这个队列也是由数据包组成，用于等待DataNode收到数据返回确认数据包，当数据管道中的所有DataNode都表示已经收到了确认信息的时候，这时ack quenc才会把对应的数据包移除掉。</li>
<li>客户端完成写数据后，调用close()方法关闭写入数据流。</li>
<li>客户端通知NameNode把文件标记为已完成。然后NameNode把文件写成功的结果反馈给客户端。此时就表示客户端已完成整个HDFS的写数据流程。</li>
</ol>
<h5 id="如果写数据的过程中某个DataNode发生错误，会采取以下的步骤处理。"><a href="#如果写数据的过程中某个DataNode发生错误，会采取以下的步骤处理。" class="headerlink" title="如果写数据的过程中某个DataNode发生错误，会采取以下的步骤处理。"></a>如果写数据的过程中某个DataNode发生错误，会采取以下的步骤处理。</h5><ol>
<li>管道关闭</li>
<li>正常的DataNode上正在写的块会有一个新ID(需要和NameNode通信)，而失败的DataNode上的那个不完整的块会在上报心跳的时候被删除。</li>
<li>失败的DataNode会被移除出数据管道，块中剩余的数据包继续写入管道中的其他两个DataNode。</li>
<li>NameNode会标记这个块的副本个数少于指定值，块的副本会稍后在另一个DataNode创建。</li>
<li>有些时候多个DataNode会失败，只要<code>dfs.replication.min</code>(缺省是1个)属性定义的指定个数的DataNode写入数据成功了，整个写入过程就算成功，缺少的副本会进行异步的恢复。</li>
</ol>
<p><code>注意:</code> 只有调用sync()方法，客户端才确保该文件的写操作已经全部完成， 当客户端调用close()方法时，会默认调用sync()方法。</p>
<h3 id="pid文件"><a href="#pid文件" class="headerlink" title="pid文件"></a>pid文件</h3><p>pid文件具体作用请参考我的另外一个博客:<a href="https://yerias.github.io/2018/10/12/DataWarehouse/hadoop/6/">https://yerias.github.io/2018/10/12/DataWarehouse/hadoop/6/</a></p>
<p>在这里我们只简单说一下修改pid文件的生成目录的步骤，在修改hadoop文件的时候，hadoop最好是stop状态，否则需要kill进程。</p>
<ol>
<li><p>创建/home/hadoop/tmp目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir /home/hadoop/tmp</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改/home/hadoop/tmp的权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chmod -R 777 /hadoop/hadoop/tmp</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改hadoop-env.sh文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HADOOP_PID_DIR=/home/hadoop/tmp</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改yarn-env.sh</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export YARN_PID_DIR=/home/hadoop/tmp</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="hdfs-dfs-常用命令"><a href="#hdfs-dfs-常用命令" class="headerlink" title="hdfs dfs 常用命令"></a>hdfs dfs 常用命令</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">[-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [&lt;path&gt; ...]]</span><br><span class="line">[-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">[-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span><br></pre></td></tr></table></figure>

<p>这些命令很点单，和linux的作用一样，这里不做演示。。。</p>
<h3 id="回收站"><a href="#回收站" class="headerlink" title="回收站"></a>回收站</h3><p>回收站的作用是把hdfs上删除的文件保存一定的时间然后自动删除，Apache是默认关闭的，CDH默认是开启的。</p>
<p>Apache的参数是由<code>core-default.xml</code>文件控制的<code>fs.trash.interval</code>属性</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>fs.trash.interval</td>
<td>0</td>
<td>检查点被删除的分钟数。如果为零，则禁用垃圾特性。单位秒</td>
</tr>
</tbody></table>
<p>一般在生产环境下设置保存7天</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun hadoop]# vim core-site.xml </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;10080&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p><code>注意:</code> 切记检查生产环境是否开启回收站,开了回收站，慎用 <code>-skipTrash</code></p>
<h3 id="安全模式"><a href="#安全模式" class="headerlink" title="安全模式"></a>安全模式</h3><p>安全模式是hadoop的一种保护机制，安全模式下不能进行修改文件的操作，但是可以浏览目录结构、查看文件内容的。</p>
<p>如果NN的log显示<code>Name node is in safe mode</code> ，正常手动让其离开安全模式，这种操作很少做。</p>
<p><strong>一般进入safemode情况有:</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1. 启动或者重新启动hdfs时</span><br></pre></td></tr></table></figure>
<pre><code>2. HDFS维护升级时
3. 块文件损坏等。。。</code></pre><p>可以使用<code>fsck</code>检查一下HDFS的健康度，然后进行下一步操作</p>
<p><code>hdfs fsck / :</code> 用这个命令可以检查整个文件系统的健康状况,但是要注意它不会主动恢复备份缺失的block,这个是由NameNode单独的线程异步处理的</p>
<p><strong>fsck相关介绍:</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs fsck</span><br><span class="line">　　　　Usage:DFSck &lt;path&gt; [-move | -delete | -openforwrite] [-files [-blocks [-locations | -racks]]]</span><br><span class="line">　　　　&lt;path&gt; 检查这个目录中的文件是否完整</span><br><span class="line">　　　　-move 破损的文件移至/lost+found目录</span><br><span class="line">　　　　-delete 删除破损的文件</span><br><span class="line">　　　　-openforwrite 打印正在打开写操作的文件</span><br><span class="line">　　　　-files 打印正在check的文件名</span><br><span class="line">　　　　-blocks 打印block报告(需要和-files参数一起使用)</span><br><span class="line">　　　　-locations 打印每个block的位置信息(需要和-files参数一起使用)</span><br><span class="line">　　　　-racks 打印位置信息的网络拓扑图(需要和-files参数一起使用)</span><br></pre></td></tr></table></figure>

<p>一般我们会查看 / 目录下的损坏文件，然后根据损坏文件的路径手动进行<code>hdfs debug</code>修复</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs fsck / -list-corruptfileblocks</span><br></pre></td></tr></table></figure>

<h3 id="多节点的磁盘均衡"><a href="#多节点的磁盘均衡" class="headerlink" title="多节点的磁盘均衡"></a>多节点的磁盘均衡</h3><p>由于集群中的一些服务器如CPU、磁盘、网络的差异，副本存放并不会一直保持均衡，这就造成某一些服务器的磁盘占用率达到90%，而另外一些服务器的磁盘占用率只有60%或者80%。所以就有必要手动进行均衡操作，事实上hadoop的sbin目录下也有这个命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-rwxr-xr-x 1 hadoop hadoop 1128 Jun  3  2019 start-balancer.sh  #开始</span><br><span class="line">-rwxr-xr-x 1 hadoop hadoop 1179 Jun  3  2019 stop-balancer.sh	#停止</span><br></pre></td></tr></table></figure>

<p>那么集群中的磁盘占用率怎么才算正常？这个由参数<code>threshold</code>控制，默认threshold=10，即各个服务器保持所有服务器的磁盘占用空间的平均值上下浮动10%，可能不好理解，我们用上面的占用率90%、60%和80%算一下。</p>
<p>这三台的平均占用率是:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(90+60+80)/3=76%</span><br></pre></td></tr></table></figure>

<p>那么<code>threshold</code>参数就控制这三台其中的任意一台的磁盘占用率不得超过86%，不得低于66%。</p>
<h4 id="那么怎么做呢？"><a href="#那么怎么做呢？" class="headerlink" title="那么怎么做呢？"></a>那么怎么做呢？</h4><p>在进行磁盘均衡之前，我们需要重新设置一下balancer的带宽限制，在<code>hdfs-default.xml</code>文件中的<code>dfs.datanode.balance.bandwidthPerSec</code>属性，默认是10M，生产环境下一般设置为30M</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.datanode.balance.bandwidthPerSec</td>
<td>10m</td>
<td>指定每个datanode可用于平衡目的的最大带宽(以每秒字节数为单位)。</td>
</tr>
</tbody></table>
<p>在<code>hadoop</code>的<code>hdfs-site.xml</code>文件中覆盖一下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun hadoop]# vim hdfs-site.xml </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.balance.bandwidthPerSec&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;30m&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>怎么做？</p>
<p>写个shell脚本，每天凌晨执行<code>./start-balancer.sh</code>调度一次，达到数据平衡，毛刺修正，调度执行完成自动关闭，不需要执行<code>./stop-balancer.sh</code>手段关闭，除非特殊情况。</p>
<h3 id="单节点的磁盘均衡"><a href="#单节点的磁盘均衡" class="headerlink" title="单节点的磁盘均衡"></a>单节点的磁盘均衡</h3><p>在官网中的描述: </p>
<p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html" target="_blank" rel="noopener">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html</a></p>
<p>在官网中的描述中有这么一句: <code>dfs.disk.balancer.enabled must be set to true in hdfs-site.xml.</code></p>
<p>翻译过来就是: 必须在<code>hdfs-site.xml</code>中将<code>dfs.disk.balancer.enabled</code>设置为<code>true</code>。</p>
<p>这是因为默认情况下，群集上未启用磁盘平衡器</p>
<p>那么我们先去设置一下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun hadoop]# vim hdfs-site.xml </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.disk.balancer.enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h4 id="场景"><a href="#场景" class="headerlink" title="场景:"></a>场景:</h4><p>假如我们现在有三个数据盘</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/data01   90%</span><br><span class="line">/data02   60%</span><br><span class="line">/data03   80%</span><br></pre></td></tr></table></figure>

<p>现在磁盘用的差不多了，准备加入一个盘<code>/data04   0%</code></p>
<p>我们这时候是不是要进行单节点服务器的磁盘均衡？</p>
<h4 id="怎么做？"><a href="#怎么做？" class="headerlink" title="怎么做？"></a>怎么做？</h4><ol>
<li><p>生成hadoop001.plan.json</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -plan hadoop001	#hadoop001是datanode的主机名</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -execute hadoop001.plan.json 	#hadoop001是datanode的主机名</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询状态</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -query ruozedata001 	#hadoop001是datanode的主机名</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="什么时候手动或调度执行？"><a href="#什么时候手动或调度执行？" class="headerlink" title="什么时候手动或调度执行？"></a>什么时候手动或调度执行？</h4><ol>
<li>新盘加入</li>
<li>监控服务器的磁盘剩余空间小于阈值10%，发邮件预警 ，手动执行</li>
</ol>
<h4 id="怎么在DataNode中挂载磁盘？"><a href="#怎么在DataNode中挂载磁盘？" class="headerlink" title="怎么在DataNode中挂载磁盘？"></a>怎么在DataNode中挂载磁盘？</h4><p>由<code>hdfs-default.xml</code>文件的<code>dfs.datanode.data.dir</code>属性控制</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.datanode.data.dir</td>
<td>file://${hadoop.tmp.dir}/dfs/data</td>
<td>确定DFS数据节点应该将其块存储在本地文件系统的何处。多个目录使用逗号分隔。</td>
</tr>
</tbody></table>
<p>假如我们现在有/data01,/data02,/data03,/data04四个目录需要挂载在该DataNode节点中</p>
<p>修改<code>hdfs-site.xml</code>文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun hadoop]# vim hdfs-site.xml </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/data01,/data02,/data03,/data04&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h5 id="为什么DN的生产上要挂载多个物理的磁盘目录，而不是做一个raid-磁盘阵列"><a href="#为什么DN的生产上要挂载多个物理的磁盘目录，而不是做一个raid-磁盘阵列" class="headerlink" title="为什么DN的生产上要挂载多个物理的磁盘目录，而不是做一个raid(磁盘阵列)"></a>为什么DN的生产上要挂载多个物理的磁盘目录，而不是做一个raid(磁盘阵列)</h5><p>为了高效率写  高效率读</p>
<p><code>注意:</code> 提前规划好2-3年存储量 ，避免后期加磁盘维护的工作量</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/07/mysql/%E6%95%B0%E6%8D%AE%E9%87%8D%E5%88%B7%E6%9C%BA%E5%88%B6/">数据重刷机制(抛砖引玉)</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-07</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/MySQL/">MySQL</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/DataWarehouse/">DataWarehouse</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Data/">Data</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/SQL/">SQL</a></span><div class="content"><h4 id="先抛出几个问题"><a href="#先抛出几个问题" class="headerlink" title="先抛出几个问题"></a>先抛出几个问题</h4><ol>
<li><p>存储是不是基石？</p>
</li>
<li><p>假如存储不挂，数据真的准确吗？</p>
</li>
<li><p>存储挂了，数据还准确吗？</p>
</li>
<li><p>如何校验是否正确？如何让其正确？机制是不是必须有？</p>
</li>
</ol>
<p>注：<code>sqoop</code>抽数据，无<code>error</code>丢数据的概率很小</p>
<p>数据质量校验：数据量校验 <code>count</code>相同吗？<code>count</code>相同内容相同吗？</p>
<p>数据量相同–&gt;数据量不同 重刷机制 补or删 <code>spark</code> 95%–&gt;数据内容不同？ 抽样 5%</p>
<h4 id="现在重点理解一下重刷机制"><a href="#现在重点理解一下重刷机制" class="headerlink" title="现在重点理解一下重刷机制"></a>现在重点理解一下重刷机制</h4><p>背景：用<code>count</code>校验上下游的数据不准确</p>
<p>引入重刷机制：通过对上下游的两个表求<code>full outer join</code>来对比字段的<code>null</code>值</p>
<p>上游表a</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| id   | data  | age  |</span><br><span class="line">| <span class="comment">---- | ----- | ---- |</span></span><br><span class="line">| 1    | data1 | 18   |</span><br><span class="line">| 2    | data2 | 19   |</span><br><span class="line">| 3    | data3 | 20   |</span><br><span class="line">| 7    | data7 | 22   |</span><br></pre></td></tr></table></figure>

<p>下游表b</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| id   | data  | age  |</span><br><span class="line">| <span class="comment">---- | ----- | ---- |</span></span><br><span class="line">| 1    | data1 | 18   |</span><br><span class="line">| 2    | data2 | 19   |</span><br><span class="line">| 3    | data3 | 20   |</span><br><span class="line">| 7    | data7 | 22   |</span><br></pre></td></tr></table></figure>

<p>我们发现表 a 和表 b 对比 表 a 少了 5 、6 多了 7 ，表 b 少了 2 、 7 多了 6，我们现在对两个表做 <code>full outer join</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| aid  | data   | age  | bid  |</span><br><span class="line">| <span class="comment">---- | ------ | ---- | ---- |</span></span><br><span class="line">| 1    | ruoze1 | 18   | 1    |</span><br><span class="line">| 2    | ruoze2 | 19   | null |</span><br><span class="line">| 3    | ruoze3 | 20   | 3    |</span><br><span class="line">| 7    | ruoze7 | 22   | null |</span><br><span class="line">| null | null   | null | 5    |</span><br><span class="line">| null | null   | null | 6    |</span><br></pre></td></tr></table></figure>

<p>以表 a 为标准，对生成后的大表做筛选，分别查找 <code>aid</code> 和 <code>bid</code> 为 <code>null</code> 的记录</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">from</span> t <span class="keyword">where</span> aid=<span class="literal">null</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">from</span> t <span class="keyword">where</span> bid=<span class="literal">null</span></span><br></pre></td></tr></table></figure>

<p>发现 <code>bid</code>为 5 、 6 的行 <code>aid</code> 为 <code>null</code>，说明 <code>bid</code> 下游数据多了，根据 <code>bid</code> 重新构建</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> b <span class="keyword">where</span> bid=<span class="number">5</span>     </span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> b <span class="keyword">where</span> bid=<span class="number">6</span></span><br></pre></td></tr></table></figure>

<p>发现 <code>aid</code> 为 2 、 7 的 <code>bid</code> 为<code>null</code>，说明 <code>bid</code> 下游数据少了，根据 <code>aid</code> 重新构建</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="number">2</span> ruoze2 <span class="number">19</span> </span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="number">7</span> ruoze7 <span class="number">22</span></span><br></pre></td></tr></table></figure>

<p>经过重新构建也就是重刷后的数据是</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| aid  | data   | age  | bid  |</span><br><span class="line">| <span class="comment">---- | ------ | ---- | ---- |</span></span><br><span class="line">| 1    | ruoze1 | 18   | 1    |</span><br><span class="line">| 2    | ruoze2 | 19   | 2    |</span><br><span class="line">| 3    | ruoze3 | 20   | 3    |</span><br><span class="line">| 7    | ruoze7 | 22   | 7    |</span><br></pre></td></tr></table></figure>

<h4 id="深度思考："><a href="#深度思考：" class="headerlink" title="深度思考："></a>深度思考：</h4><p><code>full outer join</code> 其实就是先 <code>left join</code> 和后 <code>right join</code> 的两个结果，为 <code>null</code> 的刚好是缺少的或者多的，而交集是上下游都有的数据，需要做的是 <code>left join</code> 为 <code>null</code> 做 <code>insert</code> 或者 <code>delete</code>，还是 <code>right join</code> 为 null 做 <code>insert</code> 或者 <code>delete</code>。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/06/hadoop/3/">理解数据块、副本数、小文件的概念&amp;掌握HDFS架构&amp;掌握NN和SNN交互流程</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-06</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/HDFS/">HDFS</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/HDFS/">HDFS</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/DataWarehouse/">DataWarehouse</a></span><div class="content"><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol>
<li>整理对 块大小 副本数的理解</li>
<li>整理对小文件的理解</li>
<li>整理HDFS架构</li>
<li>整理SNN流程</li>
</ol>
<h3 id="修改hdfs的数据保存文件"><a href="#修改hdfs的数据保存文件" class="headerlink" title="修改hdfs的数据保存文件"></a>修改hdfs的数据保存文件</h3><p>在开始完成今天的目标之前，我们还要做一个事情，那就是修改hdfs的nn、nd、snn文件保存的目录，这个目录默认保存在/tmp目录下，那么为什么会保存在/tmp目录下呢，实际上是由<code>core-default.xml</code>默认参数决定的</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>hadoop.tmp.dir</td>
<td>/tmp/hadoop-${user.name}</td>
<td>A base for other temporary directories.</td>
</tr>
</tbody></table>
<p>因为/tmp目录具有固定周期清除文件的特性，所以我们这里需要改变hadoop的存储文件路径，防止丢失文件</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ vim core-site.xml </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>在这之后我们还要对/home/hadoop/tmp目录做一下调整</p>
<ul>
<li><p><code>chmod -R 777 /home/hadoop/tmp</code></p>
</li>
<li><p><code>mv /tmp/hadoop-hadoop/dfs /home/hadoop/tmp/</code></p>
</li>
<li><p><code>test</code> </p>
</li>
</ul>
<h3 id="对块大小和副本数的理解"><a href="#对块大小和副本数的理解" class="headerlink" title="对块大小和副本数的理解"></a>对块大小和副本数的理解</h3><h4 id="块的理解"><a href="#块的理解" class="headerlink" title="块的理解"></a>块的理解</h4><p>hadoop1.x的块大小是64M，hadoop2.x的块大小是128M，块的大小是<code>hdfs-default.xml</code>文件中的<code>dfs.blocksize</code>属性控制，它是hdfs存储处理数据的最小单元，可以根据实际需求改变块大小，但是一般不建议这么做。</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.blocksize</td>
<td>134217728(128M)</td>
</tr>
</tbody></table>
<p>hadoop1.x的块大小是64M，hadoop2.x的块大小是128M，块的大小是<code>hdfs-default.xml</code>文件中的<code>dfs.blocksize</code>属性控制，它是hdfs存储处理数据的最小单元，可以根据实际需求改变块大小，但是一般不建议这么做。</p>
<p><code>块大小为什么要设计成128M？</code></p>
<p>是为了最小化寻址时间，目前磁盘的传输速率普遍是在100M/S左右，所以设计成128M每块。</p>
<h4 id="副本数的理解"><a href="#副本数的理解" class="headerlink" title="副本数的理解"></a>副本数的理解</h4><p>副本的设置让hadoop具有高可靠性的特点，数据不会轻易丢失。副本是存储在dn中的，由<code>hdfs-default.xml</code>文件的<code>dfs.replication</code>参数控制，伪分布式部署是1份，集群部署是3份，不建议修改。</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.replication</td>
<td>3</td>
</tr>
</tbody></table>
<h3 id="对小文件的理解"><a href="#对小文件的理解" class="headerlink" title="对小文件的理解"></a>对小文件的理解</h3><p>一般来说，小文件是文件大小小于10M的数据，由于hadoop的架构特性，它只能有一台主nn，如果小文件特别多的话，小文件的块也特别多，nn需要维护的块的元数据信息的条数也多，所以我们一般把小文件合并成大文件再放到hdfs上，也有上传hdfs后合并，这样来减少nn维护的块的元数据数量。具体合并的方式，以后再讲。</p>
<h3 id="整理HDFS架构"><a href="#整理HDFS架构" class="headerlink" title="整理HDFS架构"></a>整理HDFS架构</h3><p>HDFS由NameNode、SecondaryNameNode、DataNode三个组件组成</p>
<h4 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h4><p>NameNode也被称为名称节点或元数据节点，是HDFS主从架构中的主节点，相当于HDFS的大脑，它管理文件系统的命名空间，维护着整个文件系统的目录树以及目录树中所有子目录和文件。</p>
<p>这些信息以两个文件的形式持久化保存在本地磁盘上，一个是命令空间镜像FSImage(File System Image)，主要是用来存储HDFS的元数据信息。还有一个是命令空间镜像的编辑日志(Editlog)，该文件保存用户对命令空间镜像的修改信息。</p>
<h4 id="SecondeayNameNode"><a href="#SecondeayNameNode" class="headerlink" title="SecondeayNameNode"></a>SecondeayNameNode</h4><p>SecondaryNameNode也被称为元数据节点，是HDFS主从架构中的备用节点，主要用于定期合并命名空间镜像(FSImage)和命令空间镜像的操作日志(Editlog)，是一个辅助NameNode的守护进程。</p>
<p>定期合并FSImage和Editlog的周期时间是由<code>hdfs-default.xml</code>文件的<code>dfs.namenode.checkpoint.period</code>属性决定的，默认一小时合并一次，同时如果Editlog操作日志记录满 1000000条也会触发合并机制，由<code>dfs.namenode.checkpoint.txns</code>属性控制，两者满足一个即可。</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.namenode.checkpoint.period</td>
<td>3600</td>
<td>两个周期性检查点之间的秒数。</td>
</tr>
<tr>
<td>dfs.namenode.checkpoint.txns</td>
<td>1000000</td>
<td>两个周期性检查点之间的名称空间记录数。</td>
</tr>
</tbody></table>
<p>虽然SecondaryNameNode能够减轻单点故障，但是还会有风险，因为总有一段时间的数据是没有同步的。</p>
<h5 id="问题-为什么SecondaryNameNode要辅助NameNode定期合并FSImage文件和Editlog文件？"><a href="#问题-为什么SecondaryNameNode要辅助NameNode定期合并FSImage文件和Editlog文件？" class="headerlink" title="问题: 为什么SecondaryNameNode要辅助NameNode定期合并FSImage文件和Editlog文件？"></a>问题: 为什么SecondaryNameNode要辅助NameNode定期合并FSImage文件和Editlog文件？</h5><p>FSImage文件实际上是HDFS文件系统中元数据的一个永久性检查点(checkpoint)，但也并不是每一个写操作都会更新到这个文件中，因为FSImage是一个大型文件，如果频繁地执行写操作，会导致系统运行极其缓慢，那么如何解决呢？</p>
<p>解决方案就是NameNode将命令空间的改动信息写入命令空间的Editlog，但随着时间的推移，Editlog文件会越来越大，一旦发生故障，那么将需要花费很长的时间进行回滚操作，所以可以像传统的关系型数据库一样，定期地合并FSImage和Editlog，但是如果由NameNode来做合并操作，由于NameNode在为集群提供服务的同时可能无法提供足够的资源，所以为了解决这一问题，SecondaryNameNode就应运而生了。</p>
<h4 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h4><p>DataNode也被称为数据节点，它是HDFS主从架构在的从节点，它存储数据块和数据块校验和它在NameNode的指导下完成数据的IO操作。</p>
<p><img src="https://yerias.github.io/hadoop_img/image-20191202110000146.png" alt="数据块和数据库校验和"></p>
<p>DataNode会不断地向NameNode发送心跳和块报告信息，并执行来自NameNode的指令。</p>
<p>发送心跳是为了告诉nn我还活着，通过<code>hdfs-default.xml</code>文件的<code>dfs.heartbeat.interval</code>参数可以得知，每3秒发送一次</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.heartbeat.interval</td>
<td>3</td>
<td>Determines datanode heartbeat interval in seconds.</td>
</tr>
</tbody></table>
<p>发送块报告信息是为了扫描数据目录并协调内存块和磁盘块之间的差异的，从<code>hdfs-default.xml</code>文件的<code>dfs.datanode.directoryscan.interval</code>属性和<code>dfs.blockreport.intervalMsec</code>可以得知每6小时发送一次块报告，生产环境下建议缩短周期（3小时）</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.datanode.directoryscan.interval</td>
<td>21600</td>
<td>DataNode扫描数据目录并协调内存块和磁盘块之间的差异的时间间隔(以秒为单位)，发现损坏块</td>
</tr>
<tr>
<td>dfs.blockreport.intervalMsec</td>
<td>21600000</td>
<td>确定以毫秒为单位的块报告间隔，恢复数据块</td>
</tr>
</tbody></table>
<p>在这里我们需要知道一个hadoop命令，该命令仅适用于高级用户，不正确的使用可能会导致数据丢失。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun subdir0]$ hdfs debug</span><br><span class="line">Usage: hdfs debug &lt;command&gt; [arguments]</span><br><span class="line"></span><br><span class="line">These commands are for advanced users only.</span><br><span class="line"></span><br><span class="line">Incorrect usages may result in data loss. Use at your own risk.</span><br><span class="line"></span><br><span class="line">verifyMeta -meta &lt;metadata-file&gt; [-block &lt;block-file&gt;]</span><br><span class="line">computeMeta -block &lt;block-file&gt; -out &lt;output-metadata-file&gt;</span><br><span class="line">recoverLease -path &lt;path&gt; [-retries &lt;num-retries&gt;]</span><br><span class="line">[hadoop@aliyun subdir0]$</span><br></pre></td></tr></table></figure>

<h5 id="手动修复"><a href="#手动修复" class="headerlink" title="手动修复"></a>手动修复</h5><p><code>hdfs debug</code>的作用是在多副本的环境下手动修复元数据、块或者副本，我们在这里只说修改副本，这里的xxx是指副本路径，该路径必须驻留在HDFS文件系统上，由<code>hdfs fsck</code>命令查找。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">[hadoop@aluyuncurrent]$</span><span class="bash"> hdfs debug recoverLease -path xxx -retries 10</span></span><br></pre></td></tr></table></figure>

<h5 id="自动修复"><a href="#自动修复" class="headerlink" title="自动修复"></a><code>自动修复</code></h5><p><a href="https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/" target="_blank" rel="noopener">https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/</a></p>
<p>但是有可能: 手动修复 + 自动修复都是失败的 </p>
<p>这就需要保证数据仓库的数据质量和数据重刷机制恢复 </p>
<h5 id="问题-DataNode是如何存储和管理数据块的？"><a href="#问题-DataNode是如何存储和管理数据块的？" class="headerlink" title="问题: DataNode是如何存储和管理数据块的？"></a>问题: DataNode是如何存储和管理数据块的？</h5><ol>
<li>DataNode节点是以数据块的形式在本地Linux文件系统上保存HDFS文件的内容，并对外提供文件数据访问功能。</li>
<li>DataNode节点的一个基本功能就是管理这些保存在Linux文件系统上的数据</li>
<li>DataNode节点是将数据块以Linux文件的形式保存在本地的存储系统上</li>
</ol>
<h3 id="SecondaryNameNode和NameNode的交互流程"><a href="#SecondaryNameNode和NameNode的交互流程" class="headerlink" title="SecondaryNameNode和NameNode的交互流程"></a>SecondaryNameNode和NameNode的交互流程</h3><p><img src="https://yerias.github.io/hadoop_img/IMG_0614(20191202-125538).JPG" alt="SecondaryNameNode和NameNode的交互流程"></p>
<ol>
<li><code>SecondaryNameNode</code>引导<code>NameNode</code>滚动更新操作日志，并开始将新的操作日志写进<code>edits.new</code>。</li>
<li><code>SecondaryNameNode</code>将<code>NameNode</code>的<code>FSImage</code>文件和<code>Edits</code>文件复制到本地的检查点目录。</li>
<li><code>SecondaryNameNode</code>将<code>FSImage</code>文件导入内存，回放编辑日志<code>Edits</code>文件，将其合并到<code>FSImage.ckpt</code>文件，并将新的<code>FSImage.ckpt</code>文件压缩后写入磁盘。</li>
<li><code>SecondaryNameNode</code>将新的<code>FSImage.ckpt</code>文件传回<code>NameNode</code>。</li>
<li><code>NameNode</code>在接收新的<code>FSImage.ckpt</code>文件后，将<code>FSImage.ckpt</code>替换为<code>FSImage</code>，然后直接加载和启用该文件</li>
<li><code>NameNode</code>将<code>Edits.new</code>更名为<code>Edits</code>。默认情况下，该过程1小时内发生1次，或者当编辑日志达到默认值1000000条也会触发。</li>
</ol>
<h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><ol>
<li><p>NN的fsimage的个数默认是保留2个</p>
<p><img src="https://yerias.github.io/hadoop_img/57Y5RSNU_6%5DY886.jpg" alt="fsimage文件"></p>
<p>控制的参数是<code>hdfs-default.xml</code>文件的<code>dfs.namenode.num.checkpoints.retained</code>参数</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.namenode.num.checkpoints.retained</td>
<td>2</td>
</tr>
</tbody></table>
</li>
<li><p>NN的editlog文件不会保留所有的，至于保留的个数还是周期，解决中。。。</p>
<p><img src="https://yerias.github.io/hadoop_img/C(J%60DYZC@_%7BEA%5B3(GMDRK%7DI.jpg" alt="editlog文件"></p>
</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/05/hadoop/2/">yarn的伪分布式部署&amp;jps的原理&amp;oom-killer&amp;/tmp目录的clean机制</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-05</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/Yarn/">Yarn</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/MapReduce/">MapReduce</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Yarn/">Yarn</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Linux/">Linux</a></span><div class="content"><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol>
<li>搭建 yarn伪分布式</li>
<li>跑mr count案例</li>
<li>整理 jps命令</li>
<li>Linux两个机制 oom  clean</li>
</ol>
<h3 id="Yarn伪分布式部署-amp-单点-amp-主从架构"><a href="#Yarn伪分布式部署-amp-单点-amp-主从架构" class="headerlink" title="Yarn伪分布式部署&amp;单点&amp;主从架构"></a>Yarn伪分布式部署&amp;单点&amp;主从架构</h3><p>由于hadoop集成了MapReduce和Yarn，所以这里我们只要修改相关的配置文件即可使用</p>
<p>我们需要配置<code>mapred-site.xml</code>和<code>yarn-site.xml</code>这两个文件，由于<code>mapred-site.xml</code>不存在所以需要复制一个出来</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ cp mapred-site.xml.template mapred-site.xml</span><br></pre></td></tr></table></figure>

<p>然后用<code>vim</code>打开<code>mapred-site.xml</code>并修改</p>
<p>vim mapred-site.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>然后用<code>vim</code>打开<code>yarn-site.xml</code>并修改文件</p>
<p>vim yarn-site.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   	 <span class="tag">&lt;<span class="name">value</span>&gt;</span>aliyun:38088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="comment">&lt;!-- 日志聚集功能使能 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 日志保留时间设置7天 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><code>注意:</code> 这是使用38088替代默认的8088是为了避免云主机被挖矿</p>
<p>最后启动<code>Yarn</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ start-yarn.sh</span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/yarn-hadoop-resourcemanager-aliyun.out</span><br><span class="line">aliyun: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.16.2/logs/yarn-hadoop-nodemanager-aliyun.out</span><br><span class="line"></span><br><span class="line">[hadoop@aliyun hadoop]$ jps</span><br><span class="line">32501 NodeManager</span><br><span class="line">349 Jps</span><br><span class="line">32415 ResourceManager</span><br></pre></td></tr></table></figure>

<p>最后再用<code>ps -ef</code>命令验证<code>ResourceManager</code>程序是否真的启动完成</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ netstat -nlp|grep 32415</span><br><span class="line">(Not all processes could be identified, non-owned process info</span><br><span class="line"> will not be shown, you would have to be root to see it all.)</span><br><span class="line">tcp        0      0 172.16.39.48:38088      0.0.0.0:*               LISTEN      32415/java          </span><br><span class="line">tcp        0      0 0.0.0.0:8030            0.0.0.0:*               LISTEN      32415/java          </span><br><span class="line">tcp        0      0 0.0.0.0:8031            0.0.0.0:*               LISTEN      32415/java          </span><br><span class="line">tcp        0      0 0.0.0.0:8032            0.0.0.0:*               LISTEN      32415/java          </span><br><span class="line">tcp        0      0 0.0.0.0:8033            0.0.0.0:*               LISTEN      32415/java</span><br></pre></td></tr></table></figure>

<p>启动完成，可以去玩<code>wordcount</code>案例了</p>
<h3 id="使用MapReduce运行WordCount案例"><a href="#使用MapReduce运行WordCount案例" class="headerlink" title="使用MapReduce运行WordCount案例"></a>使用MapReduce运行WordCount案例</h3><p>在data目录下创建mapreduce的输入文件</p>
<p>[hadoop@aliyun data]$ vim name.log</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ruozedata</span><br><span class="line">ruoze</span><br><span class="line">jepson</span><br><span class="line">huhu</span><br><span class="line">ye</span><br><span class="line">tunan</span><br><span class="line">afei</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">3</span><br><span class="line">2</span><br><span class="line">1</span><br><span class="line">a b c a a aaaa bcd</span><br><span class="line">ruoze jepson</span><br></pre></td></tr></table></figure>

<p>把name.log文件put到hdfs上</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun data]$ hdfs dfs -mkdir -p /wordcount/input</span><br><span class="line">[hadoop@aliyun data]$ hdfs dfs -put name.log /wordcount/input</span><br><span class="line">[hadoop@aliyun data]$ hdfs dfs -ls /wordcount/input/</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 hadoop supergroup         86 2019-12-01 17:46 /wordcount/input/name.log</span><br></pre></td></tr></table></figure>

<p>使用hadoop jar 运行案例</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar wordcount /wordcount/input /wordcount/output</span><br></pre></td></tr></table></figure>

<p>查看结果</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ hdfs dfs -cat /wordcount/output/*</span><br><span class="line">1	2</span><br><span class="line">2	2</span><br><span class="line">3	2</span><br><span class="line">a	3</span><br><span class="line">aaaa	1</span><br><span class="line">afei	1</span><br><span class="line">b	1</span><br><span class="line">bcd	1</span><br><span class="line">c	1</span><br><span class="line">huhu	1</span><br><span class="line">jepson	2</span><br><span class="line">ruoze	2</span><br><span class="line">ruozedata	1</span><br><span class="line">tunan	1</span><br><span class="line">ye	1</span><br></pre></td></tr></table></figure>

<h3 id="jps命令不为人知的地方"><a href="#jps命令不为人知的地方" class="headerlink" title="jps命令不为人知的地方"></a>jps命令不为人知的地方</h3><p>我们想既然jps可以直接运行，肯定在APTH路径下，我们何不which一下看看</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ which jps</span><br><span class="line">/usr/java/jdk/bin/jps</span><br></pre></td></tr></table></figure>

<p>原来jps是一个java命令，我们使用一下jps看看它有什么作用</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ jps</span><br><span class="line">883 DataNode</span><br><span class="line">2117 Jps</span><br><span class="line">32501 NodeManager</span><br><span class="line">1082 SecondaryNameNode</span><br><span class="line">781 NameNode</span><br><span class="line">32415 ResourceManager</span><br></pre></td></tr></table></figure>

<p>jps命令打印出来了hadoop组件里面的程序pid和程序名，引申出这些程序都属于java程序，结合jps属于java命令，得出jps打印出来所有运行的java程序</p>
<p>我们可以从启动程序的脚本中得到pid和程序名存放的文件，这里就不去debug，他们默认存放在/tmp目录下一个叫做hsperfdata_用户名的文件下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun tmp]$ ll hsperfdata_hadoop/</span><br><span class="line">total 160</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:08 1082</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:08 32415</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:08 32501</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:08 781</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:08 883</span><br></pre></td></tr></table></figure>

<p>每个pid对应是一些二进制文件，没啥好看的，我们发现这个文件下还有一些以pid结尾的文件，文件保存的其实也是各自对应的pid号码，但是这里的.pid文件是进程自己创建的。用来管理和结束进程的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun tmp]$ ll grep *.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 4 Dec  1 17:39 hadoop-hadoop-datanode.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 4 Dec  1 17:39 hadoop-hadoop-namenode.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 5 Dec  1 17:39 hadoop-hadoop-secondarynamenode.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 6 Dec  1 17:37 yarn-hadoop-nodemanager.pid</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 6 Dec  1 17:37 yarn-hadoop-resourcemanager.pid</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun tmp]$ cat hadoop-hadoop-datanode.pid </span><br><span class="line">883</span><br></pre></td></tr></table></figure>

<p>所以jps命令的作用就明白了，在使用jsp命令时，会去/tmp目录下/hsperfdata_username目录下找到程序的pid和程序名并打印出来，/tmp/hsperfdata_username目录会存放该用户所有已经启动的java进程信息。</p>
<p>这里需要get到一个很关键的知识点，它可能让你在shell脚本中犯错，那就是进程所属的用户去执行 jps命令，只显示自己的相关的进程信息，也就是说，其他用户使用jps命令查看不到本用户启动的程序，root用户可以看所有的，但是显示不可用，我们这里就用root用户尝试一下</p>
<p>我在这里翻车了，使用root显示出来了所有的进程，原因不明，但是其他普通用户无法显示</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">aliyun:mysqladmin:/usr/local/mysql:&gt;jps</span><br><span class="line">2346 Jps</span><br></pre></td></tr></table></figure>

<p>这同样能得出我们想要的结果，那就是其他用户使用jps不能查看到本用户启动的java程序。其原理我们看看这个文件夹的权限就知道了</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun tmp]$ ll hsperfdata_hadoop/</span><br><span class="line">total 128</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:26 1082</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:26 32415</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:26 32501</span><br><span class="line">-rw------- 1 hadoop hadoop 32768 Dec  1 18:26 781</span><br></pre></td></tr></table></figure>

<p>最后总结一下我们在判断一个程序是否在运行时，切不可使用jps命令，因为jps命令只能查看本用户的java进程，那我们在shell脚本中应该如何判断一个程序是否存在？使用万能的ps -ef。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun tmp]$ ps -ef|grep 781 | grep -v grep | wc -l</span><br><span class="line">1</span><br></pre></td></tr></table></figure>

<h3 id="Linux的oom-kill和clean机制"><a href="#Linux的oom-kill和clean机制" class="headerlink" title="Linux的oom-kill和clean机制"></a>Linux的oom-kill和clean机制</h3><p>在最后我们说一下在使用hadoop时会遇到的两个坑，而且还都是Linux自带的特性，以及解决的方法。</p>
<p><code>oom-killer机制:</code> 大数据程序是非常吃内存的，而在Linux内核检测到系统内存不足后，会触发oom-killer，挑选占用最大内存的进程杀掉。</p>
<p>如果我们的进程突然断了，首先查看日志找<code>EROOR</code>，有<code>ERROR</code>具体分析，没有<code>ERROR</code>但是<code>INFO</code>信息断了，很可能就是触发了<code>oom-killer机制</code>，使用<code>free -h</code>命令查看内存使用情况，再使用<code>cat /var/log/messages | grep oom</code>命令查看有没有类似于<code>Killed process</code> 的命令，如果有，就是触发了<code>oom-killer机制</code></p>
<p><code>clean机制:</code> Linux的/tmp目录是一个临时目录，它有一个机制，默认清理超过30天的内容，而前面使用<code>jps</code>命令的时候就发现，<code>hadoop</code>的进程<code>pid</code>都存放在<code>/tmp</code>目录中，启动进程的时候去<code>/tmp</code>目录下创建对应的<code>pid</code>文件，结束进程的时候去<code>/tmp</code>目录下找到程序对应的<code>pid</code>用来结束进程并删除<code>pid</code>文件，那么引申出来一个问题，如果我们的<code>hadoop</code>组件进程启动时间超过了30天了呢，<code>pid</code>文件被清理，结束命令找不到<code>pid</code>号，会再重新创建一个<code>pid</code>，结果就是<code>pid</code>号紊乱，进程无法正常结束。</p>
<p><code>解决的办法</code>就是在家目录下面创建一个tmp目录，然后把hdfs和yarn的pid号管理文件夹设置成家目录下的tmp目录即可。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ vi hadoop-env.sh</span><br><span class="line">export HADOOP_PID_DIR=/home/hadoop/tmp</span><br><span class="line"></span><br><span class="line">[hadoop@aliyun hadoop]$ cat yarn-env.sh</span><br><span class="line">export YARN_PID_DIR=/home/hadoop/tmp</span><br></pre></td></tr></table></figure>

</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/04/hadoop/1/">HDFS的伪分布式部署&amp;HADOOP的常用命令</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-04</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/HDFS/">HDFS</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/HDFS/">HDFS</a></span><div class="content"><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol>
<li>安装hadoop的hdfs伪分布式部署</li>
<li>hadoop fs常规命令</li>
<li>配置文件在官方哪里找</li>
<li>整理 jdk、ssh、hosts文件</li>
</ol>
<h3 id="1-安装hadoop的hdfs伪分布式部署"><a href="#1-安装hadoop的hdfs伪分布式部署" class="headerlink" title="1.安装hadoop的hdfs伪分布式部署"></a>1.安装hadoop的hdfs伪分布式部署</h3><ol>
<li><p>创建用户和目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun ~]# useradd hadoop</span><br><span class="line">[root@aliyun ~]# su - hadoop</span><br><span class="line">[hadoop@aliyun ~]$ mkdir app software sourcecode log tmp data lib</span><br><span class="line">[hadoop@aliyun ~]$ ll</span><br><span class="line">total 28</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 app　　　　#解压的文件夹  软连接</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 data　　　#数据</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 lib　　　　#第三方的jar</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 log　　　　#日志文件夹</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 software　#压缩包</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 sourcecode　　#源代码编译</span><br><span class="line">drwxrwxr-x 2 hadoop hadoop 4096 Nov 28 11:26 tmp　　　　#临时文件夹</span><br></pre></td></tr></table></figure>
</li>
<li><p>下载/上传压缩包</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ cd software/</span><br><span class="line">[hadoop@aliyun software]$ wget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz</span><br></pre></td></tr></table></figure>
</li>
<li><p>解压</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun software]$ tar -xzvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ../app/</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">[hadoop@aliyun software]$ cd ../app/</span><br><span class="line">[hadoop@aliyun app]$ ln -s hadoop-2.6.0-cdh5.16.2/ hadoop</span><br><span class="line">[hadoop@aliyun app]$ ll</span><br><span class="line">total 4</span><br><span class="line">lrwxrwxrwx  1 hadoop hadoop   23 Nov 28 11:36 hadoop -&gt; hadoop-2.6.0-cdh5.16.2/</span><br><span class="line">drwxr-xr-x 14 hadoop hadoop 4096 Jun  3 19:11 hadoop-2.6.0-cdh5.16.2</span><br></pre></td></tr></table></figure>
</li>
<li><p>环境要求</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun java]# mkdir /usr/java</span><br><span class="line">[root@aliyun java]# cd /usr/java</span><br><span class="line">[root@aliyun java]# rz -E</span><br><span class="line">[root@aliyun java]# tar -xzvf jdk-8u144-linux-x64.tar.gz</span><br><span class="line">[root@aliyun java]# chown -R  root:root jdk1.8.0_144/</span><br><span class="line">[root@aliyun java]# ln -s jdk1.8.0_144/ jdk</span><br><span class="line">[root@aliyun java]# ll</span><br><span class="line">total 4</span><br><span class="line">lrwxrwxrwx 1 root root   13 Nov 28 12:01 jdk -&gt; jdk1.8.0_144/</span><br><span class="line">drwxr-xr-x 8 root root 4096 Jul 22  2017 jdk1.8.0_144</span><br><span class="line">[root@aliyun java]# vim /etc/profile</span><br><span class="line">    #env</span><br><span class="line">    export JAVA_HOME=/usr/java/jdk</span><br><span class="line">    export PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">[root@aliyun java]# source /etc/profile</span><br><span class="line">[root@aliyun java]# which java</span><br><span class="line">/usr/java/jdk/bin/java</span><br></pre></td></tr></table></figure>
</li>
<li><p>JAVA_HOME 显性配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ vi hadoop-env.sh</span><br><span class="line">export JAVA_HOME=/usr/java/jdk</span><br><span class="line">[root@aliyun java]# cat /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">172.16.39.48 aliyun</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">etc/hadoop/core-site.xml:</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://aliyun:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line">etc/hadoop/hdfs-site.xml:</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>ssh无密码信任关系</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">家目录下输入</span><br><span class="line"><span class="meta">  $</span><span class="bash"> ssh-keygen -t rsa -P <span class="string">''</span> -f ~/.ssh/id_rsa</span></span><br><span class="line"><span class="meta">  $</span><span class="bash"> cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span></span><br><span class="line"><span class="meta">  $</span><span class="bash"> chmod 0600 ~/.ssh/authorized_keys</span></span><br><span class="line">[hadoop@aliyun ~]$ ssh aliyun date</span><br><span class="line">Thu Nov 28 12:15:08 CST 2019</span><br></pre></td></tr></table></figure>
</li>
<li><p>环境变量 hadoop</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ vi .bashrc</span><br><span class="line">export HADOOP_HOME=/home/hadoop/app/hadoop</span><br><span class="line">export PATH=$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin:$PATH</span><br><span class="line">[hadoop@aliyun ~]$ source .bashrc </span><br><span class="line">[hadoop@aliyun ~]$ which hadoop</span><br><span class="line">~/app/hadoop/bin/hadoop</span><br></pre></td></tr></table></figure>
</li>
<li><p>格式化</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ hdfs namenode -format</span><br><span class="line">has been successfully formatted.</span><br></pre></td></tr></table></figure>
</li>
<li><p>第一次启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ start-dfs.sh </span><br><span class="line">[hadoop@aliyun ~]$ jps</span><br><span class="line">10804 SecondaryNameNode</span><br><span class="line">10536 NameNode</span><br><span class="line">10907 Jps</span><br><span class="line">10654 DataNode</span><br><span class="line">[hadoop@aliyun ~]$</span><br></pre></td></tr></table></figure>

<p>坑：第一次启动会输入yes确定信任关系，我们打开./ssh下的known_hosts文件，这个文件中存放信任关系</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun .ssh]$ cat known_hosts</span><br><span class="line">aliyun,172.16.39.48 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBCjHBKn/7LF5sfbae1OLkK5QoWm11Xn8RZs1JTc7K8v4RFum1OKIjArocvRjLOYPsq5ezYo8TlBHTrAgeUcvkBM=</span><br><span class="line">localhost ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBCjHBKn/7LF5sfbae1OLkK5QoWm11Xn8RZs1JTc7K8v4RFum1OKIjArocvRjLOYPsq5ezYo8TlBHTrAgeUcvkBM=</span><br><span class="line">0.0.0.0 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBCjHBKn/7LF5sfbae1OLkK5QoWm11Xn8RZs1JTc7K8v4RFum1OKIjArocvRjLOYPsq5ezYo8TlBHTrAgeUcvkBM=</span><br></pre></td></tr></table></figure>

<p>将来也许在启动hadoop的时候一直要输入密码，就是这里面已经存在了主机的信任关系，但是密匙对是新的，删除这个文件或者内容即可</p>
</li>
<li><p>DN SNN都以 aliyun启动</p>
</li>
</ol>
<ul>
<li><p>NN：core-site.xml fs.defaultFS控制</p>
</li>
<li><p>DN: slaves文件</p>
</li>
<li><p>2NN:hdfs-site.xml</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;aliyun:50090&lt;/value&gt;       #注意端口号，新旧版本有区别</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;aliyun:50091&lt;/value&gt;       #注意端口号，新旧版本有区别</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="2-hadoop-fs常规命令"><a href="#2-hadoop-fs常规命令" class="headerlink" title="2.hadoop fs常规命令"></a>2.hadoop fs常规命令</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -mkdir /</span><br><span class="line">hadoop fs -put</span><br><span class="line">hadoop fs -get</span><br><span class="line">hadoop fs -cat</span><br><span class="line">hadoop fs -rm</span><br><span class="line">hadoop fs -ls</span><br></pre></td></tr></table></figure>

<h3 id="3-配置文件在官方哪里找"><a href="#3-配置文件在官方哪里找" class="headerlink" title="3.配置文件在官方哪里找"></a><strong>3.配置文件在官方哪里找</strong></h3><p><strong><a href="https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation" target="_blank" rel="noopener">https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation</a></strong></p>
<h3 id="4-整理-jdk、ssh、hosts文件"><a href="#4-整理-jdk、ssh、hosts文件" class="headerlink" title="4.整理 jdk、ssh、hosts文件"></a>4.整理 jdk、ssh、hosts文件</h3><p>jdk和ssh是hadoop运行的先决条件</p>
<p>hosts文件存放主机名和ip地址的映射</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/03/mysql/3/">掌握where、group、join语句和写SQL</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-03</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/MySQL/">MySQL</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/DataWarehouse/">DataWarehouse</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/MySQL/">MySQL</a></span><div class="content"><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol>
<li>整理 sql的where各种条件</li>
<li>整理 sql的group</li>
<li>整理 sql的join</li>
<li>04txt文件的案例 9句sql</li>
<li>整理刚才分享的小知识点</li>
<li>补充资料文件夹 去看看执行</li>
<li>彩蛋 视频 sql</li>
</ol>
<h3 id="1-整理-sql-的-where-各种条件"><a href="#1-整理-sql-的-where-各种条件" class="headerlink" title="1.整理 sql 的 where 各种条件"></a>1.整理 sql 的 where 各种条件</h3><p><code>where 子句:</code> 如需有条件地从表中选取数据，可将 where 子句添加到 SELECT 语句。</p>
<p>语法</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> 列名称 <span class="keyword">FROM</span> 表名称 <span class="keyword">WHERE</span> 列 运算符 值</span><br></pre></td></tr></table></figure>

<p>下面的运算符可在 where 子句中使用：</p>
<table>
<thead>
<tr>
<th>操作符</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>=</td>
<td>等于</td>
</tr>
<tr>
<td>&lt;&gt;</td>
<td>不等于</td>
</tr>
<tr>
<td>&gt;</td>
<td>大于</td>
</tr>
<tr>
<td>&lt;</td>
<td>小于</td>
</tr>
<tr>
<td>&gt;=</td>
<td>大于等于</td>
</tr>
<tr>
<td>&lt;=</td>
<td>小于等于</td>
</tr>
<tr>
<td>BETWEEN</td>
<td>在某个范围内</td>
</tr>
<tr>
<td>LIKE</td>
<td>搜索某种模式</td>
</tr>
</tbody></table>
<h3 id="2-整理-sql-的-group"><a href="#2-整理-sql-的-group" class="headerlink" title="2.整理 sql 的 group"></a>2.整理 sql 的 group</h3><p>聚合函数 (比如 SUM) 常常需要添加 group by语句。</p>
<p><code>group by语句:</code> group by语句用于结合聚合函数，根据一个或多个列对结果集进行分组。</p>
<p>语法</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name, aggregate_function(column_name)</span><br><span class="line"><span class="keyword">FROM</span> table_name</span><br><span class="line"><span class="keyword">where</span> column_name <span class="keyword">operator</span> <span class="keyword">value</span></span><br><span class="line"><span class="keyword">group</span> bycolumn_name</span><br></pre></td></tr></table></figure>

<h3 id="3-整理-sql-的-join"><a href="#3-整理-sql-的-join" class="headerlink" title="3.整理 sql 的 join"></a>3.整理 sql 的 join</h3><p>join分为inner join、left join、right join，分别表示内联结，左联结，右联结</p>
<p><code>inner join:</code> 在表中存在至少一个匹配时，INNER JOIN 关键字返回行。</p>
<p>语法</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name(s)</span><br><span class="line"><span class="keyword">FROM</span> table_name1</span><br><span class="line"><span class="keyword">INNER</span> <span class="keyword">JOIN</span> table_name2 </span><br><span class="line"><span class="keyword">ON</span> table_name1.column_name=table_name2.column_name</span><br></pre></td></tr></table></figure>

<p><code>注意:</code> inner 与 join是相同的。</p>
<p><code>left join:</code> left join 关键字会从左表 (table_name1) 那里返回所有的行，即使在右表 (table_name2) 中没有匹配的行。</p>
<p>语法</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name(s)</span><br><span class="line"><span class="keyword">FROM</span> table_name1</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> table_name2 </span><br><span class="line"><span class="keyword">ON</span> table_name1.column_name=table_name2.column_name</span><br></pre></td></tr></table></figure>

<p><code>注意:</code> 在某些数据库中， left join 称为 left outer join。</p>
<p><code>right join</code> right join 关键字会右表 (table_name2) 那里返回所有的行，即使在左表 (table_name1) 中没有匹配的行。</p>
<p>语法</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name(s)</span><br><span class="line"><span class="keyword">FROM</span> table_name1</span><br><span class="line"><span class="keyword">right</span> <span class="keyword">join</span> table_name2 </span><br><span class="line"><span class="keyword">ON</span> table_name1.column_name=table_name2.column_name</span><br></pre></td></tr></table></figure>

<p><code>注意:</code> 在某些数据库中， right join 称为 right outer join。</p>
<h3 id="4-04txt文件的案例9句sql"><a href="#4-04txt文件的案例9句sql" class="headerlink" title="4. 04txt文件的案例9句sql"></a>4. 04txt文件的案例9句sql</h3><ul>
<li>查询出部门编号为30的所有员工的编号和姓名</li>
<li>找出部门编号为10中所有经理，和部门编号为20中所有销售员的详细资料。</li>
<li>查询所有员工详细信息，用工资降序排序，如果工资相同使用入职日期升序排序</li>
<li>列出薪金大于1500的各种工作及从事此工作的员工人数。</li>
<li>列出在销售部工作的员工的姓名，假定不知道销售部的部门编号。</li>
<li>查询姓名以S开头的\以S结尾\包含S字符\第二个字母为L __</li>
<li>查询每种工作的最高工资、最低工资、人数</li>
<li>列出薪金高于公司平均薪金的所有员工号，员工姓名，所在部门名称，上级领导，工资，工资等级</li>
<li>列出薪金高于在部门30工作的 所有/任何一个员工的薪金的员工姓名和薪金、部门名称。</li>
</ul>
<h3 id="5-整理刚才分享的小知识点"><a href="#5-整理刚才分享的小知识点" class="headerlink" title="5.整理刚才分享的小知识点"></a>5.整理刚才分享的小知识点</h3><ol>
<li><p>关于count()的使用细节</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">使用count(id)替换count(*)的使用，可以提升性能</span><br></pre></td></tr></table></figure>
</li>
<li><p>sum()与count()的区别与联想</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sum()计算统计字段的和，count()统计字段的数量，容易混淆，建议使用sum()联想count()，使用count()联想sum()，并区分</span><br></pre></td></tr></table></figure>
</li>
<li><p>sql语句的执行顺序</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> yyyyy <span class="keyword">from</span> rzdata</span><br><span class="line"><span class="keyword">where</span> xxx</span><br><span class="line"><span class="keyword">group</span> byxxx <span class="keyword">having</span> xxx </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> xxx</span><br><span class="line"><span class="keyword">limit</span> xxx ;</span><br></pre></td></tr></table></figure>
</li>
<li><p>all和any的区别</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">all()的用法是表示要满足字段中所有值即最大值，any()的用法表示的是满足字段值任意一个值即最小值</span><br></pre></td></tr></table></figure>
</li>
<li><p>聚合函数中的null值</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">如果在进行数值计算的时候，字段中存在null值，则计算的结果是null值，在进行数值运算的时候使用IFNULL(expression, alt_value)替换null值为0，则能计算出结果</span><br><span class="line">如果第一个参数的表达式 expression 为 NULL，则返回第二个参数的备用值。</span><br></pre></td></tr></table></figure>
</li>
<li><p>unoin和union all的区别</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">union的用法是把联合查询中的语句如果整体重复则去重，unoin all不会去重</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="6-补充资料文件夹-去看看执行"><a href="#6-补充资料文件夹-去看看执行" class="headerlink" title="6.补充资料文件夹 去看看执行"></a>6.补充资料文件夹 去看看执行</h3><p>　　资料文件中</p>
<h3 id="7-彩蛋-视频-sql"><a href="#7-彩蛋-视频-sql" class="headerlink" title="7.彩蛋 视频 sql"></a>7.彩蛋 视频 sql</h3><p>　　百度云中</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/02/mysql/2/">掌握MySQL的建表规范、DDL语句和权限操作</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-02</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/MySQL/">MySQL</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/DataWarehouse/">DataWarehouse</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/MySQL/">MySQL</a></span><div class="content"><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol>
<li>整理 建表规范</li>
<li>整理 DDL语句的</li>
<li>整理 三句话</li>
</ol>
<h3 id="首先看一个建表例子，再去研究应该遵循哪些规范"><a href="#首先看一个建表例子，再去研究应该遵循哪些规范" class="headerlink" title="首先看一个建表例子，再去研究应该遵循哪些规范"></a>首先看一个建表例子，再去研究应该遵循哪些规范</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> rzdata(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">not</span> <span class="literal">null</span> auto_increment,</span><br><span class="line"></span><br><span class="line"><span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">200</span>),</span><br><span class="line">age  <span class="built_in">int</span>(<span class="number">3</span>),</span><br><span class="line"></span><br><span class="line">createuser <span class="built_in">varchar</span>(<span class="number">200</span>) ,</span><br><span class="line">createtime <span class="built_in">timestamp</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">default</span> <span class="keyword">current_timestamp</span>,</span><br><span class="line">updateuser <span class="built_in">varchar</span>(<span class="number">200</span>) ,</span><br><span class="line">updatetime <span class="built_in">timestamp</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">default</span> <span class="keyword">current_timestamp</span> <span class="keyword">on</span> <span class="keyword">update</span> <span class="keyword">current_timestamp</span>,</span><br><span class="line"></span><br><span class="line">primary <span class="keyword">key</span> (<span class="keyword">id</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<ol>
<li><p>表名</p>
<p>不能是中文，不能是汉语拼音 ，不然很low</p>
</li>
<li><p>风格统一</p>
<p>统一所有表的风格，可以从已有的表中查看，或者找leader检查，方便后期维护</p>
</li>
<li><p>第一个字段</p>
<p>第一个字段必须是id，并且自增长，是主键，没有意义 –&gt;拓展: 为什么？</p>
</li>
<li><p>主键</p>
<p>一张表只有一个主键，primary key == unique+not null</p>
</li>
<li><p>后四个字段</p>
<p>后四个字段包括：用户、创建时间、修改用户、修改时间</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">createuser varchar(200) ,</span><br><span class="line">createtime timestamp not null default current_timestamp,</span><br><span class="line">updateuser varchar(200) ,</span><br><span class="line">updatetime timestamp not null default current_timestamp on <span class="keyword">update</span> <span class="keyword">current_timestamp</span>,</span><br></pre></td></tr></table></figure>
</li>
<li><p>业务字段</p>
<p>业务字段需要唯一存在，使用unique约束，如订单号</p>
<p>业务字段都必须加上注释</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">COMMENT</span> <span class="string">'用户名称'</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>字符集CHARSET</p>
<p>查看字符集</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">&gt;&gt; show variables like '%char%';</span><br><span class="line">+<span class="comment">--------------------------+---------------------------------------------------------------+</span></span><br><span class="line">| Variable_name            | Value                                                         |</span><br><span class="line">+<span class="comment">--------------------------+---------------------------------------------------------------+</span></span><br><span class="line">| character_set_client     | utf8                                                          |</span><br><span class="line">| character_set_connection | utf8                                                          |</span><br><span class="line">| character_set_database   | latin1                                                        |</span><br><span class="line">| character_set_filesystem | binary                                                        |</span><br><span class="line">| character_set_results    | utf8                                                          |</span><br><span class="line">| character_set_server     | latin1                                                        |</span><br><span class="line">| character_set_system     | utf8                                                          |</span><br><span class="line">| character_sets_dir       | /usr/local/mysql-5.6.23-linux-glibc2.5-x86_64/share/charsets/ |</span><br><span class="line">+<span class="comment">--------------------------+---------------------------------------------------------------+</span></span><br><span class="line">8 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="DDL语句以及需要注意的点"><a href="#DDL语句以及需要注意的点" class="headerlink" title="DDL语句以及需要注意的点"></a>DDL语句以及需要注意的点</h3><p><code>查询语句：</code>select 查询字段 from 表 ;</p>
<p>注意：</p>
<ol>
<li><p>生产环境下不要用 * 代替所有字段</p>
<p>错误示范：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from tb_user;</span><br><span class="line">+<span class="comment">----+----------+----------------------------------+-------------+---------------------+----------------------------------+</span></span><br><span class="line">| id | username | password                         | phone       | created             | salt                             |</span><br><span class="line">+<span class="comment">----+----------+----------------------------------+-------------+---------------------+----------------------------------+</span></span><br><span class="line">| 28 | zhangsan | e21d44f200365b57fab2641cd31226d4 | 13600527634 | 2018-05-25 17:52:03 | 05b0f203987e49d2b72b20b95e0e57d9 |</span><br><span class="line">| 30 | leyou    | 4de9a93b3f95d468874a3c1bf3b25a48 | 15855410440 | 2018-09-30 11:37:30 | 4565613d4b0e434cb496d4eb87feb45f |</span><br><span class="line">+<span class="comment">----+----------+----------------------------------+-------------+---------------------+----------------------------------+</span></span><br><span class="line">2 rows in <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure>

<p>正确示范：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; select username,password from tb_user;</span><br><span class="line">+<span class="comment">----------+----------------------------------+</span></span><br><span class="line">| username | password                         |</span><br><span class="line">+<span class="comment">----------+----------------------------------+</span></span><br><span class="line">| zhangsan | e21d44f200365b57fab2641cd31226d4 |</span><br><span class="line">| leyou    | 4de9a93b3f95d468874a3c1bf3b25a48 |</span><br><span class="line">+<span class="comment">----------+----------------------------------+</span></span><br><span class="line">2 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询语句如果数据量特别大必须使用where 或者 limit，否则需要使用大量的资源</p>
<p>错误示范：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,image,letter <span class="keyword">from</span> tb_brand;</span><br></pre></td></tr></table></figure>

<p>正确示范：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,image,letter <span class="keyword">from</span> tb_brand <span class="keyword">limit</span> <span class="number">100</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,image,letter <span class="keyword">from</span> tb_brand <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">100</span>;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p><code>新增语句：</code>insert into 表名（字段1，字段2…） values（数据1，数据2…）;</p>
<p>注意：在表名后加上对应要添加的字段名</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tb_brand (<span class="keyword">name</span>，letter) <span class="keyword">values</span>(tunan，T);</span><br></pre></td></tr></table></figure>

<p><code>修改语句：</code>update 表名 set 修改后的字段 where 条件；</p>
<p>注意：一定要加上条件，否则是全局修改</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> tb_brand <span class="keyword">set</span> <span class="keyword">name</span>=<span class="string">"xiaoqi"</span> <span class="keyword">where</span> <span class="keyword">id</span> = <span class="number">1</span>;</span><br></pre></td></tr></table></figure>

<p><code>删除语句：</code>delete from 表名 where 条件；</p>
<p>注意：一定要加上条件，否则是全局删除；</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> tb_brand tb <span class="keyword">where</span> <span class="keyword">id</span> = <span class="number">1</span>;</span><br></pre></td></tr></table></figure>

<h3 id="当某条SQL验证拖累进程时怎么办？"><a href="#当某条SQL验证拖累进程时怎么办？" class="headerlink" title="当某条SQL验证拖累进程时怎么办？"></a>当某条SQL验证拖累进程时怎么办？</h3><p>使用 show processlist；查看mysql中的 sql 进程</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; show processlist;</span><br><span class="line">+<span class="comment">-----+------+---------------------+--------+---------+------+----------+------------------+</span></span><br><span class="line">| Id  | User | Host                | db     | Command | Time | State    | Info             |</span><br><span class="line">+<span class="comment">-----+------+---------------------+--------+---------+------+----------+------------------+</span></span><br><span class="line">| 272 | root | localhost           | leyou1 | Query   |    0 | starting | <span class="keyword">show</span> <span class="keyword">processlist</span> |</span><br><span class="line">| <span class="number">273</span> | root | <span class="number">121.62</span><span class="number">.184</span><span class="number">.34</span>:<span class="number">56629</span> | leyou1 | <span class="keyword">Sleep</span>   |  <span class="number">448</span> |          | <span class="literal">NULL</span>             |</span><br><span class="line">| <span class="number">274</span> | root | <span class="number">121.62</span><span class="number">.184</span><span class="number">.34</span>:<span class="number">56631</span> | leyou1 | <span class="keyword">Sleep</span>   |  <span class="number">592</span> |          | <span class="literal">NULL</span>             |</span><br><span class="line">+<span class="comment">-----+------+---------------------+--------+---------+------+----------+------------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure>

<p>然后根据 id 删除即可</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">kill</span> <span class="keyword">id</span>;</span><br></pre></td></tr></table></figure>

<h3 id="必须要记住的三条命令"><a href="#必须要记住的三条命令" class="headerlink" title="必须要记住的三条命令"></a>必须要记住的三条命令</h3><p><code>修改密码：</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> <span class="keyword">user</span> <span class="keyword">set</span> <span class="keyword">password</span>=<span class="keyword">password</span>(<span class="string">'密码'</span>) <span class="keyword">where</span> <span class="keyword">user</span>=<span class="string">'用户名'</span>;</span><br></pre></td></tr></table></figure>

<p><code>修改权限:</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> 用户名@<span class="string">'%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'密码'</span>;</span><br></pre></td></tr></table></figure>

<p><code>刷新权限：</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span><br></pre></td></tr></table></figure></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/01/mysql/1/">MySQL二进制部署和DBeaver连接MySQL</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-01</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/MySQL/">MySQL</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/DataWarehouse/">DataWarehouse</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/MySQL/">MySQL</a></span><div class="content"><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol>
<li>二进制部署mysql</li>
<li>重新部署</li>
<li>部署dbeaver 打通 mysql</li>
</ol>
<h3 id="二进制部署mysql"><a href="#二进制部署mysql" class="headerlink" title="二进制部署mysql"></a>二进制部署mysql</h3><p>[<a href="https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt]" target="_blank" rel="noopener">https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt]</a>(<a href="https://github.com/Hackeruncle/MySQL/blob/master/MySQL" target="_blank" rel="noopener">https://github.com/Hackeruncle/MySQL/blob/master/MySQL</a> 5.6.23 Install.txt)</p>
<p>京东云下部署代码：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">scripts/mysql_install_db  \</span><br><span class="line"><span class="comment">--user=mysqladmin \</span></span><br><span class="line"><span class="comment">--basedir=/usr/local/mysql \</span></span><br><span class="line"><span class="comment">--datadir=/usr/local/mysql/data</span></span><br></pre></td></tr></table></figure>

<p>必须要装三个环境：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">yum <span class="keyword">install</span> -y perl</span><br><span class="line">yum <span class="keyword">install</span> -y autoconf</span><br><span class="line">yum <span class="keyword">install</span> -y libaio</span><br></pre></td></tr></table></figure>

<h3 id="重新部署"><a href="#重新部署" class="headerlink" title="重新部署"></a>重新部署</h3><ol>
<li><p>删除压缩文件和数据文件</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">rm -rf arch<span class="comment">/* data/*</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>重置执行脚本文件</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">scripts/mysql_install_db  \</span><br><span class="line"><span class="comment">--user=mysqladmin \</span></span><br><span class="line"><span class="comment">--basedir=/usr/local/mysql \</span></span><br><span class="line"><span class="comment">--datadir=/usr/local/mysql/data</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="部署dbeaver-打通-mysql"><a href="#部署dbeaver-打通-mysql" class="headerlink" title="部署dbeaver 打通 mysql"></a>部署dbeaver 打通 mysql</h3><p>修改用户密码：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> <span class="keyword">user</span> <span class="keyword">set</span> <span class="keyword">password</span>=<span class="keyword">password</span>(<span class="string">'ruozedata'</span>) <span class="keyword">where</span> <span class="keyword">user</span>=<span class="string">'root'</span>;</span><br></pre></td></tr></table></figure>

<p>查看用户权限信息：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">user</span>,<span class="keyword">password</span>,host <span class="keyword">from</span> <span class="keyword">user</span>;</span><br></pre></td></tr></table></figure>

<p>给用户添加权限：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> root@<span class="string">'%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'ruozedata'</span>;</span><br><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;　　<span class="comment">#刷新权限</span></span><br></pre></td></tr></table></figure>

<p>DBeaver连接Mysql：</p>
<p><img src="https://img2018.cnblogs.com/i-beta/1657732/201911/1657732-20191121181225170-233024101.png" alt="DBeaver连接设置"> </p>
<p> <img src="https://img2018.cnblogs.com/i-beta/1657732/201911/1657732-20191121181334218-1241892255.png" alt="DBeaver连接添加jar包"></p>
<h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><p>为了保证 ip 地址的安全性和可用性</p>
<p>　　1. 在 linux 的 /etc/hosts 文件中配置 内网ip 地址和主机名的映射环境，这样在shell脚本或者代码中使用主机名替代 ip 使用</p>
<p>　　2. 如果是云主机，在windows中的hosts文件中配置外网ip地址和主机名的映射</p>
<p><code>注意：</code>hosts文件中的前两行切记不能删，否则可能带来bug</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/30/linux/shell/9/">Shell的test命令</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-30</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Linux/Shell/">Shell</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Linux/">Linux</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Shell/">Shell</a></span><div class="content"><p>Shell中的 test 命令用于检查某个条件是否成立，它可以进行<code>数值</code>、<code>字符</code>和<code>文件</code>三个方面的测试。</p>
<h3 id="数值测试"><a href="#数值测试" class="headerlink" title="数值测试"></a>数值测试</h3><table>
<thead>
<tr>
<th align="left">参数</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">-eq</td>
<td align="left">等于则为真</td>
</tr>
<tr>
<td align="left">-ne</td>
<td align="left">不等于则为真</td>
</tr>
<tr>
<td align="left">-gt</td>
<td align="left">大于则为真</td>
</tr>
<tr>
<td align="left">-ge</td>
<td align="left">大于等于则为真</td>
</tr>
<tr>
<td align="left">-lt</td>
<td align="left">小于则为真</td>
</tr>
<tr>
<td align="left">-le</td>
<td align="left">小于等于则为真</td>
</tr>
</tbody></table>
<p>实例演示：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">num1=100</span><br><span class="line">num2=100</span><br><span class="line">if test $[num1] -eq $[num2]</span><br><span class="line">then</span><br><span class="line">    echo '两个数相等！'</span><br><span class="line">else</span><br><span class="line">    echo '两个数不相等！'</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">两个数相等！</span><br></pre></td></tr></table></figure>

<p>代码中的 [] 执行基本的算数运算，如：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">a=5</span><br><span class="line">b=6</span><br><span class="line"></span><br><span class="line">result=$[a+b] # 注意等号两边不能有空格</span><br><span class="line">echo "result 为： $result"</span><br></pre></td></tr></table></figure>

<p>结果为:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">result 为： 11</span><br></pre></td></tr></table></figure>

<h3 id="字符串测试"><a href="#字符串测试" class="headerlink" title="字符串测试"></a>字符串测试</h3><table>
<thead>
<tr>
<th align="left">参数</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">=</td>
<td align="left">等于则为真</td>
</tr>
<tr>
<td align="left">!=</td>
<td align="left">不相等则为真</td>
</tr>
<tr>
<td align="left">-z 字符串</td>
<td align="left">字符串的长度为零则为真</td>
</tr>
<tr>
<td align="left">-n 字符串</td>
<td align="left">字符串的长度不为零则为真</td>
</tr>
</tbody></table>
<p>实例演示：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">num1="ru1noob"</span><br><span class="line">num2="runoob"</span><br><span class="line">if test $num1 = $num2</span><br><span class="line">then</span><br><span class="line">    echo '两个字符串相等!'</span><br><span class="line">else</span><br><span class="line">    echo '两个字符串不相等!'</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">两个字符串不相等!</span><br></pre></td></tr></table></figure>

<h3 id="文件测试"><a href="#文件测试" class="headerlink" title="文件测试"></a>文件测试</h3><table>
<thead>
<tr>
<th align="left">参数</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">-e 文件名</td>
<td align="left">如果文件存在则为真</td>
</tr>
<tr>
<td align="left">-r 文件名</td>
<td align="left">如果文件存在且可读则为真</td>
</tr>
<tr>
<td align="left">-w 文件名</td>
<td align="left">如果文件存在且可写则为真</td>
</tr>
<tr>
<td align="left">-x 文件名</td>
<td align="left">如果文件存在且可执行则为真</td>
</tr>
<tr>
<td align="left">-s 文件名</td>
<td align="left">如果文件存在且至少有一个字符则为真</td>
</tr>
<tr>
<td align="left">-d 文件名</td>
<td align="left">如果文件存在且为目录则为真</td>
</tr>
<tr>
<td align="left">-f 文件名</td>
<td align="left">如果文件存在且为普通文件则为真</td>
</tr>
<tr>
<td align="left">-c 文件名</td>
<td align="left">如果文件存在且为字符型特殊文件则为真</td>
</tr>
<tr>
<td align="left">-b 文件名</td>
<td align="left">如果文件存在且为块特殊文件则为真</td>
</tr>
</tbody></table>
<p>实例演示：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /bin</span><br><span class="line">if test -e ./bash</span><br><span class="line">then</span><br><span class="line">    echo '文件已存在!'</span><br><span class="line">else</span><br><span class="line">    echo '文件不存在!'</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">文件已存在!</span><br></pre></td></tr></table></figure>

<p>另外，Shell还提供了与( -a )、或( -o )、非( ! )三个逻辑操作符用于将测试条件连接起来，其优先级为：”!”最高，”-a”次之，”-o”最低。例如：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /bin</span><br><span class="line">if test -e ./notFile -o -e ./bash</span><br><span class="line">then</span><br><span class="line">    echo '至少有一个文件存在!'</span><br><span class="line">else</span><br><span class="line">    echo '两个文件都不存在'</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">至少有一个文件存在!</span><br></pre></td></tr></table></figure></div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/11/">&lt;&lt;</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><span class="page-number current">12</span><a class="page-number" href="/page/13/">13</a><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/13/">&gt;&gt;</a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>