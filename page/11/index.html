<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="BigData Developer"><meta name="keywords" content="yerias,TUNANのBlog,BigData"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017 | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">129</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">30</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">26</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">TUNANのBlog</div><div id="site-sub-title">感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017</div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/10/PE/2/">HDFS Block损坏恢复</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-10</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E7%94%9F%E4%BA%A7%E6%95%85%E9%9A%9C%E6%A1%88%E4%BE%8B/">生产故障案例</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E7%94%9F%E4%BA%A7%E6%95%85%E9%9A%9C%E6%A1%88%E4%BE%8B/">生产故障案例</a></span><div class="content"><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">上传:</span><br><span class="line">-bash-4.2$ hdfs dfs -mkdir /blockrecover</span><br><span class="line">-bash-4.2$ echo &quot;www.ruozedata.com&quot; &gt; ruozedata.md</span><br><span class="line"></span><br><span class="line">-bash-4.2$ hdfs dfs -put ruozedata.md /blockrecover</span><br><span class="line">-bash-4.2$ hdfs dfs -ls /blockrecover</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 hdfs supergroup         18 2019-03-03 14:42 /blockrecover/ruozedata.md</span><br><span class="line">-bash-4.2$ </span><br><span class="line"></span><br><span class="line">校验: 健康状态</span><br><span class="line">-bash-4.2$ hdfs fsck /</span><br><span class="line">Connecting to namenode via http://yws76:50070/fsck?ugi=hdfs&amp;path=%2F</span><br><span class="line">FSCK started by hdfs (auth:SIMPLE) from /192.168.0.76 for path / at Sun Mar 03 14:44:44 CST 2019</span><br><span class="line">...............................................................................Status: HEALTHY</span><br><span class="line"> Total size:    50194618424 B</span><br><span class="line"> Total dirs:    354</span><br><span class="line"> Total files:   1079</span><br><span class="line"> Total symlinks:                0</span><br><span class="line"> Total blocks (validated):      992 (avg. block size 50599413 B)</span><br><span class="line"> Minimally replicated blocks:   992 (100.0 %)</span><br><span class="line"> Over-replicated blocks:        0 (0.0 %)</span><br><span class="line"> Under-replicated blocks:       0 (0.0 %)</span><br><span class="line"> Mis-replicated blocks:         0 (0.0 %)</span><br><span class="line"> Default replication factor:    3</span><br><span class="line"> Average block replication:     3.0</span><br><span class="line"> Corrupt blocks:                0</span><br><span class="line"> Missing replicas:              0 (0.0 %)</span><br><span class="line"> Number of data-nodes:          3</span><br><span class="line"> Number of racks:               1</span><br><span class="line">FSCK ended at Sun Mar 03 14:44:45 CST 2019 in 76 milliseconds</span><br><span class="line"></span><br><span class="line">The filesystem under path &apos;/&apos; is HEALTHY</span><br><span class="line">-bash-4.2$</span><br></pre></td></tr></table></figure>

<h3 id="直接DN节点上删除文件一个block的一个副本-3副本"><a href="#直接DN节点上删除文件一个block的一个副本-3副本" class="headerlink" title="直接DN节点上删除文件一个block的一个副本(3副本)"></a>直接DN节点上删除文件一个block的一个副本(3副本)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">删除块和meta文件:</span><br><span class="line">[root@yws87 subdir135]# rm -rf blk_1075808214 blk_1075808214_2068515.meta</span><br><span class="line"></span><br><span class="line">直接重启HDFS，直接模拟损坏效果，然后fsck检查:</span><br><span class="line">-bash-4.2$ hdfs fsck /</span><br><span class="line">Connecting to namenode via http://yws77:50070/fsck?ugi=hdfs&amp;path=%2F</span><br><span class="line">FSCK started by hdfs (auth:SIMPLE) from /192.168.0.76 for path / at Sun Mar 03 16:02:04 CST 2019</span><br><span class="line">.</span><br><span class="line">/blockrecover/ruozedata.md:  Under replicated BP-1513979236-192.168.0.76-1514982530341:blk_1075808214_2068515. Target Replicas is 3 but found 2 live replica(s), 0 decommissioned replica(s), 0 decommissioning replica(s).</span><br><span class="line">...............................................................................Status: HEALTHY</span><br><span class="line"> Total size:    50194618424 B</span><br><span class="line"> Total dirs:    354</span><br><span class="line"> Total files:   1079</span><br><span class="line"> Total symlinks:                0</span><br><span class="line"> Total blocks (validated):      992 (avg. block size 50599413 B)</span><br><span class="line"> Minimally replicated blocks:   992 (100.0 %)</span><br><span class="line"> Over-replicated blocks:        0 (0.0 %)</span><br><span class="line"> Under-replicated blocks:       1 (0.10080645 %)</span><br><span class="line"> Mis-replicated blocks:         0 (0.0 %)</span><br><span class="line"> Default replication factor:    3</span><br><span class="line"> Average block replication:     2.998992</span><br><span class="line"> Corrupt blocks:                0</span><br><span class="line"> Missing replicas:              1 (0.033602152 %)</span><br><span class="line"> Number of data-nodes:          3</span><br><span class="line"> Number of racks:               1</span><br><span class="line">FSCK ended at Sun Mar 03 16:02:04 CST 2019 in 148 milliseconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The filesystem under path &apos;/&apos; is HEALTHY</span><br><span class="line">-bash-4.2$</span><br></pre></td></tr></table></figure>

<h3 id="手动修复hdfs-debug"><a href="#手动修复hdfs-debug" class="headerlink" title="手动修复hdfs debug"></a>手动修复hdfs debug</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-bash-4.2$ hdfs |grep debug</span><br><span class="line">没有输出debug参数的任何信息结果！</span><br><span class="line">故hdfs命令帮助是没有debug的，但是确实有hdfs debug这个组合命令，切记。</span><br><span class="line"></span><br><span class="line">修复命令:</span><br><span class="line">-bash-4.2$ hdfs debug  recoverLease  -path /blockrecover/ruozedata.md -retries 10</span><br><span class="line">recoverLease SUCCEEDED on /blockrecover/ruozedata.md</span><br><span class="line">-bash-4.2$ </span><br><span class="line"></span><br><span class="line">直接DN节点查看，block文件和meta文件恢复:</span><br><span class="line">[root@yws87 subdir135]# ll</span><br><span class="line">total 8</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 56 Mar  3 14:28 blk_1075808202</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 14:28 blk_1075808202_2068503.meta</span><br><span class="line">[root@yws87 subdir135]# ll</span><br><span class="line">total 24</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 56 Mar  3 14:28 blk_1075808202</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 14:28 blk_1075808202_2068503.meta</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 18 Mar  3 15:23 blk_1075808214</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 15:23 blk_1075808214_2068515.meta</span><br></pre></td></tr></table></figure>

<h3 id="自动修复"><a href="#自动修复" class="headerlink" title="自动修复"></a>自动修复</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">当数据块损坏后，DN节点执行directoryscan操作之前，都不会发现损坏；</span><br><span class="line">也就是directoryscan操作是间隔6h</span><br><span class="line">dfs.datanode.directoryscan.interval : 21600</span><br><span class="line"></span><br><span class="line">在DN向NN进行blockreport前，都不会恢复数据块;</span><br><span class="line">也就是blockreport操作是间隔6h</span><br><span class="line">dfs.blockreport.intervalMsec : 21600000</span><br><span class="line"></span><br><span class="line">当NN收到blockreport才会进行恢复操作。</span><br></pre></td></tr></table></figure>

<p>具体参考生产上HDFS（CDH5.12.0）对应的版本的文档参数:<a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.12.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.12.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>生产上本人一般倾向于使用 手动修复方式，但是前提要手动删除损坏的block块。</p>
<p>切记，是删除损坏block文件和meta文件，而不是删除hdfs文件。</p>
<p>当然还可以先把文件get下载，然后hdfs删除，再对应上传。</p>
<p>切记删除不要执行: hdfs fsck / -delete 这是删除损坏的文件， 那么数据不就丢了嘛；除非无所谓丢数据，或者有信心从其他地方可以补数据到hdfs！</p>
<h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><ul>
<li>那么如何确定一个文件的损失的块位置，哪几种方法呢？</li>
<li>CDH的配置里搜索没有这两个参数，怎么调整生效呢？</li>
</ul>
<p>转载来源: [<a href="https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/]" target="_blank" rel="noopener">https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/]</a>(<a href="https://ruozedata.github.io/2019/06/06/生产HDFS" target="_blank" rel="noopener">https://ruozedata.github.io/2019/06/06/生产HDFS</a> Block损坏恢复最佳实践(含思考题)/)</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/10/hadoop/7/">MR的执行流程&amp;初探文件压缩&amp;初探文件格式&amp;分片数与任务数&amp;shuffle的执行流程&amp;WordCount的执行流程</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-10</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/MapReduce/">MapReduce</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/HDFS/">HDFS</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/MapReduce/">MapReduce</a></span><div class="content"><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol>
<li>整理 mr on yarn流程 </li>
<li>整理 文件格式有哪些 优缺点 </li>
<li>整理 压缩格式有哪些 优缺点 </li>
<li>spilt–&gt;map task关系 </li>
<li>wordcount的剖解图 </li>
<li>shuffle的理解 </li>
</ol>
<h3 id="mr-on-yarn流程"><a href="#mr-on-yarn流程" class="headerlink" title="mr on yarn流程"></a>mr on yarn流程</h3><p><img src="https://yerias.github.io/hadoop_img/rm_on_yarn.JPG" alt="rm_on_yarn"></p>
<h4 id="mr-on-yarn的工作流程简略分为两步"><a href="#mr-on-yarn的工作流程简略分为两步" class="headerlink" title="mr on yarn的工作流程简略分为两步:"></a>mr on yarn的工作流程简略分为两步:</h4><ol>
<li>启动应用程序管理器，申请资源。</li>
<li>运行任务，直到任务运行完成。</li>
</ol>
<h4 id="mr-on-yarn的工作流程详细分为八步"><a href="#mr-on-yarn的工作流程详细分为八步" class="headerlink" title="mr on yarn的工作流程详细分为八步:"></a>mr on yarn的工作流程详细分为八步:</h4><ol>
<li>用户向资源管理器(ResourceManager)提交作业，作业包括MapReduce应用程序管理器，启动MapReduce应用程序管理器的程序和用户自己编写的MapReduce程序。用于提交的所有作业都由ApplicationManager(全局应用程序管理器)管理。</li>
<li>资源管理器为该应用程序分配一个容器(Container)，并与对应的节点管理器(NodeManager)通信，要求它在这个容器中启动MapReduce应用程序管理器。</li>
<li>MapReduce应用程序管理器首先向资源管理器注册，这样用户可以直接通过资源管理器查看应用程序的运行状态，然后它将为各个任务申请资源，并监控他们的运行状态，直到运行结束，即重复步骤4-7。</li>
<li>MapReduce应用程序管理器采用轮询的方式通过RPC协议向资源管理器申请和领取资源。</li>
<li>MapReduce应用程序管理器申请到资源后，便与对应的节点管理器通信，要求启动任务。</li>
<li>节点管理器为任务设置好运行环境，包括环境变量、Jar包、二进制程序等，然后将任务启动命令写到另外一个脚本中，并通过该脚本启动任务。</li>
<li>各个任务通过RPC协议向MapReduce应用程序管理器汇报自己的状态和进度，MapReduce应用程序随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。在应用程序运行过程中，用户可以随时通过RPC协议向MapReduce应用程序管理器查询应用程序当前的运行状态。</li>
<li>应用程序运行完成后，MapReduce应用程序管理器向资源管理器注销并关闭自己。</li>
</ol>
<h3 id="文件格式有哪些-优缺点"><a href="#文件格式有哪些-优缺点" class="headerlink" title="文件格式有哪些 优缺点"></a>文件格式有哪些 优缺点</h3><p><strong>Hadoop中的文件格式大致上分为面向行和面向列两类：</strong></p>
<ol>
<li><p>面向行：同一行的数据存储在一起，即连续存储。SequenceFile,MapFile,Avro Datafile。采用这种方式，如果只需要访问行的一小部分数据，亦需要将整行读入内存，推迟序列化一定程度上可以缓解这个问题，但是从磁盘读取整行数据的开销却无法避免。面向行的存储适合于整行数据需要同时处理的情况。</p>
</li>
<li><p>面向列：整个文件被切割为若干列数据，每一列数据一起存储。Parquet , RCFile,ORCFile。面向列的格式使得读取数据时，可以跳过不需要的列，适合于只处于行的一小部分字段的情况。但是这种格式的读写需要更多的内存空间，因为需要缓存行在内存中（为了获取多行中的某一列）。同时不适合流式写入，因为一旦写入失败，当前文件无法恢复，而面向行的数据在写入失败时可以重新同步到最后一个同步点，所以Flume采用的是面向行的存储格式。</p>
</li>
</ol>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-0c5b3f7a2eceaaef.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="普通二维表"></p>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-dbe2595ee1e293b1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="行存储和列存储"></p>
<p>下面介绍几种相关的文件格式，它们在Hadoop体系上被广泛使用：</p>
<h4 id="SequenceFile"><a href="#SequenceFile" class="headerlink" title="SequenceFile"></a>SequenceFile</h4><p>SequenceFile是Hadoop API 提供的一种二进制文件,它将数据以的形式序列化到文件中。这种二进制文件内部使用Hadoop 的标准的Writable 接口实现序列化和反序列化。它与Hadoop API中的MapFile 是互相兼容的。Hive 中的SequenceFile 继承自Hadoop API 的SequenceFile,不过它的key为空,使用value 存放实际的值, 这样是为了避免MR 在运行map 阶段的排序过程。如果你用Java API 编写SequenceFile,并让Hive 读取的话,请确保使用value字段存放数据,否则你需要自定义读取这种SequenceFile 的InputFormat class 和OutputFormat class。</p>
<p>SequenceFile的文件结构如下：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-3f5cd8d90742ec24.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="SequenceFile详解图1"></p>
<p>根据是否压缩，以及采用记录压缩还是块压缩，存储格式有所不同：</p>
<ul>
<li><p>不压缩:</p>
<p>按照记录长度、Key长度、Value程度、Key值、Value值依次存储。长度是指字节数。采用指定的Serialization进行序列化。</p>
</li>
<li><p>Record压缩:</p>
<p>只有value被压缩，压缩的codec保存在Header中。</p>
</li>
<li><p>Block压缩:</p>
<p>多条记录被压缩在一起，可以利用记录之间的相似性，更节省空间。Block前后都加入了同步标识。Block的最小值由io.seqfile.compress.blocksize属性设置。 </p>
</li>
</ul>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-d21745547eb4c021.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="SequenceFile详解图1"></p>
<h4 id="Avro"><a href="#Avro" class="headerlink" title="Avro"></a>Avro</h4><p>Avro是一种用于支持数据密集型的二进制文件格式。它的文件格式更为紧凑,若要读取大量数据时,Avro能够提供更好的序列化和反序列化性能。并 且Avro数据文件天生是带Schema定义的,所以它不需要开发者在API 级别实现自己的Writable对象。最近多个Hadoop 子项目都支持Avro 数据格式,如Pig 、Hive、Flume、Sqoop和Hcatalog。</p>
<h4 id="RCFile"><a href="#RCFile" class="headerlink" title="RCFile"></a>RCFile</h4><p>RCFile是Hive推出的一种专门面向列的数据格式。 它遵循“先按列划分,再垂直划分”的设计理念。当查询过程中,针对它并不关心的列时,它会在IO上跳过这些列。需要说明的是,RCFile在map阶段从 远端拷贝仍然是拷贝整个数据块,并且拷贝到本地目录后RCFile并不是真正直接跳过不需要的列,并跳到需要读取的列, 而是通过扫描每一个row group的头部定义来实现的,但是在整个HDFS Block 级别的头部并没有定义每个列从哪个row group起始到哪个row group结束。所以在读取所有列的情况下,RCFile的性能反而没有SequenceFile高。</p>
<p>Hive的Record Columnar File,这种类型的文件先将数据按行划分成Row Group，在Row Group内部，再将数据按列划分存储。其结构如下：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-0a6f19b8bb6ee4e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/688/format/webp" alt="RCFile详解图1"></p>
<p>相比较于单纯地面向行和面向列：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-0df474935c56807d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/649/format/webp" alt="RCFile详解图2"></p>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-6d56c39e3445288e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/615/format/webp" alt="RCFile详解图3"></p>
<h4 id="ORC"><a href="#ORC" class="headerlink" title="ORC"></a>ORC</h4><p>ORC（Optimized Record Columnar File)提供了一种比RCFile更加高效的文件格式。其内部将数据划分为默认大小为250M的Stripe。每个Stripe包括索引、数据和Footer。索引存储每一列的最大最小值，以及列中每一行的位置。</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-1bb66728d866b469.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/580/format/webp" alt="ORCFile详解图"></p>
<p>在Hive中，如下命令用于使用ORCFile：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE ... STORED AS ORC</span><br><span class="line"></span><br><span class="line">ALTER TABLE ... SET FILEFORMAT ORC</span><br><span class="line"></span><br><span class="line">SET hive.default.fileformat=ORC</span><br></pre></td></tr></table></figure>

<h4 id="Parquet"><a href="#Parquet" class="headerlink" title="Parquet"></a>Parquet</h4><p>一种通用的面向列的存储格式，基于Google的Dremel。特别擅长处理深度嵌套的数据。</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-b45e32049ab54cbb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1124/format/webp" alt="img"></p>
<p>对于嵌套结构，Parquet将其转换为平面的列存储，嵌套结构通过Repeat Level和Definition Level来表示（R和D），在读取数据重构整条记录的时候，使用元数据重构记录的结构。下面是R和D的一个例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">AddressBook &#123;</span><br><span class="line"> contacts: &#123;</span><br><span class="line">  phoneNumber: &quot;555 987 6543&quot;</span><br><span class="line"> &#125;</span><br><span class="line"> contacts: &#123;</span><br><span class="line">&#125;&#125;</span><br><span class="line">AddressBook &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-36b68fc1d8e2b99b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/577/format/webp" alt="Parquet详解图"></p>
<h4 id="文件存储大小比较与分析"><a href="#文件存储大小比较与分析" class="headerlink" title="文件存储大小比较与分析"></a>文件存储大小比较与分析</h4><p>我们选取一个TPC-H标准测试来说明不同的文件格式在存储上的开销。因为此数据是公开的,所以读者如果对此结果感兴趣,也可以对照后面的实验自行 做一遍。Orders 表文本格式的原始大小为1.62G。 我们将其装载进Hadoop 并使用Hive 将其转化成以上几种格式,在同一种LZO 压缩模式下测试形成的文件的大小</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-34e05b3cb0e72740.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/849/format/webp" alt="文件存储大小比较与分析"></p>
<h4 id="不同格式文件大小对比"><a href="#不同格式文件大小对比" class="headerlink" title="不同格式文件大小对比"></a>不同格式文件大小对比</h4><ul>
<li><p>从上述实验结果可以看到,SequenceFile无论在压缩和非压缩的情况下都比原始纯文本TextFile大,其中非压缩模式下大11%, 压缩模式下大6.4%。这跟SequenceFile的文件格式的定义有关: SequenceFile在文件头中定义了其元数据,元数据的大小会根据压缩模式的不同略有不同。一般情况下,压缩都是选取block 级别进行的,每一个block都包含key的长度和value的长度,另外每4K字节会有一个sync-marker的标记。对于TextFile文件格 式来说不同列之间只需要用一个行间隔符来切分,所以TextFile文件格式比SequenceFile文件格式要小。但是TextFile 文件格式不定义列的长度,所以它必须逐个字符判断每个字符是不是分隔符和行结束符。因此TextFile 的反序列化开销会比其他二进制的文件格式高几十倍以上。</p>
</li>
<li><p>RCFile文件格式同样也会保存每个列的每个字段的长度。但是它是连续储存在头部元数据块中,它储存实际数据值也是连续的。另外RCFile 会每隔一定块大小重写一次头部的元数据块(称为row group,由hive.io.rcfile.record.buffer.size控制,其默认大小为4M),这种做法对于新出现的列是必须的,但是如 果是重复的列则不需要。RCFile 本来应该会比SequenceFile 文件大,但是RCFile 在定义头部时对于字段长度使用了Run Length Encoding进行压缩,所以RCFile 比SequenceFile又小一些。Run length Encoding针对固定长度的数据格式有非常高的压缩效率,比如Integer、Double和Long等占固定长度的数据类型。在此提一个特例—— Hive 0.8引入的TimeStamp 时间类型,如果其格式不包括毫秒,可表示为”YYYY-MM-DD HH:MM:SS”,那么就是固定长度占8个字节。如果带毫秒,则表示为”YYYY-MM-DD HH:MM:SS.fffffffff”,后面毫秒的部分则是可变的。</p>
</li>
<li><p>Avro文件格式也按group进行划分。但是它会在头部定义整个数据的模式(Schema), 而不像RCFile那样每隔一个row group就定义列的类型,并且重复多次。另外,Avro在使用部分类型的时候会使用更小的数据类型,比如Short或者Byte类型,所以Avro的数 据块比RCFile 的文件格式块更小。</p>
</li>
</ul>
<h3 id="压缩格式有哪些-优缺点"><a href="#压缩格式有哪些-优缺点" class="headerlink" title="压缩格式有哪些 优缺点"></a>压缩格式有哪些 优缺点</h3><h4 id="压缩的好处和坏处"><a href="#压缩的好处和坏处" class="headerlink" title="压缩的好处和坏处"></a>压缩的好处和坏处</h4><p><strong>好处</strong></p>
<ul>
<li>减少存储磁盘空间</li>
<li>降低IO(网络的IO和磁盘的IO)</li>
<li>加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度</li>
</ul>
<p><strong>坏处</strong></p>
<ul>
<li>由于使用数据时，需要先将数据解压，加重CPU负荷</li>
</ul>
<h4 id="压缩格式"><a href="#压缩格式" class="headerlink" title="压缩格式"></a>压缩格式</h4><p><img src="https://ruozedata.github.io/assets/blogImg/%E5%8E%8B%E7%BC%A91.png" alt="压缩格式"></p>
<p><img src="https://ruozedata.github.io/assets/blogImg/%E5%8E%8B%E7%BC%A92.png" alt="压缩空间比较"></p>
<p><img src="https://ruozedata.github.io/assets/blogImg/yasuo3.png" alt="压缩时间比较"></p>
<p>可以看出，压缩空间比值越高，压缩时间越长，压缩比：<code>Snappy&gt;LZ4&gt;LZO&gt;GZIP&gt;BZIP2</code></p>
<table>
<thead>
<tr>
<th align="left">压缩格式</th>
<th align="left">优点</th>
<th align="left">缺点</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>gzip</strong></td>
<td align="left">压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便</td>
<td align="left">不支持split</td>
<td></td>
</tr>
<tr>
<td align="left"><strong>lzo</strong></td>
<td align="left">压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便</td>
<td align="left">压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）</td>
<td></td>
</tr>
<tr>
<td align="left"><strong>snappy</strong></td>
<td align="left">压缩速度快；支持hadoop native库</td>
<td align="left">不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令d. bzip2</td>
<td></td>
</tr>
<tr>
<td align="left"><strong>bzip2</strong></td>
<td align="left">支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便</td>
<td align="left">压缩/解压速度慢；不支持native</td>
<td></td>
</tr>
</tbody></table>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>不同的场景选择不同的压缩方式，肯定没有一个一劳永逸的方法，如果选择高压缩比，那么对于cpu的性能要求要高，同时压缩、解压时间耗费也多；选择压缩比低的，对于磁盘io、网络io的时间要多，空间占据要多；对于支持分割的，可以实现并行处理。</p>
<h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><p>一般在HDFS 、Hive、HBase中会使用；<br>当然一般较多的是结合Spark 来一起使用。</p>
<h3 id="Spilt–-gt-Map-Task关系"><a href="#Spilt–-gt-Map-Task关系" class="headerlink" title="Spilt–&gt;Map Task关系"></a>Spilt–&gt;Map Task关系</h3><p>Reduce Task默认是1个，Map Task默认是2个，但是实际运行场景下，Map Task的个数和切片的个数保持一致，而切片的个数又与文件数和文件大小相关联。切片默认大小决定文件被分成多少个切片，执行多少个Map Task。</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.job.maps</td>
<td>2</td>
<td>The default number of map tasks per job.</td>
</tr>
<tr>
<td>mapreduce.job.reduces</td>
<td>1</td>
<td>The default number of reduce tasks per job.</td>
</tr>
</tbody></table>
<h3 id="shuffle的理解"><a href="#shuffle的理解" class="headerlink" title="shuffle的理解"></a>shuffle的理解</h3><p>俩字: 洗牌</p>
<p>shuffle阶段又可以分为Map端的shuff和reduce端的shuffle</p>
<p><img src="https:/yerias.github.io/hadoop_img/shuffer.jpg" alt="shuffe过程"></p>
<h4 id="map端的shuffle"><a href="#map端的shuffle" class="headerlink" title="map端的shuffle"></a>map端的shuffle</h4><ul>
<li>map端会处理出入数据并产生中间结果，这个中间结果会写到本地磁盘，而不是HDFS。每个map的输出会先写到内存缓冲区中，当写入的数据达到设定的阈值时，系统将会启动一个线程将缓冲区的数据写到磁盘，这个过程叫做spill(溢写)。</li>
<li>在spill之前，会先进行两次排序，首先根据数据所属的partition进行排序，然后每个partition中的数据再按key来排序，partition的目的是将记录划分到不同的reduce上，以期望能达到负载均衡，以后的reduce就会根据partition来读取自己对应的数据。接着运行combiner(如果设置了的话)，combiner的本质也是一个reduce，其目的是对将要写入到磁盘的文件先进行一次处理，这样，写入到磁盘的数据量就会减少。最后将数据写到本地磁盘产生spill文件(spill文件保存在{mapred.local.dir}指定的目录中，map任务结束后就会被删除)。</li>
<li>最后，每个map任务可能产生多个spill文件，在每个map任务完成前，会通过多路归并算法将这些spill文件合并成一个文件。至此，map的shuffle过程就结束了。</li>
</ul>
<h4 id="reduce端的shuffle"><a href="#reduce端的shuffle" class="headerlink" title="reduce端的shuffle"></a>reduce端的shuffle</h4><ul>
<li>reduce端的shuffle主要包括三个阶段，copy、sort(merge)和reduce</li>
<li>首先将map端产生的输出文件拷贝到reduce端，但每个reduce如何知道自己应该处理哪些数据呢？因为map端进行partition的时候，实际上就相当于指定了每个reduce要处理的数据(partition就对应了reduce)，所以reduce在拷贝的数据的时候只需拷贝与自己对应的partition中的数据即可。每个reduce会处理一个或多个partiton，但需要先将自己对应的partition中的数据从每个map的输出结果中拷贝出来。</li>
<li>接下来就是sort阶段，也称为merge阶段，因为这个阶段的主要工作是执行了归并排序。从map端拷贝到reduce端的数据都是有序的，所以很适合归并排序。最终在reduce端生产一个较大的文件作为reduce的输入。</li>
<li>最后就是reduce阶段了，在这个过程中产生最终的输出结果，并将其写到HDFS上。</li>
</ul>
<h3 id="WordCount的剖解图"><a href="#WordCount的剖解图" class="headerlink" title="WordCount的剖解图"></a>WordCount的剖解图</h3><p><img src="https:/yerias.github.io/hadoop_img/wordcount.jpg" alt="wordcount执行流程"></p>
<h4 id="Map任务处理"><a href="#Map任务处理" class="headerlink" title="Map任务处理"></a>Map任务处理</h4><ol>
<li>读取HDFS中的文件，每一行解析成一个&lt;K,V&gt;值。每个键值对调用一次map函数。</li>
<li>重写map()方法，接收1产生的&lt;K,V&gt;值进行处理，转为新的&lt;K,V&gt;输出。</li>
<li>对2输出的&lt;K,V&gt;值进行分区，默认一盒分区。</li>
<li>对不同分区中的数据进行排序(按照K)、分组。分组指的是相同Key的Value放到一个集合中。</li>
<li>(可选)对分组后的数据进行合并。</li>
</ol>
<h4 id="Reduce任务处理"><a href="#Reduce任务处理" class="headerlink" title="Reduce任务处理"></a>Reduce任务处理</h4><ol>
<li>多个Map任务的输出，按照不同的分区，通过网络copy到不同的Reduce节点上。</li>
<li>对多个map的输出进行合并、排序。重写reduce()方法，接收的是分组后的数据，实现自己的业务逻辑，处理后产生新的&lt;K,V&gt;值输出</li>
<li>对reduce输出的&lt;K,V&gt;写到HDFS中。</li>
</ol>
<hr>
<p>整理来自：<a href="https://ruozedata.github.io/2018/04/18/、https://www.jianshu.com/p/43630456a18a" target="_blank" rel="noopener">https://ruozedata.github.io/2018/04/18/、https://www.jianshu.com/p/43630456a18a</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/09/PE/1/">DataNode OOM溢出</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-09</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E7%94%9F%E4%BA%A7%E6%95%85%E9%9A%9C%E6%A1%88%E4%BE%8B/">生产故障案例</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E7%94%9F%E4%BA%A7%E6%95%85%E9%9A%9C%E6%A1%88%E4%BE%8B/">生产故障案例</a></span><div class="content"><h3 id="DataNode的内存溢出报错"><a href="#DataNode的内存溢出报错" class="headerlink" title="DataNode的内存溢出报错"></a>DataNode的内存溢出报错</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2017-12-17 23:58:14,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1437036909-![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.36-1509097205664:blk_1074725940_987917, type=HAS_DOWNSTREAM_IN_PIPELINE terminating</span><br><span class="line">2017-12-17 23:58:31,425 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode is out of memory. Will retry in 30 seconds.</span><br><span class="line">java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">	at java.lang.Thread.start0(Native Method)</span><br><span class="line">	at java.lang.Thread.start(Thread.java:714)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:154)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">2017-12-17 23:59:01,426 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode is out of memory. Will retry in 30 seconds.</span><br><span class="line">java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">	at java.lang.Thread.start0(Native Method)</span><br><span class="line">	at java.lang.Thread.start(Thread.java:714)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:154)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">2017-12-17 23:59:05,520 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode is out of memory. Will retry in 30 seconds.</span><br><span class="line">java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">	at java.lang.Thread.start0(Native Method)</span><br><span class="line">	at java.lang.Thread.start(Thread.java:714)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:154)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">2017-12-17 23:59:31,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1437036909-![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.36-1509097205664:blk_1074725951_987928 src: /![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.54:40478 dest: /![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.48:50010</span><br></pre></td></tr></table></figure>

<h3 id="CDH查看DataNode的内存情况"><a href="#CDH查看DataNode的内存情况" class="headerlink" title="CDH查看DataNode的内存情况"></a>CDH查看DataNode的内存情况</h3><p><img src="https://yerias.github.io/hadoop_img/20191205092342.jpg" alt="datanode内存使用图"></p>
<h3 id="明明1G的内存都没有使用，为什么会报OOM？"><a href="#明明1G的内存都没有使用，为什么会报OOM？" class="headerlink" title="明明1G的内存都没有使用，为什么会报OOM？"></a>明明1G的内存都没有使用，为什么会报OOM？</h3><p>可以确定是操作系统哪里设置错了，我想应该是把产品环境的某个参数配置错了，系统本身的影响肯定不会有了，因为产品环境上我们只create了800左右个线程，就OOM了，那应该就是配置的问题了</p>
<p>解决方法：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1.</span><br><span class="line"></span><br><span class="line">echo "kernel.threads-max=196605" &gt;&gt; /etc/sysctl.conf</span><br><span class="line">echo "kernel.pid_max=196605" &gt;&gt; /etc/sysctl.conf</span><br><span class="line">echo "vm.max_map_count=393210" &gt;&gt; /etc/sysctl.conf</span><br><span class="line">sysctl -p </span><br><span class="line"></span><br><span class="line">2.</span><br><span class="line"></span><br><span class="line">/etc/security/limits.conf</span><br><span class="line">* soft nofile 196605</span><br><span class="line">* hard nofile 196605</span><br><span class="line">* soft nproc 196605</span><br><span class="line">* hard nproc 196605</span><br></pre></td></tr></table></figure>

</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/09/hadoop/6/">Hadoop的Pid文件</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-09</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/HDFS/">HDFS</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/HDFS/">HDFS</a></span><div class="content"><h3 id="存储位置"><a href="#存储位置" class="headerlink" title="存储位置"></a>存储位置</h3><p>hadoop启动之后，pid文件是存储哪里？<br>我们可以通过查看 hadoop-env.sh文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat etc/hadoop/hadoop-env.sh从下图可以看出hadoop默认的pid文件是存储到/tmp目录的</span><br></pre></td></tr></table></figure>

<p>从下图可以看出hadoop默认的pid文件是存储到/tmp目录的</p>
<p>从下图可以看出hadoop默认的pid文件是存储到/tmp目录的</p>
<p><img src="https://img-blog.csdnimg.cn/20190729214437420.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="hadoop-env.sh文件"></p>
<p>从下图可以看出，后缀名是.pid的就是hadoop的pid文件</p>
<p><img src="https://img-blog.csdnimg.cn/20190729214915879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="/tmp目录"></p>
<h3 id="启动和停止"><a href="#启动和停止" class="headerlink" title="启动和停止"></a>启动和停止</h3><p>我们启动的时候，是执行sbin/start-df.sh文件，我们看一看这个文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20190729215631410.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="start-df.sh文件"></p>
<p>从上面这个图可以看出，启动namenode节点的时候，调用了hadoop-daemons.sh文件了，我们再看看这个文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat sbin/hadoop-daemons.sh</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20190729215911670.png" alt="hadoop-daemons.sh文件"></p>
<p>从上图可以看出，在最后一行又调用了hadoop-daemon.sh文件，我们在看看这个文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat sbin/hadoop-daemon.sh</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20190729220412390.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="hadoop-daemon.sh文件"></p>
<p><strong>从上面两张图可以得出结论:</strong></p>
<ol>
<li>hadoop启动的时候，会生成pid文件，并把进程号写入到pid文件</li>
<li>hadoop停止的时候，会到pid文件中获取进程号，然后停止进程，最后删除pid文件</li>
</ol>
<p><strong>下面我们做一下验证：</strong></p>
<ol>
<li><p>看下namenode的进程号是不是和pid文件里的进程号一样</p>
<p><img src="https://img-blog.csdnimg.cn/20190729221743612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="start-jps命令"></p>
<p>从上图可以看出，进程号是一样的，说明我们前面的推理是正确的</p>
</li>
<li><p>我们把生成号的namenode的pid文件名字改一下，停止的时候脚本会找不到pid文件，也就不会停止namenode进程了</p>
<p><img src="https://img-blog.csdnimg.cn/20190729222300551.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="stop-jps命令"></p>
<p>从上图可以看出，我们的两个推理是正确的</p>
</li>
<li><p>tmp目录的弊端<br>linux的/tmp目录会自动清理一段时间没有访问的文件，一般都是30天，假如hadoop启动了30天以上，那么pid文件会被删除，再调用停止的时候会停止不了，生产上一般不会放在/tmp目录下，下面我们自己创建个目录存放pid文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">创建文件夹</span></span><br><span class="line">mkdir -p /data/tmp</span><br><span class="line"><span class="meta">#</span><span class="bash">赋予权限</span></span><br><span class="line">chmod 777 -R /data/tmp</span><br><span class="line">然后修改etc/hadoop/hadoop-env.sh文件</span><br></pre></td></tr></table></figure>

<p>然后修改etc/hadoop/hadoop-env.sh文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vi /etc/hadoop/hadoop-env.sh</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20190730105216506.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="修改hadoop-env.sh文件的pid目录"></p>
<p>然后启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<p>启动之后，我们查看pid文件</p>
<p><img src="https://img-blog.csdnimg.cn/20190730105343192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="新的pid目录"></p>
</li>
</ol>
<hr>
<p>原文链接：<a href="https://blog.csdn.net/u010452388/article/details/97686380" target="_blank" rel="noopener">https://blog.csdn.net/u010452388/article/details/97686380</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/08/hadoop/5/">Hadoop2.7.6之前和Hadoop2.8.4之后的副本存放策略</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-08</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/HDFS/">HDFS</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/HDFS/">HDFS</a></span><div class="content"><h3 id="新旧版本的副本存放策略比较"><a href="#新旧版本的副本存放策略比较" class="headerlink" title="新旧版本的副本存放策略比较"></a>新旧版本的副本存放策略比较</h3><p>Hadoop2.7.6及以下版本是按照旧的策略进行副本存放的，官网文档描述如下：</p>
<p><img src="https://img-blog.csdnimg.cn/20191017161508850.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FjY3B0YW5nZ2FuZw==,size_16,color_FFFFFF,t_70" alt="旧版本的副本存放策略"></p>
<p>在常见情况下，当复制因子为3时，HDFS的放置策略是将一个副本放置在本地机架中的一个节点上，将另一个副本放置在本地机架中的另一个节点上，最后一个副本放置在不同机架中的另一个节点上。</p>
<p>Hadoop2.8.4及以上版本是按照新的策略进行副本存放的，官网文档描述如下：  </p>
<p><img src="https://img-blog.csdnimg.cn/20191017161742230.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FjY3B0YW5nZ2FuZw==,size_16,color_FFFFFF,t_70" alt="新版本的副本存放策略"></p>
<p>在常见情况下，当复制因子为3时，HDFS的放置策略是：如果写入器在数据节点上，则将一个副本放置在本地计算机上；否则，在随机数据节点上，HDFS将另一个副本放置在不同（远程）机架中的节点上的，最后一个位于同一远程机架中的其他节点上。</p>
<h3 id="新版本的副本存放策略思想"><a href="#新版本的副本存放策略思想" class="headerlink" title="新版本的副本存放策略思想"></a>新版本的副本存放策略思想</h3><p>最后，再把新版本的副本存放策略的基本思想描述如下：</p>
<p>第一个副本存放Client所在的节点上（假设Client不在集群的范围内，则第一个副本存储节点是随机选取的。当然系统会不选择那些太满或者太忙的节点）</p>
<p>第二个副本存放在与第一个节点不同机架中的一个节点上。</p>
<p>第三个副本和第二个在同一个机架，随机放在不同的节点上。</p>
<p>如果还有很多其他的副本就随机放在集群中的各个节点上。</p>
<hr>
<p>原文链接：<a href="https://blog.csdn.net/accptanggang/article/details/102609318" target="_blank" rel="noopener">https://blog.csdn.net/accptanggang/article/details/102609318</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/08/mysql/4/">MySQL中的Top N</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-08</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/MySQL/">MySQL</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/DataWarehouse/">DataWarehouse</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/MySQL/">MySQL</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Rank/">Rank</a></span><div class="content"><h3 id="切入点"><a href="#切入点" class="headerlink" title="切入点"></a>切入点</h3><p>MySQL没有获取Top N的这种函数，但是在MySQL中求Top N又是必须掌握的点</p>
<p>比如查询分组后的最大值、最小值所在的整行记录或者分组后的Top N行记录</p>
<p>下面我们就如何在MySQL中求Top N做出深度的思考和验证</p>
<h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>测试表结构如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">&gt;&gt; CREATE TABLE `student` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `name` varchar(20) DEFAULT NULL,</span><br><span class="line">  `course` varchar(20) DEFAULT NULL,</span><br><span class="line">  `score` int(11) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8</span><br></pre></td></tr></table></figure>

<p> 插入数据：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">&gt;&gt; insert into student(name,course,score)</span><br><span class="line"><span class="keyword">values</span></span><br><span class="line">(<span class="string">'张三'</span>,<span class="string">'语文'</span>,<span class="number">80</span>),</span><br><span class="line">(<span class="string">'李四'</span>,<span class="string">'语文'</span>,<span class="number">90</span>),</span><br><span class="line">(<span class="string">'王五'</span>,<span class="string">'语文'</span>,<span class="number">93</span>),</span><br><span class="line">(<span class="string">'张三'</span>,<span class="string">'数学'</span>,<span class="number">77</span>),</span><br><span class="line">(<span class="string">'李四'</span>,<span class="string">'数学'</span>,<span class="number">68</span>),</span><br><span class="line">(<span class="string">'王五'</span>,<span class="string">'数学'</span>,<span class="number">99</span>),</span><br><span class="line">(<span class="string">'张三'</span>,<span class="string">'英语'</span>,<span class="number">90</span>),</span><br><span class="line">(<span class="string">'李四'</span>,<span class="string">'英语'</span>,<span class="number">50</span>),</span><br><span class="line">(<span class="string">'王五'</span>,<span class="string">'英语'</span>,<span class="number">89</span>);</span><br></pre></td></tr></table></figure>

<p>查看结果：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">&gt;&gt; select * from student;</span><br><span class="line">+<span class="comment">----+--------+--------+-------+</span></span><br><span class="line">| id | name   | course | score |</span><br><span class="line">+<span class="comment">----+--------+--------+-------+</span></span><br><span class="line">|  1 | 张三   | 语文   |    80 |</span><br><span class="line">|  2 | 李四   | 语文   |    90 |</span><br><span class="line">|  3 | 王五   | 语文   |    93 |</span><br><span class="line">|  4 | 张三   | 数学   |    77 |</span><br><span class="line">|  5 | 李四   | 数学   |    68 |</span><br><span class="line">|  6 | 王五   | 数学   |    99 |</span><br><span class="line">|  7 | 张三   | 英语   |    90 |</span><br><span class="line">|  8 | 李四   | 英语   |    50 |</span><br><span class="line">|  9 | 王五   | 英语   |    89 |</span><br><span class="line">+<span class="comment">----+--------+--------+-------+</span></span><br></pre></td></tr></table></figure>

<h3 id="TOP-1"><a href="#TOP-1" class="headerlink" title="TOP 1"></a>TOP 1</h3><p>查询每门课程分数最高的学生以及成绩</p>
<ol>
<li><p>我们先拆分题目，这是一题查询分组求最大值的题目，拆分后的题目是：查询 每门课程 分数最高 的学生以及成绩</p>
<p>我们首先按照常规思路来写SQL: </p>
<p>select 学生姓名，学生分数</p>
<p>group by 课程</p>
<p>max(分数) </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	s.name,s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">	s.course;</span><br></pre></td></tr></table></figure>

<p>得出的查询结果是:</p>
<table>
<thead>
<tr>
<th>name</th>
<th>course</th>
<th>score</th>
</tr>
</thead>
<tbody><tr>
<td>张三</td>
<td>数学</td>
<td>99</td>
</tr>
<tr>
<td>张三</td>
<td>英语</td>
<td>90</td>
</tr>
<tr>
<td>张三</td>
<td>语文</td>
<td>93</td>
</tr>
</tbody></table>
<h4 id="问题-为什么姓名都是张三？课程和对应的成绩又全是对的？"><a href="#问题-为什么姓名都是张三？课程和对应的成绩又全是对的？" class="headerlink" title="问题: 为什么姓名都是张三？课程和对应的成绩又全是对的？"></a>问题: 为什么姓名都是张三？课程和对应的成绩又全是对的？</h4><p>我预测是因为没有把姓名加入group的分组字段，那么我们把姓名加入group的分组字段后试试看</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	s.name,s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">	s.name,s.course;</span><br></pre></td></tr></table></figure>

<p>得出的查询结果是:</p>
<table>
<thead>
<tr>
<th>name</th>
<th>course</th>
<th>score</th>
</tr>
</thead>
<tbody><tr>
<td>张三</td>
<td>数学</td>
<td>77</td>
</tr>
<tr>
<td>张三</td>
<td>英语</td>
<td>90</td>
</tr>
<tr>
<td>张三</td>
<td>语文</td>
<td>80</td>
</tr>
<tr>
<td>李四</td>
<td>数学</td>
<td>68</td>
</tr>
<tr>
<td>李四</td>
<td>英语</td>
<td>50</td>
</tr>
<tr>
<td>李四</td>
<td>语文</td>
<td>90</td>
</tr>
<tr>
<td>王五</td>
<td>数学</td>
<td>99</td>
</tr>
<tr>
<td>王五</td>
<td>英语</td>
<td>89</td>
</tr>
<tr>
<td>王五</td>
<td>语文</td>
<td>93</td>
</tr>
</tbody></table>
<p>结果还是不对，这次把所有的字段都查询出来了，字段的排序规则是先按姓名分组，再按课程分组，因为课程是唯一的，所以跟直接查询的结果一样。</p>
</li>
<li><p>我们回到上一步，上一步的课程和成绩对应上了，姓名没有对应上，我们干脆就不要姓名和，拿课程和成绩作为一张表再和自己联结一次，以课程和成绩作为过滤字段，说不定就能得到想要的姓名字段。</p>
<p>思路：</p>
<ol>
<li><p>先课程分组求出最高的分数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">	s.course;</span><br></pre></td></tr></table></figure>
</li>
<li><p>把前面得出的结果作为表t再自联结一次</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	s.name,s.score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	student s</span><br><span class="line"><span class="keyword">join</span> t </span><br><span class="line"><span class="keyword">on</span> s.course=t.course <span class="keyword">and</span> s.score=t.score;</span><br></pre></td></tr></table></figure>
</li>
<li><p>把t替换成查询出来的结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	s.name,s.course,s.score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	student s</span><br><span class="line"><span class="keyword">join</span> (<span class="keyword">select</span> </span><br><span class="line">	s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">	s.course) t </span><br><span class="line"><span class="keyword">on</span> s.course=t.course <span class="keyword">and</span> s.score=t.score;</span><br></pre></td></tr></table></figure>

<p>得出的查询结果是:</p>
<table>
<thead>
<tr>
<th>name</th>
<th>course</th>
<th>score</th>
</tr>
</thead>
<tbody><tr>
<td>王五</td>
<td>语文</td>
<td>93</td>
</tr>
<tr>
<td>王五</td>
<td>数学</td>
<td>99</td>
</tr>
<tr>
<td>张三</td>
<td>英语</td>
<td>90</td>
</tr>
</tbody></table>
<p>和原数据比较，这就是我们要得到的每门课程的top1。</p>
</li>
</ol>
</li>
</ol>
<h3 id="TOP-N"><a href="#TOP-N" class="headerlink" title="TOP N"></a>TOP N</h3><p>查询每门课程前两名的学生以及成绩</p>
<p>首先求Top 1的方法不适用与Top N，然后毫无头绪。。。</p>
<p>翻看其他人的博客后，发现求Top N的核心是: <code>自联结表的需求字段比较，也就是自己跟自己比较，然后把比较的结果求count()，最后控制过滤的记录数即可</code></p>
<h4 id="注意-自己和自己比较，可以通过联结和子查询，我们这里使用子查询"><a href="#注意-自己和自己比较，可以通过联结和子查询，我们这里使用子查询" class="headerlink" title="注意:  自己和自己比较，可以通过联结和子查询，我们这里使用子查询"></a>注意:  自己和自己比较，可以通过联结和子查询，我们这里使用子查询</h4><p>思路:</p>
<ol>
<li><p>首先是子查询，然后是自己和自己比较，得出一个count()值，最后使用where过滤这个count值</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	a.name,a.course,a.score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	student a</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">	<span class="number">2</span>&gt;(<span class="keyword">select</span> <span class="keyword">count</span>(b.score) <span class="keyword">from</span> student b <span class="keyword">where</span> a.course=b.course <span class="keyword">and</span> a.score&lt;b.score)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> a.course <span class="keyword">desc</span>,a.score <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>

<p>梳理这段SQL，select字段不难，from字段不难，order by字段不难，难就难在where字段，我们先不看为什么使用2大于这个子查询，先把注意力放在子查询的where字段中的两个表成绩比较，实际上理解了为什么这么比较这题就解出来了。</p>
</li>
<li><p>我们画图来解释。。。只是做为示范，所以只取一个课程的成绩比较，其他的一样</p>
<p><img src="https://yerias.github.io/hadoop_img/20191205012529.jpg" alt="a.score&lt;b.score比较图"></p>
<p>可以看出，表a中的成绩越大，满足<code>a.score&lt;b.score</code>的次数越少，<code>where条件过滤count()的值越少越满足Top N的条件，可以根据where条件灵活控制过滤的记录数,我们这里是2，即取Top 2的记录。</code></p>
<h4 id="再提出一个问题，为什么要用a-score-lt-b-score而不是a-score-gt-b-score？"><a href="#再提出一个问题，为什么要用a-score-lt-b-score而不是a-score-gt-b-score？" class="headerlink" title="再提出一个问题，为什么要用a.score&lt;b.score而不是a.score&gt;b.score？"></a>再提出一个问题，为什么要用<code>a.score&lt;b.score</code>而不是<code>a.score&gt;b.score</code>？</h4><p>通过结果可以倒推出来，我们<code>select</code>语句中要的是表a，根据题意表a必定是比较中较大的值。如果使用<code>a.score&gt;b.score</code>，where条件限制的是满足最少的条件，把表a中最大的值给过滤了，那么得出的的count()结果是反的。</p>
</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/08/mysql/5/">Linux下MySQL进程死掉的可能解决方案</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-08</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/MySQL/">MySQL</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/MySQL/">MySQL</a></span><div class="content"><p>linux下mysql进程死掉，且无法启动mysql服务，查看myql日志，发现如下日志：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">2019-10-10 18:11:03 9772 [Note] InnoDB: Initializing buffer pool, size = 128.0M</span><br><span class="line">InnoDB: mmap(136019968 bytes) failed; errno 12</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] InnoDB: Cannot allocate memory for the buffer pool</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Plugin 'InnoDB' init function returned error.</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Unknown/unsupported storage engine: InnoDB</span><br><span class="line">2019-10-10 18:11:03 9772 [ERROR] Aborting</span><br></pre></td></tr></table></figure>

<p>其中InnoDB: mmap(136019968 bytes) failed; errno 12是关键的错误信息。<br>从网上查资料，有人说修改innodb_buffer_pool_size，经过测试无效。<br>有人说是swap分区为0导致的此错误，使用free -m命令查看系统内存，发现swap确实为0。使用如下命令建立一个临时的swap分区：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">d if=/dev/zero of=/swap bs=1M count=512  //创建一个swap文件，大小为512M</span><br><span class="line">mkswap /swap                              //将swap文件变为swap分区文件</span><br><span class="line">swapon /swap                              //将其映射为swap分区</span><br></pre></td></tr></table></figure>

<p>此时使用<code>free -m</code>命令即可看到swap分区已存在了，然后启动mysql服务即可。<br>为了保证下次系统启动后，此swap分区被自动加载，需要修改系统的fstab文件，操作如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">vi /etc/fstab</span><br><span class="line">//在其中添加如下一行</span><br><span class="line">/swap swap swap defaults 0 0</span><br></pre></td></tr></table></figure>

</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/07/hadoop/4/">HDFS的副本存放策略&amp;HDFS的读写流程&amp;Pid文件详解&amp;HDFS常用命令&amp;HDFS的回收站机制&amp;安全模式详解&amp;单、多节点的磁盘均衡策略</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-07</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/HDFS/">HDFS</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/HDFS/">HDFS</a></span><div class="content"><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol>
<li>整理副本放置策略</li>
<li>整理读写流程</li>
<li>整理pid文件</li>
<li>整理hdfs dfs 常用命令</li>
<li>整理多节点，单节点的磁盘均衡</li>
<li>整理安全模式</li>
</ol>
<h3 id="副本放置策略"><a href="#副本放置策略" class="headerlink" title="副本放置策略"></a>副本放置策略</h3><p>副本存放策略存在新旧两个版本</p>
<p>具体可参考我的另一个博客:<a href="https://yerias.github.io/2018/10/11/DataWarehouse/hadoop/5/">https://yerias.github.io/2018/10/11/DataWarehouse/hadoop/5/</a></p>
<h4 id="hadoop2-7-6之前的副本存放策略"><a href="#hadoop2-7-6之前的副本存放策略" class="headerlink" title="hadoop2.7.6之前的副本存放策略"></a>hadoop2.7.6之前的副本存放策略</h4><ul>
<li>副本一：同机架的不同节点 </li>
<li>副本二：同机架的另一节点 </li>
<li>副本三：不同机架的另一节点 </li>
<li>其他副本：随机挑选</li>
</ul>
<h4 id="hadoop2-8-4之后的副本存放策略"><a href="#hadoop2-8-4之后的副本存放策略" class="headerlink" title="hadoop2.8.4之后的副本存放策略"></a>hadoop2.8.4之后的副本存放策略</h4><ul>
<li>副本一：同Client的节点上 </li>
<li>副本二：不同机架中的节点上 </li>
<li>副本三：同第二个副本的机架中的另一个节点上</li>
<li>其他副本：随机挑选</li>
</ul>
<h4 id="副本存放策略优点"><a href="#副本存放策略优点" class="headerlink" title="副本存放策略优点"></a>副本存放策略优点</h4><ul>
<li>提高系统的可靠性</li>
<li>提供负载均衡</li>
<li>提高访问效率</li>
</ul>
<h3 id="读写流程"><a href="#读写流程" class="headerlink" title="读写流程"></a>读写流程</h3><h4 id="读取流程-FSData-InputStream"><a href="#读取流程-FSData-InputStream" class="headerlink" title="读取流程(FSData InputStream)"></a>读取流程(FSData InputStream)</h4><p><img src="http://yerias.github.io/hadoop_img/read.jpg" alt="hdfs读取数据流程"></p>
<ol>
<li>首先调用FileSystem对象的open()方法，获得一个分布式文件系统(DistributedFileSystem)的实例。</li>
<li>分布式文件系统(DistributedFileSystem)通过RPC获得文件的第一批块(Block)的位置信息，<a href="">同一个块按照副本数会返回多个位置信息</a>，这些位置信息按照Hadoop拓扑结构排序，距离客户端近的排在前面。</li>
<li>前两步会返回一个文件系统数据输入流(FSDataInputStream)对象，该对象会被封装为分布式文件系统输入流(DFSInputStream)对象，DFSInputStream可以方便地管理DataNode和NameNode的数据流。客户端调用read方法，DFSInputStream会找出离客户端最近的DataNode并连接。</li>
<li>数据从DataNode源源不断地流向客户端</li>
<li>如果第一个块的数据读完了，就会管理指向第一个块的DataNode的连接，接着读取下一个块。这些操作对客户端来说是透明的，从客户端的角度看来只是在读一个持续不断的数据流。</li>
<li>如果第一批块都读取完了，DFSInputStream就会去NameNode拿下一批块的位置信息，然后继续读，如果所有的块都读完了，这时就会关闭掉所有的流。</li>
</ol>
<p><code>注意:</code> 如果在读数据的时候，DFSInputStream和DataNode的通信发生异常，就会尝试连接正在读的块的排序第二近的DataNode，并且会记录哪个DataNode发生错误，剩余的块读的时候就会直接跳过该DataNode。DFSInputStream也会检查块的校验和，如果发现一个坏的块，就会先报告到NameNode，然后DFSIputStream在其它的DataNode上读取该块的数据。</p>
<h4 id="写入流程-FSData-OutputStream"><a href="#写入流程-FSData-OutputStream" class="headerlink" title="写入流程(FSData OutputStream)"></a>写入流程(FSData OutputStream)</h4><p><img src="http://yerias.github.io/hadoop_img/write.jpg" alt="hdfs写入数据流程"></p>
<ol>
<li>客户端在同过调用分布式文件系统(DistributedFileSystem)的create()方法创建新文件</li>
<li>DistributedFileSystem通过RPC调用NameNode去创建一个没有块关联的新文件，创建前NameNode会做各种校验，比如文件是否存在，客户端有没有权限等。如果通过校验，NameNode就会记录下新文件，否则就会抛出I/O异常。</li>
<li>前两步结合，会返回文件系统数据输出流(FSDataOutputStream)的对象，与读文件的时候相似，DistributedFileSystem被封装成分布式文件系统的输出流(DFSOutputStream)。DFSOutputStream可以协调NameNode和DataNode的通信。客户端开始写数据到DFSOutputStream，DFSOutputStream会把数据切分成一个个的数据包(packet)，然后排成数据队列(data quenc)</li>
<li>接下来，数据队列中的数据包首先输出到数据管道(多个datanode节点组成数据管道)中的第一个DataNode(写数据包)，第一个DataNode又把数据包输出到第二个DataNode中，依次类推。</li>
<li>DFSOutputStream还维护着一个队列叫做确认队列(ack quenc)，这个队列也是由数据包组成，用于等待DataNode收到数据返回确认数据包，当数据管道中的所有DataNode都表示已经收到了确认信息的时候，这时ack quenc才会把对应的数据包移除掉。</li>
<li>客户端完成写数据后，调用close()方法关闭写入数据流。</li>
<li>客户端通知NameNode把文件标记为已完成。然后NameNode把文件写成功的结果反馈给客户端。此时就表示客户端已完成整个HDFS的写数据流程。</li>
</ol>
<h5 id="如果写数据的过程中某个DataNode发生错误，会采取以下的步骤处理。"><a href="#如果写数据的过程中某个DataNode发生错误，会采取以下的步骤处理。" class="headerlink" title="如果写数据的过程中某个DataNode发生错误，会采取以下的步骤处理。"></a>如果写数据的过程中某个DataNode发生错误，会采取以下的步骤处理。</h5><ol>
<li>管道关闭</li>
<li>正常的DataNode上正在写的块会有一个新ID(需要和NameNode通信)，而失败的DataNode上的那个不完整的块会在上报心跳的时候被删除。</li>
<li>失败的DataNode会被移除出数据管道，块中剩余的数据包继续写入管道中的其他两个DataNode。</li>
<li>NameNode会标记这个块的副本个数少于指定值，块的副本会稍后在另一个DataNode创建。</li>
<li>有些时候多个DataNode会失败，只要<code>dfs.replication.min</code>(缺省是1个)属性定义的指定个数的DataNode写入数据成功了，整个写入过程就算成功，缺少的副本会进行异步的恢复。</li>
</ol>
<p><code>注意:</code> 只有调用sync()方法，客户端才确保该文件的写操作已经全部完成， 当客户端调用close()方法时，会默认调用sync()方法。</p>
<h3 id="pid文件"><a href="#pid文件" class="headerlink" title="pid文件"></a>pid文件</h3><p>pid文件具体作用请参考我的另外一个博客:<a href="https://yerias.github.io/2018/10/12/DataWarehouse/hadoop/6/">https://yerias.github.io/2018/10/12/DataWarehouse/hadoop/6/</a></p>
<p>在这里我们只简单说一下修改pid文件的生成目录的步骤，在修改hadoop文件的时候，hadoop最好是stop状态，否则需要kill进程。</p>
<ol>
<li><p>创建/home/hadoop/tmp目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir /home/hadoop/tmp</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改/home/hadoop/tmp的权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chmod -R 777 /hadoop/hadoop/tmp</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改hadoop-env.sh文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HADOOP_PID_DIR=/home/hadoop/tmp</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改yarn-env.sh</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export YARN_PID_DIR=/home/hadoop/tmp</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="hdfs-dfs-常用命令"><a href="#hdfs-dfs-常用命令" class="headerlink" title="hdfs dfs 常用命令"></a>hdfs dfs 常用命令</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">[-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [&lt;path&gt; ...]]</span><br><span class="line">[-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">[-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span><br></pre></td></tr></table></figure>

<p>这些命令很点单，和linux的作用一样，这里不做演示。。。</p>
<h3 id="回收站"><a href="#回收站" class="headerlink" title="回收站"></a>回收站</h3><p>回收站的作用是把hdfs上删除的文件保存一定的时间然后自动删除，Apache是默认关闭的，CDH默认是开启的。</p>
<p>Apache的参数是由<code>core-default.xml</code>文件控制的<code>fs.trash.interval</code>属性</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>fs.trash.interval</td>
<td>0</td>
<td>检查点被删除的分钟数。如果为零，则禁用垃圾特性。单位秒</td>
</tr>
</tbody></table>
<p>一般在生产环境下设置保存7天</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun hadoop]# vim core-site.xml </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;10080&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p><code>注意:</code> 切记检查生产环境是否开启回收站,开了回收站，慎用 <code>-skipTrash</code></p>
<h3 id="安全模式"><a href="#安全模式" class="headerlink" title="安全模式"></a>安全模式</h3><p>安全模式是hadoop的一种保护机制，安全模式下不能进行修改文件的操作，但是可以浏览目录结构、查看文件内容的。</p>
<p>如果NN的log显示<code>Name node is in safe mode</code> ，正常手动让其离开安全模式，这种操作很少做。</p>
<p><strong>一般进入safemode情况有:</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1. 启动或者重新启动hdfs时</span><br></pre></td></tr></table></figure>
<pre><code>2. HDFS维护升级时
3. 块文件损坏等。。。</code></pre><p>可以使用<code>fsck</code>检查一下HDFS的健康度，然后进行下一步操作</p>
<p><code>hdfs fsck / :</code> 用这个命令可以检查整个文件系统的健康状况,但是要注意它不会主动恢复备份缺失的block,这个是由NameNode单独的线程异步处理的</p>
<p><strong>fsck相关介绍:</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs fsck</span><br><span class="line">　　　　Usage:DFSck &lt;path&gt; [-move | -delete | -openforwrite] [-files [-blocks [-locations | -racks]]]</span><br><span class="line">　　　　&lt;path&gt; 检查这个目录中的文件是否完整</span><br><span class="line">　　　　-move 破损的文件移至/lost+found目录</span><br><span class="line">　　　　-delete 删除破损的文件</span><br><span class="line">　　　　-openforwrite 打印正在打开写操作的文件</span><br><span class="line">　　　　-files 打印正在check的文件名</span><br><span class="line">　　　　-blocks 打印block报告(需要和-files参数一起使用)</span><br><span class="line">　　　　-locations 打印每个block的位置信息(需要和-files参数一起使用)</span><br><span class="line">　　　　-racks 打印位置信息的网络拓扑图(需要和-files参数一起使用)</span><br></pre></td></tr></table></figure>

<p>一般我们会查看 / 目录下的损坏文件，然后根据损坏文件的路径手动进行<code>hdfs debug</code>修复</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs fsck / -list-corruptfileblocks</span><br></pre></td></tr></table></figure>

<h3 id="多节点的磁盘均衡"><a href="#多节点的磁盘均衡" class="headerlink" title="多节点的磁盘均衡"></a>多节点的磁盘均衡</h3><p>由于集群中的一些服务器如CPU、磁盘、网络的差异，副本存放并不会一直保持均衡，这就造成某一些服务器的磁盘占用率达到90%，而另外一些服务器的磁盘占用率只有60%或者80%。所以就有必要手动进行均衡操作，事实上hadoop的sbin目录下也有这个命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-rwxr-xr-x 1 hadoop hadoop 1128 Jun  3  2019 start-balancer.sh  #开始</span><br><span class="line">-rwxr-xr-x 1 hadoop hadoop 1179 Jun  3  2019 stop-balancer.sh	#停止</span><br></pre></td></tr></table></figure>

<p>那么集群中的磁盘占用率怎么才算正常？这个由参数<code>threshold</code>控制，默认threshold=10，即各个服务器保持所有服务器的磁盘占用空间的平均值上下浮动10%，可能不好理解，我们用上面的占用率90%、60%和80%算一下。</p>
<p>这三台的平均占用率是:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(90+60+80)/3=76%</span><br></pre></td></tr></table></figure>

<p>那么<code>threshold</code>参数就控制这三台其中的任意一台的磁盘占用率不得超过86%，不得低于66%。</p>
<h4 id="那么怎么做呢？"><a href="#那么怎么做呢？" class="headerlink" title="那么怎么做呢？"></a>那么怎么做呢？</h4><p>在进行磁盘均衡之前，我们需要重新设置一下balancer的带宽限制，在<code>hdfs-default.xml</code>文件中的<code>dfs.datanode.balance.bandwidthPerSec</code>属性，默认是10M，生产环境下一般设置为30M</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.datanode.balance.bandwidthPerSec</td>
<td>10m</td>
<td>指定每个datanode可用于平衡目的的最大带宽(以每秒字节数为单位)。</td>
</tr>
</tbody></table>
<p>在<code>hadoop</code>的<code>hdfs-site.xml</code>文件中覆盖一下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun hadoop]# vim hdfs-site.xml </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.balance.bandwidthPerSec&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;30m&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>怎么做？</p>
<p>写个shell脚本，每天凌晨执行<code>./start-balancer.sh</code>调度一次，达到数据平衡，毛刺修正，调度执行完成自动关闭，不需要执行<code>./stop-balancer.sh</code>手段关闭，除非特殊情况。</p>
<h3 id="单节点的磁盘均衡"><a href="#单节点的磁盘均衡" class="headerlink" title="单节点的磁盘均衡"></a>单节点的磁盘均衡</h3><p>在官网中的描述: </p>
<p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html" target="_blank" rel="noopener">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html</a></p>
<p>在官网中的描述中有这么一句: <code>dfs.disk.balancer.enabled must be set to true in hdfs-site.xml.</code></p>
<p>翻译过来就是: 必须在<code>hdfs-site.xml</code>中将<code>dfs.disk.balancer.enabled</code>设置为<code>true</code>。</p>
<p>这是因为默认情况下，群集上未启用磁盘平衡器</p>
<p>那么我们先去设置一下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun hadoop]# vim hdfs-site.xml </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.disk.balancer.enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h4 id="场景"><a href="#场景" class="headerlink" title="场景:"></a>场景:</h4><p>假如我们现在有三个数据盘</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/data01   90%</span><br><span class="line">/data02   60%</span><br><span class="line">/data03   80%</span><br></pre></td></tr></table></figure>

<p>现在磁盘用的差不多了，准备加入一个盘<code>/data04   0%</code></p>
<p>我们这时候是不是要进行单节点服务器的磁盘均衡？</p>
<h4 id="怎么做？"><a href="#怎么做？" class="headerlink" title="怎么做？"></a>怎么做？</h4><ol>
<li><p>生成hadoop001.plan.json</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -plan hadoop001	#hadoop001是datanode的主机名</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -execute hadoop001.plan.json 	#hadoop001是datanode的主机名</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询状态</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -query ruozedata001 	#hadoop001是datanode的主机名</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="什么时候手动或调度执行？"><a href="#什么时候手动或调度执行？" class="headerlink" title="什么时候手动或调度执行？"></a>什么时候手动或调度执行？</h4><ol>
<li>新盘加入</li>
<li>监控服务器的磁盘剩余空间小于阈值10%，发邮件预警 ，手动执行</li>
</ol>
<h4 id="怎么在DataNode中挂载磁盘？"><a href="#怎么在DataNode中挂载磁盘？" class="headerlink" title="怎么在DataNode中挂载磁盘？"></a>怎么在DataNode中挂载磁盘？</h4><p>由<code>hdfs-default.xml</code>文件的<code>dfs.datanode.data.dir</code>属性控制</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.datanode.data.dir</td>
<td>file://${hadoop.tmp.dir}/dfs/data</td>
<td>确定DFS数据节点应该将其块存储在本地文件系统的何处。多个目录使用逗号分隔。</td>
</tr>
</tbody></table>
<p>假如我们现在有/data01,/data02,/data03,/data04四个目录需要挂载在该DataNode节点中</p>
<p>修改<code>hdfs-site.xml</code>文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun hadoop]# vim hdfs-site.xml </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/data01,/data02,/data03,/data04&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h5 id="为什么DN的生产上要挂载多个物理的磁盘目录，而不是做一个raid-磁盘阵列"><a href="#为什么DN的生产上要挂载多个物理的磁盘目录，而不是做一个raid-磁盘阵列" class="headerlink" title="为什么DN的生产上要挂载多个物理的磁盘目录，而不是做一个raid(磁盘阵列)"></a>为什么DN的生产上要挂载多个物理的磁盘目录，而不是做一个raid(磁盘阵列)</h5><p>为了高效率写  高效率读</p>
<p><code>注意:</code> 提前规划好2-3年存储量 ，避免后期加磁盘维护的工作量</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/07/mysql/%E6%95%B0%E6%8D%AE%E9%87%8D%E5%88%B7%E6%9C%BA%E5%88%B6/">数据重刷机制(抛砖引玉)</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-07</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/MySQL/">MySQL</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/DataWarehouse/">DataWarehouse</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Data/">Data</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/SQL/">SQL</a></span><div class="content"><h4 id="先抛出几个问题"><a href="#先抛出几个问题" class="headerlink" title="先抛出几个问题"></a>先抛出几个问题</h4><ol>
<li><p>存储是不是基石？</p>
</li>
<li><p>假如存储不挂，数据真的准确吗？</p>
</li>
<li><p>存储挂了，数据还准确吗？</p>
</li>
<li><p>如何校验是否正确？如何让其正确？机制是不是必须有？</p>
</li>
</ol>
<p>注：<code>sqoop</code>抽数据，无<code>error</code>丢数据的概率很小</p>
<p>数据质量校验：数据量校验 <code>count</code>相同吗？<code>count</code>相同内容相同吗？</p>
<p>数据量相同–&gt;数据量不同 重刷机制 补or删 <code>spark</code> 95%–&gt;数据内容不同？ 抽样 5%</p>
<h4 id="现在重点理解一下重刷机制"><a href="#现在重点理解一下重刷机制" class="headerlink" title="现在重点理解一下重刷机制"></a>现在重点理解一下重刷机制</h4><p>背景：用<code>count</code>校验上下游的数据不准确</p>
<p>引入重刷机制：通过对上下游的两个表求<code>full outer join</code>来对比字段的<code>null</code>值</p>
<p>上游表a</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| id   | data  | age  |</span><br><span class="line">| <span class="comment">---- | ----- | ---- |</span></span><br><span class="line">| 1    | data1 | 18   |</span><br><span class="line">| 2    | data2 | 19   |</span><br><span class="line">| 3    | data3 | 20   |</span><br><span class="line">| 7    | data7 | 22   |</span><br></pre></td></tr></table></figure>

<p>下游表b</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| id   | data  | age  |</span><br><span class="line">| <span class="comment">---- | ----- | ---- |</span></span><br><span class="line">| 1    | data1 | 18   |</span><br><span class="line">| 2    | data2 | 19   |</span><br><span class="line">| 3    | data3 | 20   |</span><br><span class="line">| 7    | data7 | 22   |</span><br></pre></td></tr></table></figure>

<p>我们发现表 a 和表 b 对比 表 a 少了 5 、6 多了 7 ，表 b 少了 2 、 7 多了 6，我们现在对两个表做 <code>full outer join</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| aid  | data   | age  | bid  |</span><br><span class="line">| <span class="comment">---- | ------ | ---- | ---- |</span></span><br><span class="line">| 1    | ruoze1 | 18   | 1    |</span><br><span class="line">| 2    | ruoze2 | 19   | null |</span><br><span class="line">| 3    | ruoze3 | 20   | 3    |</span><br><span class="line">| 7    | ruoze7 | 22   | null |</span><br><span class="line">| null | null   | null | 5    |</span><br><span class="line">| null | null   | null | 6    |</span><br></pre></td></tr></table></figure>

<p>以表 a 为标准，对生成后的大表做筛选，分别查找 <code>aid</code> 和 <code>bid</code> 为 <code>null</code> 的记录</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">from</span> t <span class="keyword">where</span> aid=<span class="literal">null</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">from</span> t <span class="keyword">where</span> bid=<span class="literal">null</span></span><br></pre></td></tr></table></figure>

<p>发现 <code>bid</code>为 5 、 6 的行 <code>aid</code> 为 <code>null</code>，说明 <code>bid</code> 下游数据多了，根据 <code>bid</code> 重新构建</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> b <span class="keyword">where</span> bid=<span class="number">5</span>     </span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> b <span class="keyword">where</span> bid=<span class="number">6</span></span><br></pre></td></tr></table></figure>

<p>发现 <code>aid</code> 为 2 、 7 的 <code>bid</code> 为<code>null</code>，说明 <code>bid</code> 下游数据少了，根据 <code>aid</code> 重新构建</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="number">2</span> ruoze2 <span class="number">19</span> </span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="number">7</span> ruoze7 <span class="number">22</span></span><br></pre></td></tr></table></figure>

<p>经过重新构建也就是重刷后的数据是</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">| aid  | data   | age  | bid  |</span><br><span class="line">| <span class="comment">---- | ------ | ---- | ---- |</span></span><br><span class="line">| 1    | ruoze1 | 18   | 1    |</span><br><span class="line">| 2    | ruoze2 | 19   | 2    |</span><br><span class="line">| 3    | ruoze3 | 20   | 3    |</span><br><span class="line">| 7    | ruoze7 | 22   | 7    |</span><br></pre></td></tr></table></figure>

<h4 id="深度思考："><a href="#深度思考：" class="headerlink" title="深度思考："></a>深度思考：</h4><p><code>full outer join</code> 其实就是先 <code>left join</code> 和后 <code>right join</code> 的两个结果，为 <code>null</code> 的刚好是缺少的或者多的，而交集是上下游都有的数据，需要做的是 <code>left join</code> 为 <code>null</code> 做 <code>insert</code> 或者 <code>delete</code>，还是 <code>right join</code> 为 null 做 <code>insert</code> 或者 <code>delete</code>。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/06/hadoop/3/">理解数据块、副本数、小文件的概念&amp;掌握HDFS架构&amp;掌握NN和SNN交互流程</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-06</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/HDFS/">HDFS</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/HDFS/">HDFS</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/DataWarehouse/">DataWarehouse</a></span><div class="content"><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol>
<li>整理对 块大小 副本数的理解</li>
<li>整理对小文件的理解</li>
<li>整理HDFS架构</li>
<li>整理SNN流程</li>
</ol>
<h3 id="修改hdfs的数据保存文件"><a href="#修改hdfs的数据保存文件" class="headerlink" title="修改hdfs的数据保存文件"></a>修改hdfs的数据保存文件</h3><p>在开始完成今天的目标之前，我们还要做一个事情，那就是修改hdfs的nn、nd、snn文件保存的目录，这个目录默认保存在/tmp目录下，那么为什么会保存在/tmp目录下呢，实际上是由<code>core-default.xml</code>默认参数决定的</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>hadoop.tmp.dir</td>
<td>/tmp/hadoop-${user.name}</td>
<td>A base for other temporary directories.</td>
</tr>
</tbody></table>
<p>因为/tmp目录具有固定周期清除文件的特性，所以我们这里需要改变hadoop的存储文件路径，防止丢失文件</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop]$ vim core-site.xml </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>在这之后我们还要对/home/hadoop/tmp目录做一下调整</p>
<ul>
<li><p><code>chmod -R 777 /home/hadoop/tmp</code></p>
</li>
<li><p><code>mv /tmp/hadoop-hadoop/dfs /home/hadoop/tmp/</code></p>
</li>
<li><p><code>test</code> </p>
</li>
</ul>
<h3 id="对块大小和副本数的理解"><a href="#对块大小和副本数的理解" class="headerlink" title="对块大小和副本数的理解"></a>对块大小和副本数的理解</h3><h4 id="块的理解"><a href="#块的理解" class="headerlink" title="块的理解"></a>块的理解</h4><p>hadoop1.x的块大小是64M，hadoop2.x的块大小是128M，块的大小是<code>hdfs-default.xml</code>文件中的<code>dfs.blocksize</code>属性控制，它是hdfs存储处理数据的最小单元，可以根据实际需求改变块大小，但是一般不建议这么做。</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.blocksize</td>
<td>134217728(128M)</td>
</tr>
</tbody></table>
<p>hadoop1.x的块大小是64M，hadoop2.x的块大小是128M，块的大小是<code>hdfs-default.xml</code>文件中的<code>dfs.blocksize</code>属性控制，它是hdfs存储处理数据的最小单元，可以根据实际需求改变块大小，但是一般不建议这么做。</p>
<p><code>块大小为什么要设计成128M？</code></p>
<p>是为了最小化寻址时间，目前磁盘的传输速率普遍是在100M/S左右，所以设计成128M每块。</p>
<h4 id="副本数的理解"><a href="#副本数的理解" class="headerlink" title="副本数的理解"></a>副本数的理解</h4><p>副本的设置让hadoop具有高可靠性的特点，数据不会轻易丢失。副本是存储在dn中的，由<code>hdfs-default.xml</code>文件的<code>dfs.replication</code>参数控制，伪分布式部署是1份，集群部署是3份，不建议修改。</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.replication</td>
<td>3</td>
</tr>
</tbody></table>
<h3 id="对小文件的理解"><a href="#对小文件的理解" class="headerlink" title="对小文件的理解"></a>对小文件的理解</h3><p>一般来说，小文件是文件大小小于10M的数据，由于hadoop的架构特性，它只能有一台主nn，如果小文件特别多的话，小文件的块也特别多，nn需要维护的块的元数据信息的条数也多，所以我们一般把小文件合并成大文件再放到hdfs上，也有上传hdfs后合并，这样来减少nn维护的块的元数据数量。具体合并的方式，以后再讲。</p>
<h3 id="整理HDFS架构"><a href="#整理HDFS架构" class="headerlink" title="整理HDFS架构"></a>整理HDFS架构</h3><p>HDFS由NameNode、SecondaryNameNode、DataNode三个组件组成</p>
<h4 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h4><p>NameNode也被称为名称节点或元数据节点，是HDFS主从架构中的主节点，相当于HDFS的大脑，它管理文件系统的命名空间，维护着整个文件系统的目录树以及目录树中所有子目录和文件。</p>
<p>这些信息以两个文件的形式持久化保存在本地磁盘上，一个是命令空间镜像FSImage(File System Image)，主要是用来存储HDFS的元数据信息。还有一个是命令空间镜像的编辑日志(Editlog)，该文件保存用户对命令空间镜像的修改信息。</p>
<h4 id="SecondeayNameNode"><a href="#SecondeayNameNode" class="headerlink" title="SecondeayNameNode"></a>SecondeayNameNode</h4><p>SecondaryNameNode也被称为元数据节点，是HDFS主从架构中的备用节点，主要用于定期合并命名空间镜像(FSImage)和命令空间镜像的操作日志(Editlog)，是一个辅助NameNode的守护进程。</p>
<p>定期合并FSImage和Editlog的周期时间是由<code>hdfs-default.xml</code>文件的<code>dfs.namenode.checkpoint.period</code>属性决定的，默认一小时合并一次，同时如果Editlog操作日志记录满 1000000条也会触发合并机制，由<code>dfs.namenode.checkpoint.txns</code>属性控制，两者满足一个即可。</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.namenode.checkpoint.period</td>
<td>3600</td>
<td>两个周期性检查点之间的秒数。</td>
</tr>
<tr>
<td>dfs.namenode.checkpoint.txns</td>
<td>1000000</td>
<td>两个周期性检查点之间的名称空间记录数。</td>
</tr>
</tbody></table>
<p>虽然SecondaryNameNode能够减轻单点故障，但是还会有风险，因为总有一段时间的数据是没有同步的。</p>
<h5 id="问题-为什么SecondaryNameNode要辅助NameNode定期合并FSImage文件和Editlog文件？"><a href="#问题-为什么SecondaryNameNode要辅助NameNode定期合并FSImage文件和Editlog文件？" class="headerlink" title="问题: 为什么SecondaryNameNode要辅助NameNode定期合并FSImage文件和Editlog文件？"></a>问题: 为什么SecondaryNameNode要辅助NameNode定期合并FSImage文件和Editlog文件？</h5><p>FSImage文件实际上是HDFS文件系统中元数据的一个永久性检查点(checkpoint)，但也并不是每一个写操作都会更新到这个文件中，因为FSImage是一个大型文件，如果频繁地执行写操作，会导致系统运行极其缓慢，那么如何解决呢？</p>
<p>解决方案就是NameNode将命令空间的改动信息写入命令空间的Editlog，但随着时间的推移，Editlog文件会越来越大，一旦发生故障，那么将需要花费很长的时间进行回滚操作，所以可以像传统的关系型数据库一样，定期地合并FSImage和Editlog，但是如果由NameNode来做合并操作，由于NameNode在为集群提供服务的同时可能无法提供足够的资源，所以为了解决这一问题，SecondaryNameNode就应运而生了。</p>
<h4 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h4><p>DataNode也被称为数据节点，它是HDFS主从架构在的从节点，它存储数据块和数据块校验和它在NameNode的指导下完成数据的IO操作。</p>
<p><img src="https://yerias.github.io/hadoop_img/image-20191202110000146.png" alt="数据块和数据库校验和"></p>
<p>DataNode会不断地向NameNode发送心跳和块报告信息，并执行来自NameNode的指令。</p>
<p>发送心跳是为了告诉nn我还活着，通过<code>hdfs-default.xml</code>文件的<code>dfs.heartbeat.interval</code>参数可以得知，每3秒发送一次</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.heartbeat.interval</td>
<td>3</td>
<td>Determines datanode heartbeat interval in seconds.</td>
</tr>
</tbody></table>
<p>发送块报告信息是为了扫描数据目录并协调内存块和磁盘块之间的差异的，从<code>hdfs-default.xml</code>文件的<code>dfs.datanode.directoryscan.interval</code>属性和<code>dfs.blockreport.intervalMsec</code>可以得知每6小时发送一次块报告，生产环境下建议缩短周期（3小时）</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.datanode.directoryscan.interval</td>
<td>21600</td>
<td>DataNode扫描数据目录并协调内存块和磁盘块之间的差异的时间间隔(以秒为单位)，发现损坏块</td>
</tr>
<tr>
<td>dfs.blockreport.intervalMsec</td>
<td>21600000</td>
<td>确定以毫秒为单位的块报告间隔，恢复数据块</td>
</tr>
</tbody></table>
<p>在这里我们需要知道一个hadoop命令，该命令仅适用于高级用户，不正确的使用可能会导致数据丢失。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun subdir0]$ hdfs debug</span><br><span class="line">Usage: hdfs debug &lt;command&gt; [arguments]</span><br><span class="line"></span><br><span class="line">These commands are for advanced users only.</span><br><span class="line"></span><br><span class="line">Incorrect usages may result in data loss. Use at your own risk.</span><br><span class="line"></span><br><span class="line">verifyMeta -meta &lt;metadata-file&gt; [-block &lt;block-file&gt;]</span><br><span class="line">computeMeta -block &lt;block-file&gt; -out &lt;output-metadata-file&gt;</span><br><span class="line">recoverLease -path &lt;path&gt; [-retries &lt;num-retries&gt;]</span><br><span class="line">[hadoop@aliyun subdir0]$</span><br></pre></td></tr></table></figure>

<h5 id="手动修复"><a href="#手动修复" class="headerlink" title="手动修复"></a>手动修复</h5><p><code>hdfs debug</code>的作用是在多副本的环境下手动修复元数据、块或者副本，我们在这里只说修改副本，这里的xxx是指副本路径，该路径必须驻留在HDFS文件系统上，由<code>hdfs fsck</code>命令查找。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">[hadoop@aluyuncurrent]$</span><span class="bash"> hdfs debug recoverLease -path xxx -retries 10</span></span><br></pre></td></tr></table></figure>

<h5 id="自动修复"><a href="#自动修复" class="headerlink" title="自动修复"></a><code>自动修复</code></h5><p><a href="https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/" target="_blank" rel="noopener">https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/</a></p>
<p>但是有可能: 手动修复 + 自动修复都是失败的 </p>
<p>这就需要保证数据仓库的数据质量和数据重刷机制恢复 </p>
<h5 id="问题-DataNode是如何存储和管理数据块的？"><a href="#问题-DataNode是如何存储和管理数据块的？" class="headerlink" title="问题: DataNode是如何存储和管理数据块的？"></a>问题: DataNode是如何存储和管理数据块的？</h5><ol>
<li>DataNode节点是以数据块的形式在本地Linux文件系统上保存HDFS文件的内容，并对外提供文件数据访问功能。</li>
<li>DataNode节点的一个基本功能就是管理这些保存在Linux文件系统上的数据</li>
<li>DataNode节点是将数据块以Linux文件的形式保存在本地的存储系统上</li>
</ol>
<h3 id="SecondaryNameNode和NameNode的交互流程"><a href="#SecondaryNameNode和NameNode的交互流程" class="headerlink" title="SecondaryNameNode和NameNode的交互流程"></a>SecondaryNameNode和NameNode的交互流程</h3><p><img src="https://yerias.github.io/hadoop_img/IMG_0614(20191202-125538).JPG" alt="SecondaryNameNode和NameNode的交互流程"></p>
<ol>
<li><code>SecondaryNameNode</code>引导<code>NameNode</code>滚动更新操作日志，并开始将新的操作日志写进<code>edits.new</code>。</li>
<li><code>SecondaryNameNode</code>将<code>NameNode</code>的<code>FSImage</code>文件和<code>Edits</code>文件复制到本地的检查点目录。</li>
<li><code>SecondaryNameNode</code>将<code>FSImage</code>文件导入内存，回放编辑日志<code>Edits</code>文件，将其合并到<code>FSImage.ckpt</code>文件，并将新的<code>FSImage.ckpt</code>文件压缩后写入磁盘。</li>
<li><code>SecondaryNameNode</code>将新的<code>FSImage.ckpt</code>文件传回<code>NameNode</code>。</li>
<li><code>NameNode</code>在接收新的<code>FSImage.ckpt</code>文件后，将<code>FSImage.ckpt</code>替换为<code>FSImage</code>，然后直接加载和启用该文件</li>
<li><code>NameNode</code>将<code>Edits.new</code>更名为<code>Edits</code>。默认情况下，该过程1小时内发生1次，或者当编辑日志达到默认值1000000条也会触发。</li>
</ol>
<h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><ol>
<li><p>NN的fsimage的个数默认是保留2个</p>
<p><img src="https://yerias.github.io/hadoop_img/57Y5RSNU_6%5DY886.jpg" alt="fsimage文件"></p>
<p>控制的参数是<code>hdfs-default.xml</code>文件的<code>dfs.namenode.num.checkpoints.retained</code>参数</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.namenode.num.checkpoints.retained</td>
<td>2</td>
</tr>
</tbody></table>
</li>
<li><p>NN的editlog文件不会保留所有的，至于保留的个数还是周期，解决中。。。</p>
<p><img src="https://yerias.github.io/hadoop_img/C(J%60DYZC@_%7BEA%5B3(GMDRK%7DI.jpg" alt="editlog文件"></p>
</li>
</ol>
</div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/10/">&lt;&lt;</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/12/">&gt;&gt;</a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>