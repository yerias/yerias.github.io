<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="BigData Developer"><meta name="keywords" content="yerias,TUNANのBlog,BigData"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017 | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">133</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">30</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">26</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">TUNANのBlog</div><div id="site-sub-title">感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017</div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/14/hadoop/11/">数据倾斜&amp;MRchain解决数据倾斜&amp;大小表Reduce Join&amp;大小表Map Join&amp;SQL的执行计划</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-14</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/MapReduce/">MapReduce</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/MapReduce/">MapReduce</a></span><div class="content"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>数据倾斜</li>
<li>MRchain解决数据倾斜</li>
<li>大小表Reduce Join</li>
<li>大小表Map Join</li>
<li>SQL的执行计划</li>
</ol>
<h2 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h2><ol>
<li><p>数据倾斜怎么造成的</p>
<p>mapreduce计算是将map相同的key丢到reduce，在reduce中进行聚合操作,在map和reduce中间有个shuffle操作，shuffle会将map阶段相同的key划分到reduce阶段中的一个reduce中去，数据倾斜就是数据的key 的分化严重不均，造成一部分数据很多，一部分数据很少的局面。</p>
</li>
<li><p>数据倾斜产生的问题</p>
<ul>
<li><p>有一个或多个reduce卡住</p>
</li>
<li><p>各种container报错OOM</p>
</li>
<li><p>读写的数据量极大，至少远远超过其它正常的reduce</p>
</li>
<li><p>伴随着数据倾斜，会出现任务被kill等各种诡异的表现。</p>
</li>
</ul>
</li>
<li><p>原因和解决方法</p>
<p>原因:</p>
<ul>
<li>单个值有大量记录(1.内存的限制存在，2.可能会对集群其他任务的运行产生不稳定的影响)</li>
<li>唯一值较多(单个唯一值的记录数占用内存不会超过分配给reduce的内存)</li>
</ul>
<p>解决办法:</p>
<ul>
<li><p>增加reduce个数</p>
</li>
<li><p>使用自定义partitioner</p>
</li>
<li><p>增加reduce 的jvm内存（效果不好）</p>
</li>
<li><p>map 阶段将造成倾斜的key 先分成多组加随机数并且在reduce阶段去除随机数</p>
</li>
<li><p>从业务和数据上解决数据倾斜</p>
<p>我们通过设计的角度尝试解决它</p>
<ul>
<li>数据预处理，过滤掉异常值</li>
<li>将数据打散让它的并行度变大，再汇集</li>
</ul>
</li>
<li><p>平台的优化方法</p>
<ul>
<li>join 操作中，使用 map join 在 map 端就先进行 join ，免得到reduce 时卡住</li>
<li>能先进行 group 操作的时候先进行 group 操作，把 key 先进行一次 reduce,之后再进行 count 或者 distinct count 操作</li>
<li>设置map端输出、中间结果压缩</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="MRchain解决数据倾斜"><a href="#MRchain解决数据倾斜" class="headerlink" title="MRchain解决数据倾斜"></a>MRchain解决数据倾斜</h2><p>核心思想: 第一个mapredue把具有数据倾斜特性的数据加盐(随机数)，进行聚合；第二个mapreduce把第一个mapreduce的加盐结果进行去盐，再聚合，问题是两个MR IO高。</p>
<p>参考代码:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.item;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.utils.FileUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.chain.ChainMapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.chain.ChainReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.Random;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-02-04 14:50</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ChainMRDriver</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Random r;</span><br><span class="line">    String in = <span class="string">"data/skew/access.txt"</span>;</span><br><span class="line">    String out1 = <span class="string">"out/mr1"</span>;</span><br><span class="line">    String out2 = <span class="string">"out/mr2"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(conf, <span class="keyword">new</span> ChainMRDriver(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">super</span>.getConf();</span><br><span class="line">        FileUtil.checkFileIsExists(conf, out1);</span><br><span class="line">        FileUtil.checkFileIsExists(conf, out2);</span><br><span class="line"></span><br><span class="line">        Job job1 = Job.getInstance(conf);</span><br><span class="line">        Job job2 = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        job1.setJarByClass(ChainMRDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job2.setJarByClass(ChainMRDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        ChainMapper.addMapper(job1, ChainMRDriver.incRanDomMapper<span class="class">.<span class="keyword">class</span>, <span class="title">LongWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line">        ChainReducer.setReducer(job1, ChainMRDriver.incRanDomReduver<span class="class">.<span class="keyword">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line"></span><br><span class="line">        ChainMapper.addMapper(job2, ChainMRDriver.decRanDomMapper<span class="class">.<span class="keyword">class</span>, <span class="title">LongWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line">        ChainReducer.setReducer(job2, ChainMRDriver.decRanDomReduver<span class="class">.<span class="keyword">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job1, <span class="keyword">new</span> Path(in));</span><br><span class="line">        FileOutputFormat.setOutputPath(job1, <span class="keyword">new</span> Path(out1));</span><br><span class="line">        FileInputFormat.setInputPaths(job2, <span class="keyword">new</span> Path(out1));</span><br><span class="line">        FileOutputFormat.setOutputPath(job2, <span class="keyword">new</span> Path(out2));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//提交job1和job2 job1--&gt;job2 必须按照顺序提交</span></span><br><span class="line">        System.out.println(<span class="string">"=============第一阶段=============="</span>);</span><br><span class="line">        <span class="keyword">boolean</span> b = job1.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">if</span> (b) &#123;</span><br><span class="line">            System.out.println(<span class="string">"=============第二阶段=============="</span>);</span><br><span class="line">            <span class="keyword">boolean</span> b1 = job2.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">            <span class="keyword">return</span> b1 ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">incRanDomMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//创建对象</span></span><br><span class="line">            r = <span class="keyword">new</span> Random();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//把数据读出来，加盐  www.baidu.com   2</span></span><br><span class="line">            String[] line = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">            String incR = r.nextInt(<span class="number">10</span>) +<span class="string">"_"</span>+line[<span class="number">0</span>];</span><br><span class="line">            <span class="keyword">int</span> number = Integer.parseInt(line[<span class="number">1</span>]);</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(incR), <span class="keyword">new</span> IntWritable(number));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (r != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">//回收对象</span></span><br><span class="line">                r = <span class="keyword">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">incRanDomReduver</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">int</span> sums = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (IntWritable sum : values) &#123;</span><br><span class="line">                sums += sum.get();</span><br><span class="line">            &#125;</span><br><span class="line">            context.write(key, <span class="keyword">new</span> IntWritable(sums));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">decRanDomMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="comment">//去盐 聚合</span></span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            String[] line = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">            String decWord = line[<span class="number">0</span>].split(<span class="string">"_"</span>)[<span class="number">1</span>];</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(decWord), <span class="keyword">new</span> IntWritable(Integer.parseInt(line[<span class="number">1</span>])));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">decRanDomReduver</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">int</span> sums = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (IntWritable sum : values) &#123;</span><br><span class="line">                sums += sum.get();</span><br><span class="line">            &#125;</span><br><span class="line">            context.write(key, <span class="keyword">new</span> IntWritable(sums));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="SQL的执行计划"><a href="#SQL的执行计划" class="headerlink" title="SQL的执行计划"></a>SQL的执行计划</h2><p>如何运行SQL的执行计划</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">EXPLAIN</span> [<span class="keyword">EXTENDED</span>] Syntax</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">EXPLAIN</span> <span class="keyword">select</span> e.empno,e.ename,d.deptno,d.dname <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure>

<p>解析这句SQL的执行计划</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">+-----------------------------------------------------------------+--+</span><br><span class="line">                      		Explain                       </span><br><span class="line">+-----------------------------------------------------------------+--+</span><br><span class="line"> STAGE DEPENDENCIES:     <span class="comment">//阶段性依赖                           </span></span><br><span class="line">   Stage-<span class="number">4</span> is a root stage   <span class="comment">//这是一个根依赖                       </span></span><br><span class="line">   Stage-<span class="number">3</span> depends on stages: Stage-<span class="number">4</span>    <span class="comment">//Stage-3依赖Stage-4           </span></span><br><span class="line">   Stage-<span class="number">0</span> depends on stages: Stage-<span class="number">3</span>    <span class="comment">//Stage-0依赖Stage-3           </span></span><br><span class="line">                                                    </span><br><span class="line"> STAGE PLANS:   <span class="comment">// 阶段性计划                                    </span></span><br><span class="line">   Stage: Stage-<span class="number">4</span>     <span class="comment">//阶段4                              </span></span><br><span class="line">     Map Reduce Local Work   <span class="comment">//这是一个本地作业                    </span></span><br><span class="line">       Alias -&gt; Map Local Tables:    <span class="comment">// Map本地表的别名为d 即表dept              </span></span><br><span class="line">         d                                          </span><br><span class="line">           Fetch Operator   <span class="comment">//抓取                        </span></span><br><span class="line">             limit: -<span class="number">1</span>    <span class="comment">//limit为-1，即把数据全部读出来了                   </span></span><br><span class="line">       Alias -&gt; Map Local Operator Tree:  <span class="comment">//Map本地操作树          </span></span><br><span class="line">         d                                          </span><br><span class="line">           TableScan     <span class="comment">//表扫描                           </span></span><br><span class="line">             alias: d    <span class="comment">//别名d                           </span></span><br><span class="line">             Statistics: Num rows: <span class="number">2</span> Data size: <span class="number">284</span> Basic stats: PARTIAL Column stats: NONE 	<span class="comment">//统计 </span></span><br><span class="line">             Filter Operator  <span class="comment">//过滤                      </span></span><br><span class="line">               predicate: <span class="function">deptno is not <span class="title">null</span> <span class="params">(type: <span class="keyword">boolean</span>)</span>  <span class="comment">//断定：deptno字段 不为空</span></span></span><br><span class="line"><span class="function">               Statistics: Num rows: 1 Data size: 142 Basic stats: COMPLETE Column stats: NONE 	   <span class="comment">//统计</span></span></span><br><span class="line"><span class="function">               HashTable Sink Operator  <span class="comment">//输出类型为HashTable           </span></span></span><br><span class="line"><span class="function">                 keys:                              </span></span><br><span class="line"><span class="function">                   0 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                   1 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                                                    </span></span><br><span class="line"><span class="function">   Stage: Stage-3   <span class="comment">//阶段3                                </span></span></span><br><span class="line"><span class="function">     Map Reduce                                     </span></span><br><span class="line"><span class="function">       Map Operator Tree:    <span class="comment">//Map操作树                       </span></span></span><br><span class="line"><span class="function">           TableScan   <span class="comment">//表扫描                             </span></span></span><br><span class="line"><span class="function">             alias: e  <span class="comment">//e表 即emp表                             </span></span></span><br><span class="line"><span class="function">             Statistics: Num rows: 6 Data size: 657 Basic stats: COMPLETE Column stats: NONE  	<span class="comment">//统计		</span></span></span><br><span class="line"><span class="function">             Filter Operator    <span class="comment">//过滤                    </span></span></span><br><span class="line"><span class="function">               predicate: deptno is not <span class="title">null</span> <span class="params">(type: <span class="keyword">boolean</span>)</span> 	<span class="comment">//断定：deptno字段 不为空</span></span></span><br><span class="line"><span class="function">               Statistics: Num rows: 3 Data size: 328 Basic stats: COMPLETE Column stats: NONE 	  <span class="comment">//统计</span></span></span><br><span class="line"><span class="function">               Map Join Operator    <span class="comment">// Map Join  操作               </span></span></span><br><span class="line"><span class="function">                 condition map:   <span class="comment">//Map条件                 </span></span></span><br><span class="line"><span class="function">                      Inner Join 0 to 1             </span></span><br><span class="line"><span class="function">                 keys:                              </span></span><br><span class="line"><span class="function">                   0 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                   1 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                 outputColumnNames: _col0, _col1, _col11, _col12  <span class="comment">//数值字段在表中的位置</span></span></span><br><span class="line"><span class="function">                 Statistics: Num rows: 3 Data size: 360 Basic stats: COMPLETE Column stats: NONE	<span class="comment">//统计 </span></span></span><br><span class="line"><span class="function">                 Select Operator   <span class="comment">//Select操作                 </span></span></span><br><span class="line"><span class="function">                   expressions: <span class="title">_col0</span> <span class="params">(type: <span class="keyword">int</span>)</span>, <span class="title">_col1</span> <span class="params">(type: string)</span>, <span class="title">_col11</span> <span class="params">(type: <span class="keyword">int</span>)</span>, <span class="title">_col12</span> <span class="params">(type: string)</span> 	<span class="comment">//表达式</span></span></span><br><span class="line"><span class="function">                   outputColumnNames: _col0, _col1, _col2, _col3 	<span class="comment">//数值字段在表中的位置</span></span></span><br><span class="line"><span class="function">                   Statistics: Num rows: 3 Data size: 360 Basic stats: COMPLETE Column stats: NONE 	  <span class="comment">//统计</span></span></span><br><span class="line"><span class="function">                   File Output Operator   <span class="comment">//文件输出操作          </span></span></span><br><span class="line"><span class="function">                     compressed: <span class="keyword">false</span>    <span class="comment">//是否压缩：否          </span></span></span><br><span class="line"><span class="function">                     Statistics: Num rows: 3 Data size: 360 Basic stats: COMPLETE Column stats: NONE	<span class="comment">//统计 </span></span></span><br><span class="line"><span class="function">                     table:   <span class="comment">//表文件的输入、输出、序列化类型                      </span></span></span><br><span class="line"><span class="function">                         input format: org.apache.hadoop.mapred.TextInputFormat <span class="comment">//文件输入格式</span></span></span><br><span class="line"><span class="function">                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat <span class="comment">//文件输出格式</span></span></span><br><span class="line"><span class="function">                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe <span class="comment">//反序列化</span></span></span><br><span class="line"><span class="function">       Local Work:                                  </span></span><br><span class="line"><span class="function">         Map Reduce Local Work                      </span></span><br><span class="line"><span class="function">                                                    </span></span><br><span class="line"><span class="function">   Stage: Stage-0   <span class="comment">//阶段0                                </span></span></span><br><span class="line"><span class="function">     Fetch Operator                                 </span></span><br><span class="line"><span class="function">       limit: -1     <span class="comment">// limit设置的值                               </span></span></span><br><span class="line"><span class="function">       Processor Tree:                              </span></span><br><span class="line"><span class="function">         ListSink                                   </span></span><br><span class="line"><span class="function">                                                    </span></span><br><span class="line"><span class="function">+-----------------------------------------------------------------+--+</span></span><br></pre></td></tr></table></figure>

<p>从执行计划得知，hive中执行SQL语句底层执行的是MapReduce。</p>
<p>我们在SQL中关联了两张表分别是emp dept，并从两张表中取出某些字段，在SQL执行计划中共分为三个阶段，分别是stage4、stage3、stage0。</p>
<p>stage4是根stage，stage3依赖stage4，同时stage0依赖stage3。</p>
<p>stage4是一个本地作业，读取的是dept表，输出一个Map类型的hashTable，关联的key是两张表的deptno，在执行计划中表现为0 deptno和 1 deptno，即执行的MapReduce中的Key是deptno字段。</p>
<p>stage3是MapReduce中的Map阶段，扫描emp表，执行一个Map Join操作，条件是两张表的dept字段相等(内连接)，现在我们得到的是一张包含所有字段的大表，得到需要的字段的对应位置，并且匹配字段的类型，在输出的时候检查是否需要压缩，以及输入、输出、和序列化类型</p>
<p>Stage-0阶段取出limit中指定的记录数</p>
<p>总结: 我们发现执行该SQL没有Reduce阶段，在现有的版本中默认设置<code>hive.auto.convert.join</code>(是否自动转换为mapjoin)为true，该参数配合<code>hive.mapjoin.smalltable.filesize</code>参数(小表的最大文件大小)默认为25M。即小于25M的表为小表，自动转为mapjoin，小表上传到hadoop缓存，提供给各个大表join使用。大表和小表根据关联的key形成一张大表，取出select需要的字段，最后根据limit设置的值取出对应的记录数。</p>
<p>参考参数：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--是否自动转换为mapjoin</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">true</span>;</span><br><span class="line"><span class="comment">--小表的最大文件大小，默认为25000000，即25M</span></span><br><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize = <span class="number">25000000</span>;</span><br><span class="line"><span class="comment">--是否将多个mapjoin合并为一个</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask = <span class="literal">true</span>;</span><br><span class="line"><span class="comment">--多个mapjoin转换为1个时，所有小表的文件大小总和的最大值。</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask.size = <span class="number">10000000</span>;</span><br></pre></td></tr></table></figure>

<h2 id="大小表Reduce-Join-emp、dept"><a href="#大小表Reduce-Join-emp、dept" class="headerlink" title="大小表Reduce Join(emp、dept)"></a>大小表Reduce Join(emp、dept)</h2><p>Reduce Join的核心思路是定义输出字段作为一个实体类，用来作为输出，实体类中定义一个标志用来区分表的来源</p>
<ol>
<li><p>将大小两个表在SQL中join的字段作为MapReduce中的key，原因是MapReduce中的key具有排序和分区的作用</p>
</li>
<li><p>Map中获取context中切片所在的文件名，按行获取文件中的数据并且根据获取的文件名分别将数据set到对象中，并写出Map。</p>
</li>
<li><p>Reduce中每次获取key相同的一组value值数据，这组value值既有dept中的数据，也</p>
</li>
</ol>
<p>   有emp中的数据，只要他们有相同的key，就会在shuffle中丢到一个reduce，这时候获取这组数据的值，根据flag来判断来自哪个表，如果是dept表则将数据设置到新new出来的对象中，添加到List列表中，同时创建一个保存emp表中数据的变量，由于emp表是小表，emp表中需要的数据对应dept/emp中的key的字段是唯一的，所以只需要把value中所有的对象都遍历循环出来，dept表数据添加到了List列表，emp表的数据添加到了变量中，最后循环List列表把变量set到每一个对象中，即完成了全部对象的全部成员属性。最后输出即可。</p>
<p>   参考代码:</p>
   <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.hadoop.join;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.pojo.JoinMain;</span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.utils.FileUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-01-29 16:39</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReduceJoinDriver</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    String in = <span class="string">"data/join/"</span>;</span><br><span class="line">    String out = <span class="string">"out/"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(conf, <span class="keyword">new</span> ReduceJoinDriver(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//获得configuration</span></span><br><span class="line">        Configuration conf = <span class="keyword">super</span>.getConf();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//检查文件夹</span></span><br><span class="line">        FileUtil.checkFileIsExists(conf, out);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用新方法这里怎么操作?</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置驱动类</span></span><br><span class="line">        job.setJarByClass(ReduceJoinDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置Map/Reducer类</span></span><br><span class="line">        job.setMapperClass(JoinMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(JoinReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置Map参数类型</span></span><br><span class="line">        job.setMapOutputKeyClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(JoinMain<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//job.setNumReduceTasks(3);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置Reducer参数类型</span></span><br><span class="line">        job.setOutputKeyClass(JoinMain<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置文件的输入输出</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(in));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(out));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//提交任务</span></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> b ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">JoinMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">JoinMain</span>&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            FileSplit fileSplit = (FileSplit) context.getInputSplit();</span><br><span class="line">            name = fileSplit.getPath().getName();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//做一个传入的表的判断</span></span><br><span class="line">            <span class="keyword">if</span> (name.contains(<span class="string">"emp"</span>))&#123;  <span class="comment">//emp</span></span><br><span class="line">                <span class="comment">//读取数据</span></span><br><span class="line">                String[] lines = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">                <span class="keyword">if</span> (lines.length ==<span class="number">8</span>)&#123;</span><br><span class="line">                    <span class="comment">//细粒度划分</span></span><br><span class="line">                    Integer empno = Integer.parseInt(lines[<span class="number">0</span>].trim());</span><br><span class="line">                    String ename = lines[<span class="number">1</span>];</span><br><span class="line">                    Integer deptno = Integer.parseInt(lines[lines.length-<span class="number">1</span>].trim());</span><br><span class="line">                    <span class="comment">//写入数据</span></span><br><span class="line">                    context.write(<span class="keyword">new</span> IntWritable(deptno),<span class="keyword">new</span> JoinMain(empno,ename,deptno,<span class="string">""</span>,<span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;      <span class="comment">//dept</span></span><br><span class="line">                <span class="comment">//读取数据</span></span><br><span class="line">                String[] lines = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">                <span class="keyword">if</span> (lines.length ==<span class="number">3</span>)&#123;</span><br><span class="line">                    <span class="keyword">int</span> deptno = Integer.parseInt(lines[<span class="number">0</span>].trim());</span><br><span class="line">                    String dname = lines[<span class="number">1</span>];</span><br><span class="line">                    context.write(<span class="keyword">new</span> IntWritable(deptno),<span class="keyword">new</span> JoinMain(<span class="number">0</span>, <span class="string">""</span>, deptno, dname, <span class="number">2</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">super</span>.cleanup(context);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">JoinReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">IntWritable</span>, <span class="title">JoinMain</span>,<span class="title">JoinMain</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="comment">//核心思路在 每个deptno组 进一次reduce ，前提是map中的key是deptno</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(IntWritable key, Iterable&lt;JoinMain&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            List&lt;JoinMain&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            String dname=<span class="string">""</span>;</span><br><span class="line">            <span class="comment">// 1.取出map中每行数据，判断flag值</span></span><br><span class="line">            <span class="comment">// 2.取出所有的emp中数据放入list中</span></span><br><span class="line">            <span class="comment">// 3.取出dept中的dname赋值给变量</span></span><br><span class="line">            <span class="comment">// 4.取出属于这个deptno中的所有数据，并给dname赋值</span></span><br><span class="line">            <span class="comment">// 5.每条赋值dname的数据写入reduce</span></span><br><span class="line">            <span class="keyword">for</span> (JoinMain main : values)&#123;</span><br><span class="line">                <span class="comment">// emp表</span></span><br><span class="line">                <span class="keyword">if</span> (main.getFlag() == <span class="number">1</span>)&#123;</span><br><span class="line">                    <span class="comment">//给emp表全部行重新赋值</span></span><br><span class="line">                    JoinMain m = <span class="keyword">new</span> JoinMain();</span><br><span class="line">                    m.setDeptno(main.getDeptno());</span><br><span class="line">                    m.setEmpno(main.getEmpno());</span><br><span class="line">                    m.setEname(main.getEname());</span><br><span class="line">                    <span class="comment">//写出到list</span></span><br><span class="line">                    list.add(m);</span><br><span class="line">                &#125;<span class="keyword">else</span> <span class="keyword">if</span> (main.getFlag() ==<span class="number">2</span> )&#123; <span class="comment">//dept</span></span><br><span class="line">                    <span class="comment">//拿到dept表中的dname</span></span><br><span class="line">                dname = main.getDname();</span><br><span class="line">            &#125;&#125;</span><br><span class="line">            <span class="comment">//循环赋值</span></span><br><span class="line">            <span class="keyword">for</span> (JoinMain bean : list) &#123;</span><br><span class="line">                bean.setDname(dname);</span><br><span class="line">                context.write(bean,NullWritable.get());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="大小表Map-Join-emp、dept"><a href="#大小表Map-Join-emp、dept" class="headerlink" title="大小表Map Join(emp、dept)"></a>大小表Map Join(emp、dept)</h2><p>Map Join的核心思想是把小表添加到缓存中(Map中)，在map中读取大表每行数据时set到对象值时取出小表(Map)对应key的值即可</p>
<ol>
<li><p>setup中，通过context获取小表文件切片的路径，然后通过读取流的方式读取为字符，按行获取到字符后切分，使用HashMap结构设置key和value分别为map方法中大表join时需要的键和值。</p>
</li>
<li><p>在map方法中读取文件数据，并且根据key取出HashMap(小表)中的value，一起set到对象中即可，最后写出，写出时，可以把value设置为NullWritable。</p>
<p>参考代码:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.hadoop.join;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.pojo.JoinMain;</span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.utils.FileUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"><span class="keyword">import</span> org.codehaus.groovy.runtime.wrappers.LongWrapper;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-02-01 23:10</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapJoinDriver</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String in = <span class="string">"data/join/emp.txt"</span>;</span><br><span class="line">    <span class="keyword">private</span> String out = <span class="string">"out"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(conf, <span class="keyword">new</span> MapJoinDriver(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> strings</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> : int</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@describe</span> : 设置配置文件，不用设置reduce</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@author</span> : tunan</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@date</span> : 2020/2/1 23:14</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">super</span>.getConf();</span><br><span class="line">        FileUtil.checkFileIsExists(conf,out);</span><br><span class="line"></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(MapJoinDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"data/join/dept.txt"</span>));</span><br><span class="line"></span><br><span class="line">        job.setMapperClass(MapperJoin<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(JoinMain<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(in));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(out));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> b ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MapperJoin</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">JoinMain</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> HashMap&lt;Integer, String&gt; hashMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//得到缓存文件路径</span></span><br><span class="line">            <span class="comment">//String path = context.getCacheFiles()[0].getPath().toString();</span></span><br><span class="line">            String path = context.getCacheFiles()[<span class="number">0</span>].getPath();</span><br><span class="line">            <span class="comment">/*URI[] files = context.getCacheFiles();   //URI 通过getPath()解码 没有toString()方法</span></span><br><span class="line"><span class="comment">            String s = files[0].getPath();*/</span></span><br><span class="line">            <span class="comment">//得到文件</span></span><br><span class="line">            <span class="comment">//File file = new File(cacheFiles[0]);</span></span><br><span class="line">            <span class="comment">//String path = file.getPath();</span></span><br><span class="line">            <span class="comment">//得到文件的流        InputStreamReader将字节转换为字符</span></span><br><span class="line">            BufferedReader br = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(<span class="keyword">new</span> FileInputStream(path)));</span><br><span class="line">            <span class="comment">//读取文件为字符串</span></span><br><span class="line">            String line ;</span><br><span class="line">            <span class="keyword">while</span>(StringUtils.isNotEmpty(line=br.readLine()))&#123;</span><br><span class="line">                <span class="comment">//切分字符串得到字符串数组</span></span><br><span class="line">                String[] split = line.split(<span class="string">"\t"</span>);</span><br><span class="line">                hashMap.put(Integer.parseInt(split[<span class="number">0</span>]),split[<span class="number">1</span>]);</span><br><span class="line">            &#125;</span><br><span class="line">            IOUtils.closeStream(br);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@describe</span> :</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@author</span> : tunan</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> key</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@return</span> : void</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@date</span> : 2020/2/1 23:38</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            String[] line = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">            <span class="keyword">if</span> (line.length &gt;= <span class="number">8</span>)&#123;</span><br><span class="line">                Integer empno = Integer.parseInt(line[<span class="number">0</span>].trim());</span><br><span class="line">                String ename = line[<span class="number">1</span>];</span><br><span class="line">                Integer deptno = Integer.parseInt(line[line.length-<span class="number">1</span>].trim());</span><br><span class="line">                String dname = hashMap.get(deptno);</span><br><span class="line">                context.write(<span class="keyword">new</span> JoinMain(empno,ename,deptno,dname),NullWritable.get());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">super</span>.cleanup(context);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/13/hadoop/10/">InputFormat&amp;Partitioner&amp;Conbiner&amp;Sort&amp;OutputFormat</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-13</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/MapReduce/">MapReduce</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/MapReduce/">MapReduce</a></span><div class="content"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>InputFormat</li>
<li>Partitioner</li>
<li>Conbiner</li>
<li>Sort</li>
<li>OutputFormat</li>
</ol>
<h2 id="InputFormat"><a href="#InputFormat" class="headerlink" title="InputFormat"></a>InputFormat</h2><p>在数据进入map之前，会进过一系列的格式化操作</p>
<ol>
<li>在客户端submitJob()方法提交作业前，会获取配置信息，形成一个任务分配的规划</li>
<li>提交文件分片(文件夹)和应用程序jar包</li>
<li>MR运行MapTask根据InputFormat读取文件，这里将详细介绍InputFormat</li>
</ol>
<p>InputFormat是一个抽象类，MR默认是用TextInputFormat方法读取文件</p>
<p>TextInputFormat是按行读取文件中的数据，实际上TextInputFormat中只实现了createRecordReader()和isSplitable()两个方法，它的具体实现在FileInputFormat中就已经实现的，FileInputFormat也是一个抽象类。</p>
<p>NLineInputFormat也继承于FileInputFormat，它的特点是按特定的行数读取数据</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 设置指定的InputFormat(重点)</span></span><br><span class="line">NLineInputFormat.setNumLinesPerSplit(job, <span class="number">3</span>);</span><br><span class="line">job.setInputFormatClass(NLineInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>

<p>DBInputFormat继承于DBInputFormat的同时实现了InputFormat，这个方法可以从数据库中读取数据，写入HDFS，类似于Sqoop，需要注意的是它的实体类要同时继承DBWritable和Writable，提交到HDFS上执行的时候需要指定jdbc的jar包(不推介使用)。</p>
<h2 id="Partitioner"><a href="#Partitioner" class="headerlink" title="Partitioner"></a>Partitioner</h2><p>MR的默认分区规则是按照key分区，相同的key到一个reduce方法中去，<strong>Partitoner可以自定义分区规则</strong>，自定义类继承Partitioner&lt;Text, Flow&gt;，泛型是map输出的key和value类型</p>
<p>参考代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Partitoner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">Flow</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, Flow flow, <span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">        String phone = text.toString();</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"137"</span>.equals(phone.substring(<span class="number">0</span>,<span class="number">3</span>)))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"136"</span>.equals(phone.substring(<span class="number">0</span>,<span class="number">3</span>)))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>要在Driver中指定Partitioner的类，并且指定reduce的个数，这里的<strong>reduce设置的个数一定要和Partitoner分区中返回的分区个数相同</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 设置Partitoner</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">3</span>);</span><br><span class="line">job.setPartitionerClass(Partitoner<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>

<p>如果reduce设置的数量大于分区个数，则会产生空的输出文件，即空的reduce。</p>
<p>如果reduce设置的个数小于分区个数，则会报错，表示多余的数据没有分区可去。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">java.io.IOException: Illegal partition <span class="keyword">for</span> <span class="number">13826544101</span> (<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Conbiner"><a href="#Conbiner" class="headerlink" title="Conbiner"></a>Conbiner</h2><p>Conbiner是合并，即是map阶段的reduce，可以自定义，也可以直接使用reduce方法，需要在Driver中指定，需要注意的是不能改变业务逻辑(不适用于乘积)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//设置conbiner</span></span><br><span class="line">job.setCombinerClass(FlowReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>

<h2 id="Sort"><a href="#Sort" class="headerlink" title="Sort"></a>Sort</h2><p>Sort是分区排序，需要知道的是MR的key默认是有序的，如果要自定义排序规则需要将实体类实现<code>WritableComparable&lt;FlowSort&gt;</code>接口，泛型就传入实体类的类名，WritableComparable实际上是继承了Writable和Comparable<T>，Writable是Hadoop自己实现的，Comparable是Java中的类</p>
<p>实体类参考代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowSort o)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Long.compare(o.sum, <span class="keyword">this</span>.sum);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>需要注意的是使用自定义排序的实体类要放到mapreduce方法key的位置，使之有序。</p>
<p>主要注意的是Sort是每个reduce中有序，如果设置了多个reduce，则只能保证每个reduce内部有序</p>
<h2 id="OutputFormat"><a href="#OutputFormat" class="headerlink" title="OutputFormat"></a>OutputFormat</h2><p>有一类很常见的需求：按照一定的规则把数据给我写到某个文件中去</p>
<p>OutputFormat是一个接口，实现它的类有FileOutputFormat和DBOutputFormat，使用和InputFormat差不多，用的不多，不写了</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/12/hadoop/9/">Spark、IDEA和Maven的环境准备&amp;Hadoop的依赖以及常用API&amp;WordCount Debug流程&amp;map、reduce方法的参数类型和作用&amp;瘦包在服务器上的jar包依赖</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-12</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/MapReduce/">MapReduce</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/MapReduce/">MapReduce</a></span><div class="content"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>Spark、IDEA和Maven的环境准备</li>
<li>hadoop的依赖以及常用API</li>
<li>WordCount Debug流程</li>
<li>map、reduce方法的参数类型和作用</li>
<li>Writable和WritableComparable的作用</li>
<li>瘦包在服务器上的jar包依赖</li>
</ol>
<h2 id="Spark、IDEA和Maven的环境准备"><a href="#Spark、IDEA和Maven的环境准备" class="headerlink" title="Spark、IDEA和Maven的环境准备"></a>Spark、IDEA和Maven的环境准备</h2><p>环境:</p>
<ol>
<li>Spark3.0</li>
<li>IDEA19.3</li>
<li>Maven3.6.3(安装配置阿里云的镜像)</li>
</ol>
<h2 id="Hadoop的依赖以及常用API"><a href="#Hadoop的依赖以及常用API" class="headerlink" title="Hadoop的依赖以及常用API"></a>Hadoop的依赖以及常用API</h2><p>依赖:</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.6.0-cdh5.16.2&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p>常用API:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">FileSystem fileSystem; <span class="comment">//核心</span></span><br><span class="line">open()	<span class="comment">//打开文件返回流</span></span><br><span class="line">mkdirs()	<span class="comment">//创建目录</span></span><br><span class="line">create()	<span class="comment">//创建文件</span></span><br><span class="line">copyFromLocalFile()	<span class="comment">//从本地复制文件到hdfs，类似于get</span></span><br><span class="line">copyToLocalFile()	<span class="comment">//从hdfs复制文件到本地，类似于put</span></span><br><span class="line">listFiles()	<span class="comment">//列出目录下的所有文件，可以迭代</span></span><br><span class="line">delete()	<span class="comment">//删除文件，不存在报错</span></span><br><span class="line">deleteOnExit()	<span class="comment">//删除存在的文件,不存在不报错</span></span><br></pre></td></tr></table></figure>

<h2 id="WordCount-Debug流程"><a href="#WordCount-Debug流程" class="headerlink" title="WordCount Debug流程"></a>WordCount Debug流程</h2><ol>
<li><p>编译</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">job.waitForCompletion(<span class="keyword">true</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>提交</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (state == JobState.DEFINE) &#123;</span><br><span class="line">   submit();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>兼容新老API</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">setUseNewAPI();</span><br></pre></td></tr></table></figure>
</li>
<li><p>本地连接/服务器连接</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">connect();</span><br></pre></td></tr></table></figure>
</li>
<li><p>检查配置、输出路径等</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> JobSubmitter submitter = getJobSubmitter(cluster.getFileSystem(),cluster.getClient());</span><br><span class="line"><span class="function"><span class="keyword">public</span> JobStatus <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException,ClassNotFoundException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster);</span><br><span class="line">&#125;</span><br><span class="line">checkSpecs(job);	<span class="comment">//validate the jobs output specs</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>把该作业的配置信息加到分布式缓存中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Configuration conf = job.getConfiguration();</span><br><span class="line">addMRFrameworkToDistributedCache(conf);</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建该Job对应的存放目录</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br></pre></td></tr></table></figure>
</li>
<li><p>拿到该Job对应的ID(local/application)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">JobID jobId = submitClient.getNewJobID();</span><br></pre></td></tr></table></figure>
</li>
<li><p>jobStagingArea/jobid</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Path submitJobDir = <span class="keyword">new</span> Path(jobStagingArea, jobId.toString());</span><br></pre></td></tr></table></figure>
</li>
<li><p>拷贝job对应的信息到jobStagingArea/jobid</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">copyAndConfigureFiles(job, submitJobDir);</span><br></pre></td></tr></table></figure>
</li>
<li><p>完成我们输入数据的切片(默认128MB，预留10%浮动空间)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> maps = writeSplits(job, submitJobDir);</span><br></pre></td></tr></table></figure>
</li>
<li><p>作业文件提交到指定目录</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">writeConf(conf, submitJobFile);</span><br></pre></td></tr></table></figure>
</li>
<li><p>提交作业</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="map、reduce方法的参数类型和作用"><a href="#map、reduce方法的参数类型和作用" class="headerlink" title="map、reduce方法的参数类型和作用"></a>map、reduce方法的参数类型和作用</h2><ul>
<li><p>继承Mapper后实现map方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br></pre></td></tr></table></figure>

<p>该方法中的参数分别是<code>LongWritable key, Text value, Context context</code></p>
<p>前两个参数是map方法中输入的键和值，输入的键和值必须是LongWritable类型和Text类型，因为是按offset读取每行本文数据，最后一个参数context，它是MapReduce的上下文。什么都可以拿出来。</p>
</li>
<li><p>继承Reducer后实现reduce方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span></span></span><br></pre></td></tr></table></figure>

<p>该方法中的参数分别是<code>Text key, Iterable&lt;IntWritable&gt; values, Context context</code></p>
<p>前两个参数是reduce方法中输入的键和值，输入的键和值对应map中输出的键值类型，并且值是一个Iterable类型，因为在shuffle阶段相同key的value分到了一起，是一个可迭代的参数。因为是按offset读取每行本文数据，最后一个参数context，它是MapReduce的上下文。什么都可以拿出来。</p>
</li>
</ul>
<h2 id="Writable和WritableComparable的作用"><a href="#Writable和WritableComparable的作用" class="headerlink" title="Writable和WritableComparable的作用"></a>Writable和WritableComparable的作用</h2><p>Writable是hadoop中的序列化接口，是一个接口，只定义了两个方法，分别是<code>write()</code>和<code>readFields()</code>方法，用于hadoop序列化时的读和写；<code>WritableComparable</code>也是一个序列化接口，只是在序列化的同时同时实现了java中的<code>Comparable&lt;T&gt;</code>接口，具有排序的特性。</p>
<p>hadoop是java写的，那么为什么hadoop要实现自己的序列化接口</p>
<ul>
<li>java序列化数据结果比较大、传输效率比较低、不能跨语言对接</li>
</ul>
<p>hadoop使用的是RPC协议传送数据，且hadoop是应用在大集群上，所以hadoop的序列化必须做到</p>
<ul>
<li>占用空间更小</li>
<li>传输速度更快</li>
<li>扩展性更强，支持多种格式的序列化</li>
<li>兼容性更好，需要支持多种语言，如java、scala、python等</li>
</ul>
<p>所以hadoop实现了自己的序列化接口Writable：<code>压缩</code>、<code>速度</code>、<code>扩展性</code>、<code>兼容性</code>都比java更优秀</p>
<p>另外:</p>
<ol>
<li><em>序列化的对象，他们超越了JVM的生死，不顾生他们的母亲，化作永恒。</em>static和transient修饰的属性除外，因为static修饰的属性是在编译时静态生成的，而对象是动态生成的，又因为transient修饰的属性禁止了属性的序列化。</li>
<li><em>把“活的”对象序列化，就是把“活的”对象转化成一串字节，而“反序列化”，就是从一串字节里解析出“活的”对象。</em></li>
</ol>
<h2 id="瘦包在服务器上的jar包依赖"><a href="#瘦包在服务器上的jar包依赖" class="headerlink" title="瘦包在服务器上的jar包依赖"></a>瘦包在服务器上的jar包依赖</h2><p>打包好的mapreduce程序上传到云主机，由于是瘦包，缺少某些依赖，比如连接mysql的的jar包，现在我们就解决缺少依赖的问题</p>
<ol>
<li><p>将下载好的jar包上传到云主机上</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">cp mysql-connector-java-<span class="number">5.1</span><span class="number">.27</span>-bin.jar ~/lib/</span><br></pre></td></tr></table></figure>
</li>
<li><p>将jar包加载到hadoop的classpath中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:~/lib/mysql-connector-java-<span class="number">5.1</span><span class="number">.27</span>-bin.jar</span><br></pre></td></tr></table></figure>
</li>
<li><p>用hadoop jar 执行jar文件时，加上-libjars参数</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop jar hadoop-mapreduce-examples-<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.16</span><span class="number">.2</span>.jar wordcount -libjars /home/hadoop/lib/mysql-connector-java-<span class="number">5.1</span><span class="number">.27</span>-bin.jar  /<span class="number">1</span>.txt /out</span><br></pre></td></tr></table></figure>


</li>
</ol>
<p>如果上诉方法有问题可以使用hadoop的分布式缓存</p>
<ol>
<li><p>把jar包传到集群上，命令如下</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop fs -put mysql-connector-java-<span class="number">5.1</span><span class="number">.27</span>.jar /lib</span><br></pre></td></tr></table></figure>
</li>
<li><p>在mr程序提交job前，添加一下语句：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">job.addArchiveToClassPath(<span class="keyword">new</span> Path(<span class="string">"hdfs://aliyun:9000/lib/mysql-connector-java-5.1.27.jar"</span>));</span><br></pre></td></tr></table></figure>

</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/11/hadoop/8/">YARN的调优&amp;YARN的三种调度器</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-11</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/Yarn/">Yarn</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Yarn/">Yarn</a></span><div class="content"><h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><ol>
<li>梳理YARN资源调优参数</li>
<li>调度器整理三种，区别是什么，CDH默认是什么</li>
</ol>
<h3 id="YARN的资源调优"><a href="#YARN的资源调优" class="headerlink" title="YARN的资源调优"></a>YARN的资源调优</h3><p><code>背景:</code> 假设每台服务器拥有内存128G 16物理core，怎么分配？</p>
<ol>
<li><p>装完CentOS，消耗内存1G</p>
</li>
<li><p>系统预览15%-20%内存(包含1.1)，以防全部使用导致系统夯住 和 oom机制事件，或者给未来部署组件预览点空间(<code>128*20%=25.6G==26G</code>)</p>
</li>
<li><p>假设只有DN NM节点，余下内存: <code>128-26=102G</code></p>
<ol>
<li><p>给DN进程(自身)2G，给NM进程(自身)4G，剩余<code>102-2-4=96G</code></p>
</li>
<li><p>container内存的分配共96G</p>
<ul>
<li><p><code>yarn.nodemanager.resource.memory-mb</code>     共 96G</p>
</li>
<li><p><code>yarn.scheduler.minimum-allocation-mb</code>    最少1G    极限情况下，只有96个container 内存1G</p>
</li>
<li><p><code>yarn.scheduler.maximum-allocation-mb</code>    最多96G  极限情况下，只有1个container 内存96G</p>
<p>container的内存会自动增加，默认1G递增，那么container的个数的范围: 1-96个</p>
</li>
</ul>
</li>
<li><p>container物理核分配 (物理核:虚拟核 =1:2 ==&gt;16:32)</p>
<ul>
<li><p><code>yarn.nodemanager.resource.cpu-vcores</code>            共 32个</p>
</li>
<li><p><code>yarn.scheduler.minimum-allocation-vcores</code>     最少1个   极限情况下，只有32个container    </p>
</li>
<li><p><code>yarn.scheduler.maximum-allocation-vcores</code>     最多32个 极限情况下，只有1个container</p>
<p>container的物理核会自动增加，默认1个递增，那么container的个数的范围: :1-32个</p>
</li>
</ul>
</li>
<li><p><code>关键:</code> cloudera公司推荐，一个container的vcore最好不要超过5，那么我们设置4</p>
<ul>
<li><p><code>yarn.scheduler.maximum-allocation-vcores</code>    4   </p>
<p>目前为止，极限情况下，共有8个container (32/4)</p>
</li>
</ul>
</li>
<li><p>综合memory+vcore的分配</p>
<ol>
<li><p>一共有32个vcore，一个container的vcore是4个，那么分配container一共有8个</p>
</li>
<li><p>重新分配核</p>
<ul>
<li><p><code>yarn.nodemanager.resource.cpu-vcores</code>            共32个</p>
</li>
<li><p><code>yarn.scheduler.minimum-allocation-vcores</code>    最少4个    </p>
</li>
<li><p><code>yarn.scheduler.maximum-allocation-vcores</code>    最多4个   极限container 8个</p>
</li>
</ul>
</li>
<li><p>根据物理核重新分配内存</p>
<ul>
<li><p><code>yarn.nodemanager.resource.memory-mb</code>      共96G</p>
</li>
<li><p><code>yarn.scheduler.minimum-allocation-mb</code>    最少12G  </p>
</li>
<li><p><code>yarn.scheduler.maximum-allocation-mb</code>    最多12G   (极限container 8个)</p>
</li>
</ul>
</li>
<li><p>分配后的每个container的物理核数是4个，内存大小是12G，当然spark计算时内存不够大，这个参数肯定要调大，那么这种理想化的设置个数必然要打破，以memory为主</p>
</li>
</ol>
</li>
<li><p>假如 256G内存 56core，请问参数如何设置</p>
<ol>
<li><p>首先减去系统内存开销和其他进程开销，</p>
<ul>
<li><p>系统开销: 256*0.2=52G</p>
</li>
<li><p>DN开销: 2G</p>
</li>
<li><p>NM开销: 4G</p>
</li>
<li><p>Hbase开销: 暂无</p>
</li>
<li><p>升序内存容量: 256-52-2-4=198G</p>
</li>
</ul>
</li>
<li><p>确定每个container的物理核数量是4个，56/4=14个container容器</p>
</li>
<li><p>确定了最多分配14个container容器，每个容器的内存应该分配的容量是: 198/14==&gt;14G</p>
<p><strong>那么每个container的最大核数设置4，最大内存数设置14G</strong></p>
</li>
</ol>
</li>
<li><p>假如该节点还有组件，比如hbase regionserver进程，那么该如何设置？</p>
<p>总容量减就完事了。    </p>
</li>
</ol>
<p>所有的配置信息在<code>yarn-default.xm</code>l文件中</p>
<h4 id="内存参数默认值"><a href="#内存参数默认值" class="headerlink" title="内存参数默认值:"></a>内存参数默认值:</h4><table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>yarn.nodemanager.resource.memory-mb</td>
<td>-1</td>
<td>可以分配给容器的物理内存总量(以MB为单位)。</td>
</tr>
<tr>
<td>yarn.scheduler.minimum-allocation-mb</td>
<td>1024</td>
<td>RM上每个容器请求的最小分配</td>
</tr>
<tr>
<td>yarn.scheduler.maximum-allocation-mb</td>
<td>8192</td>
<td>RM上每个容器请求的最大分配</td>
</tr>
</tbody></table>
<h4 id="核数参数默认值"><a href="#核数参数默认值" class="headerlink" title="核数参数默认值:"></a>核数参数默认值:</h4><table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>yarn.nodemanager.resource.cpu-vcores</td>
<td>-1</td>
<td>可以为容器分配的vcore总数量。</td>
</tr>
<tr>
<td>yarn.scheduler.minimum-allocation-vcores</td>
<td>1</td>
<td>RM上每个容器请求的最小虚拟CPU核心分配。</td>
</tr>
<tr>
<td>yarn.scheduler.maximum-allocation-vcores</td>
<td>4</td>
<td>RM上每个容器请求的最大虚拟CPU核心分配。</td>
</tr>
</tbody></table>
</li>
</ol>
<h3 id="Yarn的三种调度器"><a href="#Yarn的三种调度器" class="headerlink" title="Yarn的三种调度器"></a>Yarn的三种调度器</h3><ul>
<li><p>Apache hadoop2.x的默认调度器是Capacity Scheduler(计算调度器)</p>
</li>
<li><p>CDH的默认调度器是Fair Scheduler(公平调度器)</p>
</li>
</ul>
<h4 id="Yarn三种调度策略对比"><a href="#Yarn三种调度策略对比" class="headerlink" title="Yarn三种调度策略对比"></a>Yarn三种调度策略对比</h4><p>在Yarn中有三种调度器可以选择：FIFO Scheduler ，Capacity Scheduler，FairScheduler。</p>
<ol>
<li><p>FIFO Scheduler</p>
<p><img src="https://img-blog.csdnimg.cn/20181101090612286.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MzgyNDUz,size_16,color_FFFFFF,t_70" alt="队列调度器"></p>
<p>FIFO Scheduler把应用按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的应用进行分配资源，待最头上的应用需求满足后再给下一个分配，以此类推。</p>
<p>FIFO Scheduler是最简单也是最容易理解的调度器，也不需要任何配置，但它并不适用于共享集群。大的应用可能会占用所有集群资源，这就导致其它应用被阻塞。在共享集群中，更适合采用Capacity Scheduler或Fair Scheduler，这两个调度器都允许大任务和小任务在提交的同时获得一定的系统资源。从图中可以看出，在FIFO 调度器中，小任务会被大任务阻塞。</p>
</li>
<li><p>Capacity Scheduler</p>
<p><img src="https://img-blog.csdnimg.cn/20181101091012607.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MzgyNDUz,size_16,color_FFFFFF,t_70" alt="计算调度器"></p>
<p>而对于Capacity调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间。</p>
</li>
<li><p>Fair Scheduler</p>
<p><img src="https://img-blog.csdnimg.cn/20181101091843173.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MzgyNDUz,size_16,color_FFFFFF,t_70" alt="公平调度器"></p>
<p>在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。如上图所示，当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。</p>
<p>需要注意的是，在上图Fair调度器中，从第二个任务提交到获得资源会有一定的延迟，因为它需要等待第一个任务释放占用的Container。小任务执行完成之后也会释放自己占用的资源，大任务又获得了全部的系统资源。最终的效果就是Fair调度器即得到了高的资源利用率又能保证小任务及时完成。</p>
</li>
</ol>
<h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn application -kill &lt;Application ID&gt;	#杀死进程</span><br></pre></td></tr></table></figure>

</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/10/PE/2/">HDFS Block损坏恢复</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-10</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E7%94%9F%E4%BA%A7%E6%95%85%E9%9A%9C%E6%A1%88%E4%BE%8B/">生产故障案例</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E7%94%9F%E4%BA%A7%E6%95%85%E9%9A%9C%E6%A1%88%E4%BE%8B/">生产故障案例</a></span><div class="content"><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">上传:</span><br><span class="line">-bash-4.2$ hdfs dfs -mkdir /blockrecover</span><br><span class="line">-bash-4.2$ echo &quot;www.ruozedata.com&quot; &gt; ruozedata.md</span><br><span class="line"></span><br><span class="line">-bash-4.2$ hdfs dfs -put ruozedata.md /blockrecover</span><br><span class="line">-bash-4.2$ hdfs dfs -ls /blockrecover</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 hdfs supergroup         18 2019-03-03 14:42 /blockrecover/ruozedata.md</span><br><span class="line">-bash-4.2$ </span><br><span class="line"></span><br><span class="line">校验: 健康状态</span><br><span class="line">-bash-4.2$ hdfs fsck /</span><br><span class="line">Connecting to namenode via http://yws76:50070/fsck?ugi=hdfs&amp;path=%2F</span><br><span class="line">FSCK started by hdfs (auth:SIMPLE) from /192.168.0.76 for path / at Sun Mar 03 14:44:44 CST 2019</span><br><span class="line">...............................................................................Status: HEALTHY</span><br><span class="line"> Total size:    50194618424 B</span><br><span class="line"> Total dirs:    354</span><br><span class="line"> Total files:   1079</span><br><span class="line"> Total symlinks:                0</span><br><span class="line"> Total blocks (validated):      992 (avg. block size 50599413 B)</span><br><span class="line"> Minimally replicated blocks:   992 (100.0 %)</span><br><span class="line"> Over-replicated blocks:        0 (0.0 %)</span><br><span class="line"> Under-replicated blocks:       0 (0.0 %)</span><br><span class="line"> Mis-replicated blocks:         0 (0.0 %)</span><br><span class="line"> Default replication factor:    3</span><br><span class="line"> Average block replication:     3.0</span><br><span class="line"> Corrupt blocks:                0</span><br><span class="line"> Missing replicas:              0 (0.0 %)</span><br><span class="line"> Number of data-nodes:          3</span><br><span class="line"> Number of racks:               1</span><br><span class="line">FSCK ended at Sun Mar 03 14:44:45 CST 2019 in 76 milliseconds</span><br><span class="line"></span><br><span class="line">The filesystem under path &apos;/&apos; is HEALTHY</span><br><span class="line">-bash-4.2$</span><br></pre></td></tr></table></figure>

<h3 id="直接DN节点上删除文件一个block的一个副本-3副本"><a href="#直接DN节点上删除文件一个block的一个副本-3副本" class="headerlink" title="直接DN节点上删除文件一个block的一个副本(3副本)"></a>直接DN节点上删除文件一个block的一个副本(3副本)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">删除块和meta文件:</span><br><span class="line">[root@yws87 subdir135]# rm -rf blk_1075808214 blk_1075808214_2068515.meta</span><br><span class="line"></span><br><span class="line">直接重启HDFS，直接模拟损坏效果，然后fsck检查:</span><br><span class="line">-bash-4.2$ hdfs fsck /</span><br><span class="line">Connecting to namenode via http://yws77:50070/fsck?ugi=hdfs&amp;path=%2F</span><br><span class="line">FSCK started by hdfs (auth:SIMPLE) from /192.168.0.76 for path / at Sun Mar 03 16:02:04 CST 2019</span><br><span class="line">.</span><br><span class="line">/blockrecover/ruozedata.md:  Under replicated BP-1513979236-192.168.0.76-1514982530341:blk_1075808214_2068515. Target Replicas is 3 but found 2 live replica(s), 0 decommissioned replica(s), 0 decommissioning replica(s).</span><br><span class="line">...............................................................................Status: HEALTHY</span><br><span class="line"> Total size:    50194618424 B</span><br><span class="line"> Total dirs:    354</span><br><span class="line"> Total files:   1079</span><br><span class="line"> Total symlinks:                0</span><br><span class="line"> Total blocks (validated):      992 (avg. block size 50599413 B)</span><br><span class="line"> Minimally replicated blocks:   992 (100.0 %)</span><br><span class="line"> Over-replicated blocks:        0 (0.0 %)</span><br><span class="line"> Under-replicated blocks:       1 (0.10080645 %)</span><br><span class="line"> Mis-replicated blocks:         0 (0.0 %)</span><br><span class="line"> Default replication factor:    3</span><br><span class="line"> Average block replication:     2.998992</span><br><span class="line"> Corrupt blocks:                0</span><br><span class="line"> Missing replicas:              1 (0.033602152 %)</span><br><span class="line"> Number of data-nodes:          3</span><br><span class="line"> Number of racks:               1</span><br><span class="line">FSCK ended at Sun Mar 03 16:02:04 CST 2019 in 148 milliseconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The filesystem under path &apos;/&apos; is HEALTHY</span><br><span class="line">-bash-4.2$</span><br></pre></td></tr></table></figure>

<h3 id="手动修复hdfs-debug"><a href="#手动修复hdfs-debug" class="headerlink" title="手动修复hdfs debug"></a>手动修复hdfs debug</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-bash-4.2$ hdfs |grep debug</span><br><span class="line">没有输出debug参数的任何信息结果！</span><br><span class="line">故hdfs命令帮助是没有debug的，但是确实有hdfs debug这个组合命令，切记。</span><br><span class="line"></span><br><span class="line">修复命令:</span><br><span class="line">-bash-4.2$ hdfs debug  recoverLease  -path /blockrecover/ruozedata.md -retries 10</span><br><span class="line">recoverLease SUCCEEDED on /blockrecover/ruozedata.md</span><br><span class="line">-bash-4.2$ </span><br><span class="line"></span><br><span class="line">直接DN节点查看，block文件和meta文件恢复:</span><br><span class="line">[root@yws87 subdir135]# ll</span><br><span class="line">total 8</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 56 Mar  3 14:28 blk_1075808202</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 14:28 blk_1075808202_2068503.meta</span><br><span class="line">[root@yws87 subdir135]# ll</span><br><span class="line">total 24</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 56 Mar  3 14:28 blk_1075808202</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 14:28 blk_1075808202_2068503.meta</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 18 Mar  3 15:23 blk_1075808214</span><br><span class="line">-rw-r--r-- 1 hdfs hdfs 11 Mar  3 15:23 blk_1075808214_2068515.meta</span><br></pre></td></tr></table></figure>

<h3 id="自动修复"><a href="#自动修复" class="headerlink" title="自动修复"></a>自动修复</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">当数据块损坏后，DN节点执行directoryscan操作之前，都不会发现损坏；</span><br><span class="line">也就是directoryscan操作是间隔6h</span><br><span class="line">dfs.datanode.directoryscan.interval : 21600</span><br><span class="line"></span><br><span class="line">在DN向NN进行blockreport前，都不会恢复数据块;</span><br><span class="line">也就是blockreport操作是间隔6h</span><br><span class="line">dfs.blockreport.intervalMsec : 21600000</span><br><span class="line"></span><br><span class="line">当NN收到blockreport才会进行恢复操作。</span><br></pre></td></tr></table></figure>

<p>具体参考生产上HDFS（CDH5.12.0）对应的版本的文档参数:<a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.12.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.12.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>生产上本人一般倾向于使用 手动修复方式，但是前提要手动删除损坏的block块。</p>
<p>切记，是删除损坏block文件和meta文件，而不是删除hdfs文件。</p>
<p>当然还可以先把文件get下载，然后hdfs删除，再对应上传。</p>
<p>切记删除不要执行: hdfs fsck / -delete 这是删除损坏的文件， 那么数据不就丢了嘛；除非无所谓丢数据，或者有信心从其他地方可以补数据到hdfs！</p>
<h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><ul>
<li>那么如何确定一个文件的损失的块位置，哪几种方法呢？</li>
<li>CDH的配置里搜索没有这两个参数，怎么调整生效呢？</li>
</ul>
<p>转载来源: [<a href="https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/]" target="_blank" rel="noopener">https://ruozedata.github.io/2019/06/06/%E7%94%9F%E4%BA%A7HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5(%E5%90%AB%E6%80%9D%E8%80%83%E9%A2%98)/]</a>(<a href="https://ruozedata.github.io/2019/06/06/生产HDFS" target="_blank" rel="noopener">https://ruozedata.github.io/2019/06/06/生产HDFS</a> Block损坏恢复最佳实践(含思考题)/)</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/10/hadoop/7/">MR的执行流程&amp;初探文件压缩&amp;初探文件格式&amp;分片数与任务数&amp;shuffle的执行流程&amp;WordCount的执行流程</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-10</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/MapReduce/">MapReduce</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/HDFS/">HDFS</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/MapReduce/">MapReduce</a></span><div class="content"><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ol>
<li>整理 mr on yarn流程 </li>
<li>整理 文件格式有哪些 优缺点 </li>
<li>整理 压缩格式有哪些 优缺点 </li>
<li>spilt–&gt;map task关系 </li>
<li>wordcount的剖解图 </li>
<li>shuffle的理解 </li>
</ol>
<h3 id="mr-on-yarn流程"><a href="#mr-on-yarn流程" class="headerlink" title="mr on yarn流程"></a>mr on yarn流程</h3><p><img src="https://yerias.github.io/hadoop_img/rm_on_yarn.JPG" alt="rm_on_yarn"></p>
<h4 id="mr-on-yarn的工作流程简略分为两步"><a href="#mr-on-yarn的工作流程简略分为两步" class="headerlink" title="mr on yarn的工作流程简略分为两步:"></a>mr on yarn的工作流程简略分为两步:</h4><ol>
<li>启动应用程序管理器，申请资源。</li>
<li>运行任务，直到任务运行完成。</li>
</ol>
<h4 id="mr-on-yarn的工作流程详细分为八步"><a href="#mr-on-yarn的工作流程详细分为八步" class="headerlink" title="mr on yarn的工作流程详细分为八步:"></a>mr on yarn的工作流程详细分为八步:</h4><ol>
<li>用户向资源管理器(ResourceManager)提交作业，作业包括MapReduce应用程序管理器，启动MapReduce应用程序管理器的程序和用户自己编写的MapReduce程序。用于提交的所有作业都由ApplicationManager(全局应用程序管理器)管理。</li>
<li>资源管理器为该应用程序分配一个容器(Container)，并与对应的节点管理器(NodeManager)通信，要求它在这个容器中启动MapReduce应用程序管理器。</li>
<li>MapReduce应用程序管理器首先向资源管理器注册，这样用户可以直接通过资源管理器查看应用程序的运行状态，然后它将为各个任务申请资源，并监控他们的运行状态，直到运行结束，即重复步骤4-7。</li>
<li>MapReduce应用程序管理器采用轮询的方式通过RPC协议向资源管理器申请和领取资源。</li>
<li>MapReduce应用程序管理器申请到资源后，便与对应的节点管理器通信，要求启动任务。</li>
<li>节点管理器为任务设置好运行环境，包括环境变量、Jar包、二进制程序等，然后将任务启动命令写到另外一个脚本中，并通过该脚本启动任务。</li>
<li>各个任务通过RPC协议向MapReduce应用程序管理器汇报自己的状态和进度，MapReduce应用程序随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。在应用程序运行过程中，用户可以随时通过RPC协议向MapReduce应用程序管理器查询应用程序当前的运行状态。</li>
<li>应用程序运行完成后，MapReduce应用程序管理器向资源管理器注销并关闭自己。</li>
</ol>
<h3 id="文件格式有哪些-优缺点"><a href="#文件格式有哪些-优缺点" class="headerlink" title="文件格式有哪些 优缺点"></a>文件格式有哪些 优缺点</h3><p><strong>Hadoop中的文件格式大致上分为面向行和面向列两类：</strong></p>
<ol>
<li><p>面向行：同一行的数据存储在一起，即连续存储。SequenceFile,MapFile,Avro Datafile。采用这种方式，如果只需要访问行的一小部分数据，亦需要将整行读入内存，推迟序列化一定程度上可以缓解这个问题，但是从磁盘读取整行数据的开销却无法避免。面向行的存储适合于整行数据需要同时处理的情况。</p>
</li>
<li><p>面向列：整个文件被切割为若干列数据，每一列数据一起存储。Parquet , RCFile,ORCFile。面向列的格式使得读取数据时，可以跳过不需要的列，适合于只处于行的一小部分字段的情况。但是这种格式的读写需要更多的内存空间，因为需要缓存行在内存中（为了获取多行中的某一列）。同时不适合流式写入，因为一旦写入失败，当前文件无法恢复，而面向行的数据在写入失败时可以重新同步到最后一个同步点，所以Flume采用的是面向行的存储格式。</p>
</li>
</ol>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-0c5b3f7a2eceaaef.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="普通二维表"></p>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-dbe2595ee1e293b1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="行存储和列存储"></p>
<p>下面介绍几种相关的文件格式，它们在Hadoop体系上被广泛使用：</p>
<h4 id="SequenceFile"><a href="#SequenceFile" class="headerlink" title="SequenceFile"></a>SequenceFile</h4><p>SequenceFile是Hadoop API 提供的一种二进制文件,它将数据以的形式序列化到文件中。这种二进制文件内部使用Hadoop 的标准的Writable 接口实现序列化和反序列化。它与Hadoop API中的MapFile 是互相兼容的。Hive 中的SequenceFile 继承自Hadoop API 的SequenceFile,不过它的key为空,使用value 存放实际的值, 这样是为了避免MR 在运行map 阶段的排序过程。如果你用Java API 编写SequenceFile,并让Hive 读取的话,请确保使用value字段存放数据,否则你需要自定义读取这种SequenceFile 的InputFormat class 和OutputFormat class。</p>
<p>SequenceFile的文件结构如下：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-3f5cd8d90742ec24.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="SequenceFile详解图1"></p>
<p>根据是否压缩，以及采用记录压缩还是块压缩，存储格式有所不同：</p>
<ul>
<li><p>不压缩:</p>
<p>按照记录长度、Key长度、Value程度、Key值、Value值依次存储。长度是指字节数。采用指定的Serialization进行序列化。</p>
</li>
<li><p>Record压缩:</p>
<p>只有value被压缩，压缩的codec保存在Header中。</p>
</li>
<li><p>Block压缩:</p>
<p>多条记录被压缩在一起，可以利用记录之间的相似性，更节省空间。Block前后都加入了同步标识。Block的最小值由io.seqfile.compress.blocksize属性设置。 </p>
</li>
</ul>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-d21745547eb4c021.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="SequenceFile详解图1"></p>
<h4 id="Avro"><a href="#Avro" class="headerlink" title="Avro"></a>Avro</h4><p>Avro是一种用于支持数据密集型的二进制文件格式。它的文件格式更为紧凑,若要读取大量数据时,Avro能够提供更好的序列化和反序列化性能。并 且Avro数据文件天生是带Schema定义的,所以它不需要开发者在API 级别实现自己的Writable对象。最近多个Hadoop 子项目都支持Avro 数据格式,如Pig 、Hive、Flume、Sqoop和Hcatalog。</p>
<h4 id="RCFile"><a href="#RCFile" class="headerlink" title="RCFile"></a>RCFile</h4><p>RCFile是Hive推出的一种专门面向列的数据格式。 它遵循“先按列划分,再垂直划分”的设计理念。当查询过程中,针对它并不关心的列时,它会在IO上跳过这些列。需要说明的是,RCFile在map阶段从 远端拷贝仍然是拷贝整个数据块,并且拷贝到本地目录后RCFile并不是真正直接跳过不需要的列,并跳到需要读取的列, 而是通过扫描每一个row group的头部定义来实现的,但是在整个HDFS Block 级别的头部并没有定义每个列从哪个row group起始到哪个row group结束。所以在读取所有列的情况下,RCFile的性能反而没有SequenceFile高。</p>
<p>Hive的Record Columnar File,这种类型的文件先将数据按行划分成Row Group，在Row Group内部，再将数据按列划分存储。其结构如下：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-0a6f19b8bb6ee4e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/688/format/webp" alt="RCFile详解图1"></p>
<p>相比较于单纯地面向行和面向列：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-0df474935c56807d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/649/format/webp" alt="RCFile详解图2"></p>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-6d56c39e3445288e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/615/format/webp" alt="RCFile详解图3"></p>
<h4 id="ORC"><a href="#ORC" class="headerlink" title="ORC"></a>ORC</h4><p>ORC（Optimized Record Columnar File)提供了一种比RCFile更加高效的文件格式。其内部将数据划分为默认大小为250M的Stripe。每个Stripe包括索引、数据和Footer。索引存储每一列的最大最小值，以及列中每一行的位置。</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-1bb66728d866b469.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/580/format/webp" alt="ORCFile详解图"></p>
<p>在Hive中，如下命令用于使用ORCFile：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE ... STORED AS ORC</span><br><span class="line"></span><br><span class="line">ALTER TABLE ... SET FILEFORMAT ORC</span><br><span class="line"></span><br><span class="line">SET hive.default.fileformat=ORC</span><br></pre></td></tr></table></figure>

<h4 id="Parquet"><a href="#Parquet" class="headerlink" title="Parquet"></a>Parquet</h4><p>一种通用的面向列的存储格式，基于Google的Dremel。特别擅长处理深度嵌套的数据。</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-b45e32049ab54cbb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1124/format/webp" alt="img"></p>
<p>对于嵌套结构，Parquet将其转换为平面的列存储，嵌套结构通过Repeat Level和Definition Level来表示（R和D），在读取数据重构整条记录的时候，使用元数据重构记录的结构。下面是R和D的一个例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">AddressBook &#123;</span><br><span class="line"> contacts: &#123;</span><br><span class="line">  phoneNumber: &quot;555 987 6543&quot;</span><br><span class="line"> &#125;</span><br><span class="line"> contacts: &#123;</span><br><span class="line">&#125;&#125;</span><br><span class="line">AddressBook &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-36b68fc1d8e2b99b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/577/format/webp" alt="Parquet详解图"></p>
<h4 id="文件存储大小比较与分析"><a href="#文件存储大小比较与分析" class="headerlink" title="文件存储大小比较与分析"></a>文件存储大小比较与分析</h4><p>我们选取一个TPC-H标准测试来说明不同的文件格式在存储上的开销。因为此数据是公开的,所以读者如果对此结果感兴趣,也可以对照后面的实验自行 做一遍。Orders 表文本格式的原始大小为1.62G。 我们将其装载进Hadoop 并使用Hive 将其转化成以上几种格式,在同一种LZO 压缩模式下测试形成的文件的大小</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/6450093-34e05b3cb0e72740.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/849/format/webp" alt="文件存储大小比较与分析"></p>
<h4 id="不同格式文件大小对比"><a href="#不同格式文件大小对比" class="headerlink" title="不同格式文件大小对比"></a>不同格式文件大小对比</h4><ul>
<li><p>从上述实验结果可以看到,SequenceFile无论在压缩和非压缩的情况下都比原始纯文本TextFile大,其中非压缩模式下大11%, 压缩模式下大6.4%。这跟SequenceFile的文件格式的定义有关: SequenceFile在文件头中定义了其元数据,元数据的大小会根据压缩模式的不同略有不同。一般情况下,压缩都是选取block 级别进行的,每一个block都包含key的长度和value的长度,另外每4K字节会有一个sync-marker的标记。对于TextFile文件格 式来说不同列之间只需要用一个行间隔符来切分,所以TextFile文件格式比SequenceFile文件格式要小。但是TextFile 文件格式不定义列的长度,所以它必须逐个字符判断每个字符是不是分隔符和行结束符。因此TextFile 的反序列化开销会比其他二进制的文件格式高几十倍以上。</p>
</li>
<li><p>RCFile文件格式同样也会保存每个列的每个字段的长度。但是它是连续储存在头部元数据块中,它储存实际数据值也是连续的。另外RCFile 会每隔一定块大小重写一次头部的元数据块(称为row group,由hive.io.rcfile.record.buffer.size控制,其默认大小为4M),这种做法对于新出现的列是必须的,但是如 果是重复的列则不需要。RCFile 本来应该会比SequenceFile 文件大,但是RCFile 在定义头部时对于字段长度使用了Run Length Encoding进行压缩,所以RCFile 比SequenceFile又小一些。Run length Encoding针对固定长度的数据格式有非常高的压缩效率,比如Integer、Double和Long等占固定长度的数据类型。在此提一个特例—— Hive 0.8引入的TimeStamp 时间类型,如果其格式不包括毫秒,可表示为”YYYY-MM-DD HH:MM:SS”,那么就是固定长度占8个字节。如果带毫秒,则表示为”YYYY-MM-DD HH:MM:SS.fffffffff”,后面毫秒的部分则是可变的。</p>
</li>
<li><p>Avro文件格式也按group进行划分。但是它会在头部定义整个数据的模式(Schema), 而不像RCFile那样每隔一个row group就定义列的类型,并且重复多次。另外,Avro在使用部分类型的时候会使用更小的数据类型,比如Short或者Byte类型,所以Avro的数 据块比RCFile 的文件格式块更小。</p>
</li>
</ul>
<h3 id="压缩格式有哪些-优缺点"><a href="#压缩格式有哪些-优缺点" class="headerlink" title="压缩格式有哪些 优缺点"></a>压缩格式有哪些 优缺点</h3><h4 id="压缩的好处和坏处"><a href="#压缩的好处和坏处" class="headerlink" title="压缩的好处和坏处"></a>压缩的好处和坏处</h4><p><strong>好处</strong></p>
<ul>
<li>减少存储磁盘空间</li>
<li>降低IO(网络的IO和磁盘的IO)</li>
<li>加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度</li>
</ul>
<p><strong>坏处</strong></p>
<ul>
<li>由于使用数据时，需要先将数据解压，加重CPU负荷</li>
</ul>
<h4 id="压缩格式"><a href="#压缩格式" class="headerlink" title="压缩格式"></a>压缩格式</h4><p><img src="https://ruozedata.github.io/assets/blogImg/%E5%8E%8B%E7%BC%A91.png" alt="压缩格式"></p>
<p><img src="https://ruozedata.github.io/assets/blogImg/%E5%8E%8B%E7%BC%A92.png" alt="压缩空间比较"></p>
<p><img src="https://ruozedata.github.io/assets/blogImg/yasuo3.png" alt="压缩时间比较"></p>
<p>可以看出，压缩空间比值越高，压缩时间越长，压缩比：<code>Snappy&gt;LZ4&gt;LZO&gt;GZIP&gt;BZIP2</code></p>
<table>
<thead>
<tr>
<th align="left">压缩格式</th>
<th align="left">优点</th>
<th align="left">缺点</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>gzip</strong></td>
<td align="left">压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便</td>
<td align="left">不支持split</td>
<td></td>
</tr>
<tr>
<td align="left"><strong>lzo</strong></td>
<td align="left">压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便</td>
<td align="left">压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）</td>
<td></td>
</tr>
<tr>
<td align="left"><strong>snappy</strong></td>
<td align="left">压缩速度快；支持hadoop native库</td>
<td align="left">不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令d. bzip2</td>
<td></td>
</tr>
<tr>
<td align="left"><strong>bzip2</strong></td>
<td align="left">支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便</td>
<td align="left">压缩/解压速度慢；不支持native</td>
<td></td>
</tr>
</tbody></table>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>不同的场景选择不同的压缩方式，肯定没有一个一劳永逸的方法，如果选择高压缩比，那么对于cpu的性能要求要高，同时压缩、解压时间耗费也多；选择压缩比低的，对于磁盘io、网络io的时间要多，空间占据要多；对于支持分割的，可以实现并行处理。</p>
<h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><p>一般在HDFS 、Hive、HBase中会使用；<br>当然一般较多的是结合Spark 来一起使用。</p>
<h3 id="Spilt–-gt-Map-Task关系"><a href="#Spilt–-gt-Map-Task关系" class="headerlink" title="Spilt–&gt;Map Task关系"></a>Spilt–&gt;Map Task关系</h3><p>Reduce Task默认是1个，Map Task默认是2个，但是实际运行场景下，Map Task的个数和切片的个数保持一致，而切片的个数又与文件数和文件大小相关联。切片默认大小决定文件被分成多少个切片，执行多少个Map Task。</p>
<table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.job.maps</td>
<td>2</td>
<td>The default number of map tasks per job.</td>
</tr>
<tr>
<td>mapreduce.job.reduces</td>
<td>1</td>
<td>The default number of reduce tasks per job.</td>
</tr>
</tbody></table>
<h3 id="shuffle的理解"><a href="#shuffle的理解" class="headerlink" title="shuffle的理解"></a>shuffle的理解</h3><p>俩字: 洗牌</p>
<p>shuffle阶段又可以分为Map端的shuff和reduce端的shuffle</p>
<p><img src="https:/yerias.github.io/hadoop_img/shuffer.jpg" alt="shuffe过程"></p>
<h4 id="map端的shuffle"><a href="#map端的shuffle" class="headerlink" title="map端的shuffle"></a>map端的shuffle</h4><ul>
<li>map端会处理出入数据并产生中间结果，这个中间结果会写到本地磁盘，而不是HDFS。每个map的输出会先写到内存缓冲区中，当写入的数据达到设定的阈值时，系统将会启动一个线程将缓冲区的数据写到磁盘，这个过程叫做spill(溢写)。</li>
<li>在spill之前，会先进行两次排序，首先根据数据所属的partition进行排序，然后每个partition中的数据再按key来排序，partition的目的是将记录划分到不同的reduce上，以期望能达到负载均衡，以后的reduce就会根据partition来读取自己对应的数据。接着运行combiner(如果设置了的话)，combiner的本质也是一个reduce，其目的是对将要写入到磁盘的文件先进行一次处理，这样，写入到磁盘的数据量就会减少。最后将数据写到本地磁盘产生spill文件(spill文件保存在{mapred.local.dir}指定的目录中，map任务结束后就会被删除)。</li>
<li>最后，每个map任务可能产生多个spill文件，在每个map任务完成前，会通过多路归并算法将这些spill文件合并成一个文件。至此，map的shuffle过程就结束了。</li>
</ul>
<h4 id="reduce端的shuffle"><a href="#reduce端的shuffle" class="headerlink" title="reduce端的shuffle"></a>reduce端的shuffle</h4><ul>
<li>reduce端的shuffle主要包括三个阶段，copy、sort(merge)和reduce</li>
<li>首先将map端产生的输出文件拷贝到reduce端，但每个reduce如何知道自己应该处理哪些数据呢？因为map端进行partition的时候，实际上就相当于指定了每个reduce要处理的数据(partition就对应了reduce)，所以reduce在拷贝的数据的时候只需拷贝与自己对应的partition中的数据即可。每个reduce会处理一个或多个partiton，但需要先将自己对应的partition中的数据从每个map的输出结果中拷贝出来。</li>
<li>接下来就是sort阶段，也称为merge阶段，因为这个阶段的主要工作是执行了归并排序。从map端拷贝到reduce端的数据都是有序的，所以很适合归并排序。最终在reduce端生产一个较大的文件作为reduce的输入。</li>
<li>最后就是reduce阶段了，在这个过程中产生最终的输出结果，并将其写到HDFS上。</li>
</ul>
<h3 id="WordCount的剖解图"><a href="#WordCount的剖解图" class="headerlink" title="WordCount的剖解图"></a>WordCount的剖解图</h3><p><img src="https:/yerias.github.io/hadoop_img/wordcount.jpg" alt="wordcount执行流程"></p>
<h4 id="Map任务处理"><a href="#Map任务处理" class="headerlink" title="Map任务处理"></a>Map任务处理</h4><ol>
<li>读取HDFS中的文件，每一行解析成一个&lt;K,V&gt;值。每个键值对调用一次map函数。</li>
<li>重写map()方法，接收1产生的&lt;K,V&gt;值进行处理，转为新的&lt;K,V&gt;输出。</li>
<li>对2输出的&lt;K,V&gt;值进行分区，默认一盒分区。</li>
<li>对不同分区中的数据进行排序(按照K)、分组。分组指的是相同Key的Value放到一个集合中。</li>
<li>(可选)对分组后的数据进行合并。</li>
</ol>
<h4 id="Reduce任务处理"><a href="#Reduce任务处理" class="headerlink" title="Reduce任务处理"></a>Reduce任务处理</h4><ol>
<li>多个Map任务的输出，按照不同的分区，通过网络copy到不同的Reduce节点上。</li>
<li>对多个map的输出进行合并、排序。重写reduce()方法，接收的是分组后的数据，实现自己的业务逻辑，处理后产生新的&lt;K,V&gt;值输出</li>
<li>对reduce输出的&lt;K,V&gt;写到HDFS中。</li>
</ol>
<hr>
<p>整理来自：<a href="https://ruozedata.github.io/2018/04/18/、https://www.jianshu.com/p/43630456a18a" target="_blank" rel="noopener">https://ruozedata.github.io/2018/04/18/、https://www.jianshu.com/p/43630456a18a</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/09/hadoop/6/">Hadoop的Pid文件</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-09</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/HDFS/">HDFS</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/HDFS/">HDFS</a></span><div class="content"><h3 id="存储位置"><a href="#存储位置" class="headerlink" title="存储位置"></a>存储位置</h3><p>hadoop启动之后，pid文件是存储哪里？<br>我们可以通过查看 hadoop-env.sh文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat etc/hadoop/hadoop-env.sh从下图可以看出hadoop默认的pid文件是存储到/tmp目录的</span><br></pre></td></tr></table></figure>

<p>从下图可以看出hadoop默认的pid文件是存储到/tmp目录的</p>
<p>从下图可以看出hadoop默认的pid文件是存储到/tmp目录的</p>
<p><img src="https://img-blog.csdnimg.cn/20190729214437420.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="hadoop-env.sh文件"></p>
<p>从下图可以看出，后缀名是.pid的就是hadoop的pid文件</p>
<p><img src="https://img-blog.csdnimg.cn/20190729214915879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="/tmp目录"></p>
<h3 id="启动和停止"><a href="#启动和停止" class="headerlink" title="启动和停止"></a>启动和停止</h3><p>我们启动的时候，是执行sbin/start-df.sh文件，我们看一看这个文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20190729215631410.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="start-df.sh文件"></p>
<p>从上面这个图可以看出，启动namenode节点的时候，调用了hadoop-daemons.sh文件了，我们再看看这个文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat sbin/hadoop-daemons.sh</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20190729215911670.png" alt="hadoop-daemons.sh文件"></p>
<p>从上图可以看出，在最后一行又调用了hadoop-daemon.sh文件，我们在看看这个文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat sbin/hadoop-daemon.sh</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20190729220412390.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="hadoop-daemon.sh文件"></p>
<p><strong>从上面两张图可以得出结论:</strong></p>
<ol>
<li>hadoop启动的时候，会生成pid文件，并把进程号写入到pid文件</li>
<li>hadoop停止的时候，会到pid文件中获取进程号，然后停止进程，最后删除pid文件</li>
</ol>
<p><strong>下面我们做一下验证：</strong></p>
<ol>
<li><p>看下namenode的进程号是不是和pid文件里的进程号一样</p>
<p><img src="https://img-blog.csdnimg.cn/20190729221743612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="start-jps命令"></p>
<p>从上图可以看出，进程号是一样的，说明我们前面的推理是正确的</p>
</li>
<li><p>我们把生成号的namenode的pid文件名字改一下，停止的时候脚本会找不到pid文件，也就不会停止namenode进程了</p>
<p><img src="https://img-blog.csdnimg.cn/20190729222300551.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="stop-jps命令"></p>
<p>从上图可以看出，我们的两个推理是正确的</p>
</li>
<li><p>tmp目录的弊端<br>linux的/tmp目录会自动清理一段时间没有访问的文件，一般都是30天，假如hadoop启动了30天以上，那么pid文件会被删除，再调用停止的时候会停止不了，生产上一般不会放在/tmp目录下，下面我们自己创建个目录存放pid文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">创建文件夹</span></span><br><span class="line">mkdir -p /data/tmp</span><br><span class="line"><span class="meta">#</span><span class="bash">赋予权限</span></span><br><span class="line">chmod 777 -R /data/tmp</span><br><span class="line">然后修改etc/hadoop/hadoop-env.sh文件</span><br></pre></td></tr></table></figure>

<p>然后修改etc/hadoop/hadoop-env.sh文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vi /etc/hadoop/hadoop-env.sh</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20190730105216506.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="修改hadoop-env.sh文件的pid目录"></p>
<p>然后启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<p>启动之后，我们查看pid文件</p>
<p><img src="https://img-blog.csdnimg.cn/20190730105343192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0NTIzODg=,size_16,color_FFFFFF,t_70" alt="新的pid目录"></p>
</li>
</ol>
<hr>
<p>原文链接：<a href="https://blog.csdn.net/u010452388/article/details/97686380" target="_blank" rel="noopener">https://blog.csdn.net/u010452388/article/details/97686380</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/09/PE/1/">DataNode OOM溢出</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-09</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/%E7%94%9F%E4%BA%A7%E6%95%85%E9%9A%9C%E6%A1%88%E4%BE%8B/">生产故障案例</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/%E7%94%9F%E4%BA%A7%E6%95%85%E9%9A%9C%E6%A1%88%E4%BE%8B/">生产故障案例</a></span><div class="content"><h3 id="DataNode的内存溢出报错"><a href="#DataNode的内存溢出报错" class="headerlink" title="DataNode的内存溢出报错"></a>DataNode的内存溢出报错</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2017-12-17 23:58:14,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1437036909-![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.36-1509097205664:blk_1074725940_987917, type=HAS_DOWNSTREAM_IN_PIPELINE terminating</span><br><span class="line">2017-12-17 23:58:31,425 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode is out of memory. Will retry in 30 seconds.</span><br><span class="line">java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">	at java.lang.Thread.start0(Native Method)</span><br><span class="line">	at java.lang.Thread.start(Thread.java:714)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:154)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">2017-12-17 23:59:01,426 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode is out of memory. Will retry in 30 seconds.</span><br><span class="line">java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">	at java.lang.Thread.start0(Native Method)</span><br><span class="line">	at java.lang.Thread.start(Thread.java:714)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:154)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">2017-12-17 23:59:05,520 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode is out of memory. Will retry in 30 seconds.</span><br><span class="line">java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">	at java.lang.Thread.start0(Native Method)</span><br><span class="line">	at java.lang.Thread.start(Thread.java:714)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:154)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:745)</span><br><span class="line">2017-12-17 23:59:31,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1437036909-![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.36-1509097205664:blk_1074725951_987928 src: /![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.54:40478 dest: /![img](file:///C:\Users\Administrator\AppData\Roaming\Tencent\QQTempSys\%W@GJ$ACOF(TYDYECOKVDYB.png)192.168.17.48:50010</span><br></pre></td></tr></table></figure>

<h3 id="CDH查看DataNode的内存情况"><a href="#CDH查看DataNode的内存情况" class="headerlink" title="CDH查看DataNode的内存情况"></a>CDH查看DataNode的内存情况</h3><p><img src="https://yerias.github.io/hadoop_img/20191205092342.jpg" alt="datanode内存使用图"></p>
<h3 id="明明1G的内存都没有使用，为什么会报OOM？"><a href="#明明1G的内存都没有使用，为什么会报OOM？" class="headerlink" title="明明1G的内存都没有使用，为什么会报OOM？"></a>明明1G的内存都没有使用，为什么会报OOM？</h3><p>可以确定是操作系统哪里设置错了，我想应该是把产品环境的某个参数配置错了，系统本身的影响肯定不会有了，因为产品环境上我们只create了800左右个线程，就OOM了，那应该就是配置的问题了</p>
<p>解决方法：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1.</span><br><span class="line"></span><br><span class="line">echo "kernel.threads-max=196605" &gt;&gt; /etc/sysctl.conf</span><br><span class="line">echo "kernel.pid_max=196605" &gt;&gt; /etc/sysctl.conf</span><br><span class="line">echo "vm.max_map_count=393210" &gt;&gt; /etc/sysctl.conf</span><br><span class="line">sysctl -p </span><br><span class="line"></span><br><span class="line">2.</span><br><span class="line"></span><br><span class="line">/etc/security/limits.conf</span><br><span class="line">* soft nofile 196605</span><br><span class="line">* hard nofile 196605</span><br><span class="line">* soft nproc 196605</span><br><span class="line">* hard nproc 196605</span><br></pre></td></tr></table></figure>

</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/08/hadoop/5/">Hadoop2.7.6之前和Hadoop2.8.4之后的副本存放策略</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-08</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/HDFS/">HDFS</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/HDFS/">HDFS</a></span><div class="content"><h3 id="新旧版本的副本存放策略比较"><a href="#新旧版本的副本存放策略比较" class="headerlink" title="新旧版本的副本存放策略比较"></a>新旧版本的副本存放策略比较</h3><p>Hadoop2.7.6及以下版本是按照旧的策略进行副本存放的，官网文档描述如下：</p>
<p><img src="https://img-blog.csdnimg.cn/20191017161508850.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FjY3B0YW5nZ2FuZw==,size_16,color_FFFFFF,t_70" alt="旧版本的副本存放策略"></p>
<p>在常见情况下，当复制因子为3时，HDFS的放置策略是将一个副本放置在本地机架中的一个节点上，将另一个副本放置在本地机架中的另一个节点上，最后一个副本放置在不同机架中的另一个节点上。</p>
<p>Hadoop2.8.4及以上版本是按照新的策略进行副本存放的，官网文档描述如下：  </p>
<p><img src="https://img-blog.csdnimg.cn/20191017161742230.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FjY3B0YW5nZ2FuZw==,size_16,color_FFFFFF,t_70" alt="新版本的副本存放策略"></p>
<p>在常见情况下，当复制因子为3时，HDFS的放置策略是：如果写入器在数据节点上，则将一个副本放置在本地计算机上；否则，在随机数据节点上，HDFS将另一个副本放置在不同（远程）机架中的节点上的，最后一个位于同一远程机架中的其他节点上。</p>
<h3 id="新版本的副本存放策略思想"><a href="#新版本的副本存放策略思想" class="headerlink" title="新版本的副本存放策略思想"></a>新版本的副本存放策略思想</h3><p>最后，再把新版本的副本存放策略的基本思想描述如下：</p>
<p>第一个副本存放Client所在的节点上（假设Client不在集群的范围内，则第一个副本存储节点是随机选取的。当然系统会不选择那些太满或者太忙的节点）</p>
<p>第二个副本存放在与第一个节点不同机架中的一个节点上。</p>
<p>第三个副本和第二个在同一个机架，随机放在不同的节点上。</p>
<p>如果还有很多其他的副本就随机放在集群中的各个节点上。</p>
<hr>
<p>原文链接：<a href="https://blog.csdn.net/accptanggang/article/details/102609318" target="_blank" rel="noopener">https://blog.csdn.net/accptanggang/article/details/102609318</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/08/mysql/4/">MySQL中的Top N</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-08</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/MySQL/">MySQL</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/DataWarehouse/">DataWarehouse</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/MySQL/">MySQL</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Rank/">Rank</a></span><div class="content"><h3 id="切入点"><a href="#切入点" class="headerlink" title="切入点"></a>切入点</h3><p>MySQL没有获取Top N的这种函数，但是在MySQL中求Top N又是必须掌握的点</p>
<p>比如查询分组后的最大值、最小值所在的整行记录或者分组后的Top N行记录</p>
<p>下面我们就如何在MySQL中求Top N做出深度的思考和验证</p>
<h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>测试表结构如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">&gt;&gt; CREATE TABLE `student` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `name` varchar(20) DEFAULT NULL,</span><br><span class="line">  `course` varchar(20) DEFAULT NULL,</span><br><span class="line">  `score` int(11) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8</span><br></pre></td></tr></table></figure>

<p> 插入数据：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">&gt;&gt; insert into student(name,course,score)</span><br><span class="line"><span class="keyword">values</span></span><br><span class="line">(<span class="string">'张三'</span>,<span class="string">'语文'</span>,<span class="number">80</span>),</span><br><span class="line">(<span class="string">'李四'</span>,<span class="string">'语文'</span>,<span class="number">90</span>),</span><br><span class="line">(<span class="string">'王五'</span>,<span class="string">'语文'</span>,<span class="number">93</span>),</span><br><span class="line">(<span class="string">'张三'</span>,<span class="string">'数学'</span>,<span class="number">77</span>),</span><br><span class="line">(<span class="string">'李四'</span>,<span class="string">'数学'</span>,<span class="number">68</span>),</span><br><span class="line">(<span class="string">'王五'</span>,<span class="string">'数学'</span>,<span class="number">99</span>),</span><br><span class="line">(<span class="string">'张三'</span>,<span class="string">'英语'</span>,<span class="number">90</span>),</span><br><span class="line">(<span class="string">'李四'</span>,<span class="string">'英语'</span>,<span class="number">50</span>),</span><br><span class="line">(<span class="string">'王五'</span>,<span class="string">'英语'</span>,<span class="number">89</span>);</span><br></pre></td></tr></table></figure>

<p>查看结果：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">&gt;&gt; select * from student;</span><br><span class="line">+<span class="comment">----+--------+--------+-------+</span></span><br><span class="line">| id | name   | course | score |</span><br><span class="line">+<span class="comment">----+--------+--------+-------+</span></span><br><span class="line">|  1 | 张三   | 语文   |    80 |</span><br><span class="line">|  2 | 李四   | 语文   |    90 |</span><br><span class="line">|  3 | 王五   | 语文   |    93 |</span><br><span class="line">|  4 | 张三   | 数学   |    77 |</span><br><span class="line">|  5 | 李四   | 数学   |    68 |</span><br><span class="line">|  6 | 王五   | 数学   |    99 |</span><br><span class="line">|  7 | 张三   | 英语   |    90 |</span><br><span class="line">|  8 | 李四   | 英语   |    50 |</span><br><span class="line">|  9 | 王五   | 英语   |    89 |</span><br><span class="line">+<span class="comment">----+--------+--------+-------+</span></span><br></pre></td></tr></table></figure>

<h3 id="TOP-1"><a href="#TOP-1" class="headerlink" title="TOP 1"></a>TOP 1</h3><p>查询每门课程分数最高的学生以及成绩</p>
<ol>
<li><p>我们先拆分题目，这是一题查询分组求最大值的题目，拆分后的题目是：查询 每门课程 分数最高 的学生以及成绩</p>
<p>我们首先按照常规思路来写SQL: </p>
<p>select 学生姓名，学生分数</p>
<p>group by 课程</p>
<p>max(分数) </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	s.name,s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">	s.course;</span><br></pre></td></tr></table></figure>

<p>得出的查询结果是:</p>
<table>
<thead>
<tr>
<th>name</th>
<th>course</th>
<th>score</th>
</tr>
</thead>
<tbody><tr>
<td>张三</td>
<td>数学</td>
<td>99</td>
</tr>
<tr>
<td>张三</td>
<td>英语</td>
<td>90</td>
</tr>
<tr>
<td>张三</td>
<td>语文</td>
<td>93</td>
</tr>
</tbody></table>
<h4 id="问题-为什么姓名都是张三？课程和对应的成绩又全是对的？"><a href="#问题-为什么姓名都是张三？课程和对应的成绩又全是对的？" class="headerlink" title="问题: 为什么姓名都是张三？课程和对应的成绩又全是对的？"></a>问题: 为什么姓名都是张三？课程和对应的成绩又全是对的？</h4><p>我预测是因为没有把姓名加入group的分组字段，那么我们把姓名加入group的分组字段后试试看</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	s.name,s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">	s.name,s.course;</span><br></pre></td></tr></table></figure>

<p>得出的查询结果是:</p>
<table>
<thead>
<tr>
<th>name</th>
<th>course</th>
<th>score</th>
</tr>
</thead>
<tbody><tr>
<td>张三</td>
<td>数学</td>
<td>77</td>
</tr>
<tr>
<td>张三</td>
<td>英语</td>
<td>90</td>
</tr>
<tr>
<td>张三</td>
<td>语文</td>
<td>80</td>
</tr>
<tr>
<td>李四</td>
<td>数学</td>
<td>68</td>
</tr>
<tr>
<td>李四</td>
<td>英语</td>
<td>50</td>
</tr>
<tr>
<td>李四</td>
<td>语文</td>
<td>90</td>
</tr>
<tr>
<td>王五</td>
<td>数学</td>
<td>99</td>
</tr>
<tr>
<td>王五</td>
<td>英语</td>
<td>89</td>
</tr>
<tr>
<td>王五</td>
<td>语文</td>
<td>93</td>
</tr>
</tbody></table>
<p>结果还是不对，这次把所有的字段都查询出来了，字段的排序规则是先按姓名分组，再按课程分组，因为课程是唯一的，所以跟直接查询的结果一样。</p>
</li>
<li><p>我们回到上一步，上一步的课程和成绩对应上了，姓名没有对应上，我们干脆就不要姓名和，拿课程和成绩作为一张表再和自己联结一次，以课程和成绩作为过滤字段，说不定就能得到想要的姓名字段。</p>
<p>思路：</p>
<ol>
<li><p>先课程分组求出最高的分数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">	s.course;</span><br></pre></td></tr></table></figure>
</li>
<li><p>把前面得出的结果作为表t再自联结一次</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	s.name,s.score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	student s</span><br><span class="line"><span class="keyword">join</span> t </span><br><span class="line"><span class="keyword">on</span> s.course=t.course <span class="keyword">and</span> s.score=t.score;</span><br></pre></td></tr></table></figure>
</li>
<li><p>把t替换成查询出来的结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	s.name,s.course,s.score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	student s</span><br><span class="line"><span class="keyword">join</span> (<span class="keyword">select</span> </span><br><span class="line">	s.course,<span class="keyword">max</span>(s.score) <span class="keyword">as</span> score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	student s</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">	s.course) t </span><br><span class="line"><span class="keyword">on</span> s.course=t.course <span class="keyword">and</span> s.score=t.score;</span><br></pre></td></tr></table></figure>

<p>得出的查询结果是:</p>
<table>
<thead>
<tr>
<th>name</th>
<th>course</th>
<th>score</th>
</tr>
</thead>
<tbody><tr>
<td>王五</td>
<td>语文</td>
<td>93</td>
</tr>
<tr>
<td>王五</td>
<td>数学</td>
<td>99</td>
</tr>
<tr>
<td>张三</td>
<td>英语</td>
<td>90</td>
</tr>
</tbody></table>
<p>和原数据比较，这就是我们要得到的每门课程的top1。</p>
</li>
</ol>
</li>
</ol>
<h3 id="TOP-N"><a href="#TOP-N" class="headerlink" title="TOP N"></a>TOP N</h3><p>查询每门课程前两名的学生以及成绩</p>
<p>首先求Top 1的方法不适用与Top N，然后毫无头绪。。。</p>
<p>翻看其他人的博客后，发现求Top N的核心是: <code>自联结表的需求字段比较，也就是自己跟自己比较，然后把比较的结果求count()，最后控制过滤的记录数即可</code></p>
<h4 id="注意-自己和自己比较，可以通过联结和子查询，我们这里使用子查询"><a href="#注意-自己和自己比较，可以通过联结和子查询，我们这里使用子查询" class="headerlink" title="注意:  自己和自己比较，可以通过联结和子查询，我们这里使用子查询"></a>注意:  自己和自己比较，可以通过联结和子查询，我们这里使用子查询</h4><p>思路:</p>
<ol>
<li><p>首先是子查询，然后是自己和自己比较，得出一个count()值，最后使用where过滤这个count值</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	a.name,a.course,a.score</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	student a</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">	<span class="number">2</span>&gt;(<span class="keyword">select</span> <span class="keyword">count</span>(b.score) <span class="keyword">from</span> student b <span class="keyword">where</span> a.course=b.course <span class="keyword">and</span> a.score&lt;b.score)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> a.course <span class="keyword">desc</span>,a.score <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>

<p>梳理这段SQL，select字段不难，from字段不难，order by字段不难，难就难在where字段，我们先不看为什么使用2大于这个子查询，先把注意力放在子查询的where字段中的两个表成绩比较，实际上理解了为什么这么比较这题就解出来了。</p>
</li>
<li><p>我们画图来解释。。。只是做为示范，所以只取一个课程的成绩比较，其他的一样</p>
<p><img src="https://yerias.github.io/hadoop_img/20191205012529.jpg" alt="a.score&lt;b.score比较图"></p>
<p>可以看出，表a中的成绩越大，满足<code>a.score&lt;b.score</code>的次数越少，<code>where条件过滤count()的值越少越满足Top N的条件，可以根据where条件灵活控制过滤的记录数,我们这里是2，即取Top 2的记录。</code></p>
<h4 id="再提出一个问题，为什么要用a-score-lt-b-score而不是a-score-gt-b-score？"><a href="#再提出一个问题，为什么要用a-score-lt-b-score而不是a-score-gt-b-score？" class="headerlink" title="再提出一个问题，为什么要用a.score&lt;b.score而不是a.score&gt;b.score？"></a>再提出一个问题，为什么要用<code>a.score&lt;b.score</code>而不是<code>a.score&gt;b.score</code>？</h4><p>通过结果可以倒推出来，我们<code>select</code>语句中要的是表a，根据题意表a必定是比较中较大的值。如果使用<code>a.score&gt;b.score</code>，where条件限制的是满足最少的条件，把表a中最大的值给过滤了，那么得出的的count()结果是反的。</p>
</li>
</ol>
</div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/10/">&lt;&lt;</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/12/">&gt;&gt;</a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>