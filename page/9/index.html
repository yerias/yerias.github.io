<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="BigData Developer"><meta name="keywords" content="yerias,TUNANのBlog,BigData"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017 | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">133</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">30</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">26</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">TUNANのBlog</div><div id="site-sub-title">感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017</div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2018/12/02/flume/2/">案例&amp;Flume单源单出口&amp;Flume单源多出口&amp;Flume多源单出口</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-12-02</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Flume/">Flume</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Flume/">Flume</a></span><div class="content"><h2 id="安装地址"><a href="#安装地址" class="headerlink" title="安装地址"></a>安装地址</h2><ol>
<li><p>Flume官网地址</p>
<p><a href="http://flume.apache.org/" target="_blank" rel="noopener">http://flume.apache.org/</a></p>
</li>
<li><p>文档查看地址</p>
<p><a href="http://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="noopener">http://flume.apache.org/FlumeUserGuide.html</a></p>
</li>
<li><p>下载地址</p>
<p><a href="http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.16.2-src.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.16.2-src.tar.gz</a></p>
</li>
</ol>
<h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><ol>
<li><p>将apache-flume-1.7.0-bin.tar.gz上传到linux的/opt/software目录下</p>
</li>
<li><p>解压apache-flume-1.7.0-bin.tar.gz到/opt/module/目录下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun software]$ tar -zxf apache-flume-1.7.0-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改apache-flume-1.7.0-bin的名称为flume</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun module]$ mv apache-flume-1.7.0-bin flume</span><br></pre></td></tr></table></figure>
</li>
<li><p>将flume/conf下的flume-env.sh.template文件修改为flume-env.sh，并配置flume-env.sh文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun conf]$ mv flume-env.sh.template flume-env.sh</span><br><span class="line">[aliyun@aliyun conf]$ vi flume-env.sh</span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><ol>
<li><p>安装netcat工具</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun software]$ sudo yum install -y nc</span><br></pre></td></tr></table></figure>
</li>
<li><p>判断44444端口是否被占用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume-telnet]$ sudo netstat -tunlp | grep 44444</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建Flume Agent配置文件flume-netcat-logger.conf</p>
<p>在flume目录下创建job文件夹并进入job文件夹。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ mkdir job</span><br><span class="line">[aliyun@aliyun flume]$ cd job/</span><br></pre></td></tr></table></figure>
</li>
<li><p>在job文件夹下创建Flume Agent配置文件flume-netcat-logger.conf。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ vim flume-netcat-logger.conf</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent    #a1表示agent的名称</span><br><span class="line">a1.sources = r1		#r1表示a1的输入源</span><br><span class="line">a1.sinks = k1		#k1表示a1的输出目的地</span><br><span class="line">a1.channels = c1	#c1表示a1的缓冲区</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat		#表示a1的输入源类型为netcat端口类型</span><br><span class="line">a1.sources.r1.bind = localhost	#表示a1的监听的主机</span><br><span class="line">a1.sources.r1.port = 44444		#表示a1的监听的端口号</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger		#表示a1的输出目的地是控制台logger类型</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory		#表示a1的channel类型是memory内存型</span><br><span class="line">a1.channels.c1.capacity = 1000		#表示a1的channel总容量1000个event</span><br><span class="line">a1.channels.c1.transactionCapacity = 10		#表示a1的channel传输时收集到了100条event以后再去提交事务</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1		#将r1和c1连接起来</span><br><span class="line">a1.sinks.k1.channel = c1		#将k1和c1连接起来</span><br></pre></td></tr></table></figure>
</li>
<li><p>先开启flume监听端口</p>
<p>第一种写法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>第二种写法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ bin/flume-ng agent -c conf/ -n a1 –f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<p>​        <code>--conf conf/：</code>表示配置文件存储在conf/目录</p>
<p>​        <code>--name a1：</code>表示给agent起名为a1</p>
<pre><code>`--conf-file job/flume-netcat.conf ：`flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。</code></pre><p>​        <code>--Dflume.root.logger==INFO,console ：</code>-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error。</p>
</li>
<li><p>使用netcat工具向本机的44444端口发送内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun ~]$ nc localhost 44444</span><br><span class="line">hello </span><br><span class="line">aliyun</span><br></pre></td></tr></table></figure>
</li>
<li><p>在Flume监听页面观察接收数据情况</p>
</li>
</ol>
<h2 id="实时读取本地文件到HDFS案例"><a href="#实时读取本地文件到HDFS案例" class="headerlink" title="实时读取本地文件到HDFS案例"></a>实时读取本地文件到HDFS案例</h2><p>案例需求：实时监控Hive日志，并上传到HDFS中</p>
<p><img src="https://yerias.github.io/flume_img/%E5%8D%95%E6%BA%90%E5%8D%95%E5%87%BA%E5%8F%A31.jpg" alt="单源单出口1"></p>
<ol>
<li><p>给Flume的lib目录下添加aliyun相关的jar包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">commons-configuration-1.6.jar</span><br><span class="line">aliyun-auth-2.7.2.jar</span><br><span class="line">aliyun-common-2.7.2.jar</span><br><span class="line">aliyun-hdfs-2.7.2.jar</span><br><span class="line">commons-io-2.4.jar</span><br><span class="line">htrace-core-3.1.0-incubating.jar</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建flume-file-hdfs.conf文件</p>
<p>创建文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ touch flume-file-hdfs.conf</span><br></pre></td></tr></table></figure>

<p>注：要想读取Linux系统中的文件，就得按照Linux命令的规则执行命令。由于Hive日志在Linux系统中所以读取文件的类型选择：exec即execute执行的意思。表示执行Linux命令来读取文件。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ vim flume-file-hdfs.conf</span><br></pre></td></tr></table></figure>

<p>添加如下内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources = r2		#定义source</span><br><span class="line">a2.sinks = k2		#定义sink</span><br><span class="line">a2.channels = c2	#定义channels</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a2.sources.r2.type = exec				#定义source类型为exec可执行命令的</span><br><span class="line">a2.sources.r2.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line">a2.sources.r2.shell = /bin/bash -c		#执行shell脚本的绝对路径</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k2.type = hdfs</span><br><span class="line">a2.sinks.k2.hdfs.path = hdfs://aliyun:9000/flume/%Y%m%d/%H</span><br><span class="line">a2.sinks.k2.hdfs.filePrefix = logs-			#上传文件的前缀</span><br><span class="line">a2.sinks.k2.hdfs.round = true				#是否按照时间滚动文件夹		</span><br><span class="line">a2.sinks.k2.hdfs.roundValue = 1				#多少时间单位创建一个新的文件夹	</span><br><span class="line">a2.sinks.k2.hdfs.roundUnit = hour			#重新定义时间单位</span><br><span class="line">a2.sinks.k2.hdfs.useLocalTimeStamp = true	#是否使用本地时间戳</span><br><span class="line">a2.sinks.k2.hdfs.batchSize = 1000			#积攒多少个Event才flush到HDFS一次</span><br><span class="line">a2.sinks.k2.hdfs.fileType = DataStream		#设置文件类型，可支持压缩</span><br><span class="line">a2.sinks.k2.hdfs.rollInterval = 60			#多久生成一个新的文件</span><br><span class="line">a2.sinks.k2.hdfs.rollSize = 134217700		#设置每个文件的滚动大小</span><br><span class="line">a2.sinks.k2.hdfs.rollCount = 0				#文件的滚动与Event数量无关</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a2.channels.c2.type = memory</span><br><span class="line">a2.channels.c2.capacity = 1000</span><br><span class="line">a2.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r2.channels = c2</span><br><span class="line">a2.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>：</p>
<p>对于所有与时间相关的转义序列，Event Header中必须存在以 “timestamp”的key（除非hdfs.useLocalTimeStamp设置为true，此方法会使用TimestampInterceptor自动添加timestamp）。</p>
</li>
<li><p>执行监控配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/flume-file-hdfs.conf</span><br></pre></td></tr></table></figure>
</li>
<li><p>开启aliyun和Hive并操作Hive产生日志</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun aliyun-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[aliyun@aliyun103 aliyun-2.7.2]$ sbin/start-yarn.sh</span><br><span class="line"></span><br><span class="line">[aliyun@aliyun hive]$ bin/hive</span><br><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在HDFS上查看文件。</p>
</li>
</ol>
<h2 id="实时读取目录文件到HDFS案例"><a href="#实时读取目录文件到HDFS案例" class="headerlink" title="实时读取目录文件到HDFS案例"></a>实时读取目录文件到HDFS案例</h2><p>案例需求：使用Flume监听整个目录的文件</p>
<p><img src="https://yerias.github.io/flume_img/%E5%8D%95%E6%BA%90%E5%8D%95%E5%87%BA%E5%8F%A32.jpg" alt="单源单出口2"></p>
<ol>
<li><p>创建配置文件flume-dir-hdfs.conf</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ touch flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure>
</li>
<li><p>打开文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun job]$ vim flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加如下内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a3.sources = r3		#定义sources</span><br><span class="line">a3.sinks = k3		#定义sink</span><br><span class="line">a3.channels = c3	#定义channel</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a3.sources.r3.type = spooldir						#定义souce类型为目录</span><br><span class="line">a3.sources.r3.spoolDir = /opt/module/flume/upload	#定义监控目录</span><br><span class="line">a3.sources.r3.fileSuffix = .COMPLETED				#定义文件上传完的后缀</span><br><span class="line">a3.sources.r3.fileHeader = true						#是否有文件头</span><br><span class="line">a3.sources.r3.ignorePattern = ([^ ]*\.tmp)			#忽略所有以.tmp结尾的文件，不上传</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a3.sinks.k3.type = hdfs							#sink类型为hdfs</span><br><span class="line">a3.sinks.k3.hdfs.pat=hdfs://aliyun:9000/flume/upload/%Y%m%d/%H  #文件上传到hdfs的路径</span><br><span class="line"></span><br><span class="line">a3.sinks.k3.hdfs.filePrefix = upload-			#上传文件到hdfs的前缀</span><br><span class="line">a3.sinks.k3.hdfs.round = true					#是否按照时间滚动文件夹</span><br><span class="line">a3.sinks.k3.hdfs.roundValue = 1					#多少时间单位创建一个新的文件夹</span><br><span class="line">a3.sinks.k3.hdfs.roundUnit = hour				#重新定义时间单位</span><br><span class="line">a3.sinks.k3.hdfs.useLocalTimeStamp = true		#是否使用本地时间戳</span><br><span class="line">a3.sinks.k3.hdfs.batchSize = 100				#积攒多少个Event才flush到HDFS一次</span><br><span class="line">a3.sinks.k3.hdfs.fileType = DataStream			#设置文件类型，可支持压缩</span><br><span class="line">a3.sinks.k3.hdfs.rollInterval = 60				#多久生成一个新的文件</span><br><span class="line">a3.sinks.k3.hdfs.rollSize = 134217700			#设置每个文件的滚动大小大概是128M</span><br><span class="line">a3.sinks.k3.hdfs.rollCount = 0					#文件的滚动与Event数量无关</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a3.channels.c3.type = memory</span><br><span class="line">a3.channels.c3.capacity = 1000</span><br><span class="line">a3.channels.c3.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a3.sources.r3.channels = c3</span><br><span class="line">a3.sinks.k3.channel = c3</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动监控文件夹命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-dir-hdfs.conf</span><br></pre></td></tr></table></figure>

<p>说明： 在使用Spooling Directory Source时</p>
<ol>
<li>不要在监控目录中创建并持续修改文件</li>
<li>上传完成的文件会以.COMPLETED结尾</li>
<li>被监控文件夹每500毫秒扫描一次文件变动</li>
</ol>
</li>
<li><p>向upload文件夹中添加文件</p>
<p>在/opt/module/flume目录下创建upload文件夹</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun flume]$ mkdir upload</span><br></pre></td></tr></table></figure>

<p>向upload文件夹中添加文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun upload]$ touch aliyun.txt</span><br><span class="line">[aliyun@aliyun upload]$ touch aliyun.tmp</span><br><span class="line">[aliyun@aliyun upload]$ touch aliyun.log</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看HDFS上的数据</p>
</li>
<li><p>等待1s，再次查询upload文件夹</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[aliyun@aliyun upload]$ ll</span><br><span class="line">总用量 0</span><br><span class="line">-rw-rw-r--. 1 aliyun aliyun 0 5月  20 22:31 aliyun.log.COMPLETED</span><br><span class="line">-rw-rw-r--. 1 aliyun aliyun 0 5月  20 22:31 aliyun.tmp</span><br><span class="line">-rw-rw-r--. 1 aliyun aliyun 0 5月  20 22:31 aliyun.txt.COMPLETED</span><br></pre></td></tr></table></figure>


</li>
</ol>
<h2 id="单数据源多出口案例-选择器"><a href="#单数据源多出口案例-选择器" class="headerlink" title="单数据源多出口案例(选择器)"></a>单数据源多出口案例(选择器)</h2><p>案例需求：使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。同时Flume-1将变动内容传递给Flume-3，Flume-3负责输出到Local FileSystem。</p>
<p><img src="https://yerias.github.io/flume_img/%E5%8D%95%E6%BA%90%E5%A4%9A%E5%87%BA%E5%8F%A31.jpg" alt="单源多出口1"></p>
<ol>
<li><p>准备工作</p>
<p>在/opt/module/flume/job目录下创建group1文件夹</p>
<p><code>[hadoop@aliyun102 job]$ cd group1/</code></p>
<p>在/opt/module/datas/目录下创建flume3文件夹</p>
<p><code>[hadoop@aliyun102 datas]$ mkdir flume3</code></p>
</li>
<li><p>创建flume-file-flume.conf</p>
<p>配置1个接收日志文件的source和两个channel、两个sink，分别输送给flume-flume-hdfs和flume-flume-dir。</p>
<p>创建配置文件并打开</p>
<p><code>[hadoop@aliyun102 group1]$ touch flume-file-flume.conf</code></p>
<p><code>[hadoop@aliyun102 group1]$ vim flume-file-flume.conf</code></p>
<p>添加如下内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"></span><br><span class="line"># 将数据流复制给所有channel</span><br><span class="line">a1.sources.r1.selector.type = replicating</span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"> </span><br><span class="line"># Describe the sink</span><br><span class="line"># sink端的avro是一个数据发送者</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun102 </span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = aliyun102</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure>

<p>注：Avro是由aliyun创始人Doug Cutting创建的一种语言无关的数据序列化和RPC框架。</p>
<p>注：RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。</p>
</li>
<li><p>创建flume-flume-hdfs.conf</p>
<p>配置上级Flume输出的Source，输出是到HDFS的Sink。</p>
<p>创建配置文件并打开</p>
<p><code>[hadoop@aliyun102 group1]$ touch flume-flume-hdfs.conf</code></p>
<p><code>[hadoop@aliyun102 group1]$ vim flume-flume-hdfs.conf</code></p>
<p>添加如下内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line"># source端的avro是一个数据接收服务</span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = aliyun102</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k1.type = hdfs</span><br><span class="line">a2.sinks.k1.hdfs.path = hdfs://aliyun102:9000/flume2/%Y%m%d/%H</span><br><span class="line">#上传文件的前缀</span><br><span class="line">a2.sinks.k1.hdfs.filePrefix = flume2-</span><br><span class="line">#是否按照时间滚动文件夹</span><br><span class="line">a2.sinks.k1.hdfs.round = true</span><br><span class="line">#多少时间单位创建一个新的文件夹</span><br><span class="line">a2.sinks.k1.hdfs.roundValue = 1</span><br><span class="line">#重新定义时间单位</span><br><span class="line">a2.sinks.k1.hdfs.roundUnit = hour</span><br><span class="line">#是否使用本地时间戳</span><br><span class="line">a2.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">#积攒多少个Event才flush到HDFS一次</span><br><span class="line">a2.sinks.k1.hdfs.batchSize = 100</span><br><span class="line">#设置文件类型，可支持压缩</span><br><span class="line">a2.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">#多久生成一个新的文件</span><br><span class="line">a2.sinks.k1.hdfs.rollInterval = 600</span><br><span class="line">#设置每个文件的滚动大小大概是128M</span><br><span class="line">a2.sinks.k1.hdfs.rollSize = 134217700</span><br><span class="line">#文件的滚动与Event数量无关</span><br><span class="line">a2.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建flume-flume-dir.conf</p>
<p>配置上级Flume输出的Source，输出是到本地目录的Sink。</p>
<p>创建配置文件并打开</p>
<p><code>[hadoop@aliyun102 group1]$ touch flume-flume-dir.conf</code></p>
<p><code>[hadoop@aliyun102 group1]$ vim flume-flume-dir.conf</code></p>
<p>添加如下内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = aliyun102</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a3.sinks.k1.type = file_roll</span><br><span class="line">a3.sinks.k1.sink.directory = /opt/module/data/flume3</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure>

<p>提示：输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录。</p>
</li>
<li><p>执行配置文件</p>
<p>分别开启对应配置文件：flume-flume-dir，flume-flume-hdfs，flume-file-flume。</p>
<p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group1/flume-flume-dir.conf</code></p>
<p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group1/flume-flume-hdfs.conf</code></p>
<p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group1/flume-file-flume.conf</code></p>
</li>
<li><p>启动aliyun和Hive</p>
<p><code>[hadoop@aliyun102 aliyun-2.7.2]$ sbin/start-dfs.sh</code></p>
<p><code>[hadoop@aliyun103 aliyun-2.7.2]$ sbin/start-yarn.sh</code></p>
<p><code>[hadoop@aliyun102 hive]$ bin/hive</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>检查HDFS上数据</p>
</li>
<li><p>检查/opt/module/datas/flume3目录中数据</p>
<p><code>[hadoop@aliyun102 flume3]$ ll</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">总用量 8</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop 5942 5月 22 00:09 1526918887550-3</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="单数据源多出口案例-Sink组"><a href="#单数据源多出口案例-Sink组" class="headerlink" title="单数据源多出口案例(Sink组)"></a>单数据源多出口案例(Sink组)</h2><p>案例需求：使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。同时Flume-1将变动内容传递给Flume-3，Flume-3也负责存储到HDFS </p>
<p><img src="https://yerias.github.io/flume_img/%E5%8D%95%E6%BA%90%E5%A4%9A%E5%87%BA%E5%8F%A32.jpg" alt="单源多出口2"></p>
<ol>
<li><p>准备工作</p>
<p>在/opt/module/flume/job目录下创建group2文件夹</p>
<p><code>[hadoop@aliyun102 job]$ cd group2/</code></p>
</li>
<li><p>创建flume-netcat-flume.conf</p>
<p>配置1个接收日志文件的source和1个channel、两个sink，分别输送给flume-flume-console1和flume-flume-console2。</p>
<p>创建配置文件并打开</p>
<p><code>[hadoop@aliyun102 group2]$ touch flume-netcat-flume.conf</code></p>
<p><code>[hadoop@aliyun102 group2]$ vim flume-netcat-flume.conf</code></p>
<p>添加如下内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">a1.sinkgroups.g1.processor.backoff = true</span><br><span class="line">a1.sinkgroups.g1.processor.selector = round_robin</span><br><span class="line">a1.sinkgroups.g1.processor.selector.maxTimeOut=10000</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun102</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = aliyun102</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure>

<p>注：Avro是由aliyun创始人Doug Cutting创建的一种语言无关的数据序列化和RPC框架。</p>
<p>注：RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。</p>
</li>
<li><p>创建flume-flume-console1.conf</p>
<p>配置上级Flume输出的Source，输出是到本地控制台。</p>
<p>创建配置文件并打开</p>
<p><code>[hadoop@aliyun102 group2]$ touch flume-flume-console1.conf</code></p>
<p><code>[hadoop@aliyun102 group2]$ vim flume-flume-console1.conf</code></p>
<p>添加如下内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = aliyun102</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建flume-flume-console2.conf</p>
<p>配置上级Flume输出的Source，输出是到本地控制台。</p>
<p>创建配置文件并打开</p>
<p><code>[hadoop@aliyun102 group2]$ touch flume-flume-console2.conf</code></p>
<p><code>[hadoop@aliyun102 group2]$ vim flume-flume-console2.conf</code></p>
<p>添加如下内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = aliyun102</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行配置文件</p>
<p>分别开启对应配置文件：flume-flume-console2，flume-flume-console1，flume-netcat-flume。</p>
<p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group2/flume-flume-console2.conf -Dflume.root.logger=INFO,console</code></p>
<p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group2/flume-flume-console1.conf -Dflume.root.logger=INFO,console</code></p>
<p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group2/flume-netcat-flume.conf</code></p>
</li>
<li><p>使用netcat工具向本机的44444端口发送内容</p>
<p><code>$ nc localhost 44444</code></p>
</li>
<li><p>查看Flume2及Flume3的控制台打印</p>
</li>
</ol>
<h2 id="多数据源汇总案例"><a href="#多数据源汇总案例" class="headerlink" title="多数据源汇总案例"></a>多数据源汇总案例</h2><p>案例需求：</p>
<p>aliyun103上的Flume-1监控文件/opt/module/group.log，</p>
<p>aliyun102上的Flume-2监控某一个端口的数据流，</p>
<p>Flume-1与Flume-2将数据发送给aliyun104上的Flume-3，Flume-3将最终数据打印到控制台。</p>
<p><img src="https://yerias.github.io/flume_img/%E5%A4%9A%E6%BA%90%E5%8D%95%E5%87%BA%E5%8F%A3.jpg" alt="多源单出口"></p>
<ol>
<li><p>准备工作</p>
<p>分发Flume</p>
<p><code>[hadoop@aliyun102 module]$ xsync flume</code></p>
<p>在aliyun102、aliyun103以及aliyun104的/opt/module/flume/job目录下创建一个group3文件夹。</p>
<p><code>[hadoop@aliyun102 job]$ mkdir group3</code></p>
<p><code>[hadoop@aliyun103 job]$ mkdir group3</code></p>
<p><code>[hadoop@aliyun104 job]$ mkdir group3</code></p>
</li>
<li><p>创建flume1-logger-flume.conf</p>
<p>配置Source用于监控hive.log文件，配置Sink输出数据到下一级Flume。</p>
<p>在aliyun103上创建配置文件并打开</p>
<p><code>[hadoop@aliyun103 group3]$ touch flume1-logger-flume.conf</code></p>
<p><code>[hadoop@aliyun103 group3]$ vim flume1-logger-flume.conf</code></p>
<p>添加如下内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/group.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = aliyun104</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建flume2-netcat-flume.conf</p>
<p>配置Source监控端口44444数据流，配置Sink数据到下一级Flume：</p>
<p>在aliyun102上创建配置文件并打开</p>
<p><code>[hadoop@aliyun102 group3]$ touch flume2-netcat-flume.conf</code></p>
<p><code>[hadoop@aliyun102 group3]$ vim flume2-netcat-flume.conf</code></p>
<p>添加如下内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a2.sources.r1.type = netcat</span><br><span class="line">a2.sources.r1.bind = aliyun102</span><br><span class="line">a2.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k1.type = avro</span><br><span class="line">a2.sinks.k1.hostname = aliyun104</span><br><span class="line">a2.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建flume3-flume-logger.conf</p>
<p>配置source用于接收flume1与flume2发送过来的数据流，最终合并后sink到控制台。</p>
<p>在aliyun104上创建配置文件并打开</p>
<p><code>[hadoop@aliyun104 group3]$ touch flume3-flume-logger.conf</code></p>
<p><code>[hadoop@aliyun104 group3]$ vim flume3-flume-logger.conf</code></p>
<p>添加如下内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = aliyun104</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line"># Describe the sink</span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行配置文件</p>
<p>分别开启对应配置文件：flume3-flume-logger.conf，flume2-netcat-flume.conf，flume1-logger-flume.conf。</p>
<p><code>[hadoop@aliyun104 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group3/flume3-flume-logger.conf -Dflume.root.logger=INFO,console</code></p>
<p><code>[hadoop@aliyun102 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group3/flume2-netcat-flume.conf</code></p>
<p><code>[hadoop@aliyun103 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group3/flume1-logger-flume.conf</code></p>
</li>
<li><p>在aliyun103上向/opt/module目录下的group.log追加内容</p>
<p><code>[hadoop@aliyun103 module]$ echo &#39;hello&#39; &gt; group.log</code></p>
</li>
<li><p>在aliyun102上向44444端口发送数据</p>
<p><code>[hadoop@aliyun102 flume]$ telnet aliyun102 44444</code></p>
</li>
<li><p>检查aliyun104上数据</p>
</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/12/01/sqoop/1/">SQOOP安装&amp;RDBMS导入HDFS&amp;RDBMS导入HIVE&amp;HDFS导入RDBMS&amp;HIVE导入RDBMS&amp;SQOOP的ETL案例&amp;在SHELL中操作MYSQL</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-12-01</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Sqoop/">Sqoop</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Sqoop/">Sqoop</a></span><div class="content"><p>首先抛出两个场景</p>
<ol>
<li>数据数据在RDBMS中，你想使用Hive进行处理，怎么做</li>
<li>使用Hive统计分析好了，数据还在Hive中，如何导到RDBMS中</li>
</ol>
<h2 id="Sqoop安装"><a href="#Sqoop安装" class="headerlink" title="Sqoop安装"></a>Sqoop安装</h2><ol>
<li><p>下载并解压</p>
<p>下载地址：<a href="http://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.16.2.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.16.2.tar.gz</a></p>
<p>上传安装包sqoop-1.4.6-cdh5.16.2.tar.gz到主机中</p>
<p>解压sqoop安装包到指定目录，如：$ tar -zxf sqoop-1.4.6-cdh5.16.2.tar.gz -C /opt/module/</p>
</li>
<li><p>修改配置文件</p>
<p>重命名配置文件</p>
<p><code>$ mv sqoop-env-template.sh sqoop-env.sh</code></p>
<p>修改配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HADOOP_COMMON_HOME=/opt/module/hadoop</span><br><span class="line">export HADOOP_MAPRED_HOME=/opt/module/hadoop</span><br><span class="line">export HIVE_HOME=/opt/module/hive</span><br><span class="line">export ZOOKEEPER_HOME=/opt/module/zookeeper</span><br><span class="line">export ZOOCFGDIR=/opt/module/zookeeper</span><br><span class="line">export HBASE_HOME=/opt/module/hbase</span><br></pre></td></tr></table></figure>
</li>
<li><p>拷贝JDBC驱动</p>
<p><code>$ cp mysql-connector-java-5.1.27-bin.jar /opt/module/sqoop/lib/</code></p>
</li>
<li><p>验证Sqoop</p>
<p><code>$ bin/sqoop help</code></p>
<p>出现一些Warning警告（警告信息已省略），并伴随着帮助命令的输出：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Available commands:</span><br><span class="line">  codegen            Generate code to interact with database records</span><br><span class="line">  create-hive-table  Import a table definition into Hive</span><br><span class="line">  eval               Evaluate a SQL statement and display the results</span><br><span class="line">  export             Export an HDFS directory to a database table</span><br><span class="line">  help               List available commands</span><br><span class="line">  import             Import a table from a database to HDFS</span><br><span class="line">  import-all-tables  Import tables from a database to HDFS</span><br><span class="line">  import-mainframe   Import datasets from a mainframe server to HDFS</span><br><span class="line">  job                Work with saved jobs</span><br><span class="line">  list-databases     List available databases on a server</span><br><span class="line">  list-tables        List available tables in a database</span><br><span class="line">  merge              Merge results of incremental imports</span><br><span class="line">  metastore          Run a standalone Sqoop metastore</span><br><span class="line">  version            Display version information</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试Sqoop是否能够成功连接数据库</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop \</span><br><span class="line">list-databases \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/ \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root</span></span><br></pre></td></tr></table></figure>

<p>数据库用到的参数:</p>
<ul>
<li>“\“： 代表换行</li>
<li>“connect”：连接数据库</li>
<li>username：用户</li>
<li>password：密码</li>
</ul>
<p>显示所有的数据库列表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">information_schema</span><br><span class="line">leyou1</span><br><span class="line">metastore</span><br><span class="line">mypython</span><br><span class="line">mysql</span><br><span class="line">performance_schema</span><br><span class="line">sys</span><br><span class="line">travel</span><br><span class="line">tunan</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="RDBMS到HDFS"><a href="#RDBMS到HDFS" class="headerlink" title="RDBMS到HDFS"></a>RDBMS到HDFS</h2><h3 id="MySQL准备表和数据"><a href="#MySQL准备表和数据" class="headerlink" title="MySQL准备表和数据"></a>MySQL准备表和数据</h3><ol>
<li><p>确定mysql服务开启正常</p>
</li>
<li><p>在mysql中创建库表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> company;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> company.staff(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">4</span>) primary <span class="keyword">key</span> <span class="keyword">not</span> <span class="literal">null</span> auto_increment, </span><br><span class="line">    <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">255</span>), </span><br><span class="line">    sex <span class="built_in">varchar</span>(<span class="number">255</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
</li>
<li><p>插入数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> company.staff(<span class="keyword">name</span>, sex) <span class="keyword">values</span>(<span class="string">'Thomas'</span>, <span class="string">'Male'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> company.staff(<span class="keyword">name</span>, sex) <span class="keyword">values</span>(<span class="string">'Catalina'</span>, <span class="string">'FeMale'</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看数据</p>
<p><code>select * from staff;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">----+----------+--------+</span></span><br><span class="line">| id | name     | sex    |</span><br><span class="line">+<span class="comment">----+----------+--------+</span></span><br><span class="line">|  1 | Thomas   | Male   |</span><br><span class="line">|  2 | Catalina | FeMale |</span><br><span class="line">+<span class="comment">----+----------+--------+</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="将MySQL数据导入到HDFS"><a href="#将MySQL数据导入到HDFS" class="headerlink" title="将MySQL数据导入到HDFS"></a>将MySQL数据导入到HDFS</h3><h4 id="全部导入"><a href="#全部导入" class="headerlink" title="全部导入"></a>全部导入</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://aliyun:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table staff \</span><br><span class="line">--target-dir /user/company \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &quot;\t&quot;</span><br></pre></td></tr></table></figure>

<p>全部导入用到的参数:</p>
<ul>
<li>table：指定被导入的表名</li>
<li>target-dir：指定导入路径</li>
<li>delete-target-dir：如果目标目录存在就删除它</li>
<li>num-mappers：mapper的个数</li>
<li>fields-terminated-by：指定字段分隔符</li>
</ul>
<h4 id="查询导入-query"><a href="#查询导入-query" class="headerlink" title="查询导入(query)"></a>查询导入(query)</h4><p>query参数就可以让用户随意写sql语句来查询了。query和table参数是互斥的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://aliyun:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--target-dir /user/company \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &quot;\t&quot; \</span><br><span class="line">--query &apos;select name,sex from staff where id &lt;=1 and $CONDITIONS;&apos;</span><br></pre></td></tr></table></figure>

<ul>
<li>query：指定查询SQL where条件要有$CONDITIONS</li>
</ul>
<p>注意:  <code>must contain &#39;$CONDITIONS&#39; in WHERE clause.</code></p>
<p>如果query后使用的是双引号，则$CONDITIONS前必须加转移符，防止shell识别为自己的变量。</p>
<h4 id="导入指定列-columns"><a href="#导入指定列-columns" class="headerlink" title="导入指定列(columns)"></a>导入指定列(columns)</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--target-dir /user/company \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--num-mappers 1 \</span></span><br><span class="line"><span class="comment">--fields-terminated-by "\t" \</span></span><br><span class="line"><span class="comment">--columns id,sex \</span></span><br><span class="line"><span class="comment">--table staff</span></span><br></pre></td></tr></table></figure>

<ul>
<li>columns：指定导入的列</li>
</ul>
<h4 id="筛选查询导入数据-where"><a href="#筛选查询导入数据-where" class="headerlink" title="筛选查询导入数据(where)"></a>筛选查询导入数据(where)</h4><p>where参数可以进行一些简单的筛选</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--target-dir /user/company \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--num-mappers 1 \</span></span><br><span class="line"><span class="comment">--fields-terminated-by "\t" \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--where "id=1"</span></span><br></pre></td></tr></table></figure>

<ul>
<li>where：指定查询过滤条件</li>
</ul>
<h4 id="增量导入"><a href="#增量导入" class="headerlink" title="增量导入"></a>增量导入</h4><p>增量导入的一个场景就是昨天导入了一批数据，今天又增加了部分数据，现在要把这部分数据也导入到hdfs中。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://localhost:3306/company \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--target-dir /user/company/ \</span></span><br><span class="line"><span class="comment">--num-mappers 1 \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--null-string "" \</span></span><br><span class="line"><span class="comment">--null-non-string "0" \</span></span><br><span class="line"><span class="comment">--check-column "id" \</span></span><br><span class="line"><span class="comment">--incremental append \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line"><span class="comment">--last-value 0</span></span><br></pre></td></tr></table></figure>

<ul>
<li>null-string：字符串为null怎么处理</li>
<li>null-non-string：其他类型为null怎么处理</li>
<li>check-column：根据哪一行做增量导入</li>
<li>last-value：开始增量导入的上个位置</li>
</ul>
<h2 id="RDBMS导入到Hive"><a href="#RDBMS导入到Hive" class="headerlink" title="RDBMS导入到Hive"></a>RDBMS导入到Hive</h2><ol>
<li><p>在导入hive之前先在hive创建一样的表结构</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> staff(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">4</span>) , </span><br><span class="line">    <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">255</span>), </span><br><span class="line">    sex <span class="built_in">varchar</span>(<span class="number">255</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用sqoop导入数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--hive-overwrite \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--null-string "" \</span></span><br><span class="line"><span class="comment">--null-non-string "0" \</span></span><br><span class="line"><span class="comment">--hive-import \</span></span><br><span class="line"><span class="comment">--hive-database default \</span></span><br><span class="line"><span class="comment">--hive-table staff \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line"><span class="comment">--num-mappers 1</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>hive-import：数据从关系数据库中导入到hive表中</p>
</li>
<li><p>hive-overwrite：覆盖掉在hive表中已经存在的数据</p>
</li>
<li><p>hive-table：后面接hive表,默认使用MySQL的表名</p>
</li>
<li><p>如果导入的是分区表，需要指定分区的key和value</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--hive-partition-key key \</span></span><br><span class="line"><span class="comment">--hive-partition-value value \</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>该过程分为两步，第一步将数据导入到HDFS，第二步将导入到HDFS的数据迁移到Hive仓库，第一步默认的临时目录是/user/hadoop/表名</p>
</li>
<li><p>查看hive表中的数据:</p>
<p><code>select * from staff;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">id|name    |sex   |</span><br><span class="line"><span class="comment">--|--------|------|</span></span><br><span class="line"> 1|Thomas  |Male  |</span><br><span class="line"> 2|Catalina|FeMale|</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="HDFS导出到RDBMS"><a href="#HDFS导出到RDBMS" class="headerlink" title="HDFS导出到RDBMS"></a>HDFS导出到RDBMS</h2><ol>
<li><p>首先保证MySQL创建了一张和Hive一样表结构的表用来接收数据</p>
<p>注意表结构和分隔符都要一样</p>
</li>
<li><p>写Sqoop代码(批量导入)</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line">-Dsqoop.export.records.per.statement=10 \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--export-dir /user/company/ \</span></span><br><span class="line"><span class="comment">--null-string "" \</span></span><br><span class="line"><span class="comment">--null-non-string "0" \</span></span><br><span class="line"><span class="comment">--columns "id,name" \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure>

<ul>
<li><code>Dsqoop.export.records.per.statement</code>：批量更新，每隔10条提交一次 </li>
<li>export-dir：导出的hdfs目录</li>
<li>table：导入的表名</li>
<li>columns：指定导入的列</li>
</ul>
<p><code>注意:</code> MySQL中如果表不存在，不会自动创建</p>
</li>
</ol>
<h2 id="Hive导出到RDBMS"><a href="#Hive导出到RDBMS" class="headerlink" title="Hive导出到RDBMS"></a>Hive导出到RDBMS</h2><ol>
<li><p>首先保证MySQL创建了一张和Hive一样表结构的表用来接收数据</p>
<p>注意表结构和分隔符都要一样</p>
</li>
<li><p>写Sqoop代码</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/company \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--table staff \</span></span><br><span class="line"><span class="comment">--num-mappers 1 \</span></span><br><span class="line"><span class="comment">--export-dir /user/hive/warehouse/staff \</span></span><br><span class="line"><span class="comment">--input-fields-terminated-by "\t"</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>export-dir：指定被导出的目录</p>
</li>
<li><p>input-fields-terminated-by：导入的分隔符格式，和导入的fields-terminated-by有区别</p>
</li>
</ul>
<p><code>注意:</code> Mysql中如果表不存在，不会自动创建</p>
</li>
<li><p>查看MySQL数据库中的数据</p>
<p><code>select * from staff;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">----+----------+--------+</span></span><br><span class="line">| id | name     | sex    |</span><br><span class="line">+<span class="comment">----+----------+--------+</span></span><br><span class="line">|  1 | Thomas   | Male   |</span><br><span class="line">|  2 | Catalina | FeMale |</span><br><span class="line">|  3 | tunan    | Male   |</span><br><span class="line">+<span class="comment">----+----------+--------+</span></span><br></pre></td></tr></table></figure>


</li>
</ol>
<h2 id="Sqoop的综合操作"><a href="#Sqoop的综合操作" class="headerlink" title="Sqoop的综合操作"></a>Sqoop的综合操作</h2><p>需求：emp和dept表是在MySQL，把MySQL数据抽取到Hive进行统计分析，然后把统计的结果回写到MySQL中</p>
<ol>
<li><p>在Hive中创建与MySQL中emp和dept表相对应的表emp_hive，dept_hive</p>
<p><code>emp_hive表</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> emp_hive(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">job <span class="keyword">string</span>,</span><br><span class="line">mgr <span class="built_in">int</span>,</span><br><span class="line">hiredate <span class="keyword">string</span>,</span><br><span class="line">sal <span class="keyword">double</span>,</span><br><span class="line">comm <span class="keyword">double</span>,</span><br><span class="line">deptno <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>

<p><code>dept_hive表</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> dept_hive(</span><br><span class="line">deptno <span class="built_in">int</span>,</span><br><span class="line">dname <span class="keyword">string</span>,</span><br><span class="line">loc <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建在Hive中的中间结果表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> mid_hive(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">deptno <span class="built_in">int</span>,</span><br><span class="line">dname <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>将MySQL中的emp表中的数据传到emp_hive中</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/tunan \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--table emp \</span></span><br><span class="line"><span class="comment">--hive-overwrite \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--null-string "-" \</span></span><br><span class="line"><span class="comment">--null-non-string "0" \</span></span><br><span class="line"><span class="comment">--hive-import \</span></span><br><span class="line"><span class="comment">--hive-database default \</span></span><br><span class="line"><span class="comment">--hive-table emp_hive \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure>
</li>
<li><p>将MySQL中的dept表中的数据传到dept_hive</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/tunan \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--table dept \</span></span><br><span class="line"><span class="comment">--delete-target-dir \</span></span><br><span class="line"><span class="comment">--null-string "-" \</span></span><br><span class="line"><span class="comment">--null-non-string '0' \</span></span><br><span class="line"><span class="comment">--hive-import \</span></span><br><span class="line"><span class="comment">--hive-database default \</span></span><br><span class="line"><span class="comment">--hive-table dept_hive \</span></span><br><span class="line"><span class="comment">--fields-terminated-by '\t' \</span></span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure>
</li>
<li><p>将Hive中的表进行业务处理</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> </span><br><span class="line">	mid_hive</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">	e.empno,e.ename,d.deptno,d.dname</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	emp_hive  e</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">	dept_hive  d</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">	e.deptno=d.deptno</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">	*</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	mid_hive;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看中间表的数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> mid_hive;</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |deptno|dname     |</span><br><span class="line"><span class="comment">-----|------|------|----------|</span></span><br><span class="line"> 7369|SMITH |    20|RESEARCH  |</span><br><span class="line"> 7499|ALLEN |    30|SALES     |</span><br><span class="line"> 7521|WARD  |    30|SALES     |</span><br><span class="line"> 7566|JONES |    20|RESEARCH  |</span><br><span class="line"> 7654|MARTIN|    30|SALES     |</span><br><span class="line"> 7698|BLAKE |    30|SALES     |</span><br><span class="line"> 7782|CLARK |    10|ACCOUNTING|</span><br><span class="line"> 7788|SCOTT |    20|RESEARCH  |</span><br><span class="line"> 7839|KING  |    10|ACCOUNTING|</span><br><span class="line"> 7844|TURNER|    30|SALES     |</span><br><span class="line"> 7876|ADAMS |    20|RESEARCH  |</span><br><span class="line"> 7900|JAMES |    30|SALES     |</span><br><span class="line"> 7902|FORD  |    20|RESEARCH  |</span><br><span class="line"> 7934|MILLER|    10|ACCOUNTING|</span><br></pre></td></tr></table></figure>
</li>
<li><p>在MySQL中创建返回数据的表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="string">`mid`</span>(</span><br><span class="line">empno <span class="built_in">int</span>(<span class="number">11</span>),</span><br><span class="line">ename <span class="built_in">varchar</span>(<span class="number">20</span>),</span><br><span class="line">deptno <span class="built_in">int</span>(<span class="number">11</span>),</span><br><span class="line">dname <span class="built_in">varchar</span>(<span class="number">20</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li><p>将处理好的数据用Sqoop发回MySQL</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://aliyun:3306/tunan \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password root \</span></span><br><span class="line"><span class="comment">--table mid \</span></span><br><span class="line">-m 1 \</span><br><span class="line"><span class="comment">--export-dir /user/hive/warehouse/mid_hive \</span></span><br><span class="line"><span class="comment">--input-fields-terminated-by '\t'</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>查看MySQL中已经发回的数据</p>
<p><code>select * from mid;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">-------+--------+--------+------------+</span></span><br><span class="line">| empno | ename  | deptno | dname      |</span><br><span class="line">+<span class="comment">-------+--------+--------+------------+</span></span><br><span class="line">|  7369 | SMITH  |     20 | RESEARCH   |</span><br><span class="line">|  7499 | ALLEN  |     30 | SALES      |</span><br><span class="line">|  7521 | WARD   |     30 | SALES      |</span><br><span class="line">|  7566 | JONES  |     20 | RESEARCH   |</span><br><span class="line">|  7654 | MARTIN |     30 | SALES      |</span><br><span class="line">|  7698 | BLAKE  |     30 | SALES      |</span><br><span class="line">|  7782 | CLARK  |     10 | ACCOUNTING |</span><br><span class="line">|  7788 | SCOTT  |     20 | RESEARCH   |</span><br><span class="line">|  7839 | KING   |     10 | ACCOUNTING |</span><br><span class="line">|  7844 | TURNER |     30 | SALES      |</span><br><span class="line">|  7876 | ADAMS  |     20 | RESEARCH   |</span><br><span class="line">|  7900 | JAMES  |     30 | SALES      |</span><br><span class="line">|  7902 | FORD   |     20 | RESEARCH   |</span><br><span class="line">|  7934 | MILLER |     10 | ACCOUNTING |</span><br><span class="line">+<span class="comment">-------+--------+--------+------------+</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="shell操作数据库"><a href="#shell操作数据库" class="headerlink" title="shell操作数据库"></a>shell操作数据库</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql -uroot -pruozedata &lt;&lt;EOF</span><br><span class="line">	<span class="keyword">use</span> sqoop;</span><br><span class="line">	<span class="keyword">truncate</span> etl_result;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/12/01/flume/1/">Flume架构摸排</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-12-01</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Flume/">Flume</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Flume/">Flume</a></span><div class="content"><h2 id="Flume定义"><a href="#Flume定义" class="headerlink" title="Flume定义"></a>Flume定义</h2><p>Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单。</p>
<h2 id="Flume的优点"><a href="#Flume的优点" class="headerlink" title="Flume的优点"></a>Flume的优点</h2><ol>
<li><p>可以和任意存储进程集成。</p>
</li>
<li><p>输入的的数据速率大于写入目的存储的速率，flume会进行缓冲，减小hdfs的压力。</p>
</li>
<li><p>flume中的事务基于channel，使用了两个事务模型（sender + receiver），确保消息被可靠发送。</p>
</li>
</ol>
<p>Flume使用两个独立的事务分别负责从soucrce到channel，以及从channel到sink的事件传递。一旦事务中所有的数据全部成功提交到channel，那么source才认为该数据读取完成。同理，只有成功被sink写出去的数据，才会从channel中移除。</p>
<h2 id="Flume组成架构"><a href="#Flume组成架构" class="headerlink" title="Flume组成架构"></a>Flume组成架构</h2><p><img src="https://yerias.github.io/flume_img/flume.jpg" alt="flume架构图"></p>
<ul>
<li><p>Agent</p>
<p>Agent是一个JVM进程，它以事件的形式将数据从源头送至目的。</p>
<p>Agent主要有3个部分组成，Source、Channel、Sink。</p>
</li>
<li><p>Source</p>
<p>Source是负责接收数据到Flume Agent的组件。</p>
<p>Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。</p>
</li>
<li><p>Channel</p>
<p>Channel是位于Source和Sink之间的缓冲区。因此，Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。</p>
<p>Flume自带两种Channel：Memory Channel和File Channel。</p>
<ol>
<li><p>Memory Channel是内存中的队列。Memory Channel在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么Memory Channel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。</p>
</li>
<li><p>File Channel将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。</p>
</li>
</ol>
</li>
<li><p>Sink</p>
<p>Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。</p>
<p>Sink是完全事务性的。在从Channel批量删除数据之前，每个Sink用Channel启动一个事务。批量事件一旦成功写出到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。</p>
<p>Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。</p>
</li>
<li><p>Event</p>
<p>传输单元，Flume数据传输的基本单元，以事件的形式将数据从源头送至目的地。 Event由可选的header和载有数据的一个byte array 构成。Header是容纳了key-value字符串对的HashMap。</p>
</li>
</ul>
<h2 id="Flume拓扑结构"><a href="#Flume拓扑结构" class="headerlink" title="Flume拓扑结构"></a>Flume拓扑结构</h2><h3 id="Flume-Agent连接"><a href="#Flume-Agent连接" class="headerlink" title="Flume Agent连接"></a>Flume Agent连接</h3><p><img src="https://yerias.github.io/flume_img/flume2.jpg" alt="flume2"></p>
<p>这种模式是将多个flume给顺序连接起来了，从最初的source开始到最终sink传送的目的存储系统。此模式不建议桥接过多的flume数量， flume数量过多不仅会影响传输速率，而且一旦传输过程中某个节点flume宕机，会影响整个传输系统。</p>
<h3 id="单source，多channel、sink"><a href="#单source，多channel、sink" class="headerlink" title="单source，多channel、sink"></a>单source，多channel、sink</h3><p><img src="https://yerias.github.io/flume_img/flume3.jpg" alt="flume3"></p>
<p>Flume支持将事件流向一个或者多个目的地。这种模式将数据源复制到多个channel中，每个channel都有相同的数据，sink可以选择传送的不同的目的地。</p>
<h3 id="Flume负载均衡"><a href="#Flume负载均衡" class="headerlink" title="Flume负载均衡"></a>Flume负载均衡</h3><p><img src="https://yerias.github.io/flume_img/flume4.jpg" alt="flume4"></p>
<p>Flume支持使用将多个sink逻辑上分到一个sink组，flume将数据发送到不同的sink，主要解决负载均衡和故障转移问题。</p>
<h3 id="Flume-Agent聚合"><a href="#Flume-Agent聚合" class="headerlink" title="Flume Agent聚合"></a>Flume Agent聚合</h3><p><img src="https://yerias.github.io/flume_img/flume5.jpg" alt="flume5"></p>
<p>这种模式是我们最常见的，也非常实用，日常web应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用flume的这种组合方式能很好的解决这一问题，每台服务器部署一个flume采集日志，传送到一个集中收集日志的flume，再由此flume上传到hdfs、hive、hbase、jms等，进行日志分析。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/14/hive/14/">Idea加载Hive源码，并且在控制台查询SQL</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-14</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h2 id="编译源码"><a href="#编译源码" class="headerlink" title="编译源码"></a>编译源码</h2><ol>
<li><p>下载Hive源码:<a href="http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.16.2-src.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.16.2-src.tar.gz</a></p>
</li>
<li><p>编译Hive源码(切记不要idea里面执行命令)：mvn clean package -DskipTests=true -Phadoop-2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mvn clean package -DskipTests=true -Phadoop-2</span><br><span class="line"></span><br><span class="line">[INFO] Reactor Summary:</span><br><span class="line">[INFO]</span><br><span class="line">[INFO] Hive 1.1.0-cdh5.16.2 ............................... SUCCESS [  3.119 s]</span><br><span class="line">[INFO] Hive Classifications ............................... SUCCESS [  2.406 s]</span><br><span class="line">[INFO] Hive Shims Common .................................. SUCCESS [  3.327 s]</span><br><span class="line">[INFO] Hive Shims 0.23 .................................... SUCCESS [  3.494 s]</span><br><span class="line">[INFO] Hive Shims Scheduler ............................... SUCCESS [  2.423 s]</span><br><span class="line">[INFO] Hive Shims ......................................... SUCCESS [  1.463 s]</span><br><span class="line">[INFO] Hive Common ........................................ SUCCESS [  8.382 s]</span><br><span class="line">[INFO] Hive Serde ......................................... SUCCESS [  8.001 s]</span><br><span class="line">[INFO] Hive Metastore ..................................... SUCCESS [ 28.285 s]</span><br><span class="line">[INFO] Hive Ant Utilities ................................. SUCCESS [  1.668 s]</span><br><span class="line">[INFO] Spark Remote Client ................................ SUCCESS [  4.915 s]</span><br><span class="line">[INFO] Hive Query Language ................................ SUCCESS [01:36 min]</span><br><span class="line">[INFO] Hive Service ....................................... SUCCESS [ 22.921 s]</span><br><span class="line">[INFO] Hive Accumulo Handler .............................. SUCCESS [  5.496 s]</span><br><span class="line">[INFO] Hive JDBC .......................................... SUCCESS [  5.797 s]</span><br><span class="line">[INFO] Hive Beeline ....................................... SUCCESS [  3.957 s]</span><br><span class="line">[INFO] Hive CLI ........................................... SUCCESS [  4.060 s]</span><br><span class="line">[INFO] Hive Contrib ....................................... SUCCESS [  4.321 s]</span><br><span class="line">[INFO] Hive HBase Handler ................................. SUCCESS [  5.518 s]</span><br><span class="line">[INFO] Hive HCatalog ...................................... SUCCESS [  1.399 s]</span><br><span class="line">[INFO] Hive HCatalog Core ................................. SUCCESS [  5.933 s]</span><br><span class="line">[INFO] Hive HCatalog Pig Adapter .......................... SUCCESS [  4.632 s]</span><br><span class="line">[INFO] Hive HCatalog Server Extensions .................... SUCCESS [  4.477 s]</span><br><span class="line">[INFO] Hive HCatalog Webhcat Java Client .................. SUCCESS [  4.903 s]</span><br><span class="line">[INFO] Hive HCatalog Webhcat .............................. SUCCESS [  7.452 s]</span><br><span class="line">[INFO] Hive HCatalog Streaming ............................ SUCCESS [  4.306 s]</span><br><span class="line">[INFO] Hive HWI ........................................... SUCCESS [  3.461 s]</span><br><span class="line">[INFO] Hive ODBC .......................................... SUCCESS [  3.061 s]</span><br><span class="line">[INFO] Hive Shims Aggregator .............................. SUCCESS [  0.840 s]</span><br><span class="line">[INFO] Hive TestUtils ..................................... SUCCESS [  1.077 s]</span><br><span class="line">[INFO] Hive Packaging 1.1.0-cdh5.16.2 ..................... SUCCESS [  4.194 s]</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 04:22 min</span><br><span class="line">[INFO] Finished at: 2020-04-12T18:50:46+08:00</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>
</li>
<li><p>IDEA导入源码</p>
</li>
</ol>
<h2 id="本地调试Hive-SQL"><a href="#本地调试Hive-SQL" class="headerlink" title="本地调试Hive SQL"></a>本地调试Hive SQL</h2><ol>
<li><p>集群服务器启动metastore:hive –service metastore -p 9083 &amp;</p>
</li>
<li><p>分别把集群的配置文件加载到hive-cli的resources下</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">core-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://ruozedata001:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">hdfs-site.xml</span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.use.datanode.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">mapred-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">yarn-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">hive-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://ruozedata001:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>在CliDriver类中加入vm参数:<code>-Djline.WindowsTerminal.directConsole=false</code>,Hive是默认Jline输入到控制太只支持Linux和mac,所以需要关闭</p>
<p><img src="https://yerias.github.io/hive_img/hive_client" alt="hive client"></p>
</li>
<li><p>运行CliDriver</p>
</li>
<li><p>控制台输入</p>
<p><img src="https://yerias.github.io/hive_img/hive_client2.png" alt="hive client"></p>
</li>
</ol>
<p><em>20200415更新：</em> idea2020.1版本支持本地调试hive，无需任何设置</p>
<hr>
<p>转载自：<a href="https://blog.csdn.net/jim8973/article/details/105503221" target="_blank" rel="noopener">https://blog.csdn.net/jim8973/article/details/105503221</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/13/hive/13/">HIVE调优(2)</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-13</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>Hadoop 框架计算特性</li>
<li>优化常用手段</li>
<li>排序选择</li>
<li>怎样做笛卡尔积</li>
<li>怎样写 in/exists 语句</li>
<li>设置合理的 maptask 数量</li>
<li>小文件合并</li>
<li>设置合理的 reduceTask 的数量</li>
<li>合理利用分桶：Bucketing 和 Sampling</li>
<li>合理利用分区：Partition </li>
<li>Join 优化</li>
<li>Group By 优化</li>
<li>合理利用文件存储格式 </li>
<li>本地模式执行 MapReduce</li>
<li>并行化处理</li>
<li>设置压缩存储</li>
</ol>
<h2 id="Hadoop-框架计算特性"><a href="#Hadoop-框架计算特性" class="headerlink" title="Hadoop 框架计算特性"></a>Hadoop 框架计算特性</h2><ol>
<li><p>数据量大不是问题，数据倾斜是个问题</p>
</li>
<li><p>jobs 数比较多的作业运行效率相对比较低，比如即使有几百行的表，如果多次关联多次 汇总，产生十几个 jobs，耗时很长。原因是 map reduce 作业初始化的时间是比较长的</p>
</li>
<li><p>sum,count,max,min 等 UDAF，不怕数据倾斜问题，hadoop 在 map 端的汇总合并优化，使 数据倾斜不成问题</p>
</li>
<li><p>count(distinct userid)，在数据量大的情况下，效率较低，如果是多 count(distinct userid,month)效率更低，因为 count(distinct)是按 group by 字段分组，按 distinct 字段排序， 一般这种分布方式是很</p>
</li>
</ol>
<p>倾斜的，比如 PV 数据，淘宝一天 30 亿的 pv，如果按性别分组，分 配 2 个 reduce，每个 reduce 期望处理 15 亿数据，但现实必定是男少女多</p>
<h2 id="优化常用手段"><a href="#优化常用手段" class="headerlink" title="优化常用手段"></a>优化常用手段</h2><ol>
<li><p>好的模型设计事半功倍</p>
</li>
<li><p>解决数据倾斜问题</p>
</li>
<li><p>减少 job 数</p>
</li>
<li><p>设置合理的 MapReduce 的 task 数，能有效提升性能。(比如，10w+级别的计算，用 160个 reduce，那是相当的浪费，1 个足够)</p>
</li>
<li><p>了解数据分布，自己动手解决数据倾斜问题是个不错的选择。这是通用的算法优化，但 算法优化有时不能适应特定业务背景，开发人员了解业务，了解数据，可以通过业务逻辑精 确有效的解决数据倾斜问题</p>
</li>
<li><p>数据量较大的情况下，慎用 count(distinct)，group by 容易产生倾斜问题</p>
</li>
<li><p>对小文件进行合并，是行之有效的提高调度效率的方法，假如所有的作业设置合理的文 件数，对云梯的整体调度效率也会产生积极的正向影响</p>
</li>
<li><p>优化时把握整体，单个作业最优不如整体最优</p>
</li>
</ol>
<h2 id="排序选择"><a href="#排序选择" class="headerlink" title="排序选择"></a>排序选择</h2><ul>
<li><p><strong>cluster by</strong>：对同一字段分桶并排序，不能和 sort by 连用</p>
</li>
<li><p><strong>distribute by + sort by</strong>：分桶，保证同一字段值只存在一个结果文件当中，结合 sort by 保证 每个 reduceTask 结果有序</p>
</li>
<li><p><strong>sort by</strong>：单机排序，单个 reduce 结果有序</p>
</li>
<li><p><strong>order by</strong>：全局排序，缺陷是只能使用一个 reduce</p>
</li>
</ul>
<p><strong>一定要区分这四种排序的使用方式和适用场景</strong></p>
<h2 id="怎样做笛卡尔积"><a href="#怎样做笛卡尔积" class="headerlink" title="怎样做笛卡尔积"></a>怎样做笛卡尔积</h2><p>当 Hive 设定为严格模式（hive.mapred.mode=strict）时，不允许在 HQL 语句中出现笛卡尔积， 这实际说明了 Hive 对笛卡尔积支持较弱。因为找不到 Join key，Hive 只能使用 1 个 reducer 来完成笛卡尔积。</p>
<p>当然也可以使用 limit 的办法来减少某个表参与 join 的数据量，但对于需要笛卡尔积语义的 需求来说，经常是一个大表和一个小表的 Join 操作，结果仍然很大（以至于无法用单机处 理），这时 MapJoin才是最好的解决办法。MapJoin，顾名思义，会在 Map 端完成 Join 操作。 这需要将 Join 操作的一个或多个表完全读入内存。</p>
<p>PS：MapJoin 在子查询中可能出现未知 BUG。在大表和小表做笛卡尔积时，规避笛卡尔积的 方法是，给 Join 添加一个 Join key，<strong>原理很简单：将小表扩充一列 join key，并将小表的条目复制数倍，join</strong> <strong>key 各不相同；将大表扩充一列 join key 为随机数。</strong></p>
<p><strong>精髓就在于复制几倍，最后就有几个 reduce 来做，而且大表的数据是前面小表扩张 key 值 范围里面随机出来的，所以复制了几倍 n，就相当于这个随机范围就有多大 n，那么相应的， 大表的数据就被随机的分为了 n 份。并且最后处理所用的 reduce 数量也是 n，而且也不会 出现数据倾斜。</strong></p>
<h2 id="怎样写-in-exists-语句"><a href="#怎样写-in-exists-语句" class="headerlink" title="怎样写 in/exists 语句"></a>怎样写 in/exists 语句</h2><p>虽然经过测验，hive1.2.1 也支持 in/exists 操作，但还是推荐使用 hive 的一个高效替代方案：<strong>left semi join</strong></p>
<p>比如说：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">where</span> a.id <span class="keyword">in</span> (<span class="keyword">select</span> b.id <span class="keyword">from</span> b);</span><br><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">where</span> <span class="keyword">exists</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> b <span class="keyword">where</span> a.id = b.id);</span><br></pre></td></tr></table></figure>

<p>应该转换成：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">left</span> <span class="keyword">semi</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.id = b.id;</span><br></pre></td></tr></table></figure>



<h2 id="设置合理的-maptask-数量"><a href="#设置合理的-maptask-数量" class="headerlink" title="设置合理的 maptask 数量"></a>设置合理的 maptask 数量</h2><ol>
<li><p>Map 数过大</p>
<p>Map 阶段输出文件太小，产生大量小文件</p>
<p>初始化和创建 Map 的开销很大</p>
</li>
<li><p>Map 数太小</p>
<p>文件处理或查询并发度小，Job 执行时间过长</p>
<p>大量作业时，容易堵塞集群 </p>
</li>
</ol>
<p>在 MapReduce 的编程案例中，我们得知，一个MR Job的 MapTask 数量是由输入分片 InputSplit 决定的。而输入分片是由 FileInputFormat.getSplit()决定的。一个输入分片对应一个 MapTask， 而输入分片是由三个参数决定的：</p>
<p><img src="https://yerias.github.io/hive_img/%E8%AE%BE%E7%BD%AEmap%E4%BB%BB%E5%8A%A1%E6%95%B0%E9%87%8F.png" alt="设置map任务数量"></p>
<p>输入分片大小的计算是这么计算出来的：</p>
<p><strong>long splitSize = Math.max(minSize, Math.min(maxSize, blockSize))</strong></p>
<p>默认情况下，输入分片大小和 HDFS 集群默认数据块大小一致，也就是默认一个数据块，启 用一个 MapTask 进行处理，这样做的好处是避免了服务器节点之间的数据传输，提高 job 处 理效率</p>
<p>两种经典的控制 MapTask 的个数方案：减少 MapTask 数或者增加 MapTask 数</p>
<ol>
<li>减少 MapTask 数是通过合并小文件来实现，这一点主要是针对数据源</li>
<li>增加 MapTask 数可以通过控制上一个 job 的 reduceTask 个数</li>
</ol>
<p>因为 Hive 语句最终要转换为一系列的 MapReduce Job 的，而每一个 MapReduce Job 是由一 系列的 MapTask 和 ReduceTask 组成的，默认情况下， MapReduce 中一个 MapTask 或者一个 ReduceTask 就会启动一个 JVM 进程，一个 Task 执行完毕后， JVM 进程就退出。这样如果任 务花费时间很短，又要多次启动 JVM 的情况下，JVM 的启动时间会变成一个比较大的消耗， 这个时候，就可以通过重用 JVM 来解决：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set mapred.job.reuse.jvm.num.tasks=5</span><br></pre></td></tr></table></figure>

<h2 id="小文件合并"><a href="#小文件合并" class="headerlink" title="小文件合并"></a>小文件合并</h2><p>文件数目过多，会给 HDFS 带来压力，并且会影响处理效率，可以通过合并 Map 和 Reduce 的 结果文件来消除这样的影响：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.merge.mapfiles = true ##在 map only 的任务结束时合并小文件</span><br><span class="line">set hive.merge.mapredfiles = false ## true 时在 MapReduce 的任务结束时合并小文件</span><br><span class="line">set hive.merge.size.per.task = 256*1000*1000 ##合并文件的大小</span><br><span class="line">set mapred.max.split.size=256000000; ##每个 Map 最大分割大小</span><br><span class="line">set mapred.min.split.size.per.node=1; ##一个节点上 split 的最少值</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; ##执行 Map 前进行小文件合并</span><br></pre></td></tr></table></figure>

<h2 id="设置合理的-reduceTask-的数量"><a href="#设置合理的-reduceTask-的数量" class="headerlink" title="设置合理的 reduceTask 的数量"></a>设置合理的 reduceTask 的数量</h2><p>Hadoop MapReduce 程序中，reducer 个数的设定极大影响执行效率，这使得 Hive 怎样决定 reducer 个数成为一个关键问题。遗憾的是 Hive 的估计机制很弱，不指定 reducer 个数的情 况下，Hive 会猜测确定一个 reducer 个数，基于以下两个设定：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive.exec.reducers.bytes.per.reducer（默认为 256000000）</span><br><span class="line">hive.exec.reducers.max（默认为 1009）</span><br><span class="line">mapreduce.job.reduces=-1（设置一个常量 reducetask 数量）</span><br></pre></td></tr></table></figure>

<p>计算 reducer 数的公式很简单： N=min(参数 2，总输入数据量/参数 1) 通常情况下，有必要手动指定 reducer 个数。考虑到 map 阶段的输出数据量通常会比输入有 大幅减少，因此即使不设定 reducer 个数，重设参数 2 还是必要的。</p>
<p><strong>依据 Hadoop 的经验，可以将参数 2 设定为 0.95*(集群中 datanode 个数)。</strong> </p>
<h2 id="合并-MapReduce-操作"><a href="#合并-MapReduce-操作" class="headerlink" title="合并 MapReduce 操作"></a>合并 MapReduce 操作</h2><p>Multi-group by 是 Hive 的一个非常好的特性，它使得 Hive 中利用中间结果变得非常方便。 例如：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">FROM (<span class="keyword">SELECT</span> a.status, b.school, b.gender <span class="keyword">FROM</span> status_updates a <span class="keyword">JOIN</span> <span class="keyword">profiles</span> b <span class="keyword">ON</span> (a.userid =</span><br><span class="line">b.userid <span class="keyword">and</span> a.ds=<span class="string">'2009-03-20'</span> ) ) subq1</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> gender_summary <span class="keyword">PARTITION</span>(ds=<span class="string">'2009-03-20'</span>)</span><br><span class="line"><span class="keyword">SELECT</span> subq1.gender, <span class="keyword">COUNT</span>(<span class="number">1</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> subq1.gender</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> school_summary <span class="keyword">PARTITION</span>(ds=<span class="string">'2009-03-20'</span>)</span><br><span class="line"><span class="keyword">SELECT</span> subq1.school, <span class="keyword">COUNT</span>(<span class="number">1</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> subq1.school</span><br></pre></td></tr></table></figure>

<p>上述查询语句使用了 multi-group by 特性连续 group by 了 2 次数据，使用不同的 group by key。 这一特性可以减少一次 MapReduce 操作</p>
<h2 id="合理利用分桶：Bucketing-和-Sampling"><a href="#合理利用分桶：Bucketing-和-Sampling" class="headerlink" title="合理利用分桶：Bucketing 和 Sampling"></a>合理利用分桶：Bucketing 和 Sampling</h2><p>Bucket 是指将数据以指定列的值为 key 进行 hash，hash 到指定数目的桶中。这样就可以支 持高效采样了。如下例就是以 userid 这一列为 bucket 的依据，共设置 32 个 buckets</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> page_view(viewTime <span class="built_in">INT</span>, userid <span class="built_in">BIGINT</span>,</span><br><span class="line"> page_url <span class="keyword">STRING</span>, referrer_url <span class="keyword">STRING</span>,</span><br><span class="line"> ip <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'IP Address of the User'</span>)</span><br><span class="line"> <span class="keyword">COMMENT</span> <span class="string">'This is the page view table'</span></span><br><span class="line"> PARTITIONED <span class="keyword">BY</span>(dt <span class="keyword">STRING</span>, country <span class="keyword">STRING</span>)</span><br><span class="line"> CLUSTERED <span class="keyword">BY</span>(userid) SORTED <span class="keyword">BY</span>(viewTime) <span class="keyword">INTO</span> <span class="number">32</span> BUCKETS</span><br><span class="line"> <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line"> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'1'</span></span><br><span class="line"> COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'2'</span></span><br><span class="line"> <span class="keyword">MAP</span> <span class="keyword">KEYS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'3'</span></span><br><span class="line"> <span class="keyword">STORED</span> <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure>

<p>通常情况下，Sampling 在全体数据上进行采样，这样效率自然就低，它要去访问所有数据。 而如果一个表已经对某一列制作了 bucket，就可以采样所有桶中指定序号的某个桶，这就 减少了访问量。</p>
<p>如下例所示就是采样了 page_view 中 32 个桶中的第三个桶的全部数据：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> page_view <span class="keyword">TABLESAMPLE</span>(<span class="keyword">BUCKET</span> <span class="number">3</span> <span class="keyword">OUT</span> <span class="keyword">OF</span> <span class="number">32</span>);</span><br></pre></td></tr></table></figure>

<p>如下例所示就是采样了 page_view 中 32 个桶中的第三个桶的一半数据：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> page_view <span class="keyword">TABLESAMPLE</span>(<span class="keyword">BUCKET</span> <span class="number">3</span> <span class="keyword">OUT</span> <span class="keyword">OF</span> <span class="number">64</span>);</span><br></pre></td></tr></table></figure>

<h2 id="合理利用分区：Partition"><a href="#合理利用分区：Partition" class="headerlink" title="合理利用分区：Partition"></a>合理利用分区：Partition</h2><p> Partition 就是分区。分区通过在创建表时启用 partitioned by 实现，用来 partition 的维度并不 是实际数据的某一列，具体分区的标志是由插入内容时给定的。当要查询某一分区的内容时 可以采用 where 语句，形似 where tablename.partition_column = a 来实现。</p>
<p>创建含分区的表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> page_view(viewTime <span class="built_in">INT</span>, userid <span class="built_in">BIGINT</span>,</span><br><span class="line"> page_url <span class="keyword">STRING</span>, referrer_url <span class="keyword">STRING</span>,</span><br><span class="line"> ip <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'IP Address of the User'</span>)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span>(<span class="built_in">date</span> <span class="keyword">STRING</span>, country <span class="keyword">STRING</span>)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'1'</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> TEXTFILE;</span><br></pre></td></tr></table></figure>

<p>载入内容，并指定分区标志</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/pv_2008-06-08_us.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> page_view</span><br><span class="line"><span class="keyword">partition</span>(<span class="built_in">date</span>=<span class="string">'2008-06-08'</span>, country=<span class="string">'US'</span>);</span><br></pre></td></tr></table></figure>

<p>查询指定标志的分区内容</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> page_views.* <span class="keyword">FROM</span> page_views</span><br><span class="line"> <span class="keyword">WHERE</span> page_views.date &gt;= <span class="string">'2008-03-01'</span> <span class="keyword">AND</span> page_views.date &lt;= <span class="string">'2008-03-31'</span> <span class="keyword">AND</span></span><br><span class="line">page_views.referrer_url <span class="keyword">like</span> <span class="string">'%xyz.com'</span>;</span><br></pre></td></tr></table></figure>

<h2 id="Join-优化"><a href="#Join-优化" class="headerlink" title="Join 优化"></a>Join 优化</h2><p>总体原则：</p>
<ol>
<li><p>优先过滤后再进行 Join 操作，最大限度的减少参与 join 的数据量</p>
</li>
<li><p>小表 join 大表，最好启动 mapjoin</p>
</li>
<li><p>Join on 的条件相同的话，最好放入同一个 job，并且 join 表的排列顺序从小到大 </p>
</li>
</ol>
<p><strong>在使用写有 Join 操作的查询语句时有一条原则：应该将条目少的表/子查询放在 Join 操作 符的左边。</strong>原因是在 Join 操作的 Reduce 阶段，位于 Join 操作符左边的表的内容会被加 载进内存，将条目少的表放在左边，可以有效减少发生 OOM 错误的几率。对于一条语句 中有多个 Join 的情况，如果 Join 的条件相同，比如查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> pv_users</span><br><span class="line"><span class="keyword">SELECT</span> pv.pageid, u.age <span class="keyword">FROM</span> page_view p</span><br><span class="line"><span class="keyword">JOIN</span> <span class="keyword">user</span> u <span class="keyword">ON</span> (pv.userid = u.userid)</span><br><span class="line"><span class="keyword">JOIN</span> newuser x <span class="keyword">ON</span> (u.userid = x.userid);</span><br></pre></td></tr></table></figure>

<p>如果 Join 的 key 相同，不管有多少个表，都会则会合并为一个 Map-Reduce 任务，而不 是”n”个，在做 OUTER JOIN 的时候也是一样</p>
<p>如果 join 的条件不相同，比如：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> pv_users</span><br><span class="line"> <span class="keyword">SELECT</span> pv.pageid, u.age <span class="keyword">FROM</span> page_view p</span><br><span class="line"> <span class="keyword">JOIN</span> <span class="keyword">user</span> u <span class="keyword">ON</span> (pv.userid = u.userid)</span><br><span class="line"> <span class="keyword">JOIN</span> newuser x <span class="keyword">on</span> (u.age = x.age);</span><br></pre></td></tr></table></figure>

<p>Map-Reduce 的任务数目和 Join 操作的数目是对应的，上述查询和以下查询是等价的</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--先 page_view 表和 user 表做链接</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tmptable</span><br><span class="line"> <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> page_view p <span class="keyword">JOIN</span> <span class="keyword">user</span> u <span class="keyword">ON</span> (pv.userid = u.userid);</span><br><span class="line"><span class="comment">-- 然后结果表 temptable 和 newuser 表做链接</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> pv_users</span><br><span class="line"> <span class="keyword">SELECT</span> x.pageid, x.age <span class="keyword">FROM</span> tmptable x <span class="keyword">JOIN</span> newuser y <span class="keyword">ON</span> (x.age = y.age);</span><br></pre></td></tr></table></figure>

<p>在编写 Join 查询语句时，如果确定是由于 join 出现的数据倾斜，那么请做如下设置：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.skewjoin.key=100000; // 这个是 join 的键对应的记录条数超过这个值则会进行 分拆，值根据具体数据量设置</span><br><span class="line">set hive.optimize.skewjoin=true; // 如果是 join 过程出现倾斜应该设置为 true</span><br></pre></td></tr></table></figure>

<h2 id="Group-By-优化"><a href="#Group-By-优化" class="headerlink" title="Group By 优化"></a>Group By 优化</h2><h3 id="1-Map-端部分聚合"><a href="#1-Map-端部分聚合" class="headerlink" title="1. Map 端部分聚合"></a>1. Map 端部分聚合</h3><p>并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进 行部分聚合，最后在Reduce 端得出最终结果。</p>
<p>MapReduce 的 combiner 组件参数包括：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.map.aggr = true 是否在 Map 端进行聚合，默认为 True</span><br><span class="line">set hive.groupby.mapaggr.checkinterval = 100000 在 Map 端进行聚合操作的条目数目</span><br></pre></td></tr></table></figure>

<h3 id="2-使用-Group-By-有数据倾斜的时候进行负载均衡"><a href="#2-使用-Group-By-有数据倾斜的时候进行负载均衡" class="headerlink" title="2. 使用 Group By 有数据倾斜的时候进行负载均衡"></a>2. 使用 Group By 有数据倾斜的时候进行负载均衡</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.groupby.skewindata = true</span><br></pre></td></tr></table></figure>

<p>当 sql 语句使用 groupby 时数据出现倾斜时，如果该变量设置为 true，那么 Hive 会自动进行 负载均衡。<strong>策略就是把 MR 任务拆分成两个：第一个先做预汇总，第二个再做最终汇总</strong></p>
<p>在 MR 的第一个阶段中，Map 的输出结果集合会缓存到 maptaks 中，每个 Reduce 做部分聚 合操作，并输出结果，这样处理的结果是相同 Group By Key 有可能被分发到不同的 Reduce 中， 从而达到负载均衡的目的；第二个阶段 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成 最终的聚合操作。</p>
<h2 id="合理利用文件存储格式"><a href="#合理利用文件存储格式" class="headerlink" title="合理利用文件存储格式"></a>合理利用文件存储格式</h2><p>创建表时，尽量使用 orc、parquet 这些列式存储格式，因为列式存储的表，每一列的数据在 物理上是存储在一起的，Hive 查询时会只遍历需要列数据，大大减少处理的数据量。</p>
<h2 id="本地模式执行-MapReduce"><a href="#本地模式执行-MapReduce" class="headerlink" title="本地模式执行 MapReduce"></a>本地模式执行 MapReduce</h2><p>Hive 在集群上查询时，默认是在集群上 N 台机器上运行， 需要多个机器进行协调运行，这 个方式很好地解决了大数据量的查询问题。但是当 Hive 查询处理的数据量比较小时，其实 没有必要启动分布式模式去执行，因为以分布式方式执行就涉及到跨网络传输、多节点协调 等，并且消耗资源。这个时间可以只使用本地模式来执行 mapreduce job，只在一台机器上 执行，速度会很快。启动本地模式涉及到三个参数：</p>
<p><img src="https://yerias.github.io/hive_img/%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F.png" alt="本地模式"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.exec.mode.local.auto=true 是打开 hive 自动判断是否启动本地模式的开关，但是只 是打开这个参数并不能保证启动本地模式，要当 map 任务数不超过</span><br><span class="line">hive.exec.mode.local.auto.input.files.max 的个数并且 map 输入文件大小不超过</span><br><span class="line">hive.exec.mode.local.auto.inputbytes.max 所指定的大小时，才能启动本地模式。</span><br></pre></td></tr></table></figure>

<h2 id="并行化处理"><a href="#并行化处理" class="headerlink" title="并行化处理"></a>并行化处理</h2><p>一个 hive sql 语句可能会转为多个 mapreduce Job，每一个 job 就是一个 stage，这些 job 顺序 执行，这个在 cli 的运行日志中也可以看到。但是有时候这些任务之间并不是是相互依赖的， 如果集群资源允许的话，可以让多个并不相互依赖 stage 并发执行，这样就节约了时间，提 高了执行速度，但是如果集群资源匮乏时，启用并行化反倒是会导致各个 job 相互抢占资源 而导致整体执行性能的下降。启用并行化：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.parallel=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number=<span class="number">8</span>; //同一个 sql 允许并行任务的最大线程数</span><br></pre></td></tr></table></figure>

<h2 id="设置压缩存储"><a href="#设置压缩存储" class="headerlink" title="设置压缩存储"></a>设置压缩存储</h2><h3 id="1-压缩的原因"><a href="#1-压缩的原因" class="headerlink" title="1. 压缩的原因"></a>1. 压缩的原因</h3><p>Hive 最终是转为 MapReduce 程序来执行的，而 MapReduce 的性能瓶颈在于网络 IO 和 磁盘 IO，要解决性能瓶颈，最主要的是减少数据量，对数据进行压缩是个好的方式。压缩 虽然是减少了数据量，但是压缩过程要消耗 CPU 的，但是在 Hadoop 中， 往往性能瓶颈不 在于 CPU，CPU 压力并不大，所以压缩充分利用了比较空闲的 CPU</p>
<h3 id="2-常用压缩方法对比"><a href="#2-常用压缩方法对比" class="headerlink" title="2. 常用压缩方法对比"></a>2. 常用压缩方法对比</h3><p><img src="https://yerias.github.io/hive_img/%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%E5%AF%B9%E6%AF%94.png" alt="压缩格式对比"></p>
<p>各个压缩方式所对应的 Class 类：</p>
<p><img src="https://yerias.github.io/hive_img/%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%E5%AF%B9%E5%BA%94%E7%9A%84%E7%B1%BB.png" alt="压缩格式对应的类"></p>
<h3 id="3-压缩方式的选择"><a href="#3-压缩方式的选择" class="headerlink" title="3. 压缩方式的选择"></a>3. 压缩方式的选择</h3><ol>
<li><p>压缩比率</p>
<p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E6%AF%94.png" alt="压缩比"></p>
</li>
<li><p>压缩解压缩速度</p>
<p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E6%97%B6%E9%97%B4.png" alt="压缩时间"></p>
</li>
<li><p>是否支持 Split</p>
<p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9.png" alt="压缩"></p>
</li>
</ol>
<h3 id="4-压缩使用"><a href="#4-压缩使用" class="headerlink" title="4. 压缩使用"></a>4. 压缩使用</h3><p>Job 输出文件按照 block 以 GZip 的方式进行压缩：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set mapreduce.output.fileoutputformat.compress=true // 默认值是 false</span><br><span class="line">set mapreduce.output.fileoutputformat.compress.type=BLOCK // 默认值是 Record</span><br><span class="line">set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec // 默认值是 org.apache.hadoop.io.compress.DefaultCodec</span><br></pre></td></tr></table></figure>

<p>Map 输出结果也以 Gzip 进行压缩：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set mapred.map.output.compress=true</span><br><span class="line">set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.GzipCodec // 默认值是 org.apache.hadoop.io.compress.DefaultCodec</span><br></pre></td></tr></table></figure>

<p>对 Hive 输出结果和中间都进行压缩：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.exec.compress.output=true // 默认值是 false，不压缩</span><br><span class="line">set hive.exec.compress.intermediate=true // 默认值是 false，为 true 时 MR 设置的压缩才启用</span><br></pre></td></tr></table></figure></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/12/hive/12/">HIVE调优之存储格式</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-12</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><p>目录</p>
<ol>
<li>行式数据库和列式数据库的对比</li>
<li>存储格式的比较</li>
<li>存储格式的应用</li>
</ol>
<h2 id="行式数据库和列式数据库的对比"><a href="#行式数据库和列式数据库的对比" class="headerlink" title="行式数据库和列式数据库的对比"></a>行式数据库和列式数据库的对比</h2><ol>
<li><p>存储比较</p>
<p>行式数据库存储在hdfs上式按行进行存储的，一个block存储一或多行数据。而列式数据库在hdfs上则是按照列进行存储，一个block可能有一列或多列数据。</p>
</li>
<li><p>压缩比较</p>
<p>对于行式数据库，必然按行压缩，当一行中有多个字段，各个字段对应的数据类型可能不一致，压缩性能压缩比就比较差。</p>
<p>对于列式数据库，必然按列压缩，每一列对应的是相同数据类型的数据，故列式数据库的压缩性能要强于行式数据库。</p>
</li>
<li><p>查询比较</p>
<p>假设执行的查询操作是：<code>select id,name from table_emp;</code></p>
<p>对于行式数据库，它要遍历一整张表将每一行中的id,name字段拼接再展现出来，这样需要查询的数据量就比较大，效率低。</p>
<p>对于列式数据库，它只需找到对应的id,name字段的列展现出来即可，需要查询的数据量小，效率高。</p>
<p>假设执行的查询操作是：<code>select * from table_emp;</code></p>
<p>对于这种查询整个表全部信息的操作，由于列式数据库需要将分散的行进行重新组合，行式数据库效率就高于列式数据库。</p>
<p><strong>但是，在大数据领域，进行全表查询的场景少之又少，进而我们使用较多的还是列式数据库及列式储存。</strong></p>
</li>
</ol>
<h2 id="存储格式的比较"><a href="#存储格式的比较" class="headerlink" title="存储格式的比较"></a>存储格式的比较</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">: SEQUENCEFILE</span><br><span class="line">| TEXTFILE  -- (Default, depending on hive.default.fileformat configuration)</span><br><span class="line">| RCFILE   -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">| ORC     -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line">| PARQUET   -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line">| AVRO    -- (Note: Available in Hive 0.14.0 and later)</span><br><span class="line">| JSONFILE  -- (Note: Available in Hive 4.0.0 and later)</span><br><span class="line">| INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong>SEQUENCEFILE:</strong> Hadoop API提供的一种二进制文件支持，其具有使用方便、可分割、可压缩的特点。 </p>
</li>
<li><p><strong>TEXTFILE:</strong> textfile为默认格式，存储方式为行存储。数据不做压缩，磁盘开销大，数据解析开销大。 </p>
</li>
<li><p><strong>RCFILE（Record Columnar File）:</strong> 一种行列存储相结合的存储方式。 </p>
</li>
<li><p><strong>ORC:</strong> 数据按照行分块，每个块按照列存储，其中每个块都存储有一个索引。hive给出的新格式，属于RCFILE的升级版,性能有大幅度提升,而且数据可以压缩存储,压缩快 快速列存取。 </p>
<p><strong>ORC这种文件格式可以作用于表或者表的分区，可以通过以下几种方式进行指定：</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">CREATE TABLE ... STORED AS ORC	(常用)</span><br><span class="line">ALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORC</span><br><span class="line">SET hive.default.fileformat=Orc</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>PARQUET:</strong> Parquet也是一种行式存储，同时具有很好的压缩性能；同时可以减少大量的表扫描和反序列化的时间。</p>
</li>
</ul>
<p>如果要使用其他格式作为hive表数据的存储格式，则必须先存在一张相同数据的存储格式为TEXTFILE的表table_t0,然后在建表时使用“<code>insert into table table_stored_file_ORC select from table_t0;</code>”创建。或者使用”<code>create table as select from table_t0;</code>”创建。</p>
<p>相同数据，分别以TextFile、SequenceFile、RcFile、ORC、Parquet存储的比较。</p>
<ol>
<li><p>源文件大小</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop data]$ hdfs dfs -du -s -h /data/page_views.dat</span><br><span class="line">18.1 M  18.1 M  /data/page_views.dat</span><br></pre></td></tr></table></figure>
</li>
<li><p>TextFile</p>
<ul>
<li><p>建表&amp;加载数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_text( </span><br><span class="line">track_time <span class="keyword">string</span>, </span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>, </span><br><span class="line">session_id <span class="keyword">string</span>, </span><br><span class="line">referer <span class="keyword">string</span>, </span><br><span class="line">ip <span class="keyword">string</span>, </span><br><span class="line">end_user_id <span class="keyword">string</span>, </span><br><span class="line">city_id <span class="keyword">string</span> </span><br><span class="line">)<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> TEXTFilE;</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载导入</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">'/data/page_views.dat'</span> <span class="keyword">into</span> <span class="keyword">table</span> page_views_text;</span><br></pre></td></tr></table></figure>
</li>
<li><p>表大小</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">18.1 M  18.1 M  /user/hive/warehouse/store_format.db/page_views_text</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询时读取数据量大小(单位字节，下面都用这个SQL测试)</p>
<p><code>select count(1) from page_views_text where session_id=&quot;B58W48U4WKZCJ5D1T3Z9ZY88RU7QA7B1&quot;;</code> </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 19024045</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>SequenceFile</p>
<ul>
<li><p>建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_seq( </span><br><span class="line">track_time <span class="keyword">string</span>, </span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>, </span><br><span class="line">session_id <span class="keyword">string</span>, </span><br><span class="line">referer <span class="keyword">string</span>, </span><br><span class="line">ip <span class="keyword">string</span>, </span><br><span class="line">end_user_id <span class="keyword">string</span>, </span><br><span class="line">city_id <span class="keyword">string</span> </span><br><span class="line">)<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> SequenceFile;</span><br><span class="line"></span><br><span class="line"><span class="comment">#查询导入</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> page_views_Seq  <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text;</span><br></pre></td></tr></table></figure>
</li>
<li><p>表大小</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">19.6 M  19.6 M  /user/hive/warehouse/store_format.db/page_views_seq</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询时读取数据量大小(字节)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 61513817</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>RcFile</p>
<ul>
<li><p>建表(CTAS)</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_rcfile</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> RcFile</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text</span><br></pre></td></tr></table></figure>
</li>
<li><p>表大小</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">17.9 M  17.9 M  /user/hive/warehouse/store_format.db/page_views_rcfile</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询时读取数据量大小(字节)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 3726738</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>ORC</p>
<ul>
<li><p>建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_orc</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> ORC</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text</span><br></pre></td></tr></table></figure>
</li>
<li><p>表大小</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 1258828</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询时读取数据量大小(字节)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2.8 M  2.8 M  /user/hive/warehouse/store_format.db/page_views_orc</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Parquet</p>
<ul>
<li><p>建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_par</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> Parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text</span><br></pre></td></tr></table></figure>
</li>
<li><p>表大小</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">13.1 M  13.1 M  /user/hive/warehouse/store_format.db/page_views_par</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询时读取数据量大小(字节)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 2688348</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ol>
<p>不同的存储格式不仅表现在存储空间上的不同，对于数据的查询，效率也不一样。因为对于不同的存储格式，执行相同的查询操作，他们访问的数据量大小是不一样的。</p>
<p>不同格式表存储大小的比较</p>
<p><img src="https://yerias.github.io/hive_img/%E8%A1%A8%E5%A4%A7%E5%B0%8F%E6%AF%94%E8%BE%83.jpg" alt="表大小比较"></p>
<p>不同格式表读取数据量比较</p>
<p><img src="https://yerias.github.io/hive_img/%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%87%8F%E6%AF%94%E8%BE%83.jpg" alt="读取数据量比较"></p>
<h2 id="存储格式的应用"><a href="#存储格式的应用" class="headerlink" title="存储格式的应用"></a>存储格式的应用</h2><p>原文件还是上面的那个</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop data]$ hdfs dfs -du -s -h /data/page_views.dat</span><br><span class="line">18.1 M  18.1 M  /data/page_views.dat</span><br></pre></td></tr></table></figure>

<ol>
<li><p>ORC+Zlip结合</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_orc_zlib</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> ORC </span><br><span class="line">TBLPROPERTIES(<span class="string">"orc.compress"</span>=<span class="string">"ZLIB"</span>)</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text;</span><br></pre></td></tr></table></figure>

<p>用ORC+Zlip之后的文件为2.8M</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2.8 M  2.8 M  /user/hive/warehouse/store_format.db/page_views_orc_zlib</span><br></pre></td></tr></table></figure>
</li>
<li><p>Parquet+gzip结合</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> parquet.compression=gzip;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_parquet_gzip</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> PARQUET </span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text;</span><br></pre></td></tr></table></figure>

<p>用Parquet+gzip之后的文件为3.9M</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">3.9 M  3.9 M  /user/hive/warehouse/store_format.db/page_views_parquet_gzip</span><br></pre></td></tr></table></figure>
</li>
<li><p>Parquet+Lzo结合</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.exec.compress.output=<span class="literal">true</span>;  </span><br><span class="line"><span class="keyword">SET</span> mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.lzopCodec;</span><br><span class="line"><span class="keyword">SET</span> mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec; </span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_parquet_lzo <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> PARQUET</span><br><span class="line">TBLPROPERTIES(<span class="string">"parquet.compression"</span>=<span class="string">"lzo"</span>)</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text;</span><br></pre></td></tr></table></figure>

<p>用Parquet+Lzo(未建立索引)之后的文件为6.2M</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">6.2 M  6.2 M  /user/hive/warehouse/store_format.db/page_views_parquet_lzo</span><br></pre></td></tr></table></figure>

<p>建立索引(表好像没啥用)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar $HADOOP_HOME/share/hadoop/common/hadoop-lzo.jar com.hadoop.compression.lzo.LzoIndexer /user/hive/warehouse/store_format.db/page_views_parquet_lzo</span><br></pre></td></tr></table></figure>

</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/11/hive/11/">HIVE Skewed Table&amp;List Bucketing</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-11</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>HIVE Skewed Table</li>
<li>Skewed Join Optimization(最优化)</li>
<li>Basic Partitioning</li>
<li>List Bucketing</li>
<li>Skewed Table vs List Bucketing Table</li>
<li>List Bucketing Validation</li>
</ol>
<h2 id="HIVE-Skewed-Table"><a href="#HIVE-Skewed-Table" class="headerlink" title="HIVE Skewed Table"></a>HIVE Skewed Table</h2><p>Skewed Table可用于提高一个或多个列具有偏斜值的表的性能。通过指定经常出现的值（严重偏斜），Hive会自动将它们拆分成单独的文件（或在列表存储的情况下为目录），并在查询过程中考虑到这一事实，以便它可以跳过或包括整个文件（或在列表存储的情况下为目录）。<strong>可以在表创建过程中在每个表级别上指定。</strong></p>
<p>若是指定了STORED AS DIRECTORIES，也就是使用列表桶（ListBucketing），hive会对倾斜的值建立子目录，查询会更加得到优化。</p>
<p>下面的示例显示具有三个偏斜值的一列，还可以选择使用STORED AS DIRECTORIES子句来指定列表存储。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> list_bucket_single (<span class="keyword">key</span> <span class="keyword">STRING</span>, <span class="keyword">value</span> <span class="keyword">STRING</span>)</span><br><span class="line">SKEWED <span class="keyword">BY</span> (<span class="keyword">key</span>) <span class="keyword">ON</span> (<span class="number">1</span>,<span class="number">5</span>,<span class="number">6</span>) [<span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES];</span><br></pre></td></tr></table></figure>

<p>这是一个带有两个倾斜列的表的示例。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> list_bucket_multiple (col1 <span class="keyword">STRING</span>, col2 <span class="built_in">int</span>, col3 <span class="keyword">STRING</span>)</span><br><span class="line">SKEWED <span class="keyword">BY</span> (col1, col2) <span class="keyword">ON</span> ((<span class="string">'s1'</span>,<span class="number">1</span>), (<span class="string">'s3'</span>,<span class="number">3</span>), (<span class="string">'s13'</span>,<span class="number">13</span>), (<span class="string">'s78'</span>,<span class="number">78</span>)) [<span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES];</span><br></pre></td></tr></table></figure>



<p>可以使用alter table语句来对已创建的表修改倾斜信息。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name SKEWED <span class="keyword">BY</span> (col_name1, col_name2, ...) <span class="keyword">ON</span> ([(col_name1_value, col_name2_value, ...) [, (col_name1_value, col_name2_value), ...][<span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES];</span><br></pre></td></tr></table></figure>

<p><code>STORED AS DIRECTORIES</code>选项确定倾斜表是否使用列表存储功能，该功能为倾斜值创建子目录。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">NOT</span> SKEWED;</span><br></pre></td></tr></table></figure>

<p><code>NOT SKEWED</code>选项使表不倾斜，并关闭列表存储功能（因为列表存储表始终是倾斜的）。这会影响在ALTER语句之后创建的分区，但对在ALTER语句之前创建的分区没有影响。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">NOT</span> <span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES;</span><br></pre></td></tr></table></figure>

<p><code>NOT STORED</code>会关闭列表存储功能，但是表仍然歪斜。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">SET</span> SKEWED LOCATION (col_name1=<span class="string">"location1"</span> [, col_name2=<span class="string">"location2"</span>, ...] );</span><br></pre></td></tr></table></figure>

<p>修改list bucketing倾斜值的存储位置映射。</p>
<h2 id="Skewed-Join-Optimization-最优化"><a href="#Skewed-Join-Optimization-最优化" class="headerlink" title="Skewed Join Optimization(最优化)"></a>Skewed Join Optimization(最优化)</h2><p>两个大数据表的连接由一组MapReduce作业完成，它们首先根据连接键对表进行排序，然后将它们连接起来，mapper将相同键的行发送到同一个reduce。<br>假设表A id字段有值1，2，3，4，并且表B也含有id列，含有值1，2，3。我们使用如下语句来进行连接。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> A.id <span class="keyword">from</span> A <span class="keyword">join</span> B <span class="keyword">on</span> A.id = B.id</span><br></pre></td></tr></table></figure>

<p>将会有一组mappers读这两个表并基于连接键id发送到reducers，假设id=1的行分发到Reducer R1，id=2的分发到R2等等，这些reducer对A和B进行交叉连接，R4从A得到id=4的所有行，但是不会产生任何结果。</p>
<p>现在我们假定A在id=1上倾斜，这样R2和R3将会很快完成但是R1会执行很长时间，因此成为job的瓶颈。若是用户知道这些倾斜信息，这种瓶颈可以使用如下方法人工避免：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> A.id <span class="keyword">from</span> A <span class="keyword">join</span> B <span class="keyword">on</span> A.id = B.id <span class="keyword">where</span> A.id &lt;&gt; <span class="number">1</span>;</span><br><span class="line"><span class="keyword">select</span> A.id <span class="keyword">from</span> A <span class="keyword">join</span> B <span class="keyword">on</span> A.id = B.id <span class="keyword">where</span> A.id = <span class="number">1</span> <span class="keyword">and</span> B.id = <span class="number">1</span>;</span><br></pre></td></tr></table></figure>

<p>第一个查询没有倾斜数据将会很快的完成，如果我们假定表B中只有少量的B.id=1的行，能够直接加载到内存中，通过将B的数据存储到内存中的哈希表中，join将会高效的完成，因此可以再mappper端进行连接，而不用reduce，效率会高很多，最后合并结果。</p>
<p>优点：</p>
<ul>
<li>如果少量的倾斜键占了很大一部分数据，它们将不会成为瓶颈。</li>
</ul>
<p>缺点：</p>
<ul>
<li><p>表A和表B需要分别读和处理两次；</p>
</li>
<li><p>结果需要合并；</p>
</li>
<li><p>需要人工的处理倾斜数据。</p>
</li>
</ul>
<p>hive为了避免上面的操作，在处理数据是对倾斜值进行特殊处理，首先读表B并且存储B.id=1的数据到内存的哈希表中，运行一组mappers来读取表A并做以下操作：</p>
<ol>
<li>若id为1，使用B的id=1的哈希表来计算结果</li>
<li>对于其他值，发送到reducer端来join，这个reduce也会从B的mapper中得到对应需要连接的数据。</li>
</ol>
<p>使用这种方法，最终我们只读取B两次，并且A中的倾斜数据在mapper中进行连接，不会被发送到reducer，其他的键值通过map/reduce。</p>
<p>假设B的行很少，而键在A中倾斜。因此可以将这些行加载到内存中。<strong>若是使用ListBucketing对倾斜值单独存储，会有更好的性能。在读倾斜的数据到内存中时可以指定到倾斜目录下的数据。</strong></p>
<h2 id="Basic-Partitioning"><a href="#Basic-Partitioning" class="headerlink" title="Basic Partitioning"></a>Basic Partitioning</h2><p>有如下问题：存在许多表是这种格式</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> T(a, b, c, ....., x) partitioned <span class="keyword">by</span> (ds);</span><br></pre></td></tr></table></figure>

<p>但是以下查询需要更加高效(这不扯蛋吗，有分区的查询条件不是分区)：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> x = <span class="number">10</span>;</span><br></pre></td></tr></table></figure>

<p>字段x中含有倾斜数据，一般情况下x的值中大约有1000个值有重度倾斜，其他值基数很小，当然，每天倾斜的X的值可能改变，上述要求可以通过以下方式解决：</p>
<p>为值“ x”创建一个分区。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> T(a,b,c, .......) partitioned <span class="keyword">by</span> (ds, x)</span><br></pre></td></tr></table></figure>

<p>优点</p>
<ul>
<li>现有的Hive就足够了。</li>
</ul>
<p>缺点</p>
<ul>
<li>HDFS可伸缩性：HDFS中的文件数量增加。</li>
<li>HDFS可伸缩性：HDFS中的中间文件数量增加。例如，如果有1000个映射器和1000个分区，并且每个映射器每个键至少获得1行，我们最终将创建100万个中间文件。</li>
<li>Metastore的可伸缩性：Metastore会随着分区数量的增长而扩展。</li>
</ul>
<h2 id="List-Bucketing"><a href="#List-Bucketing" class="headerlink" title="List Bucketing"></a>List Bucketing</h2><p>上边方法提到将含有倾斜值得列作为分区存储，但是可能产生大量的目录，为什么不把列值不倾斜的放在一起呢，将每个倾斜的值单独存放一个目录，于是有了List Bucketing。</p>
<p>这个映射在表或分区级别的Metastore中维护。倾斜键的列表存储在表级别中，这个列表可以由客户端周期的提供，并且在新的分区载入时可以被更新。</p>
<p>如下例子，一个表含有一个x字段倾斜值的列表：6，20，30，40。当一个新的分区载入时，它会创建5个目录（4个目录对应x的4个倾斜值，另外一个目录是其余值）。这个表的数据被分成了5个部分：6，20，30，40，others。这跟上一节介绍的分桶表类似，桶的个数决定了文件的个数。倾斜键的整个列表存储在每个表或者分区中。</p>
<p>当使用一下查询时，hive编译器会仅仅使用x=30对应的目录去运行map-reduce。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> ds = <span class="string">'2012-04-15'</span> <span class="keyword">and</span> x = <span class="number">30</span>;</span><br></pre></td></tr></table></figure>

<p>若是查询是x=50，则会使用x=others对应的目录去运行map-reduce作业。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> ds = <span class="string">'2012-04-15'</span> <span class="keyword">and</span> x = <span class="number">50</span>;</span><br></pre></td></tr></table></figure>

<p>这种方法在一下条件下是很有效的：</p>
<ol>
<li><strong>每个分区的倾斜键占总数据的一大部分</strong>。在上边的例子中，如果倾斜的键（6，20，30，40）只占一小部分数据（比如20%）,那么在查询x=50时依然需要扫描80%的数据。</li>
<li><strong>每个分区的倾斜键数量非常少</strong>，因为这个倾斜值列表存在在元数据库中，在元数据库中为每个分区存储100w个倾斜键是没有意义的。</li>
</ol>
<p>这种方法也可被扩展到含有多个列产生的倾斜键，例如我们想优化一下查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> x = <span class="number">10</span> <span class="keyword">and</span> y = <span class="string">'b'</span>;</span><br></pre></td></tr></table></figure>

<p>扩展以上的方法，对于（x，y）每个倾斜值,也按照上边方式单独存储，因此元数据库会有以下映射： </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">(10, 'a') <span class="comment">--&gt; 1, (10, 'b') --&gt; 2, (20, 'c') --&gt; 3, (others) --&gt; 4.</span></span><br></pre></td></tr></table></figure>

<p>因此可直接找到2对应的目录，减少要处理的数据。</p>
<p>同时以下查询也会有一定程度的优化：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> x = <span class="number">10</span>; </span><br><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> y = <span class="string">'b'</span>;</span><br></pre></td></tr></table></figure>

<p>以上两个语句在执行的过程中会裁剪掉一部分数据，例如，对x=10的查询hive编译器可以裁剪掉 ( 20 , c ) 对应的文件，对于 y = ‘b’，( 10 , ‘a’ )  和 ( 20 , ‘c’ ) 对应的文件会被裁剪掉，一定程度能够减少扫描的数据量。</p>
<p>这种方法不适用于以下场景：</p>
<ul>
<li>倾斜键数目非常多，元数据规模问题</li>
<li>许多情况下，倾斜键由多个列组成，但是在查询中，没有使用到倾斜键中的那些列。</li>
</ul>
<h2 id="Skewed-Table-vs-List-Bucketing-Table"><a href="#Skewed-Table-vs-List-Bucketing-Table" class="headerlink" title="Skewed Table vs List Bucketing Table"></a>Skewed Table vs List Bucketing Table</h2><ul>
<li>Skewed Table是一个表它含有倾斜的信息。</li>
<li>List Bucketing Table是Skewed Table，此外，它告诉hive使用列表桶的特点：为倾斜值创建子目录。</li>
</ul>
<p>以下说明两者的存储区别：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t1 (x <span class="keyword">string</span>) skewed <span class="keyword">by</span> (x) <span class="keyword">on</span> (‘a’, ‘b’) partitioned <span class="keyword">by</span> dt location ‘/<span class="keyword">user</span>/hive/warehouse/t1’;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t2 (x <span class="keyword">string</span>) skewed <span class="keyword">by</span> (x) <span class="keyword">on</span> (‘a’, ‘b’) <span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES partitioned <span class="keyword">by</span> dt location ‘/<span class="keyword">user</span>/hive/warehouse/t2’ ;</span><br></pre></td></tr></table></figure>

<p>两者存储的形式如下所示：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">/user/hive/warehouse/t1/dt=something/data.txt</span><br><span class="line">/user/hive/warehouse/t2/dt=something/x=a/data.txt</span><br><span class="line">/user/hive/warehouse/t2/dt=something/x=b/data.txt</span><br><span class="line">/user/hive/warehouse/t2/dt=something/default/data.txt</span><br></pre></td></tr></table></figure>

<h2 id="List-Bucketing-Validation"><a href="#List-Bucketing-Validation" class="headerlink" title="List Bucketing Validation"></a>List Bucketing Validation</h2><p>由于列表桶的子目录特点，它不能够与一些特征共存。</p>
<p>DDL</p>
<p>列表桶与以下共存会抛出编译错误：</p>
<ul>
<li>normal bucketing (clustered by, tablesample, etc.)</li>
<li>external table</li>
<li>“ load data …”</li>
<li>CTAS (Create Table As Select) queries</li>
</ul>
<p>DML</p>
<p>与一下DML操作共存也会跳出错误：</p>
<ul>
<li>“ insert into ”</li>
<li>normal bucketing (clustered by, tablesample, etc.)</li>
<li>external table</li>
<li>non-RCfile due to merge</li>
<li>non-partitioned table</li>
</ul>
<hr>
<p>参考文献：</p>
<p>1.<a href="https://cwiki.apache.org/confluence/display/Hive/ListBucketing" target="_blank" rel="noopener">HIVE ListBucketing</a><br>2.<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-SkewedTables" target="_blank" rel="noopener">HIVE LanguageManualDDL-SkewedTables</a><br>3.<a href="https://cwiki.apache.org/confluence/display/Hive/Skewed+Join+Optimization" target="_blank" rel="noopener">HIVE Skewed Join Optimization</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/10/hive/10/">大表Join大表&amp;大表Join小表&amp;group By解决数据倾斜</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-10</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>大表Join大表</li>
<li>大表Join小表</li>
<li>group By解决</li>
</ol>
<h2 id="大表Join大表"><a href="#大表Join大表" class="headerlink" title="大表Join大表"></a>大表Join大表</h2><h3 id="思路一：SMBJoin"><a href="#思路一：SMBJoin" class="headerlink" title="思路一：SMBJoin"></a>思路一：SMBJoin</h3><p>smb是sort  merge bucket操作，首先进行排序，继而合并，然后放到所对应的bucket中去，bucket是hive中和分区表类似的技术，就是按照key进行hash，相同的hash值都放到相同的buck中去。在进行两个表联合的时候。我们首先进行分桶，在join会大幅度的对性能进行优化。也就是说，在进行联合的时候，是bukect中的一小部分和bukect中的一小部分进行联合，table联合都是等值连接，相同的key都放到了同一个bucket中去了，那么在联合的时候就会大幅度的减小无关项的扫描。</p>
<ol>
<li><p>设置参数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join.noconditionaltask=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>两个表的bucket数量相等</p>
</li>
<li><p>Bucket列、Join列、Sort列、Skewed列为相同的字段</p>
</li>
<li><p>必须是应用在bucket mapjoin 的场景中</p>
</li>
<li><p>注意点</p>
<p>hive并不检查两个join的表是否已经做好bucket且sorted，需要用户自己去保证join的表，否则可能数据不正确。有两个办法</p>
<ul>
<li><p>hive.enforce.sorting 设置为true</p>
</li>
<li><p>手动生成符合条件的数据，通过在sql中用distributed c1 sort by c1 或者 cluster by c1，表创建时必须是CLUSTERED且SORTED，如下</p>
</li>
<li><p>创建Skewed Table提高有一个或多个列有倾斜值的表的性能，例如：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_smb_2(<span class="keyword">mid</span> <span class="keyword">string</span>,age_id <span class="keyword">string</span>)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span>(<span class="keyword">mid</span>) SORTED <span class="keyword">BY</span>(<span class="keyword">mid</span>) <span class="keyword">INTO</span> <span class="number">500</span> BUCKETS</span><br><span class="line">SKEWED <span class="keyword">BY</span>(<span class="keyword">mid</span>) <span class="keyword">on</span> (<span class="number">001</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>案例</p>
<ol>
<li><p>设置先关参数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join.noconditionaltask=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建桶表</p>
<p>user表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_info_bucket(userid <span class="keyword">string</span>,uname <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span>(userid) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line">SKEWED <span class="keyword">BY</span>(userid) <span class="keyword">on</span> (<span class="number">001</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line">STORED AS DIRECTORIES</span><br></pre></td></tr></table></figure>

<p>domain表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> domain_info_bucket(userid <span class="keyword">string</span>,domainid <span class="keyword">string</span>,<span class="keyword">domain</span> <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span> (userid) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line">SKEWED <span class="keyword">BY</span>(userid) <span class="keyword">on</span> (<span class="number">001</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line">STORED AS DIRECTORIES</span><br></pre></td></tr></table></figure>
</li>
<li><p>分别倒入数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> user_info_bucket <span class="keyword">select</span> userid ,uname <span class="keyword">from</span> <span class="keyword">user</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> domain_info_bucket <span class="keyword">select</span> userid ,domainid,<span class="keyword">domain</span> <span class="keyword">from</span> doamin</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> user_info_bucket u  <span class="keyword">join</span> domain_info_bucket d <span class="keyword">on</span>(u.userid==d.userid)</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h3 id="思路二：一分为二"><a href="#思路二：一分为二" class="headerlink" title="思路二：一分为二"></a>思路二：一分为二</h3><p>选择临时表的方式，将数据一分为二，把倾斜的key，和不倾斜的key分开处理，不倾斜的正常join，倾斜的根据情况选择mapjoin或加盐处理，最后结果union all结果</p>
<p>user表为用户基本表，domain为用户访问域名的宽表</p>
<p><strong>注意：</strong>我们其实隐含使用到了mapjoin，hive中的参数为<code>set hive.auto.convert.join=true;</code>，自动开启，默认25M，不能超过1G。</p>
<ol start="0">
<li><p>创建中间表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tmp_table(userid <span class="keyword">string</span>,uname <span class="keyword">string</span>)</span><br><span class="line">SKEWED <span class="keyword">BY</span>(userid) <span class="keyword">on</span> (<span class="number">001</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line">STORED AS DIRECTORIES</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>count(*)</code>出符合倾斜条件的数据存入中间表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> tmp_table</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">	d.userid,u.uname</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	(<span class="keyword">select</span> userid</span><br><span class="line">	<span class="keyword">from</span> </span><br><span class="line">		(<span class="keyword">select</span> </span><br><span class="line">			userid,</span><br><span class="line">			<span class="keyword">count</span>(userid) u_cunt</span><br><span class="line">			</span><br><span class="line">		<span class="keyword">from</span> </span><br><span class="line">			<span class="keyword">domain</span></span><br><span class="line">		<span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">			userid) t</span><br><span class="line">	<span class="keyword">where</span> u_cunt&gt;<span class="number">100</span>) d</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span></span><br><span class="line">	(<span class="keyword">select</span> </span><br><span class="line">		userid,uname</span><br><span class="line">	<span class="keyword">from</span></span><br><span class="line">		<span class="keyword">user</span>) u</span><br><span class="line"><span class="keyword">on</span> d.userid = u.userid;</span><br></pre></td></tr></table></figure>
</li>
<li><p>一分为二，分别查询中出结果，再union all</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	u1.userid,u1.uname,d1.domain</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	(<span class="keyword">select</span> </span><br><span class="line">		userid,uname</span><br><span class="line">	<span class="keyword">from</span></span><br><span class="line">		<span class="keyword">user</span>) u1</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">	(<span class="keyword">select</span> </span><br><span class="line">		d.userid,d.domain</span><br><span class="line">	<span class="keyword">from</span></span><br><span class="line">		<span class="keyword">domain</span> d</span><br><span class="line">	<span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span></span><br><span class="line">		tmp_table t</span><br><span class="line">	<span class="keyword">on</span> </span><br><span class="line">		d.userid = t.userid</span><br><span class="line">	<span class="keyword">where</span> </span><br><span class="line">		t.userid <span class="keyword">is</span> <span class="literal">null</span>) d1</span><br><span class="line"><span class="keyword">on</span> u1.userid = d1.userid</span><br><span class="line"></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">	u2.userid,u2.uname,d2.domain</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	(<span class="keyword">select</span> 	</span><br><span class="line">		userid,uname</span><br><span class="line">	<span class="keyword">from</span></span><br><span class="line">		<span class="keyword">user</span>) u2</span><br><span class="line"><span class="keyword">join</span> </span><br><span class="line">	(<span class="keyword">select</span> </span><br><span class="line">		d.userid,d.domain</span><br><span class="line">		<span class="keyword">from</span></span><br><span class="line">			<span class="keyword">domain</span> d</span><br><span class="line">		<span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span></span><br><span class="line">			tmp_table t</span><br><span class="line">		<span class="keyword">on</span> </span><br><span class="line">			d.userid = t.userid</span><br><span class="line">		<span class="keyword">where</span> </span><br><span class="line">			t.userid <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>) d2</span><br><span class="line"><span class="keyword">on</span> u2.userid = d2.userid</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="大表Join小表"><a href="#大表Join小表" class="headerlink" title="大表Join小表"></a>大表Join小表</h2><h3 id="思路：MapJoin"><a href="#思路：MapJoin" class="headerlink" title="思路：MapJoin"></a>思路：MapJoin</h3><p>大表Join小表很好解决，把小表放进内存，大表再去匹配即可。</p>
<p>思路：</p>
<ol>
<li><p>开启MapJoin</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>调整MapJoin小表大小，默认25M(调整为可以容忍的大小)</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果是MR，小表放进Map，大表进入Mapper匹配Map(使用对象存储结果)</p>
</li>
</ol>
<h2 id="group-By解决"><a href="#group-By解决" class="headerlink" title="group By解决"></a>group By解决</h2><h3 id="思路：加盐去盐"><a href="#思路：加盐去盐" class="headerlink" title="思路：加盐去盐"></a>思路：加盐去盐</h3><ol>
<li><p>开启数据倾斜时的负载均衡</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata=<span class="literal">true</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>对groupby的key加盐去盐</p>
<p>开启上面这个参数，hive会自动拆解成两个MR，加盐去盐，最终输出结果</p>
<p>MR需要写两个MR，一个对key加盐，一个对key去盐</p>
</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/09/hive/9/">HIVE调优(1)</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-09</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>Fetch</li>
<li>本地模式</li>
<li>JVM重用</li>
<li>map数量</li>
<li>reduce数量</li>
<li>推测执行</li>
</ol>
<h2 id="Fetch"><a href="#Fetch" class="headerlink" title="Fetch"></a>Fetch</h2><p>通过修改<code>hive.fetch.task.conversion</code>参数可以让一些select查询可以转换为单个获取任务，不需要执行MapReduce任务，从而最小化延迟。</p>
<p>目前的版本中支持none、minimal和more</p>
<ul>
<li><code>none</code>: 是禁用这一特性</li>
<li><code>minimal</code>: 允许使用<code>SELECT *</code>、<code>FILTER on partition columns (WHERE and HAVING clauses)</code>、<code>LIMIT only</code></li>
<li><code>more</code>: 最大程度的允许使用 <code>SELECT</code>, <code>FILTER</code>, <code>LIMIT only (including TABLESAMPLE, virtual columns)、where</code></li>
</ul>
<p>当前版本默认使用more</p>
<h2 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h2><p><code>hive.exec.mode.local.auto</code> 参数决定hive是否允许使用本地化，默认<code>hive.exec.mode.local.auto=false</code> 没有启用本地化</p>
<p><code>hive.exec.mode.local.auto.inputbytes.max</code> 参数规定了使用本地化处理的最大的文件字节数，默认是128M</p>
<p><code>hive.exec.mode.local.auto.input.files.max</code> 参数规定了使用本地化处理的最大文件数，默认是4个</p>
<h2 id="JVM重用"><a href="#JVM重用" class="headerlink" title="JVM重用"></a>JVM重用</h2><p>在目前使用的版本中，<code>mapreduce.job.jvm.numtasks</code> 参数可以控制Java虚拟机的回收，由于<code>mapTask</code>或者<code>reduceTask</code>都是进程，需要启用JVM，作业运行结束了关闭JVM，使用这个参数控制JVM运行完作业不关机，继续执行作业。默认是1个。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.jvm.numtasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>How many tasks to run per jvm. If set to -1, there is no limit.  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="map数量"><a href="#map数量" class="headerlink" title="map数量"></a>map数量</h2><p><code>mapTask</code>数量由输入的文件大小、文件数和输入的文件产生多少个block决定。</p>
<p>那么我们如何考虑map的数量呢?</p>
<p>理论上讲<code>mapTask</code>越多Map作业的并行度越高，但是耗费的时间和资源也越多，map、reduce作业都是进程级别。</p>
<p>场景：128M的数据只有一两个列的数据记录数有几千万个，必须拆开用NTILE(n)==&gt; 改变map任务的数量</p>
<h2 id="reduce数量"><a href="#reduce数量" class="headerlink" title="reduce数量"></a>reduce数量</h2><p><code>mapred.reduce.tasks=-1</code> 参数决定每个作业的默认数量。通常设置为接近可用主机数量的素数。通过将此属性设置为-1,Hive将自动计算出还原器的数量。默认是-1，即自动计算</p>
<p><code>hive.exec.reducers.bytes.per.reducer=256M</code> 参数决定了reduce最大的字节数，在Hive 0.14.0及以后的版本中，默认为256 MB，也就是说，如果输入大小为1 GB，那么将使用4个reduce。</p>
<p><code>hive.exec.reducers.max=1099</code> 参数决定最大可以使用的reduce数量，如果<code>mapred.reduce.tasks</code> 参数为-1，即自动计算reduce数量，那么Hive将使用这个参数作为最大的reduce数量，自动确定reduce的数量。</p>
<p>计算reducer数的公式很简单N=min( <code>hive.exec.reducers.max</code> ，总输入数据量/ <code>hive.exec.reducers.bytes.per.reducer</code> )</p>
<p>reduce的数量决定最终文件输出的数量<br>思路：reduce数量越多，小文件越多，reduce数量越少，文件大耗费的时间多，最终在reduce文件的大小和需要消耗的时间取个折中。 如果没有reduce，那么map的数据个数决定了输出文件个数 。</p>
<p>Spark3.0 自动适配</p>
<h2 id="推测执行"><a href="#推测执行" class="headerlink" title="推测执行"></a>推测执行</h2><ol>
<li><p>作业完成时间取决于最慢的任务完成时间</p>
<p>一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。</p>
<p>思考：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？</p>
</li>
<li><p>推测执行机制</p>
<p>发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果。</p>
</li>
<li><p>执行推测任务的前提条件</p>
</li>
</ol>
<ul>
<li><p>每个Task只能有一个备份任务</p>
</li>
<li><p>当前 Job 已完成的 Task 必须不小于0.05（5%）</p>
</li>
<li><p>开启推测执行参数设置。mapred-site.xml文件中默认是打开的。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some map tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some reduce tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol start="4">
<li>不能启用推测执行机制情况</li>
</ol>
<ul>
<li><p>任务间存在严重的数据倾斜，数据倾斜跑不过去的，开启多少个推测执行都跑不过去；</p>
</li>
<li><p>特殊任务，比如任务向数据库中写数据。</p>
</li>
</ul>
<ol start="5">
<li><p>生产建议</p>
<p>一般生产生禁用此功能，除非特殊场景直接命令开启</p>
</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/08/hive/8/">Windowing functions&amp;The OVER clause&amp;Analytics functions</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-08</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>The OVER clause</li>
<li>Analytics functions</li>
<li>Windowing functions</li>
</ol>
<h2 id="The-OVER-clause"><a href="#The-OVER-clause" class="headerlink" title="The OVER clause"></a>The OVER clause</h2><p>聚合函数是将多行数据按照规则聚合为一行，比如count()、sum()、min()、max()、avg()</p>
<p>窗口函数是在做聚合的基础上，要返回的数据不仅仅是一行</p>
<p>窗口函数是在窗口的基础上做统计分析，对其所作用的窗口中的每一条记录输出一条结果</p>
<p>窗口函数借助于over() 函数开窗</p>
<p>窗口函数的标准聚合函数同样包括count()、sum()、min()、max()、avg()</p>
<p>窗口可以在一个窗口子句中单独定义。窗口规范支持以下格式:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">(ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING</span><br></pre></td></tr></table></figure>

<p>窗口有以上三种定义方式，分别是<code>从某行之后到某行</code>、<code>某行之后到某行之前</code>、<code>某行到某行之前</code></p>
<p><code>PRECEDING</code>: 往前<br><code>FOLLOWING</code>: 往后<br><code>CURRENT ROW</code>: 当前行<br><code>UNBOUNDED</code>: 起点，<code>UNBOUNDED PRECEDING</code> 表示从前面的起点， <code>UNBOUNDED FOLLOWING</code>: 表示到后面的终点</p>
<ol>
<li><p>准备数据</p>
<p>window01.txt </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ruozedata,2019-04-10,1</span><br><span class="line">ruozedata,2019-04-11,5</span><br><span class="line">ruozedata,2019-04-12,7</span><br><span class="line">ruozedata,2019-04-13,3</span><br><span class="line">ruozedata,2019-04-14,2</span><br><span class="line">ruozedata,2019-04-15,4</span><br><span class="line">ruozedata,2019-04-16,4</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span>  <span class="keyword">database</span> window_over;</span><br><span class="line"><span class="keyword">use</span> window_over</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> window01(</span><br><span class="line">	<span class="keyword">name</span> <span class="keyword">String</span>,</span><br><span class="line">	<span class="built_in">date</span> <span class="keyword">String</span>,</span><br><span class="line">	grade <span class="built_in">Int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>导入数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/window/window01.txt'</span> <span class="keyword">into</span>  <span class="keyword">table</span>  window01;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询</p>
<p>问题：窗口到底怎么开? </p>
<p>核心：从什么地方开始到什么地方结束</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	<span class="keyword">name</span>,</span><br><span class="line">	<span class="built_in">date</span>,</span><br><span class="line">	grade,</span><br><span class="line">	<span class="keyword">sum</span>(grade) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> grade <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> g1,	//第一行到当前行的grade求<span class="keyword">sum</span>()</span><br><span class="line">	<span class="keyword">sum</span>(grade) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> grade <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">3</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> g2,	//第三行到当前行的grade求<span class="keyword">sum</span>()</span><br><span class="line">	<span class="keyword">sum</span>(grade) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> grade <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">3</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="number">1</span> <span class="keyword">following</span>) <span class="keyword">as</span> g3,	//第三行到后一行的grade求<span class="keyword">sum</span>()</span><br><span class="line">	<span class="keyword">sum</span>(grade) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> grade <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="keyword">unbounded</span> <span class="keyword">FOLLOWING</span>) <span class="keyword">as</span> g4	//当前行到最后一行的grade求<span class="keyword">sum</span>()</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	window01;</span><br></pre></td></tr></table></figure>

<p>结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name     |date      |grade|g1|g2|g3|g4|</span><br><span class="line"><span class="comment">---------|----------|-----|--|--|--|--|</span></span><br><span class="line">ruozedata|2019-04-10|    1| 1| 1| 3|26|</span><br><span class="line">ruozedata|2019-04-14|    2| 3| 3| 6|25|</span><br><span class="line">ruozedata|2019-04-13|    3| 6| 6|10|23|</span><br><span class="line">ruozedata|2019-04-16|    4|10|10|14|20|</span><br><span class="line">ruozedata|2019-04-15|    4|14|13|18|16|</span><br><span class="line">ruozedata|2019-04-11|    5|19|16|23|12|</span><br><span class="line">ruozedata|2019-04-12|    7|26|20|20| 7|</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="Analytics-functions"><a href="#Analytics-functions" class="headerlink" title="Analytics functions"></a>Analytics functions</h2><p>分析函数有RANK、ROW_NUMBER、DENSE_RANK、CUME_DIST、PERCENT_RANK、NTILE</p>
<p>这些函数可以分为三部分，第一部分是排序相关的RANK、ROW_NUMBER、DENSE_RANK，第二部分是占比相关的CUME_DIST、PERCENT_RANK，第三部分是把表切成指定分区的NTILE</p>
<h3 id="排序相关"><a href="#排序相关" class="headerlink" title="排序相关"></a>排序相关</h3><ol>
<li><p>准备数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">gifshow.com,2019-04-10,1</span><br><span class="line">gifshow.com,2019-04-11,5</span><br><span class="line">gifshow.com,2019-04-12,7</span><br><span class="line">gifshow.com,2019-04-13,3</span><br><span class="line">gifshow.com,2019-04-14,2</span><br><span class="line">gifshow.com,2019-04-15,4</span><br><span class="line">gifshow.com,2019-04-16,4</span><br><span class="line">yy.com,2019-04-10,2</span><br><span class="line">yy.com,2019-04-11,3</span><br><span class="line">yy.com,2019-04-12,5</span><br><span class="line">yy.com,2019-04-13,6</span><br><span class="line">yy.com,2019-04-14,3</span><br><span class="line">yy.com,2019-04-15,9</span><br><span class="line">yy.com,2019-04-16,7</span><br></pre></td></tr></table></figure>
</li>
<li><p>建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> traffic(</span><br><span class="line">	<span class="keyword">domain</span> <span class="keyword">String</span>,</span><br><span class="line">	<span class="built_in">date</span> <span class="keyword">String</span>,</span><br><span class="line">	grade <span class="built_in">Int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span>  <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>导数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/window/rank.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> traffic;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	<span class="keyword">domain</span>,</span><br><span class="line">	<span class="built_in">date</span>,</span><br><span class="line">	grade,</span><br><span class="line">	<span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> r1,</span><br><span class="line">	ROW_NUMBER() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> r2,</span><br><span class="line">	<span class="keyword">DENSE_RANK</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> r3</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	traffic;</span><br></pre></td></tr></table></figure>

<p>结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">domain     |date      |grade|r1|r2|r3|</span><br><span class="line"><span class="comment">-----------|----------|-----|--|--|--|</span></span><br><span class="line">gifshow.com|2019-04-10|    1| 1| 1| 1|</span><br><span class="line">gifshow.com|2019-04-14|    2| 2| 2| 2|</span><br><span class="line">gifshow.com|2019-04-13|    3| 3| 3| 3|</span><br><span class="line">gifshow.com|2019-04-16|    4| 4| 4| 4|</span><br><span class="line">gifshow.com|2019-04-15|    4| 4| 5| 4|</span><br><span class="line">gifshow.com|2019-04-11|    5| 6| 6| 5|</span><br><span class="line">gifshow.com|2019-04-12|    7| 7| 7| 6|</span><br><span class="line">yy.com     |2019-04-10|    2| 1| 1| 1|</span><br><span class="line">yy.com     |2019-04-11|    3| 2| 2| 2|</span><br><span class="line">yy.com     |2019-04-14|    3| 2| 3| 2|</span><br><span class="line">yy.com     |2019-04-12|    5| 4| 4| 3|</span><br><span class="line">yy.com     |2019-04-13|    6| 5| 5| 4|</span><br><span class="line">yy.com     |2019-04-16|    7| 6| 6| 5|</span><br><span class="line">yy.com     |2019-04-15|    9| 7| 7| 6|</span><br></pre></td></tr></table></figure>
</li>
<li><p>总结</p>
<p><code>RANK()</code>：分组内生成编号，排名相同相同的名词留空位<br><code>DENSE_RANK()</code>： 分组内生成编号，排名相同相同的名词不留空位<br><code>ROW_NUMBER()</code>： 从1开始，按照排序，生成分组内记录的序号(推介使用)</p>
</li>
</ol>
<h3 id="占比相关"><a href="#占比相关" class="headerlink" title="占比相关"></a>占比相关</h3><p><code>CUME_DIST()</code>: 小于等于当前行值(OVER中order by指定的字段排序)的行数/分组内的总行数<br><code>PERCENT_RANK()</code>: 分组内当前行的rank -1 / 分组内总行数 -1</p>
<ol>
<li><p>准备数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">dept01,ruoze,10000</span><br><span class="line">dept01,jepson,20000</span><br><span class="line">dept01,xingxing,30000</span><br><span class="line">dept02,zhangsan,40000</span><br><span class="line">dept02,lisi,50000</span><br></pre></td></tr></table></figure>
</li>
<li><p>建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> window02(</span><br><span class="line">	dept <span class="keyword">String</span>,</span><br><span class="line">	<span class="keyword">user</span> <span class="keyword">String</span>,</span><br><span class="line">	sal <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>导数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/window/window02.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> window02;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	dept,</span><br><span class="line">	<span class="keyword">user</span>,</span><br><span class="line">	sal,</span><br><span class="line">	<span class="keyword">CUME_DIST</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> sal) cume,</span><br><span class="line">	<span class="keyword">PERCENT_RANK</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> sal) <span class="keyword">percent</span></span><br><span class="line"><span class="keyword">from</span> window02;</span><br></pre></td></tr></table></figure>

<p>结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">dept  |user    |sal  |cume1|cume2             |percent|</span><br><span class="line"><span class="comment">------|--------|-----|-----|------------------|-------|</span></span><br><span class="line">dept01|ruoze   |10000|  0.2|0.3333333333333333|      0|</span><br><span class="line">dept01|jepson  |20000|  0.4|0.6666666666666666|    0.5|</span><br><span class="line">dept01|xingxing|30000|  0.6|                 1|      1|</span><br><span class="line">dept02|zhangsan|40000|  0.8|               0.5|      0|</span><br><span class="line">dept02|lisi    |50000|    1|                 1|      1|</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="分区相关"><a href="#分区相关" class="headerlink" title="分区相关"></a>分区相关</h3><p><code>NTILE(num)</code>：将数据按照输入的数成<code>num</code>片，并且记录分片号</p>
<p>场景：128M的数据只有一两个列的数据记录数有几千万个，必须拆开用<code>NTILE(n)</code>==&gt; 改变map任务的数量</p>
<ol>
<li><p>查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	<span class="keyword">domain</span>,</span><br><span class="line">	<span class="built_in">date</span>,</span><br><span class="line">	grade,</span><br><span class="line">	ntile(<span class="number">2</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> n1,</span><br><span class="line">	ntile(<span class="number">3</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> n2,</span><br><span class="line">	ntile(<span class="number">4</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> n3</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	traffic;</span><br></pre></td></tr></table></figure>

<p>结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">domain     |date      |grade|n1|n2|n3|</span><br><span class="line"><span class="comment">-----------|----------|-----|--|--|--|</span></span><br><span class="line">gifshow.com|2019-04-10|    1| 1| 1| 1|</span><br><span class="line">gifshow.com|2019-04-14|    2| 1| 1| 1|</span><br><span class="line">gifshow.com|2019-04-13|    3| 1| 1| 2|</span><br><span class="line">gifshow.com|2019-04-16|    4| 1| 2| 2|</span><br><span class="line">gifshow.com|2019-04-15|    4| 2| 2| 3|</span><br><span class="line">gifshow.com|2019-04-11|    5| 2| 3| 3|</span><br><span class="line">gifshow.com|2019-04-12|    7| 2| 3| 4|</span><br><span class="line">yy.com     |2019-04-10|    2| 1| 1| 1|</span><br><span class="line">yy.com     |2019-04-11|    3| 1| 1| 1|</span><br><span class="line">yy.com     |2019-04-14|    3| 1| 1| 2|</span><br><span class="line">yy.com     |2019-04-12|    5| 1| 2| 2|</span><br><span class="line">yy.com     |2019-04-13|    6| 2| 2| 3|</span><br><span class="line">yy.com     |2019-04-16|    7| 2| 3| 3|</span><br><span class="line">yy.com     |2019-04-15|    9| 2| 3| 4|</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="Windowing-functions"><a href="#Windowing-functions" class="headerlink" title="Windowing functions"></a>Windowing functions</h2><p><code>LAG(col,n,default)</code>: 窗口内往上取N行的值，如果有default就取default，没有就用null</p>
<p><code>LEAD(col,n,default)</code>: 窗口内往下取N行的值，如果有default就取default，没有就用null</p>
<ol>
<li><p>准备数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie1,2015-04-10 10:00:02,url2</span><br><span class="line">cookie1,2015-04-10 10:00:00,url1</span><br><span class="line">cookie1,2015-04-10 10:03:04,1url3</span><br><span class="line">cookie1,2015-04-10 10:50:05,url6</span><br><span class="line">cookie1,2015-04-10 11:00:00,url7</span><br><span class="line">cookie1,2015-04-10 10:10:00,url4</span><br><span class="line">cookie1,2015-04-10 10:50:01,url5</span><br><span class="line">cookie2,2015-04-10 10:00:02,url22</span><br><span class="line">cookie2,2015-04-10 10:00:00,url11</span><br><span class="line">cookie2,2015-04-10 10:03:04,1url33</span><br><span class="line">cookie2,2015-04-10 10:50:05,url66</span><br><span class="line">cookie2,2015-04-10 11:00:00,url77</span><br><span class="line">cookie2,2015-04-10 10:10:00,url44</span><br><span class="line">cookie2,2015-04-10 10:50:01,url55</span><br></pre></td></tr></table></figure>
</li>
<li><p>建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> window03(</span><br><span class="line">	cookid <span class="keyword">String</span>,</span><br><span class="line">	<span class="built_in">time</span> <span class="keyword">String</span>,</span><br><span class="line">	<span class="keyword">url</span> <span class="keyword">String</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>导数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/window/window03.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> window03;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	cookid,</span><br><span class="line">	<span class="built_in">time</span>,</span><br><span class="line">	<span class="keyword">url</span>,</span><br><span class="line">	lag(<span class="built_in">time</span>,<span class="number">1</span>,<span class="string">'1970-00-00 00:00:00'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> cookid <span class="keyword">order</span> <span class="keyword">by</span> <span class="built_in">time</span>),</span><br><span class="line">	<span class="keyword">lead</span>(<span class="built_in">time</span>,<span class="number">2</span>,<span class="string">"null"</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> cookid <span class="keyword">order</span> <span class="keyword">by</span> <span class="built_in">time</span>)</span><br><span class="line"><span class="keyword">from</span> window03;</span><br></pre></td></tr></table></figure>

<p>结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookid |time               |url   |lag                |lead               |</span><br><span class="line"><span class="comment">-------|-------------------|------|-------------------|-------------------|</span></span><br><span class="line">cookie1|2015-04-10 10:00:00|url1  |1970-00-00 00:00:00|2015-04-10 10:03:04|</span><br><span class="line">cookie1|2015-04-10 10:00:02|url2  |2015-04-10 10:00:00|2015-04-10 10:10:00|</span><br><span class="line">cookie1|2015-04-10 10:03:04|1url3 |2015-04-10 10:00:02|2015-04-10 10:50:01|</span><br><span class="line">cookie1|2015-04-10 10:10:00|url4  |2015-04-10 10:03:04|2015-04-10 10:50:05|</span><br><span class="line">cookie1|2015-04-10 10:50:01|url5  |2015-04-10 10:10:00|2015-04-10 11:00:00|</span><br><span class="line">cookie1|2015-04-10 10:50:05|url6  |2015-04-10 10:50:01|null               |</span><br><span class="line">cookie1|2015-04-10 11:00:00|url7  |2015-04-10 10:50:05|null               |</span><br><span class="line">cookie2|2015-04-10 10:00:00|url11 |1970-00-00 00:00:00|2015-04-10 10:03:04|</span><br><span class="line">cookie2|2015-04-10 10:00:02|url22 |2015-04-10 10:00:00|2015-04-10 10:10:00|</span><br><span class="line">cookie2|2015-04-10 10:03:04|1url33|2015-04-10 10:00:02|2015-04-10 10:50:01|</span><br><span class="line">cookie2|2015-04-10 10:10:00|url44 |2015-04-10 10:03:04|2015-04-10 10:50:05|</span><br><span class="line">cookie2|2015-04-10 10:50:01|url55 |2015-04-10 10:10:00|2015-04-10 11:00:00|</span><br><span class="line">cookie2|2015-04-10 10:50:05|url66 |2015-04-10 10:50:01|null               |</span><br><span class="line">cookie2|2015-04-10 11:00:00|url77 |2015-04-10 10:50:05|null               |</span><br></pre></td></tr></table></figure>

</li>
</ol>
</div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/8/">&lt;&lt;</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/10/">&gt;&gt;</a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>