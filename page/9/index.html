<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="BigData Developer"><meta name="keywords" content="yerias,TUNANのBlog,BigData"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017 | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">129</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">30</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">26</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">TUNANのBlog</div><div id="site-sub-title">感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017</div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/13/hive/13/">HIVE调优(2)</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-13</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>Hadoop 框架计算特性</li>
<li>优化常用手段</li>
<li>排序选择</li>
<li>怎样做笛卡尔积</li>
<li>怎样写 in/exists 语句</li>
<li>设置合理的 maptask 数量</li>
<li>小文件合并</li>
<li>设置合理的 reduceTask 的数量</li>
<li>合理利用分桶：Bucketing 和 Sampling</li>
<li>合理利用分区：Partition </li>
<li>Join 优化</li>
<li>Group By 优化</li>
<li>合理利用文件存储格式 </li>
<li>本地模式执行 MapReduce</li>
<li>并行化处理</li>
<li>设置压缩存储</li>
</ol>
<h2 id="Hadoop-框架计算特性"><a href="#Hadoop-框架计算特性" class="headerlink" title="Hadoop 框架计算特性"></a>Hadoop 框架计算特性</h2><ol>
<li><p>数据量大不是问题，数据倾斜是个问题</p>
</li>
<li><p>jobs 数比较多的作业运行效率相对比较低，比如即使有几百行的表，如果多次关联多次 汇总，产生十几个 jobs，耗时很长。原因是 map reduce 作业初始化的时间是比较长的</p>
</li>
<li><p>sum,count,max,min 等 UDAF，不怕数据倾斜问题，hadoop 在 map 端的汇总合并优化，使 数据倾斜不成问题</p>
</li>
<li><p>count(distinct userid)，在数据量大的情况下，效率较低，如果是多 count(distinct userid,month)效率更低，因为 count(distinct)是按 group by 字段分组，按 distinct 字段排序， 一般这种分布方式是很</p>
</li>
</ol>
<p>倾斜的，比如 PV 数据，淘宝一天 30 亿的 pv，如果按性别分组，分 配 2 个 reduce，每个 reduce 期望处理 15 亿数据，但现实必定是男少女多</p>
<h2 id="优化常用手段"><a href="#优化常用手段" class="headerlink" title="优化常用手段"></a>优化常用手段</h2><ol>
<li><p>好的模型设计事半功倍</p>
</li>
<li><p>解决数据倾斜问题</p>
</li>
<li><p>减少 job 数</p>
</li>
<li><p>设置合理的 MapReduce 的 task 数，能有效提升性能。(比如，10w+级别的计算，用 160个 reduce，那是相当的浪费，1 个足够)</p>
</li>
<li><p>了解数据分布，自己动手解决数据倾斜问题是个不错的选择。这是通用的算法优化，但 算法优化有时不能适应特定业务背景，开发人员了解业务，了解数据，可以通过业务逻辑精 确有效的解决数据倾斜问题</p>
</li>
<li><p>数据量较大的情况下，慎用 count(distinct)，group by 容易产生倾斜问题</p>
</li>
<li><p>对小文件进行合并，是行之有效的提高调度效率的方法，假如所有的作业设置合理的文 件数，对云梯的整体调度效率也会产生积极的正向影响</p>
</li>
<li><p>优化时把握整体，单个作业最优不如整体最优</p>
</li>
</ol>
<h2 id="排序选择"><a href="#排序选择" class="headerlink" title="排序选择"></a>排序选择</h2><ul>
<li><p><strong>cluster by</strong>：对同一字段分桶并排序，不能和 sort by 连用</p>
</li>
<li><p><strong>distribute by + sort by</strong>：分桶，保证同一字段值只存在一个结果文件当中，结合 sort by 保证 每个 reduceTask 结果有序</p>
</li>
<li><p><strong>sort by</strong>：单机排序，单个 reduce 结果有序</p>
</li>
<li><p><strong>order by</strong>：全局排序，缺陷是只能使用一个 reduce</p>
</li>
</ul>
<p><strong>一定要区分这四种排序的使用方式和适用场景</strong></p>
<h2 id="怎样做笛卡尔积"><a href="#怎样做笛卡尔积" class="headerlink" title="怎样做笛卡尔积"></a>怎样做笛卡尔积</h2><p>当 Hive 设定为严格模式（hive.mapred.mode=strict）时，不允许在 HQL 语句中出现笛卡尔积， 这实际说明了 Hive 对笛卡尔积支持较弱。因为找不到 Join key，Hive 只能使用 1 个 reducer 来完成笛卡尔积。</p>
<p>当然也可以使用 limit 的办法来减少某个表参与 join 的数据量，但对于需要笛卡尔积语义的 需求来说，经常是一个大表和一个小表的 Join 操作，结果仍然很大（以至于无法用单机处 理），这时 MapJoin才是最好的解决办法。MapJoin，顾名思义，会在 Map 端完成 Join 操作。 这需要将 Join 操作的一个或多个表完全读入内存。</p>
<p>PS：MapJoin 在子查询中可能出现未知 BUG。在大表和小表做笛卡尔积时，规避笛卡尔积的 方法是，给 Join 添加一个 Join key，<strong>原理很简单：将小表扩充一列 join key，并将小表的条目复制数倍，join</strong> <strong>key 各不相同；将大表扩充一列 join key 为随机数。</strong></p>
<p><strong>精髓就在于复制几倍，最后就有几个 reduce 来做，而且大表的数据是前面小表扩张 key 值 范围里面随机出来的，所以复制了几倍 n，就相当于这个随机范围就有多大 n，那么相应的， 大表的数据就被随机的分为了 n 份。并且最后处理所用的 reduce 数量也是 n，而且也不会 出现数据倾斜。</strong></p>
<h2 id="怎样写-in-exists-语句"><a href="#怎样写-in-exists-语句" class="headerlink" title="怎样写 in/exists 语句"></a>怎样写 in/exists 语句</h2><p>虽然经过测验，hive1.2.1 也支持 in/exists 操作，但还是推荐使用 hive 的一个高效替代方案：<strong>left semi join</strong></p>
<p>比如说：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">where</span> a.id <span class="keyword">in</span> (<span class="keyword">select</span> b.id <span class="keyword">from</span> b);</span><br><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">where</span> <span class="keyword">exists</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> b <span class="keyword">where</span> a.id = b.id);</span><br></pre></td></tr></table></figure>

<p>应该转换成：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">left</span> <span class="keyword">semi</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.id = b.id;</span><br></pre></td></tr></table></figure>



<h2 id="设置合理的-maptask-数量"><a href="#设置合理的-maptask-数量" class="headerlink" title="设置合理的 maptask 数量"></a>设置合理的 maptask 数量</h2><ol>
<li><p>Map 数过大</p>
<p>Map 阶段输出文件太小，产生大量小文件</p>
<p>初始化和创建 Map 的开销很大</p>
</li>
<li><p>Map 数太小</p>
<p>文件处理或查询并发度小，Job 执行时间过长</p>
<p>大量作业时，容易堵塞集群 </p>
</li>
</ol>
<p>在 MapReduce 的编程案例中，我们得知，一个MR Job的 MapTask 数量是由输入分片 InputSplit 决定的。而输入分片是由 FileInputFormat.getSplit()决定的。一个输入分片对应一个 MapTask， 而输入分片是由三个参数决定的：</p>
<p><img src="https://yerias.github.io/hive_img/%E8%AE%BE%E7%BD%AEmap%E4%BB%BB%E5%8A%A1%E6%95%B0%E9%87%8F.png" alt="设置map任务数量"></p>
<p>输入分片大小的计算是这么计算出来的：</p>
<p><strong>long splitSize = Math.max(minSize, Math.min(maxSize, blockSize))</strong></p>
<p>默认情况下，输入分片大小和 HDFS 集群默认数据块大小一致，也就是默认一个数据块，启 用一个 MapTask 进行处理，这样做的好处是避免了服务器节点之间的数据传输，提高 job 处 理效率</p>
<p>两种经典的控制 MapTask 的个数方案：减少 MapTask 数或者增加 MapTask 数</p>
<ol>
<li>减少 MapTask 数是通过合并小文件来实现，这一点主要是针对数据源</li>
<li>增加 MapTask 数可以通过控制上一个 job 的 reduceTask 个数</li>
</ol>
<p>因为 Hive 语句最终要转换为一系列的 MapReduce Job 的，而每一个 MapReduce Job 是由一 系列的 MapTask 和 ReduceTask 组成的，默认情况下， MapReduce 中一个 MapTask 或者一个 ReduceTask 就会启动一个 JVM 进程，一个 Task 执行完毕后， JVM 进程就退出。这样如果任 务花费时间很短，又要多次启动 JVM 的情况下，JVM 的启动时间会变成一个比较大的消耗， 这个时候，就可以通过重用 JVM 来解决：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set mapred.job.reuse.jvm.num.tasks=5</span><br></pre></td></tr></table></figure>

<h2 id="小文件合并"><a href="#小文件合并" class="headerlink" title="小文件合并"></a>小文件合并</h2><p>文件数目过多，会给 HDFS 带来压力，并且会影响处理效率，可以通过合并 Map 和 Reduce 的 结果文件来消除这样的影响：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.merge.mapfiles = true ##在 map only 的任务结束时合并小文件</span><br><span class="line">set hive.merge.mapredfiles = false ## true 时在 MapReduce 的任务结束时合并小文件</span><br><span class="line">set hive.merge.size.per.task = 256*1000*1000 ##合并文件的大小</span><br><span class="line">set mapred.max.split.size=256000000; ##每个 Map 最大分割大小</span><br><span class="line">set mapred.min.split.size.per.node=1; ##一个节点上 split 的最少值</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; ##执行 Map 前进行小文件合并</span><br></pre></td></tr></table></figure>

<h2 id="设置合理的-reduceTask-的数量"><a href="#设置合理的-reduceTask-的数量" class="headerlink" title="设置合理的 reduceTask 的数量"></a>设置合理的 reduceTask 的数量</h2><p>Hadoop MapReduce 程序中，reducer 个数的设定极大影响执行效率，这使得 Hive 怎样决定 reducer 个数成为一个关键问题。遗憾的是 Hive 的估计机制很弱，不指定 reducer 个数的情 况下，Hive 会猜测确定一个 reducer 个数，基于以下两个设定：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive.exec.reducers.bytes.per.reducer（默认为 256000000）</span><br><span class="line">hive.exec.reducers.max（默认为 1009）</span><br><span class="line">mapreduce.job.reduces=-1（设置一个常量 reducetask 数量）</span><br></pre></td></tr></table></figure>

<p>计算 reducer 数的公式很简单： N=min(参数 2，总输入数据量/参数 1) 通常情况下，有必要手动指定 reducer 个数。考虑到 map 阶段的输出数据量通常会比输入有 大幅减少，因此即使不设定 reducer 个数，重设参数 2 还是必要的。</p>
<p><strong>依据 Hadoop 的经验，可以将参数 2 设定为 0.95*(集群中 datanode 个数)。</strong> </p>
<h2 id="合并-MapReduce-操作"><a href="#合并-MapReduce-操作" class="headerlink" title="合并 MapReduce 操作"></a>合并 MapReduce 操作</h2><p>Multi-group by 是 Hive 的一个非常好的特性，它使得 Hive 中利用中间结果变得非常方便。 例如：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">FROM (<span class="keyword">SELECT</span> a.status, b.school, b.gender <span class="keyword">FROM</span> status_updates a <span class="keyword">JOIN</span> <span class="keyword">profiles</span> b <span class="keyword">ON</span> (a.userid =</span><br><span class="line">b.userid <span class="keyword">and</span> a.ds=<span class="string">'2009-03-20'</span> ) ) subq1</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> gender_summary <span class="keyword">PARTITION</span>(ds=<span class="string">'2009-03-20'</span>)</span><br><span class="line"><span class="keyword">SELECT</span> subq1.gender, <span class="keyword">COUNT</span>(<span class="number">1</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> subq1.gender</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> school_summary <span class="keyword">PARTITION</span>(ds=<span class="string">'2009-03-20'</span>)</span><br><span class="line"><span class="keyword">SELECT</span> subq1.school, <span class="keyword">COUNT</span>(<span class="number">1</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> subq1.school</span><br></pre></td></tr></table></figure>

<p>上述查询语句使用了 multi-group by 特性连续 group by 了 2 次数据，使用不同的 group by key。 这一特性可以减少一次 MapReduce 操作</p>
<h2 id="合理利用分桶：Bucketing-和-Sampling"><a href="#合理利用分桶：Bucketing-和-Sampling" class="headerlink" title="合理利用分桶：Bucketing 和 Sampling"></a>合理利用分桶：Bucketing 和 Sampling</h2><p>Bucket 是指将数据以指定列的值为 key 进行 hash，hash 到指定数目的桶中。这样就可以支 持高效采样了。如下例就是以 userid 这一列为 bucket 的依据，共设置 32 个 buckets</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> page_view(viewTime <span class="built_in">INT</span>, userid <span class="built_in">BIGINT</span>,</span><br><span class="line"> page_url <span class="keyword">STRING</span>, referrer_url <span class="keyword">STRING</span>,</span><br><span class="line"> ip <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'IP Address of the User'</span>)</span><br><span class="line"> <span class="keyword">COMMENT</span> <span class="string">'This is the page view table'</span></span><br><span class="line"> PARTITIONED <span class="keyword">BY</span>(dt <span class="keyword">STRING</span>, country <span class="keyword">STRING</span>)</span><br><span class="line"> CLUSTERED <span class="keyword">BY</span>(userid) SORTED <span class="keyword">BY</span>(viewTime) <span class="keyword">INTO</span> <span class="number">32</span> BUCKETS</span><br><span class="line"> <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line"> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'1'</span></span><br><span class="line"> COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'2'</span></span><br><span class="line"> <span class="keyword">MAP</span> <span class="keyword">KEYS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'3'</span></span><br><span class="line"> <span class="keyword">STORED</span> <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure>

<p>通常情况下，Sampling 在全体数据上进行采样，这样效率自然就低，它要去访问所有数据。 而如果一个表已经对某一列制作了 bucket，就可以采样所有桶中指定序号的某个桶，这就 减少了访问量。</p>
<p>如下例所示就是采样了 page_view 中 32 个桶中的第三个桶的全部数据：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> page_view <span class="keyword">TABLESAMPLE</span>(<span class="keyword">BUCKET</span> <span class="number">3</span> <span class="keyword">OUT</span> <span class="keyword">OF</span> <span class="number">32</span>);</span><br></pre></td></tr></table></figure>

<p>如下例所示就是采样了 page_view 中 32 个桶中的第三个桶的一半数据：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> page_view <span class="keyword">TABLESAMPLE</span>(<span class="keyword">BUCKET</span> <span class="number">3</span> <span class="keyword">OUT</span> <span class="keyword">OF</span> <span class="number">64</span>);</span><br></pre></td></tr></table></figure>

<h2 id="合理利用分区：Partition"><a href="#合理利用分区：Partition" class="headerlink" title="合理利用分区：Partition"></a>合理利用分区：Partition</h2><p> Partition 就是分区。分区通过在创建表时启用 partitioned by 实现，用来 partition 的维度并不 是实际数据的某一列，具体分区的标志是由插入内容时给定的。当要查询某一分区的内容时 可以采用 where 语句，形似 where tablename.partition_column = a 来实现。</p>
<p>创建含分区的表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> page_view(viewTime <span class="built_in">INT</span>, userid <span class="built_in">BIGINT</span>,</span><br><span class="line"> page_url <span class="keyword">STRING</span>, referrer_url <span class="keyword">STRING</span>,</span><br><span class="line"> ip <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'IP Address of the User'</span>)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span>(<span class="built_in">date</span> <span class="keyword">STRING</span>, country <span class="keyword">STRING</span>)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'1'</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> TEXTFILE;</span><br></pre></td></tr></table></figure>

<p>载入内容，并指定分区标志</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/pv_2008-06-08_us.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> page_view</span><br><span class="line"><span class="keyword">partition</span>(<span class="built_in">date</span>=<span class="string">'2008-06-08'</span>, country=<span class="string">'US'</span>);</span><br></pre></td></tr></table></figure>

<p>查询指定标志的分区内容</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> page_views.* <span class="keyword">FROM</span> page_views</span><br><span class="line"> <span class="keyword">WHERE</span> page_views.date &gt;= <span class="string">'2008-03-01'</span> <span class="keyword">AND</span> page_views.date &lt;= <span class="string">'2008-03-31'</span> <span class="keyword">AND</span></span><br><span class="line">page_views.referrer_url <span class="keyword">like</span> <span class="string">'%xyz.com'</span>;</span><br></pre></td></tr></table></figure>

<h2 id="Join-优化"><a href="#Join-优化" class="headerlink" title="Join 优化"></a>Join 优化</h2><p>总体原则：</p>
<ol>
<li><p>优先过滤后再进行 Join 操作，最大限度的减少参与 join 的数据量</p>
</li>
<li><p>小表 join 大表，最好启动 mapjoin</p>
</li>
<li><p>Join on 的条件相同的话，最好放入同一个 job，并且 join 表的排列顺序从小到大 </p>
</li>
</ol>
<p><strong>在使用写有 Join 操作的查询语句时有一条原则：应该将条目少的表/子查询放在 Join 操作 符的左边。</strong>原因是在 Join 操作的 Reduce 阶段，位于 Join 操作符左边的表的内容会被加 载进内存，将条目少的表放在左边，可以有效减少发生 OOM 错误的几率。对于一条语句 中有多个 Join 的情况，如果 Join 的条件相同，比如查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> pv_users</span><br><span class="line"><span class="keyword">SELECT</span> pv.pageid, u.age <span class="keyword">FROM</span> page_view p</span><br><span class="line"><span class="keyword">JOIN</span> <span class="keyword">user</span> u <span class="keyword">ON</span> (pv.userid = u.userid)</span><br><span class="line"><span class="keyword">JOIN</span> newuser x <span class="keyword">ON</span> (u.userid = x.userid);</span><br></pre></td></tr></table></figure>

<p>如果 Join 的 key 相同，不管有多少个表，都会则会合并为一个 Map-Reduce 任务，而不 是”n”个，在做 OUTER JOIN 的时候也是一样</p>
<p>如果 join 的条件不相同，比如：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> pv_users</span><br><span class="line"> <span class="keyword">SELECT</span> pv.pageid, u.age <span class="keyword">FROM</span> page_view p</span><br><span class="line"> <span class="keyword">JOIN</span> <span class="keyword">user</span> u <span class="keyword">ON</span> (pv.userid = u.userid)</span><br><span class="line"> <span class="keyword">JOIN</span> newuser x <span class="keyword">on</span> (u.age = x.age);</span><br></pre></td></tr></table></figure>

<p>Map-Reduce 的任务数目和 Join 操作的数目是对应的，上述查询和以下查询是等价的</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--先 page_view 表和 user 表做链接</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tmptable</span><br><span class="line"> <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> page_view p <span class="keyword">JOIN</span> <span class="keyword">user</span> u <span class="keyword">ON</span> (pv.userid = u.userid);</span><br><span class="line"><span class="comment">-- 然后结果表 temptable 和 newuser 表做链接</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> pv_users</span><br><span class="line"> <span class="keyword">SELECT</span> x.pageid, x.age <span class="keyword">FROM</span> tmptable x <span class="keyword">JOIN</span> newuser y <span class="keyword">ON</span> (x.age = y.age);</span><br></pre></td></tr></table></figure>

<p>在编写 Join 查询语句时，如果确定是由于 join 出现的数据倾斜，那么请做如下设置：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.skewjoin.key=100000; // 这个是 join 的键对应的记录条数超过这个值则会进行 分拆，值根据具体数据量设置</span><br><span class="line">set hive.optimize.skewjoin=true; // 如果是 join 过程出现倾斜应该设置为 true</span><br></pre></td></tr></table></figure>

<h2 id="Group-By-优化"><a href="#Group-By-优化" class="headerlink" title="Group By 优化"></a>Group By 优化</h2><h3 id="1-Map-端部分聚合"><a href="#1-Map-端部分聚合" class="headerlink" title="1. Map 端部分聚合"></a>1. Map 端部分聚合</h3><p>并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进 行部分聚合，最后在Reduce 端得出最终结果。</p>
<p>MapReduce 的 combiner 组件参数包括：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.map.aggr = true 是否在 Map 端进行聚合，默认为 True</span><br><span class="line">set hive.groupby.mapaggr.checkinterval = 100000 在 Map 端进行聚合操作的条目数目</span><br></pre></td></tr></table></figure>

<h3 id="2-使用-Group-By-有数据倾斜的时候进行负载均衡"><a href="#2-使用-Group-By-有数据倾斜的时候进行负载均衡" class="headerlink" title="2. 使用 Group By 有数据倾斜的时候进行负载均衡"></a>2. 使用 Group By 有数据倾斜的时候进行负载均衡</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.groupby.skewindata = true</span><br></pre></td></tr></table></figure>

<p>当 sql 语句使用 groupby 时数据出现倾斜时，如果该变量设置为 true，那么 Hive 会自动进行 负载均衡。<strong>策略就是把 MR 任务拆分成两个：第一个先做预汇总，第二个再做最终汇总</strong></p>
<p>在 MR 的第一个阶段中，Map 的输出结果集合会缓存到 maptaks 中，每个 Reduce 做部分聚 合操作，并输出结果，这样处理的结果是相同 Group By Key 有可能被分发到不同的 Reduce 中， 从而达到负载均衡的目的；第二个阶段 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成 最终的聚合操作。</p>
<h2 id="合理利用文件存储格式"><a href="#合理利用文件存储格式" class="headerlink" title="合理利用文件存储格式"></a>合理利用文件存储格式</h2><p>创建表时，尽量使用 orc、parquet 这些列式存储格式，因为列式存储的表，每一列的数据在 物理上是存储在一起的，Hive 查询时会只遍历需要列数据，大大减少处理的数据量。</p>
<h2 id="本地模式执行-MapReduce"><a href="#本地模式执行-MapReduce" class="headerlink" title="本地模式执行 MapReduce"></a>本地模式执行 MapReduce</h2><p>Hive 在集群上查询时，默认是在集群上 N 台机器上运行， 需要多个机器进行协调运行，这 个方式很好地解决了大数据量的查询问题。但是当 Hive 查询处理的数据量比较小时，其实 没有必要启动分布式模式去执行，因为以分布式方式执行就涉及到跨网络传输、多节点协调 等，并且消耗资源。这个时间可以只使用本地模式来执行 mapreduce job，只在一台机器上 执行，速度会很快。启动本地模式涉及到三个参数：</p>
<p><img src="https://yerias.github.io/hive_img/%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F.png" alt="本地模式"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.exec.mode.local.auto=true 是打开 hive 自动判断是否启动本地模式的开关，但是只 是打开这个参数并不能保证启动本地模式，要当 map 任务数不超过</span><br><span class="line">hive.exec.mode.local.auto.input.files.max 的个数并且 map 输入文件大小不超过</span><br><span class="line">hive.exec.mode.local.auto.inputbytes.max 所指定的大小时，才能启动本地模式。</span><br></pre></td></tr></table></figure>

<h2 id="并行化处理"><a href="#并行化处理" class="headerlink" title="并行化处理"></a>并行化处理</h2><p>一个 hive sql 语句可能会转为多个 mapreduce Job，每一个 job 就是一个 stage，这些 job 顺序 执行，这个在 cli 的运行日志中也可以看到。但是有时候这些任务之间并不是是相互依赖的， 如果集群资源允许的话，可以让多个并不相互依赖 stage 并发执行，这样就节约了时间，提 高了执行速度，但是如果集群资源匮乏时，启用并行化反倒是会导致各个 job 相互抢占资源 而导致整体执行性能的下降。启用并行化：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.parallel=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number=<span class="number">8</span>; //同一个 sql 允许并行任务的最大线程数</span><br></pre></td></tr></table></figure>

<h2 id="设置压缩存储"><a href="#设置压缩存储" class="headerlink" title="设置压缩存储"></a>设置压缩存储</h2><h3 id="1-压缩的原因"><a href="#1-压缩的原因" class="headerlink" title="1. 压缩的原因"></a>1. 压缩的原因</h3><p>Hive 最终是转为 MapReduce 程序来执行的，而 MapReduce 的性能瓶颈在于网络 IO 和 磁盘 IO，要解决性能瓶颈，最主要的是减少数据量，对数据进行压缩是个好的方式。压缩 虽然是减少了数据量，但是压缩过程要消耗 CPU 的，但是在 Hadoop 中， 往往性能瓶颈不 在于 CPU，CPU 压力并不大，所以压缩充分利用了比较空闲的 CPU</p>
<h3 id="2-常用压缩方法对比"><a href="#2-常用压缩方法对比" class="headerlink" title="2. 常用压缩方法对比"></a>2. 常用压缩方法对比</h3><p><img src="https://yerias.github.io/hive_img/%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%E5%AF%B9%E6%AF%94.png" alt="压缩格式对比"></p>
<p>各个压缩方式所对应的 Class 类：</p>
<p><img src="https://yerias.github.io/hive_img/%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%E5%AF%B9%E5%BA%94%E7%9A%84%E7%B1%BB.png" alt="压缩格式对应的类"></p>
<h3 id="3-压缩方式的选择"><a href="#3-压缩方式的选择" class="headerlink" title="3. 压缩方式的选择"></a>3. 压缩方式的选择</h3><ol>
<li><p>压缩比率</p>
<p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E6%AF%94.png" alt="压缩比"></p>
</li>
<li><p>压缩解压缩速度</p>
<p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E6%97%B6%E9%97%B4.png" alt="压缩时间"></p>
</li>
<li><p>是否支持 Split</p>
<p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9.png" alt="压缩"></p>
</li>
</ol>
<h3 id="4-压缩使用"><a href="#4-压缩使用" class="headerlink" title="4. 压缩使用"></a>4. 压缩使用</h3><p>Job 输出文件按照 block 以 GZip 的方式进行压缩：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set mapreduce.output.fileoutputformat.compress=true // 默认值是 false</span><br><span class="line">set mapreduce.output.fileoutputformat.compress.type=BLOCK // 默认值是 Record</span><br><span class="line">set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec // 默认值是 org.apache.hadoop.io.compress.DefaultCodec</span><br></pre></td></tr></table></figure>

<p>Map 输出结果也以 Gzip 进行压缩：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set mapred.map.output.compress=true</span><br><span class="line">set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.GzipCodec // 默认值是 org.apache.hadoop.io.compress.DefaultCodec</span><br></pre></td></tr></table></figure>

<p>对 Hive 输出结果和中间都进行压缩：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.exec.compress.output=true // 默认值是 false，不压缩</span><br><span class="line">set hive.exec.compress.intermediate=true // 默认值是 false，为 true 时 MR 设置的压缩才启用</span><br></pre></td></tr></table></figure></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/12/hive/12/">HIVE调优之存储格式</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-12</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><p>目录</p>
<ol>
<li>行式数据库和列式数据库的对比</li>
<li>存储格式的比较</li>
<li>存储格式的应用</li>
</ol>
<h2 id="行式数据库和列式数据库的对比"><a href="#行式数据库和列式数据库的对比" class="headerlink" title="行式数据库和列式数据库的对比"></a>行式数据库和列式数据库的对比</h2><ol>
<li><p>存储比较</p>
<p>行式数据库存储在hdfs上式按行进行存储的，一个block存储一或多行数据。而列式数据库在hdfs上则是按照列进行存储，一个block可能有一列或多列数据。</p>
</li>
<li><p>压缩比较</p>
<p>对于行式数据库，必然按行压缩，当一行中有多个字段，各个字段对应的数据类型可能不一致，压缩性能压缩比就比较差。</p>
<p>对于列式数据库，必然按列压缩，每一列对应的是相同数据类型的数据，故列式数据库的压缩性能要强于行式数据库。</p>
</li>
<li><p>查询比较</p>
<p>假设执行的查询操作是：<code>select id,name from table_emp;</code></p>
<p>对于行式数据库，它要遍历一整张表将每一行中的id,name字段拼接再展现出来，这样需要查询的数据量就比较大，效率低。</p>
<p>对于列式数据库，它只需找到对应的id,name字段的列展现出来即可，需要查询的数据量小，效率高。</p>
<p>假设执行的查询操作是：<code>select * from table_emp;</code></p>
<p>对于这种查询整个表全部信息的操作，由于列式数据库需要将分散的行进行重新组合，行式数据库效率就高于列式数据库。</p>
<p><strong>但是，在大数据领域，进行全表查询的场景少之又少，进而我们使用较多的还是列式数据库及列式储存。</strong></p>
</li>
</ol>
<h2 id="存储格式的比较"><a href="#存储格式的比较" class="headerlink" title="存储格式的比较"></a>存储格式的比较</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">: SEQUENCEFILE</span><br><span class="line">| TEXTFILE  -- (Default, depending on hive.default.fileformat configuration)</span><br><span class="line">| RCFILE   -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">| ORC     -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line">| PARQUET   -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line">| AVRO    -- (Note: Available in Hive 0.14.0 and later)</span><br><span class="line">| JSONFILE  -- (Note: Available in Hive 4.0.0 and later)</span><br><span class="line">| INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong>SEQUENCEFILE:</strong> Hadoop API提供的一种二进制文件支持，其具有使用方便、可分割、可压缩的特点。 </p>
</li>
<li><p><strong>TEXTFILE:</strong> textfile为默认格式，存储方式为行存储。数据不做压缩，磁盘开销大，数据解析开销大。 </p>
</li>
<li><p><strong>RCFILE（Record Columnar File）:</strong> 一种行列存储相结合的存储方式。 </p>
</li>
<li><p><strong>ORC:</strong> 数据按照行分块，每个块按照列存储，其中每个块都存储有一个索引。hive给出的新格式，属于RCFILE的升级版,性能有大幅度提升,而且数据可以压缩存储,压缩快 快速列存取。 </p>
<p><strong>ORC这种文件格式可以作用于表或者表的分区，可以通过以下几种方式进行指定：</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">CREATE TABLE ... STORED AS ORC	(常用)</span><br><span class="line">ALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORC</span><br><span class="line">SET hive.default.fileformat=Orc</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>PARQUET:</strong> Parquet也是一种行式存储，同时具有很好的压缩性能；同时可以减少大量的表扫描和反序列化的时间。</p>
</li>
</ul>
<p>如果要使用其他格式作为hive表数据的存储格式，则必须先存在一张相同数据的存储格式为TEXTFILE的表table_t0,然后在建表时使用“<code>insert into table table_stored_file_ORC select from table_t0;</code>”创建。或者使用”<code>create table as select from table_t0;</code>”创建。</p>
<p>相同数据，分别以TextFile、SequenceFile、RcFile、ORC、Parquet存储的比较。</p>
<ol>
<li><p>源文件大小</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop data]$ hdfs dfs -du -s -h /data/page_views.dat</span><br><span class="line">18.1 M  18.1 M  /data/page_views.dat</span><br></pre></td></tr></table></figure>
</li>
<li><p>TextFile</p>
<ul>
<li><p>建表&amp;加载数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_text( </span><br><span class="line">track_time <span class="keyword">string</span>, </span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>, </span><br><span class="line">session_id <span class="keyword">string</span>, </span><br><span class="line">referer <span class="keyword">string</span>, </span><br><span class="line">ip <span class="keyword">string</span>, </span><br><span class="line">end_user_id <span class="keyword">string</span>, </span><br><span class="line">city_id <span class="keyword">string</span> </span><br><span class="line">)<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> TEXTFilE;</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载导入</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">'/data/page_views.dat'</span> <span class="keyword">into</span> <span class="keyword">table</span> page_views_text;</span><br></pre></td></tr></table></figure>
</li>
<li><p>表大小</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">18.1 M  18.1 M  /user/hive/warehouse/store_format.db/page_views_text</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询时读取数据量大小(单位字节，下面都用这个SQL测试)</p>
<p><code>select count(1) from page_views_text where session_id=&quot;B58W48U4WKZCJ5D1T3Z9ZY88RU7QA7B1&quot;;</code> </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 19024045</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>SequenceFile</p>
<ul>
<li><p>建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_seq( </span><br><span class="line">track_time <span class="keyword">string</span>, </span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>, </span><br><span class="line">session_id <span class="keyword">string</span>, </span><br><span class="line">referer <span class="keyword">string</span>, </span><br><span class="line">ip <span class="keyword">string</span>, </span><br><span class="line">end_user_id <span class="keyword">string</span>, </span><br><span class="line">city_id <span class="keyword">string</span> </span><br><span class="line">)<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> SequenceFile;</span><br><span class="line"></span><br><span class="line"><span class="comment">#查询导入</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> page_views_Seq  <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text;</span><br></pre></td></tr></table></figure>
</li>
<li><p>表大小</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">19.6 M  19.6 M  /user/hive/warehouse/store_format.db/page_views_seq</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询时读取数据量大小(字节)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 61513817</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>RcFile</p>
<ul>
<li><p>建表(CTAS)</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_rcfile</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> RcFile</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text</span><br></pre></td></tr></table></figure>
</li>
<li><p>表大小</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">17.9 M  17.9 M  /user/hive/warehouse/store_format.db/page_views_rcfile</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询时读取数据量大小(字节)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 3726738</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>ORC</p>
<ul>
<li><p>建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_orc</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> ORC</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text</span><br></pre></td></tr></table></figure>
</li>
<li><p>表大小</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 1258828</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询时读取数据量大小(字节)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2.8 M  2.8 M  /user/hive/warehouse/store_format.db/page_views_orc</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Parquet</p>
<ul>
<li><p>建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_par</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> Parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text</span><br></pre></td></tr></table></figure>
</li>
<li><p>表大小</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">13.1 M  13.1 M  /user/hive/warehouse/store_format.db/page_views_par</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询时读取数据量大小(字节)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HDFS Read: 2688348</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ol>
<p>不同的存储格式不仅表现在存储空间上的不同，对于数据的查询，效率也不一样。因为对于不同的存储格式，执行相同的查询操作，他们访问的数据量大小是不一样的。</p>
<p>不同格式表存储大小的比较</p>
<p><img src="https://yerias.github.io/hive_img/%E8%A1%A8%E5%A4%A7%E5%B0%8F%E6%AF%94%E8%BE%83.jpg" alt="表大小比较"></p>
<p>不同格式表读取数据量比较</p>
<p><img src="https://yerias.github.io/hive_img/%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%87%8F%E6%AF%94%E8%BE%83.jpg" alt="读取数据量比较"></p>
<h2 id="存储格式的应用"><a href="#存储格式的应用" class="headerlink" title="存储格式的应用"></a>存储格式的应用</h2><p>原文件还是上面的那个</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop data]$ hdfs dfs -du -s -h /data/page_views.dat</span><br><span class="line">18.1 M  18.1 M  /data/page_views.dat</span><br></pre></td></tr></table></figure>

<ol>
<li><p>ORC+Zlip结合</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_orc_zlib</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> ORC </span><br><span class="line">TBLPROPERTIES(<span class="string">"orc.compress"</span>=<span class="string">"ZLIB"</span>)</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text;</span><br></pre></td></tr></table></figure>

<p>用ORC+Zlip之后的文件为2.8M</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2.8 M  2.8 M  /user/hive/warehouse/store_format.db/page_views_orc_zlib</span><br></pre></td></tr></table></figure>
</li>
<li><p>Parquet+gzip结合</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> parquet.compression=gzip;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_parquet_gzip</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> PARQUET </span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text;</span><br></pre></td></tr></table></figure>

<p>用Parquet+gzip之后的文件为3.9M</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">3.9 M  3.9 M  /user/hive/warehouse/store_format.db/page_views_parquet_gzip</span><br></pre></td></tr></table></figure>
</li>
<li><p>Parquet+Lzo结合</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.exec.compress.output=<span class="literal">true</span>;  </span><br><span class="line"><span class="keyword">SET</span> mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.lzopCodec;</span><br><span class="line"><span class="keyword">SET</span> mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec; </span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_parquet_lzo <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> PARQUET</span><br><span class="line">TBLPROPERTIES(<span class="string">"parquet.compression"</span>=<span class="string">"lzo"</span>)</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views_text;</span><br></pre></td></tr></table></figure>

<p>用Parquet+Lzo(未建立索引)之后的文件为6.2M</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">6.2 M  6.2 M  /user/hive/warehouse/store_format.db/page_views_parquet_lzo</span><br></pre></td></tr></table></figure>

<p>建立索引(表好像没啥用)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar $HADOOP_HOME/share/hadoop/common/hadoop-lzo.jar com.hadoop.compression.lzo.LzoIndexer /user/hive/warehouse/store_format.db/page_views_parquet_lzo</span><br></pre></td></tr></table></figure>

</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/11/hive/11/">HIVE Skewed Table&amp;List Bucketing</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-11</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>HIVE Skewed Table</li>
<li>Skewed Join Optimization(最优化)</li>
<li>Basic Partitioning</li>
<li>List Bucketing</li>
<li>Skewed Table vs List Bucketing Table</li>
<li>List Bucketing Validation</li>
</ol>
<h2 id="HIVE-Skewed-Table"><a href="#HIVE-Skewed-Table" class="headerlink" title="HIVE Skewed Table"></a>HIVE Skewed Table</h2><p>Skewed Table可用于提高一个或多个列具有偏斜值的表的性能。通过指定经常出现的值（严重偏斜），Hive会自动将它们拆分成单独的文件（或在列表存储的情况下为目录），并在查询过程中考虑到这一事实，以便它可以跳过或包括整个文件（或在列表存储的情况下为目录）。<strong>可以在表创建过程中在每个表级别上指定。</strong></p>
<p>若是指定了STORED AS DIRECTORIES，也就是使用列表桶（ListBucketing），hive会对倾斜的值建立子目录，查询会更加得到优化。</p>
<p>下面的示例显示具有三个偏斜值的一列，还可以选择使用STORED AS DIRECTORIES子句来指定列表存储。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> list_bucket_single (<span class="keyword">key</span> <span class="keyword">STRING</span>, <span class="keyword">value</span> <span class="keyword">STRING</span>)</span><br><span class="line">SKEWED <span class="keyword">BY</span> (<span class="keyword">key</span>) <span class="keyword">ON</span> (<span class="number">1</span>,<span class="number">5</span>,<span class="number">6</span>) [<span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES];</span><br></pre></td></tr></table></figure>

<p>这是一个带有两个倾斜列的表的示例。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> list_bucket_multiple (col1 <span class="keyword">STRING</span>, col2 <span class="built_in">int</span>, col3 <span class="keyword">STRING</span>)</span><br><span class="line">SKEWED <span class="keyword">BY</span> (col1, col2) <span class="keyword">ON</span> ((<span class="string">'s1'</span>,<span class="number">1</span>), (<span class="string">'s3'</span>,<span class="number">3</span>), (<span class="string">'s13'</span>,<span class="number">13</span>), (<span class="string">'s78'</span>,<span class="number">78</span>)) [<span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES];</span><br></pre></td></tr></table></figure>



<p>可以使用alter table语句来对已创建的表修改倾斜信息。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name SKEWED <span class="keyword">BY</span> (col_name1, col_name2, ...) <span class="keyword">ON</span> ([(col_name1_value, col_name2_value, ...) [, (col_name1_value, col_name2_value), ...][<span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES];</span><br></pre></td></tr></table></figure>

<p><code>STORED AS DIRECTORIES</code>选项确定倾斜表是否使用列表存储功能，该功能为倾斜值创建子目录。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">NOT</span> SKEWED;</span><br></pre></td></tr></table></figure>

<p><code>NOT SKEWED</code>选项使表不倾斜，并关闭列表存储功能（因为列表存储表始终是倾斜的）。这会影响在ALTER语句之后创建的分区，但对在ALTER语句之前创建的分区没有影响。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">NOT</span> <span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES;</span><br></pre></td></tr></table></figure>

<p><code>NOT STORED</code>会关闭列表存储功能，但是表仍然歪斜。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">SET</span> SKEWED LOCATION (col_name1=<span class="string">"location1"</span> [, col_name2=<span class="string">"location2"</span>, ...] );</span><br></pre></td></tr></table></figure>

<p>修改list bucketing倾斜值的存储位置映射。</p>
<h2 id="Skewed-Join-Optimization-最优化"><a href="#Skewed-Join-Optimization-最优化" class="headerlink" title="Skewed Join Optimization(最优化)"></a>Skewed Join Optimization(最优化)</h2><p>两个大数据表的连接由一组MapReduce作业完成，它们首先根据连接键对表进行排序，然后将它们连接起来，mapper将相同键的行发送到同一个reduce。<br>假设表A id字段有值1，2，3，4，并且表B也含有id列，含有值1，2，3。我们使用如下语句来进行连接。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> A.id <span class="keyword">from</span> A <span class="keyword">join</span> B <span class="keyword">on</span> A.id = B.id</span><br></pre></td></tr></table></figure>

<p>将会有一组mappers读这两个表并基于连接键id发送到reducers，假设id=1的行分发到Reducer R1，id=2的分发到R2等等，这些reducer对A和B进行交叉连接，R4从A得到id=4的所有行，但是不会产生任何结果。</p>
<p>现在我们假定A在id=1上倾斜，这样R2和R3将会很快完成但是R1会执行很长时间，因此成为job的瓶颈。若是用户知道这些倾斜信息，这种瓶颈可以使用如下方法人工避免：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> A.id <span class="keyword">from</span> A <span class="keyword">join</span> B <span class="keyword">on</span> A.id = B.id <span class="keyword">where</span> A.id &lt;&gt; <span class="number">1</span>;</span><br><span class="line"><span class="keyword">select</span> A.id <span class="keyword">from</span> A <span class="keyword">join</span> B <span class="keyword">on</span> A.id = B.id <span class="keyword">where</span> A.id = <span class="number">1</span> <span class="keyword">and</span> B.id = <span class="number">1</span>;</span><br></pre></td></tr></table></figure>

<p>第一个查询没有倾斜数据将会很快的完成，如果我们假定表B中只有少量的B.id=1的行，能够直接加载到内存中，通过将B的数据存储到内存中的哈希表中，join将会高效的完成，因此可以再mappper端进行连接，而不用reduce，效率会高很多，最后合并结果。</p>
<p>优点：</p>
<ul>
<li>如果少量的倾斜键占了很大一部分数据，它们将不会成为瓶颈。</li>
</ul>
<p>缺点：</p>
<ul>
<li><p>表A和表B需要分别读和处理两次；</p>
</li>
<li><p>结果需要合并；</p>
</li>
<li><p>需要人工的处理倾斜数据。</p>
</li>
</ul>
<p>hive为了避免上面的操作，在处理数据是对倾斜值进行特殊处理，首先读表B并且存储B.id=1的数据到内存的哈希表中，运行一组mappers来读取表A并做以下操作：</p>
<ol>
<li>若id为1，使用B的id=1的哈希表来计算结果</li>
<li>对于其他值，发送到reducer端来join，这个reduce也会从B的mapper中得到对应需要连接的数据。</li>
</ol>
<p>使用这种方法，最终我们只读取B两次，并且A中的倾斜数据在mapper中进行连接，不会被发送到reducer，其他的键值通过map/reduce。</p>
<p>假设B的行很少，而键在A中倾斜。因此可以将这些行加载到内存中。<strong>若是使用ListBucketing对倾斜值单独存储，会有更好的性能。在读倾斜的数据到内存中时可以指定到倾斜目录下的数据。</strong></p>
<h2 id="Basic-Partitioning"><a href="#Basic-Partitioning" class="headerlink" title="Basic Partitioning"></a>Basic Partitioning</h2><p>有如下问题：存在许多表是这种格式</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> T(a, b, c, ....., x) partitioned <span class="keyword">by</span> (ds);</span><br></pre></td></tr></table></figure>

<p>但是以下查询需要更加高效(这不扯蛋吗，有分区的查询条件不是分区)：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> x = <span class="number">10</span>;</span><br></pre></td></tr></table></figure>

<p>字段x中含有倾斜数据，一般情况下x的值中大约有1000个值有重度倾斜，其他值基数很小，当然，每天倾斜的X的值可能改变，上述要求可以通过以下方式解决：</p>
<p>为值“ x”创建一个分区。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> T(a,b,c, .......) partitioned <span class="keyword">by</span> (ds, x)</span><br></pre></td></tr></table></figure>

<p>优点</p>
<ul>
<li>现有的Hive就足够了。</li>
</ul>
<p>缺点</p>
<ul>
<li>HDFS可伸缩性：HDFS中的文件数量增加。</li>
<li>HDFS可伸缩性：HDFS中的中间文件数量增加。例如，如果有1000个映射器和1000个分区，并且每个映射器每个键至少获得1行，我们最终将创建100万个中间文件。</li>
<li>Metastore的可伸缩性：Metastore会随着分区数量的增长而扩展。</li>
</ul>
<h2 id="List-Bucketing"><a href="#List-Bucketing" class="headerlink" title="List Bucketing"></a>List Bucketing</h2><p>上边方法提到将含有倾斜值得列作为分区存储，但是可能产生大量的目录，为什么不把列值不倾斜的放在一起呢，将每个倾斜的值单独存放一个目录，于是有了List Bucketing。</p>
<p>这个映射在表或分区级别的Metastore中维护。倾斜键的列表存储在表级别中，这个列表可以由客户端周期的提供，并且在新的分区载入时可以被更新。</p>
<p>如下例子，一个表含有一个x字段倾斜值的列表：6，20，30，40。当一个新的分区载入时，它会创建5个目录（4个目录对应x的4个倾斜值，另外一个目录是其余值）。这个表的数据被分成了5个部分：6，20，30，40，others。这跟上一节介绍的分桶表类似，桶的个数决定了文件的个数。倾斜键的整个列表存储在每个表或者分区中。</p>
<p>当使用一下查询时，hive编译器会仅仅使用x=30对应的目录去运行map-reduce。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> ds = <span class="string">'2012-04-15'</span> <span class="keyword">and</span> x = <span class="number">30</span>;</span><br></pre></td></tr></table></figure>

<p>若是查询是x=50，则会使用x=others对应的目录去运行map-reduce作业。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> ds = <span class="string">'2012-04-15'</span> <span class="keyword">and</span> x = <span class="number">50</span>;</span><br></pre></td></tr></table></figure>

<p>这种方法在一下条件下是很有效的：</p>
<ol>
<li><strong>每个分区的倾斜键占总数据的一大部分</strong>。在上边的例子中，如果倾斜的键（6，20，30，40）只占一小部分数据（比如20%）,那么在查询x=50时依然需要扫描80%的数据。</li>
<li><strong>每个分区的倾斜键数量非常少</strong>，因为这个倾斜值列表存在在元数据库中，在元数据库中为每个分区存储100w个倾斜键是没有意义的。</li>
</ol>
<p>这种方法也可被扩展到含有多个列产生的倾斜键，例如我们想优化一下查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> x = <span class="number">10</span> <span class="keyword">and</span> y = <span class="string">'b'</span>;</span><br></pre></td></tr></table></figure>

<p>扩展以上的方法，对于（x，y）每个倾斜值,也按照上边方式单独存储，因此元数据库会有以下映射： </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">(10, 'a') <span class="comment">--&gt; 1, (10, 'b') --&gt; 2, (20, 'c') --&gt; 3, (others) --&gt; 4.</span></span><br></pre></td></tr></table></figure>

<p>因此可直接找到2对应的目录，减少要处理的数据。</p>
<p>同时以下查询也会有一定程度的优化：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> x = <span class="number">10</span>; </span><br><span class="line"><span class="keyword">select</span> ... <span class="keyword">from</span> T <span class="keyword">where</span> y = <span class="string">'b'</span>;</span><br></pre></td></tr></table></figure>

<p>以上两个语句在执行的过程中会裁剪掉一部分数据，例如，对x=10的查询hive编译器可以裁剪掉 ( 20 , c ) 对应的文件，对于 y = ‘b’，( 10 , ‘a’ )  和 ( 20 , ‘c’ ) 对应的文件会被裁剪掉，一定程度能够减少扫描的数据量。</p>
<p>这种方法不适用于以下场景：</p>
<ul>
<li>倾斜键数目非常多，元数据规模问题</li>
<li>许多情况下，倾斜键由多个列组成，但是在查询中，没有使用到倾斜键中的那些列。</li>
</ul>
<h2 id="Skewed-Table-vs-List-Bucketing-Table"><a href="#Skewed-Table-vs-List-Bucketing-Table" class="headerlink" title="Skewed Table vs List Bucketing Table"></a>Skewed Table vs List Bucketing Table</h2><ul>
<li>Skewed Table是一个表它含有倾斜的信息。</li>
<li>List Bucketing Table是Skewed Table，此外，它告诉hive使用列表桶的特点：为倾斜值创建子目录。</li>
</ul>
<p>以下说明两者的存储区别：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t1 (x <span class="keyword">string</span>) skewed <span class="keyword">by</span> (x) <span class="keyword">on</span> (‘a’, ‘b’) partitioned <span class="keyword">by</span> dt location ‘/<span class="keyword">user</span>/hive/warehouse/t1’;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t2 (x <span class="keyword">string</span>) skewed <span class="keyword">by</span> (x) <span class="keyword">on</span> (‘a’, ‘b’) <span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES partitioned <span class="keyword">by</span> dt location ‘/<span class="keyword">user</span>/hive/warehouse/t2’ ;</span><br></pre></td></tr></table></figure>

<p>两者存储的形式如下所示：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">/user/hive/warehouse/t1/dt=something/data.txt</span><br><span class="line">/user/hive/warehouse/t2/dt=something/x=a/data.txt</span><br><span class="line">/user/hive/warehouse/t2/dt=something/x=b/data.txt</span><br><span class="line">/user/hive/warehouse/t2/dt=something/default/data.txt</span><br></pre></td></tr></table></figure>

<h2 id="List-Bucketing-Validation"><a href="#List-Bucketing-Validation" class="headerlink" title="List Bucketing Validation"></a>List Bucketing Validation</h2><p>由于列表桶的子目录特点，它不能够与一些特征共存。</p>
<p>DDL</p>
<p>列表桶与以下共存会抛出编译错误：</p>
<ul>
<li>normal bucketing (clustered by, tablesample, etc.)</li>
<li>external table</li>
<li>“ load data …”</li>
<li>CTAS (Create Table As Select) queries</li>
</ul>
<p>DML</p>
<p>与一下DML操作共存也会跳出错误：</p>
<ul>
<li>“ insert into ”</li>
<li>normal bucketing (clustered by, tablesample, etc.)</li>
<li>external table</li>
<li>non-RCfile due to merge</li>
<li>non-partitioned table</li>
</ul>
<hr>
<p>参考文献：</p>
<p>1.<a href="https://cwiki.apache.org/confluence/display/Hive/ListBucketing" target="_blank" rel="noopener">HIVE ListBucketing</a><br>2.<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-SkewedTables" target="_blank" rel="noopener">HIVE LanguageManualDDL-SkewedTables</a><br>3.<a href="https://cwiki.apache.org/confluence/display/Hive/Skewed+Join+Optimization" target="_blank" rel="noopener">HIVE Skewed Join Optimization</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/10/hive/10/">大表Join大表&amp;大表Join小表&amp;group By解决数据倾斜</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-10</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>大表Join大表</li>
<li>大表Join小表</li>
<li>group By解决</li>
</ol>
<h2 id="大表Join大表"><a href="#大表Join大表" class="headerlink" title="大表Join大表"></a>大表Join大表</h2><h3 id="思路一：SMBJoin"><a href="#思路一：SMBJoin" class="headerlink" title="思路一：SMBJoin"></a>思路一：SMBJoin</h3><p>smb是sort  merge bucket操作，首先进行排序，继而合并，然后放到所对应的bucket中去，bucket是hive中和分区表类似的技术，就是按照key进行hash，相同的hash值都放到相同的buck中去。在进行两个表联合的时候。我们首先进行分桶，在join会大幅度的对性能进行优化。也就是说，在进行联合的时候，是bukect中的一小部分和bukect中的一小部分进行联合，table联合都是等值连接，相同的key都放到了同一个bucket中去了，那么在联合的时候就会大幅度的减小无关项的扫描。</p>
<ol>
<li><p>设置参数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join.noconditionaltask=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>两个表的bucket数量相等</p>
</li>
<li><p>Bucket列、Join列、Sort列、Skewed列为相同的字段</p>
</li>
<li><p>必须是应用在bucket mapjoin 的场景中</p>
</li>
<li><p>注意点</p>
<p>hive并不检查两个join的表是否已经做好bucket且sorted，需要用户自己去保证join的表，否则可能数据不正确。有两个办法</p>
<ul>
<li><p>hive.enforce.sorting 设置为true</p>
</li>
<li><p>手动生成符合条件的数据，通过在sql中用distributed c1 sort by c1 或者 cluster by c1，表创建时必须是CLUSTERED且SORTED，如下</p>
</li>
<li><p>创建Skewed Table提高有一个或多个列有倾斜值的表的性能，例如：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_smb_2(<span class="keyword">mid</span> <span class="keyword">string</span>,age_id <span class="keyword">string</span>)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span>(<span class="keyword">mid</span>) SORTED <span class="keyword">BY</span>(<span class="keyword">mid</span>) <span class="keyword">INTO</span> <span class="number">500</span> BUCKETS</span><br><span class="line">SKEWED <span class="keyword">BY</span>(<span class="keyword">mid</span>) <span class="keyword">on</span> (<span class="number">001</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>案例</p>
<ol>
<li><p>设置先关参数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join.noconditionaltask=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建桶表</p>
<p>user表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_info_bucket(userid <span class="keyword">string</span>,uname <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span>(userid) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line">SKEWED <span class="keyword">BY</span>(userid) <span class="keyword">on</span> (<span class="number">001</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line">STORED AS DIRECTORIES</span><br></pre></td></tr></table></figure>

<p>domain表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> domain_info_bucket(userid <span class="keyword">string</span>,domainid <span class="keyword">string</span>,<span class="keyword">domain</span> <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span> (userid) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line">SKEWED <span class="keyword">BY</span>(userid) <span class="keyword">on</span> (<span class="number">001</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line">STORED AS DIRECTORIES</span><br></pre></td></tr></table></figure>
</li>
<li><p>分别倒入数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> user_info_bucket <span class="keyword">select</span> userid ,uname <span class="keyword">from</span> <span class="keyword">user</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> domain_info_bucket <span class="keyword">select</span> userid ,domainid,<span class="keyword">domain</span> <span class="keyword">from</span> doamin</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> user_info_bucket u  <span class="keyword">join</span> domain_info_bucket d <span class="keyword">on</span>(u.userid==d.userid)</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h3 id="思路二：一分为二"><a href="#思路二：一分为二" class="headerlink" title="思路二：一分为二"></a>思路二：一分为二</h3><p>选择临时表的方式，将数据一分为二，把倾斜的key，和不倾斜的key分开处理，不倾斜的正常join，倾斜的根据情况选择mapjoin或加盐处理，最后结果union all结果</p>
<p>user表为用户基本表，domain为用户访问域名的宽表</p>
<p><strong>注意：</strong>我们其实隐含使用到了mapjoin，hive中的参数为<code>set hive.auto.convert.join=true;</code>，自动开启，默认25M，不能超过1G。</p>
<ol start="0">
<li><p>创建中间表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tmp_table(userid <span class="keyword">string</span>,uname <span class="keyword">string</span>)</span><br><span class="line">SKEWED <span class="keyword">BY</span>(userid) <span class="keyword">on</span> (<span class="number">001</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line">STORED AS DIRECTORIES</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>count(*)</code>出符合倾斜条件的数据存入中间表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> tmp_table</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">	d.userid,u.uname</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	(<span class="keyword">select</span> userid</span><br><span class="line">	<span class="keyword">from</span> </span><br><span class="line">		(<span class="keyword">select</span> </span><br><span class="line">			userid,</span><br><span class="line">			<span class="keyword">count</span>(userid) u_cunt</span><br><span class="line">			</span><br><span class="line">		<span class="keyword">from</span> </span><br><span class="line">			<span class="keyword">domain</span></span><br><span class="line">		<span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">			userid) t</span><br><span class="line">	<span class="keyword">where</span> u_cunt&gt;<span class="number">100</span>) d</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span></span><br><span class="line">	(<span class="keyword">select</span> </span><br><span class="line">		userid,uname</span><br><span class="line">	<span class="keyword">from</span></span><br><span class="line">		<span class="keyword">user</span>) u</span><br><span class="line"><span class="keyword">on</span> d.userid = u.userid;</span><br></pre></td></tr></table></figure>
</li>
<li><p>一分为二，分别查询中出结果，再union all</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	u1.userid,u1.uname,d1.domain</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	(<span class="keyword">select</span> </span><br><span class="line">		userid,uname</span><br><span class="line">	<span class="keyword">from</span></span><br><span class="line">		<span class="keyword">user</span>) u1</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">	(<span class="keyword">select</span> </span><br><span class="line">		d.userid,d.domain</span><br><span class="line">	<span class="keyword">from</span></span><br><span class="line">		<span class="keyword">domain</span> d</span><br><span class="line">	<span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span></span><br><span class="line">		tmp_table t</span><br><span class="line">	<span class="keyword">on</span> </span><br><span class="line">		d.userid = t.userid</span><br><span class="line">	<span class="keyword">where</span> </span><br><span class="line">		t.userid <span class="keyword">is</span> <span class="literal">null</span>) d1</span><br><span class="line"><span class="keyword">on</span> u1.userid = d1.userid</span><br><span class="line"></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">	u2.userid,u2.uname,d2.domain</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	(<span class="keyword">select</span> 	</span><br><span class="line">		userid,uname</span><br><span class="line">	<span class="keyword">from</span></span><br><span class="line">		<span class="keyword">user</span>) u2</span><br><span class="line"><span class="keyword">join</span> </span><br><span class="line">	(<span class="keyword">select</span> </span><br><span class="line">		d.userid,d.domain</span><br><span class="line">		<span class="keyword">from</span></span><br><span class="line">			<span class="keyword">domain</span> d</span><br><span class="line">		<span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span></span><br><span class="line">			tmp_table t</span><br><span class="line">		<span class="keyword">on</span> </span><br><span class="line">			d.userid = t.userid</span><br><span class="line">		<span class="keyword">where</span> </span><br><span class="line">			t.userid <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>) d2</span><br><span class="line"><span class="keyword">on</span> u2.userid = d2.userid</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="大表Join小表"><a href="#大表Join小表" class="headerlink" title="大表Join小表"></a>大表Join小表</h2><h3 id="思路：MapJoin"><a href="#思路：MapJoin" class="headerlink" title="思路：MapJoin"></a>思路：MapJoin</h3><p>大表Join小表很好解决，把小表放进内存，大表再去匹配即可。</p>
<p>思路：</p>
<ol>
<li><p>开启MapJoin</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>调整MapJoin小表大小，默认25M(调整为可以容忍的大小)</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果是MR，小表放进Map，大表进入Mapper匹配Map(使用对象存储结果)</p>
</li>
</ol>
<h2 id="group-By解决"><a href="#group-By解决" class="headerlink" title="group By解决"></a>group By解决</h2><h3 id="思路：加盐去盐"><a href="#思路：加盐去盐" class="headerlink" title="思路：加盐去盐"></a>思路：加盐去盐</h3><ol>
<li><p>开启数据倾斜时的负载均衡</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata=<span class="literal">true</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>对groupby的key加盐去盐</p>
<p>开启上面这个参数，hive会自动拆解成两个MR，加盐去盐，最终输出结果</p>
<p>MR需要写两个MR，一个对key加盐，一个对key去盐</p>
</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/09/hive/9/">HIVE调优(1)</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-09</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>Fetch</li>
<li>本地模式</li>
<li>JVM重用</li>
<li>map数量</li>
<li>reduce数量</li>
<li>推测执行</li>
</ol>
<h2 id="Fetch"><a href="#Fetch" class="headerlink" title="Fetch"></a>Fetch</h2><p>通过修改<code>hive.fetch.task.conversion</code>参数可以让一些select查询可以转换为单个获取任务，不需要执行MapReduce任务，从而最小化延迟。</p>
<p>目前的版本中支持none、minimal和more</p>
<ul>
<li><code>none</code>: 是禁用这一特性</li>
<li><code>minimal</code>: 允许使用<code>SELECT *</code>、<code>FILTER on partition columns (WHERE and HAVING clauses)</code>、<code>LIMIT only</code></li>
<li><code>more</code>: 最大程度的允许使用 <code>SELECT</code>, <code>FILTER</code>, <code>LIMIT only (including TABLESAMPLE, virtual columns)、where</code></li>
</ul>
<p>当前版本默认使用more</p>
<h2 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h2><p><code>hive.exec.mode.local.auto</code> 参数决定hive是否允许使用本地化，默认<code>hive.exec.mode.local.auto=false</code> 没有启用本地化</p>
<p><code>hive.exec.mode.local.auto.inputbytes.max</code> 参数规定了使用本地化处理的最大的文件字节数，默认是128M</p>
<p><code>hive.exec.mode.local.auto.input.files.max</code> 参数规定了使用本地化处理的最大文件数，默认是4个</p>
<h2 id="JVM重用"><a href="#JVM重用" class="headerlink" title="JVM重用"></a>JVM重用</h2><p>在目前使用的版本中，<code>mapreduce.job.jvm.numtasks</code> 参数可以控制Java虚拟机的回收，由于<code>mapTask</code>或者<code>reduceTask</code>都是进程，需要启用JVM，作业运行结束了关闭JVM，使用这个参数控制JVM运行完作业不关机，继续执行作业。默认是1个。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.jvm.numtasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>How many tasks to run per jvm. If set to -1, there is no limit.  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="map数量"><a href="#map数量" class="headerlink" title="map数量"></a>map数量</h2><p><code>mapTask</code>数量由输入的文件大小、文件数和输入的文件产生多少个block决定。</p>
<p>那么我们如何考虑map的数量呢?</p>
<p>理论上讲<code>mapTask</code>越多Map作业的并行度越高，但是耗费的时间和资源也越多，map、reduce作业都是进程级别。</p>
<p>场景：128M的数据只有一两个列的数据记录数有几千万个，必须拆开用NTILE(n)==&gt; 改变map任务的数量</p>
<h2 id="reduce数量"><a href="#reduce数量" class="headerlink" title="reduce数量"></a>reduce数量</h2><p><code>mapred.reduce.tasks=-1</code> 参数决定每个作业的默认数量。通常设置为接近可用主机数量的素数。通过将此属性设置为-1,Hive将自动计算出还原器的数量。默认是-1，即自动计算</p>
<p><code>hive.exec.reducers.bytes.per.reducer=256M</code> 参数决定了reduce最大的字节数，在Hive 0.14.0及以后的版本中，默认为256 MB，也就是说，如果输入大小为1 GB，那么将使用4个reduce。</p>
<p><code>hive.exec.reducers.max=1099</code> 参数决定最大可以使用的reduce数量，如果<code>mapred.reduce.tasks</code> 参数为-1，即自动计算reduce数量，那么Hive将使用这个参数作为最大的reduce数量，自动确定reduce的数量。</p>
<p>计算reducer数的公式很简单N=min( <code>hive.exec.reducers.max</code> ，总输入数据量/ <code>hive.exec.reducers.bytes.per.reducer</code> )</p>
<p>reduce的数量决定最终文件输出的数量<br>思路：reduce数量越多，小文件越多，reduce数量越少，文件大耗费的时间多，最终在reduce文件的大小和需要消耗的时间取个折中。 如果没有reduce，那么map的数据个数决定了输出文件个数 。</p>
<p>Spark3.0 自动适配</p>
<h2 id="推测执行"><a href="#推测执行" class="headerlink" title="推测执行"></a>推测执行</h2><ol>
<li><p>作业完成时间取决于最慢的任务完成时间</p>
<p>一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。</p>
<p>思考：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？</p>
</li>
<li><p>推测执行机制</p>
<p>发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果。</p>
</li>
<li><p>执行推测任务的前提条件</p>
</li>
</ol>
<ul>
<li><p>每个Task只能有一个备份任务</p>
</li>
<li><p>当前 Job 已完成的 Task 必须不小于0.05（5%）</p>
</li>
<li><p>开启推测执行参数设置。mapred-site.xml文件中默认是打开的。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some map tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some reduce tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol start="4">
<li>不能启用推测执行机制情况</li>
</ol>
<ul>
<li><p>任务间存在严重的数据倾斜，数据倾斜跑不过去的，开启多少个推测执行都跑不过去；</p>
</li>
<li><p>特殊任务，比如任务向数据库中写数据。</p>
</li>
</ul>
<ol start="5">
<li><p>生产建议</p>
<p>一般生产生禁用此功能，除非特殊场景直接命令开启</p>
</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/08/hive/8/">Windowing functions&amp;The OVER clause&amp;Analytics functions</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-08</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>The OVER clause</li>
<li>Analytics functions</li>
<li>Windowing functions</li>
</ol>
<h2 id="The-OVER-clause"><a href="#The-OVER-clause" class="headerlink" title="The OVER clause"></a>The OVER clause</h2><p>聚合函数是将多行数据按照规则聚合为一行，比如count()、sum()、min()、max()、avg()</p>
<p>窗口函数是在做聚合的基础上，要返回的数据不仅仅是一行</p>
<p>窗口函数是在窗口的基础上做统计分析，对其所作用的窗口中的每一条记录输出一条结果</p>
<p>窗口函数借助于over() 函数开窗</p>
<p>窗口函数的标准聚合函数同样包括count()、sum()、min()、max()、avg()</p>
<p>窗口可以在一个窗口子句中单独定义。窗口规范支持以下格式:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">(ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING</span><br></pre></td></tr></table></figure>

<p>窗口有以上三种定义方式，分别是<code>从某行之后到某行</code>、<code>某行之后到某行之前</code>、<code>某行到某行之前</code></p>
<p><code>PRECEDING</code>: 往前<br><code>FOLLOWING</code>: 往后<br><code>CURRENT ROW</code>: 当前行<br><code>UNBOUNDED</code>: 起点，<code>UNBOUNDED PRECEDING</code> 表示从前面的起点， <code>UNBOUNDED FOLLOWING</code>: 表示到后面的终点</p>
<ol>
<li><p>准备数据</p>
<p>window01.txt </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ruozedata,2019-04-10,1</span><br><span class="line">ruozedata,2019-04-11,5</span><br><span class="line">ruozedata,2019-04-12,7</span><br><span class="line">ruozedata,2019-04-13,3</span><br><span class="line">ruozedata,2019-04-14,2</span><br><span class="line">ruozedata,2019-04-15,4</span><br><span class="line">ruozedata,2019-04-16,4</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span>  <span class="keyword">database</span> window_over;</span><br><span class="line"><span class="keyword">use</span> window_over</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> window01(</span><br><span class="line">	<span class="keyword">name</span> <span class="keyword">String</span>,</span><br><span class="line">	<span class="built_in">date</span> <span class="keyword">String</span>,</span><br><span class="line">	grade <span class="built_in">Int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>导入数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/window/window01.txt'</span> <span class="keyword">into</span>  <span class="keyword">table</span>  window01;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询</p>
<p>问题：窗口到底怎么开? </p>
<p>核心：从什么地方开始到什么地方结束</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	<span class="keyword">name</span>,</span><br><span class="line">	<span class="built_in">date</span>,</span><br><span class="line">	grade,</span><br><span class="line">	<span class="keyword">sum</span>(grade) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> grade <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> g1,	//第一行到当前行的grade求<span class="keyword">sum</span>()</span><br><span class="line">	<span class="keyword">sum</span>(grade) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> grade <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">3</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> g2,	//第三行到当前行的grade求<span class="keyword">sum</span>()</span><br><span class="line">	<span class="keyword">sum</span>(grade) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> grade <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">3</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="number">1</span> <span class="keyword">following</span>) <span class="keyword">as</span> g3,	//第三行到后一行的grade求<span class="keyword">sum</span>()</span><br><span class="line">	<span class="keyword">sum</span>(grade) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> grade <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="keyword">unbounded</span> <span class="keyword">FOLLOWING</span>) <span class="keyword">as</span> g4	//当前行到最后一行的grade求<span class="keyword">sum</span>()</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	window01;</span><br></pre></td></tr></table></figure>

<p>结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name     |date      |grade|g1|g2|g3|g4|</span><br><span class="line"><span class="comment">---------|----------|-----|--|--|--|--|</span></span><br><span class="line">ruozedata|2019-04-10|    1| 1| 1| 3|26|</span><br><span class="line">ruozedata|2019-04-14|    2| 3| 3| 6|25|</span><br><span class="line">ruozedata|2019-04-13|    3| 6| 6|10|23|</span><br><span class="line">ruozedata|2019-04-16|    4|10|10|14|20|</span><br><span class="line">ruozedata|2019-04-15|    4|14|13|18|16|</span><br><span class="line">ruozedata|2019-04-11|    5|19|16|23|12|</span><br><span class="line">ruozedata|2019-04-12|    7|26|20|20| 7|</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="Analytics-functions"><a href="#Analytics-functions" class="headerlink" title="Analytics functions"></a>Analytics functions</h2><p>分析函数有RANK、ROW_NUMBER、DENSE_RANK、CUME_DIST、PERCENT_RANK、NTILE</p>
<p>这些函数可以分为三部分，第一部分是排序相关的RANK、ROW_NUMBER、DENSE_RANK，第二部分是占比相关的CUME_DIST、PERCENT_RANK，第三部分是把表切成指定分区的NTILE</p>
<h3 id="排序相关"><a href="#排序相关" class="headerlink" title="排序相关"></a>排序相关</h3><ol>
<li><p>准备数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">gifshow.com,2019-04-10,1</span><br><span class="line">gifshow.com,2019-04-11,5</span><br><span class="line">gifshow.com,2019-04-12,7</span><br><span class="line">gifshow.com,2019-04-13,3</span><br><span class="line">gifshow.com,2019-04-14,2</span><br><span class="line">gifshow.com,2019-04-15,4</span><br><span class="line">gifshow.com,2019-04-16,4</span><br><span class="line">yy.com,2019-04-10,2</span><br><span class="line">yy.com,2019-04-11,3</span><br><span class="line">yy.com,2019-04-12,5</span><br><span class="line">yy.com,2019-04-13,6</span><br><span class="line">yy.com,2019-04-14,3</span><br><span class="line">yy.com,2019-04-15,9</span><br><span class="line">yy.com,2019-04-16,7</span><br></pre></td></tr></table></figure>
</li>
<li><p>建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> traffic(</span><br><span class="line">	<span class="keyword">domain</span> <span class="keyword">String</span>,</span><br><span class="line">	<span class="built_in">date</span> <span class="keyword">String</span>,</span><br><span class="line">	grade <span class="built_in">Int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span>  <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>导数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/window/rank.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> traffic;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	<span class="keyword">domain</span>,</span><br><span class="line">	<span class="built_in">date</span>,</span><br><span class="line">	grade,</span><br><span class="line">	<span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> r1,</span><br><span class="line">	ROW_NUMBER() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> r2,</span><br><span class="line">	<span class="keyword">DENSE_RANK</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> r3</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	traffic;</span><br></pre></td></tr></table></figure>

<p>结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">domain     |date      |grade|r1|r2|r3|</span><br><span class="line"><span class="comment">-----------|----------|-----|--|--|--|</span></span><br><span class="line">gifshow.com|2019-04-10|    1| 1| 1| 1|</span><br><span class="line">gifshow.com|2019-04-14|    2| 2| 2| 2|</span><br><span class="line">gifshow.com|2019-04-13|    3| 3| 3| 3|</span><br><span class="line">gifshow.com|2019-04-16|    4| 4| 4| 4|</span><br><span class="line">gifshow.com|2019-04-15|    4| 4| 5| 4|</span><br><span class="line">gifshow.com|2019-04-11|    5| 6| 6| 5|</span><br><span class="line">gifshow.com|2019-04-12|    7| 7| 7| 6|</span><br><span class="line">yy.com     |2019-04-10|    2| 1| 1| 1|</span><br><span class="line">yy.com     |2019-04-11|    3| 2| 2| 2|</span><br><span class="line">yy.com     |2019-04-14|    3| 2| 3| 2|</span><br><span class="line">yy.com     |2019-04-12|    5| 4| 4| 3|</span><br><span class="line">yy.com     |2019-04-13|    6| 5| 5| 4|</span><br><span class="line">yy.com     |2019-04-16|    7| 6| 6| 5|</span><br><span class="line">yy.com     |2019-04-15|    9| 7| 7| 6|</span><br></pre></td></tr></table></figure>
</li>
<li><p>总结</p>
<p><code>RANK()</code>：分组内生成编号，排名相同相同的名词留空位<br><code>DENSE_RANK()</code>： 分组内生成编号，排名相同相同的名词不留空位<br><code>ROW_NUMBER()</code>： 从1开始，按照排序，生成分组内记录的序号(推介使用)</p>
</li>
</ol>
<h3 id="占比相关"><a href="#占比相关" class="headerlink" title="占比相关"></a>占比相关</h3><p><code>CUME_DIST()</code>: 小于等于当前行值(OVER中order by指定的字段排序)的行数/分组内的总行数<br><code>PERCENT_RANK()</code>: 分组内当前行的rank -1 / 分组内总行数 -1</p>
<ol>
<li><p>准备数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">dept01,ruoze,10000</span><br><span class="line">dept01,jepson,20000</span><br><span class="line">dept01,xingxing,30000</span><br><span class="line">dept02,zhangsan,40000</span><br><span class="line">dept02,lisi,50000</span><br></pre></td></tr></table></figure>
</li>
<li><p>建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> window02(</span><br><span class="line">	dept <span class="keyword">String</span>,</span><br><span class="line">	<span class="keyword">user</span> <span class="keyword">String</span>,</span><br><span class="line">	sal <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>导数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/window/window02.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> window02;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	dept,</span><br><span class="line">	<span class="keyword">user</span>,</span><br><span class="line">	sal,</span><br><span class="line">	<span class="keyword">CUME_DIST</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> sal) cume,</span><br><span class="line">	<span class="keyword">PERCENT_RANK</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> sal) <span class="keyword">percent</span></span><br><span class="line"><span class="keyword">from</span> window02;</span><br></pre></td></tr></table></figure>

<p>结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">dept  |user    |sal  |cume1|cume2             |percent|</span><br><span class="line"><span class="comment">------|--------|-----|-----|------------------|-------|</span></span><br><span class="line">dept01|ruoze   |10000|  0.2|0.3333333333333333|      0|</span><br><span class="line">dept01|jepson  |20000|  0.4|0.6666666666666666|    0.5|</span><br><span class="line">dept01|xingxing|30000|  0.6|                 1|      1|</span><br><span class="line">dept02|zhangsan|40000|  0.8|               0.5|      0|</span><br><span class="line">dept02|lisi    |50000|    1|                 1|      1|</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="分区相关"><a href="#分区相关" class="headerlink" title="分区相关"></a>分区相关</h3><p><code>NTILE(num)</code>：将数据按照输入的数成<code>num</code>片，并且记录分片号</p>
<p>场景：128M的数据只有一两个列的数据记录数有几千万个，必须拆开用<code>NTILE(n)</code>==&gt; 改变map任务的数量</p>
<ol>
<li><p>查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	<span class="keyword">domain</span>,</span><br><span class="line">	<span class="built_in">date</span>,</span><br><span class="line">	grade,</span><br><span class="line">	ntile(<span class="number">2</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> n1,</span><br><span class="line">	ntile(<span class="number">3</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> n2,</span><br><span class="line">	ntile(<span class="number">4</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">domain</span> <span class="keyword">order</span> <span class="keyword">by</span> grade) <span class="keyword">as</span> n3</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	traffic;</span><br></pre></td></tr></table></figure>

<p>结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">domain     |date      |grade|n1|n2|n3|</span><br><span class="line"><span class="comment">-----------|----------|-----|--|--|--|</span></span><br><span class="line">gifshow.com|2019-04-10|    1| 1| 1| 1|</span><br><span class="line">gifshow.com|2019-04-14|    2| 1| 1| 1|</span><br><span class="line">gifshow.com|2019-04-13|    3| 1| 1| 2|</span><br><span class="line">gifshow.com|2019-04-16|    4| 1| 2| 2|</span><br><span class="line">gifshow.com|2019-04-15|    4| 2| 2| 3|</span><br><span class="line">gifshow.com|2019-04-11|    5| 2| 3| 3|</span><br><span class="line">gifshow.com|2019-04-12|    7| 2| 3| 4|</span><br><span class="line">yy.com     |2019-04-10|    2| 1| 1| 1|</span><br><span class="line">yy.com     |2019-04-11|    3| 1| 1| 1|</span><br><span class="line">yy.com     |2019-04-14|    3| 1| 1| 2|</span><br><span class="line">yy.com     |2019-04-12|    5| 1| 2| 2|</span><br><span class="line">yy.com     |2019-04-13|    6| 2| 2| 3|</span><br><span class="line">yy.com     |2019-04-16|    7| 2| 3| 3|</span><br><span class="line">yy.com     |2019-04-15|    9| 2| 3| 4|</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="Windowing-functions"><a href="#Windowing-functions" class="headerlink" title="Windowing functions"></a>Windowing functions</h2><p><code>LAG(col,n,default)</code>: 窗口内往上取N行的值，如果有default就取default，没有就用null</p>
<p><code>LEAD(col,n,default)</code>: 窗口内往下取N行的值，如果有default就取default，没有就用null</p>
<ol>
<li><p>准备数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie1,2015-04-10 10:00:02,url2</span><br><span class="line">cookie1,2015-04-10 10:00:00,url1</span><br><span class="line">cookie1,2015-04-10 10:03:04,1url3</span><br><span class="line">cookie1,2015-04-10 10:50:05,url6</span><br><span class="line">cookie1,2015-04-10 11:00:00,url7</span><br><span class="line">cookie1,2015-04-10 10:10:00,url4</span><br><span class="line">cookie1,2015-04-10 10:50:01,url5</span><br><span class="line">cookie2,2015-04-10 10:00:02,url22</span><br><span class="line">cookie2,2015-04-10 10:00:00,url11</span><br><span class="line">cookie2,2015-04-10 10:03:04,1url33</span><br><span class="line">cookie2,2015-04-10 10:50:05,url66</span><br><span class="line">cookie2,2015-04-10 11:00:00,url77</span><br><span class="line">cookie2,2015-04-10 10:10:00,url44</span><br><span class="line">cookie2,2015-04-10 10:50:01,url55</span><br></pre></td></tr></table></figure>
</li>
<li><p>建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> window03(</span><br><span class="line">	cookid <span class="keyword">String</span>,</span><br><span class="line">	<span class="built_in">time</span> <span class="keyword">String</span>,</span><br><span class="line">	<span class="keyword">url</span> <span class="keyword">String</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>导数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/window/window03.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> window03;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	cookid,</span><br><span class="line">	<span class="built_in">time</span>,</span><br><span class="line">	<span class="keyword">url</span>,</span><br><span class="line">	lag(<span class="built_in">time</span>,<span class="number">1</span>,<span class="string">'1970-00-00 00:00:00'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> cookid <span class="keyword">order</span> <span class="keyword">by</span> <span class="built_in">time</span>),</span><br><span class="line">	<span class="keyword">lead</span>(<span class="built_in">time</span>,<span class="number">2</span>,<span class="string">"null"</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> cookid <span class="keyword">order</span> <span class="keyword">by</span> <span class="built_in">time</span>)</span><br><span class="line"><span class="keyword">from</span> window03;</span><br></pre></td></tr></table></figure>

<p>结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookid |time               |url   |lag                |lead               |</span><br><span class="line"><span class="comment">-------|-------------------|------|-------------------|-------------------|</span></span><br><span class="line">cookie1|2015-04-10 10:00:00|url1  |1970-00-00 00:00:00|2015-04-10 10:03:04|</span><br><span class="line">cookie1|2015-04-10 10:00:02|url2  |2015-04-10 10:00:00|2015-04-10 10:10:00|</span><br><span class="line">cookie1|2015-04-10 10:03:04|1url3 |2015-04-10 10:00:02|2015-04-10 10:50:01|</span><br><span class="line">cookie1|2015-04-10 10:10:00|url4  |2015-04-10 10:03:04|2015-04-10 10:50:05|</span><br><span class="line">cookie1|2015-04-10 10:50:01|url5  |2015-04-10 10:10:00|2015-04-10 11:00:00|</span><br><span class="line">cookie1|2015-04-10 10:50:05|url6  |2015-04-10 10:50:01|null               |</span><br><span class="line">cookie1|2015-04-10 11:00:00|url7  |2015-04-10 10:50:05|null               |</span><br><span class="line">cookie2|2015-04-10 10:00:00|url11 |1970-00-00 00:00:00|2015-04-10 10:03:04|</span><br><span class="line">cookie2|2015-04-10 10:00:02|url22 |2015-04-10 10:00:00|2015-04-10 10:10:00|</span><br><span class="line">cookie2|2015-04-10 10:03:04|1url33|2015-04-10 10:00:02|2015-04-10 10:50:01|</span><br><span class="line">cookie2|2015-04-10 10:10:00|url44 |2015-04-10 10:03:04|2015-04-10 10:50:05|</span><br><span class="line">cookie2|2015-04-10 10:50:01|url55 |2015-04-10 10:10:00|2015-04-10 11:00:00|</span><br><span class="line">cookie2|2015-04-10 10:50:05|url66 |2015-04-10 10:50:01|null               |</span><br><span class="line">cookie2|2015-04-10 11:00:00|url77 |2015-04-10 10:50:05|null               |</span><br></pre></td></tr></table></figure>

</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/07/hive/7/">创建伪表&amp;自定义UDF函数&amp;MR解决数据倾斜的问题&amp;行转列案例&amp;列转行案例&amp;使用hive实现wc&amp;修改hadoop的URI带来的hive数据库路径问题&amp;多文件多目录做wc或建表带来的问题</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-07</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>创建伪表</li>
<li>自定义UDF函数</li>
<li>MR解决数据倾斜的问题(引入)</li>
<li>行转列案例</li>
<li>列转行案例</li>
<li>使用hive实现wc</li>
<li>修改hadoop的URI带来的hive数据库路径问题</li>
<li>多文件多目录做wc或建表带来的问题</li>
</ol>
<h2 id="创建伪表"><a href="#创建伪表" class="headerlink" title="创建伪表"></a>创建伪表</h2><ol>
<li><p>创建表dual</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dual(a <span class="keyword">string</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建数据并导入到表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">touch dual.txt</span><br><span class="line">echo 'X' &gt;dual.txt</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/dual.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> dual;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="自定义UDF函数"><a href="#自定义UDF函数" class="headerlink" title="自定义UDF函数"></a>自定义UDF函数</h2><p>hive官网关于用户如何自定义UDF: <a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins1" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/HivePlugins1</a>. </p>
<p>hive查看函数: show functions;</p>
<p>hive查看jar包: list jars;</p>
<ol>
<li><p>首先，需要创建一个扩展UDF的新类，其中有一个或多个名为evaluate的方法</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">// 继承UDF</span><br><span class="line">public class UDFPrintf extends UDF &#123;</span><br><span class="line">    </span><br><span class="line">	//方法重载</span><br><span class="line">    public void evaluate()&#123;</span><br><span class="line">        System.out.println("你不要老婆吧");</span><br><span class="line">    &#125;</span><br><span class="line">    public void evaluate(String name)&#123;</span><br><span class="line">        System.out.println(name + ",你要老婆不要?");</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>对方法添加描述，先看系统的</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc function extended upper;</span><br><span class="line">upper(str) - Returns str <span class="keyword">with</span> <span class="keyword">all</span> <span class="keyword">characters</span> <span class="keyword">changed</span> <span class="keyword">to</span> uppercase</span><br><span class="line">Synonyms: <span class="keyword">ucase</span></span><br><span class="line">Example:</span><br><span class="line">  &gt; <span class="keyword">SELECT</span> <span class="keyword">upper</span>(<span class="string">'Facebook'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;</span><br><span class="line">  'FACEBOOK'</span><br><span class="line">=================================================================</span><br><span class="line">@Description(</span><br><span class="line">    name = "upper,ucase",</span><br><span class="line">    value = "_FUNC_(str) - Returns str <span class="keyword">with</span> <span class="keyword">all</span> <span class="keyword">characters</span> <span class="keyword">changed</span> <span class="keyword">to</span> uppercase<span class="string">",</span></span><br><span class="line"><span class="string">    extended = "</span>Example:\n  &gt; <span class="keyword">SELECT</span> _FUNC_(<span class="string">'Facebook'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;\n  		  </span><br><span class="line">    'FACEBOOK'"</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>我们仿照上面的给自定义的方法写个描述(Description)</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">@Description(</span><br><span class="line">    name = "my_printf,ucase",</span><br><span class="line">    value = "_FUNC_(str) - 返回一个名字加上字符串",</span><br><span class="line">    extended = "Example:\n  &gt; SELECT _FUNC_('老王') FROM src LIMIT 1;\n  </span><br><span class="line">    '老王,你要老婆不要?'"</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li><p>打包上传到Linux，启动hive，jar包添加到class path，创建<strong>临时UDF函数</strong>，只能在当前session中有效</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">add jar /home/hadoop/lib/hive-client-1.0.0.jar;</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">temporary</span> <span class="keyword">function</span> my_printf <span class="keyword">as</span> <span class="string">'org.hadoop.hive.UDF.UDFPrintf'</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>伪表使用自定义函数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> my_printf() <span class="keyword">from</span> dual;</span><br><span class="line">hello,小七</span><br><span class="line"><span class="keyword">select</span> my_printf(<span class="string">"老王"</span>) <span class="keyword">from</span> dual;</span><br><span class="line">hello,老王</span><br></pre></td></tr></table></figure>
</li>
<li><p>上面的方法只能创建临时函数，我们接下来将会创建永久函数，需要把jar把上传到hdfs</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hdfs dfs -mkdir /jar</span><br><span class="line">hdfs dfs -put /home/hadoop/lib/hive-client-1.0.0.jar /jar</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">FUNCTION</span> my_printf <span class="keyword">AS</span> <span class="string">'org.hadoop.hive.UDF.UDFPrintf'</span> <span class="keyword">USING</span> JAR <span class="string">'hdfs:///jar/hive-client-1.0.0.jar'</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>然后可以在多个窗口执行创建的永久函数</p>
</li>
</ol>
<h2 id="MR解决数据倾斜的思想"><a href="#MR解决数据倾斜的思想" class="headerlink" title="MR解决数据倾斜的思想"></a>MR解决数据倾斜的思想</h2><p>核心思想==&gt;先加盐(随机数)，再去盐(随机数)</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">public class skew &#123;</span><br><span class="line">    private List&lt;String&gt; newList = new ArrayList&lt;&gt;();</span><br><span class="line">    private Random r = new Random();</span><br><span class="line">    public void incRandom(List list)&#123;</span><br><span class="line">        newList.clear();</span><br><span class="line">        list.forEach( word -&gt;&#123;</span><br><span class="line">            int i = r.nextInt(10);</span><br><span class="line">            newList.add(i+"_"+word);</span><br><span class="line">        &#125;);</span><br><span class="line">        newList.forEach(System.out::println);</span><br><span class="line">    &#125;</span><br><span class="line">    public void decRandom()&#123;</span><br><span class="line">        newList.forEach(word-&gt;&#123;</span><br><span class="line">            System.out.println(word.substring(word.lastIndexOf("_")+1));</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        skew skew = new skew();</span><br><span class="line">        List&lt;String&gt; arr = new ArrayList&lt;&gt;();</span><br><span class="line">        arr.add("老王");</span><br><span class="line">        arr.add("狗子");</span><br><span class="line">        arr.add("张三");</span><br><span class="line">        skew.incRandom(arr);</span><br><span class="line">        System.out.println("<span class="comment">-------------------");</span></span><br><span class="line">        skew.decRandom();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">7_老王</span><br><span class="line">2_狗子</span><br><span class="line">4_张三</span><br><span class="line"><span class="comment">-------------------</span></span><br><span class="line">老王</span><br><span class="line">狗子</span><br><span class="line">张三</span><br></pre></td></tr></table></figure>

<h2 id="行转列案例"><a href="#行转列案例" class="headerlink" title="行转列案例"></a>行转列案例</h2><p>实现部门号的所有学生格式为</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">(dept01,A	laowang|wangwu)</span><br></pre></td></tr></table></figure>

<ol>
<li><p>数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">laowang	dept01	A</span><br><span class="line">zhangsan	dept02	A</span><br><span class="line">lisi	dept01	B</span><br><span class="line">wangwu	dept01	A</span><br><span class="line">zhaoliu	dept02	A</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建表结构</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> row2col(</span><br><span class="line">	<span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">	dept <span class="keyword">string</span>,</span><br><span class="line">	grade <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>导入数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/row2col.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> row2col;</span><br></pre></td></tr></table></figure>
</li>
<li><p>实现(dept01,A    laowang|wangwu)</p>
<ol>
<li><p>第一步组合dept和grade成dept_grade</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">concat_ws</span>(<span class="string">','</span>,dept,grade) dept_grade,<span class="keyword">name</span> <span class="keyword">from</span> row2col;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步按dept_grade分组求collect_set()将返回的数组使用concat_ws()函数转成字符串</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	t.dept_grade,<span class="keyword">concat_ws</span>(<span class="string">'|'</span>,collect_set(t.name)) <span class="keyword">names</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	(<span class="keyword">select</span> <span class="keyword">concat_ws</span>(<span class="string">','</span>,dept,grade) dept_grade,<span class="keyword">name</span> <span class="keyword">from</span> row2col) t</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">	t.dept_grade;</span><br></pre></td></tr></table></figure>

<p>collect_set()返回一个数组，concat_ws()返回一个指定字符切分数组的字符串</p>
</li>
<li><p>结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">dept_grade|names           |</span><br><span class="line"><span class="comment">----------|----------------|</span></span><br><span class="line">dept01,A  |laowang|wangwu  |</span><br><span class="line">dept01,B  |lisi            |</span><br><span class="line">dept02,A  |zhangsan|zhaoliu|</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h2 id="列转行案例"><a href="#列转行案例" class="headerlink" title="列转行案例"></a>列转行案例</h2><p>实现所有课程的格式为</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">1       zhangsan        化学</span><br><span class="line">1       zhangsan        物理</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<ol>
<li><p>数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">1,zhangsan,化学:物理:数学:语文</span><br><span class="line">2,lisi,化学:数学:生物:生理:卫生</span><br><span class="line">3,wangwu,化学:语文:英语:体育:生物</span><br></pre></td></tr></table></figure>
</li>
<li><p>建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> col2row(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">subjects <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">":"</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>导数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/col2row.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> col2row;</span><br></pre></td></tr></table></figure>
</li>
<li><p>实现</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	<span class="keyword">id</span>,<span class="keyword">name</span>,subject </span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	col2row</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(subjects) t <span class="keyword">as</span> subject;</span><br></pre></td></tr></table></figure>
</li>
<li><p>结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">id|name    |subject|</span><br><span class="line"><span class="comment">--|--------|-------|</span></span><br><span class="line"> 1|zhangsan|化学     |</span><br><span class="line"> 1|zhangsan|物理     |</span><br><span class="line"> 1|zhangsan|数学     |</span><br><span class="line"> 1|zhangsan|语文     |</span><br><span class="line"> 2|lisi    |化学     |</span><br><span class="line"> 2|lisi    |数学     |</span><br><span class="line"> 2|lisi    |生物     |</span><br><span class="line"> 2|lisi    |生理     |</span><br><span class="line"> 2|lisi    |卫生     |</span><br><span class="line"> 3|wangwu  |化学     |</span><br><span class="line"> 3|wangwu  |语文     |</span><br><span class="line"> 3|wangwu  |英语     |</span><br><span class="line"> 3|wangwu  |体育     |</span><br><span class="line"> 3|wangwu  |生物     |</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="使用hive实现wc"><a href="#使用hive实现wc" class="headerlink" title="使用hive实现wc"></a>使用hive实现wc</h2><ol>
<li><p>数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">化学:物理:数学:语文:英语</span><br><span class="line">化学:数学:生物:生理:卫生</span><br><span class="line">化学:语文:英语:体育:生物</span><br><span class="line">数学:语文:英语:体育:生物</span><br></pre></td></tr></table></figure>
</li>
<li><p>切分返回数组</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">split</span>(line,<span class="string">":"</span>) <span class="keyword">as</span> subjects <span class="keyword">from</span> wc</span><br></pre></td></tr></table></figure>
</li>
<li><p>炸开数组返回单词</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	words</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	(<span class="keyword">select</span> <span class="keyword">split</span>(line,<span class="string">":"</span>) subjects <span class="keyword">from</span> wc) <span class="keyword">as</span> t</span><br><span class="line"> <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(t.subjects) t1 <span class="keyword">as</span> words;</span><br></pre></td></tr></table></figure>
</li>
<li><p>每个单词分组求count()</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	t2.words,<span class="keyword">count</span>(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	(<span class="keyword">select</span></span><br><span class="line">	words</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	(<span class="keyword">select</span> <span class="keyword">split</span>(line,<span class="string">":"</span>) subjects <span class="keyword">from</span> wc) <span class="keyword">as</span> t</span><br><span class="line"> <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(t.subjects) t1 <span class="keyword">as</span> words) t2</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> t2.words;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="修改hadoop的URI带来的hive数据库路径问题"><a href="#修改hadoop的URI带来的hive数据库路径问题" class="headerlink" title="修改hadoop的URI带来的hive数据库路径问题"></a>修改hadoop的URI带来的hive数据库路径问题</h2><p>hive的数据保存在hadoop中，而hive的源数据保存在mysql中</p>
<p>这里就有一个问题，如果修改了hadoop的fs.defaultFS这个参数</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://aliyun:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>那么hive的元数据没有修改就会找mysql中保存的路径</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">*************************** 1. row ***************************</span><br><span class="line">          DB_ID: 1</span><br><span class="line">           DESC: Default Hive database</span><br><span class="line">DB_LOCATION_URI: hdfs://aliyun:9000/user/hive/warehouse</span><br><span class="line">           NAME: default</span><br><span class="line">     OWNER_NAME: public</span><br><span class="line">     OWNER_TYPE: ROLE</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>这时候需要使用到一个命令metatool来将mysql中的元数据修改为新的仓库路径</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">-updateLocation &lt;new-loc&gt; &lt;old-loc&gt;</span><br></pre></td></tr></table></figure>

<h2 id="多文件多目录做wc或建表带来的问题"><a href="#多文件多目录做wc或建表带来的问题" class="headerlink" title="多文件多目录做wc或建表带来的问题"></a>多文件多目录做wc或建表带来的问题</h2><p>我们查看一下hdfs中的目录结构</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ hdfs dfs -ls /mdir<span class="comment">/*</span></span><br><span class="line"><span class="comment">-rwxr-xr-x   1 hadoop supergroup         13 2020-02-05 00:59 /mdir/1.txt</span></span><br><span class="line"><span class="comment">-rwxr-xr-x   1 hadoop supergroup         14 2020-02-05 00:59 /mdir/2.txt</span></span><br><span class="line"><span class="comment">-rwxr-xr-x   1 hadoop supergroup         11 2020-02-05 00:59 /mdir/mdir2/3.txt</span></span><br></pre></td></tr></table></figure>

<p>存在嵌套目录，在做wc或者建表时，将会报错</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> dir (<span class="keyword">name</span> <span class="keyword">string</span>) location <span class="string">'/mdir'</span>;</span><br><span class="line">Failed <span class="keyword">with</span> <span class="keyword">exception</span> java.io.IOException:java.io.IOException: <span class="keyword">Not</span> a <span class="keyword">file</span>: hdfs://aliyun:<span class="number">9000</span>/mdir<span class="comment">/*</span></span><br></pre></td></tr></table></figure>

<p>通过设置参数<code>mapreduce.input.fileinputformat.input.dir.recursive</code>为<code>true</code>来解决这个问题，该参数默认为<code>false</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dir;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>可以直接在mapped-site.xml文件中直接配配置参数</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.input.fileinputformat.input.dir.recursive&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br></pre></td></tr></table></figure></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/06/hive/6/">Order By&amp;Sort By&amp;Distribute By&amp;Cluster By</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-06</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>全局排序（Order By）</li>
<li>Reduce内部排序（Sort By）</li>
<li>分区排序（Distribute By）</li>
<li>Cluster By</li>
</ol>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><ol>
<li><p>准备测试数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">7369	SMITH	CLERK	7902	1980-12-17	800.00		20</span><br><span class="line">7499	ALLEN	SALESMAN	7698	1981-2-20	1600.00	300.00	30</span><br><span class="line">7521	WARD	SALESMAN	7698	1981-2-22	1250.00	500.00	30</span><br><span class="line">7566	JONES	MANAGER	7839	1981-4-2	2975.00		20</span><br><span class="line">7654	MARTIN	SALESMAN	7698	1981-9-28	1250.00	1400.00	30</span><br><span class="line">7698	BLAKE	MANAGER	7839	1981-5-1	2850.00		30</span><br><span class="line">7782	CLARK	MANAGER	7839	1981-6-9	2450.00		10</span><br><span class="line">7788	SCOTT	ANALYST	7566	1987-4-19	3000.00		20</span><br><span class="line">7839	KING	PRESIDENT		1981-11-17	5000.00		10</span><br><span class="line">7844	TURNER	SALESMAN	7698	1981-9-8	1500.00	0.00	30</span><br><span class="line">7876	ADAMS	CLERK	7788	1987-5-23	1100.00		20</span><br><span class="line">7900	JAMES	CLERK	7698	1981-12-3	950.00		30</span><br><span class="line">7902	FORD	ANALYST	7566	1981-12-3	3000.00		20</span><br><span class="line">7934	MILLER	CLERK	7782	1982-1-23	1300.00		10</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建emp</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> emp(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">job <span class="keyword">string</span>,</span><br><span class="line">mgr <span class="built_in">int</span>,</span><br><span class="line">hiredate <span class="keyword">string</span>, </span><br><span class="line">sal <span class="keyword">double</span>, </span><br><span class="line">comm <span class="keyword">double</span>,</span><br><span class="line">deptno <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/emp.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> emp;</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证数据</p>
<p><code>select * from emp;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br></pre></td></tr></table></figure>


</li>
</ol>
<h2 id="全局排序（Order-By）-慎用"><a href="#全局排序（Order-By）-慎用" class="headerlink" title="全局排序（Order By）[慎用]"></a>全局排序（Order By）[慎用]</h2><p>语句格式:     <code>order by col1,col2 asc/desc</code></p>
<p>使用order by语句排序的是全局排序，只能有一个reduce作用来完成，与此对应的是sort by由多个reduce来完成</p>
<p>案例实操</p>
<ol>
<li><p>查询员工信息按工资升序排列</p>
<p><code>select * from emp order by sal;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询员工信息按工资降序排列(增加一个替换null值的功能)</p>
<p><code>select *,nvl(comm,&#39;-1&#39;) from emp order by sal desc;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|_c1   |</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|------|</span></span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|-1    |</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|-1    |</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|-1    |</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|-1    |</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|-1    |</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|-1    |</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|300.0 |</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|0.0   |</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|-1    |</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|1400.0|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|500.0 |</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|-1    |</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|-1    |</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|-1    |</span><br></pre></td></tr></table></figure>
</li>
<li><p>按照员工薪水的2倍排序</p>
<p><code>select *,sal+nvl(comm,0) as salandcomm from emp order by salandcomm;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|salandcomm|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|----------|</span></span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|       800|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|       950|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|      1100|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|      1300|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|      1500|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|      1750|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|      1900|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|      2450|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|      2650|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|      2850|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|      2975|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|      3000|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|      3000|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|      5000|</span><br></pre></td></tr></table></figure>
</li>
<li><p>按照部门和工资升序排序</p>
<p><code>select * from emp order by deptno,sal+nvl(comm,0);</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="Reduce内部排序（Sort-By）"><a href="#Reduce内部排序（Sort-By）" class="headerlink" title="Reduce内部排序（Sort By）"></a>Reduce内部排序（Sort By）</h2><p>对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用<strong>sort by</strong>。</p>
<p>Sort by为每个reducer产生一个排序文件。每个Reducer内部进行排序，对全局结果集来说不是排序。</p>
<ol>
<li><p>设置reduce个数，不然一个一个reduce没有效果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据部门编号降序查看员工信息</p>
<p><code>select * from emp order by deptno desc;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br></pre></td></tr></table></figure>
</li>
<li><p>将查询结果导入到文件中（按照部门编号降序排序）[指定了格式]</p>
<p><code>insert overwrite local directory &#39;/home/hadoop/emp&#39; row format delimited fields terminated by &quot;\t&quot; select * from emp order by deptno desc;</code></p>
<p>打开/home/hadoop/emp下的文件</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun emp.txt]$ cat 000000_0 </span><br><span class="line">7521	WARD	SALESMAN	7698	1981-2-22	1250.0	500.0	30</span><br><span class="line">7499	ALLEN	SALESMAN	7698	1981-2-20	1600.0	300.0	30</span><br><span class="line">7900	JAMES	CLERK	7698	1981-12-3	950.0	\N	30</span><br><span class="line">7844	TURNER	SALESMAN	7698	1981-9-8	1500.0	0.0	30</span><br><span class="line">7698	BLAKE	MANAGER	7839	1981-5-1	2850.0	\N	30</span><br><span class="line">7654	MARTIN	SALESMAN	7698	1981-9-28	1250.0	1400.0	30</span><br><span class="line">7876	ADAMS	CLERK	7788	1987-5-23	1100.0	\N	20</span><br><span class="line">7902	FORD	ANALYST	7566	1981-12-3	3000.0	\N	20</span><br><span class="line">7788	SCOTT	ANALYST	7566	1987-4-19	3000.0	\N	20</span><br><span class="line">7369	SMITH	CLERK	7902	1980-12-17	800.0	\N	20</span><br><span class="line">7566	JONES	MANAGER	7839	1981-4-2	2975.0	\N	20</span><br><span class="line">7934	MILLER	CLERK	7782	1982-1-23	1300.0	\N	10</span><br><span class="line">7839	KING	PRESIDENT	\N	1981-11-17	5000.0	\N	10</span><br><span class="line">7782	CLARK	MANAGER	7839	1981-6-9	2450.0	\N	10</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="发送分区排序（Distribute-By）"><a href="#发送分区排序（Distribute-By）" class="headerlink" title="发送分区排序（Distribute By）"></a>发送分区排序（Distribute By）</h2><p>在有些情况下，我们需要控制某个特定行应该到哪个reducer，通常是为了进行后续的聚集操作。<strong>distribute by</strong> 子句可以做这件事。<strong>distribute by</strong>类似MR中<strong>partition</strong>（自定义分区），进行分区，结合<strong>sort by</strong>使用。 </p>
<p>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p>
<ol>
<li><p>先按照部门编号分区，再按照员工编号降序排序。</p>
<p><code>select * from emp distribute by deptno sort by empno desc;</code> </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br></pre></td></tr></table></figure>

<p><code>注意:</code> </p>
<ol>
<li>distribute by的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。</li>
<li>Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。</li>
</ol>
</li>
</ol>
<h2 id="Cluster-By"><a href="#Cluster-By" class="headerlink" title="Cluster By"></a>Cluster By</h2><p>当<code>distribute by</code>和<code>sorts by</code>字段相同时，可以使用<code>cluster by</code>方式。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</p>
<p>当分区字段和排序字段都是部门编号的时候我们可以这么做</p>
<p><code>select * from emp cluster by deptno;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br></pre></td></tr></table></figure>

<p><code>注意:</code> 注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/05/hive/5/">HS2&amp;Hive的复杂数据结构&amp;行列互转&amp;常用函数&amp;静动态分区表&amp;桶表</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-05</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>HS2</li>
<li>复杂数据结构</li>
<li>行列互转</li>
<li>常用函数</li>
<li>静动态分区表</li>
<li>桶表</li>
</ol>
<h2 id="SH2"><a href="#SH2" class="headerlink" title="SH2"></a>SH2</h2><p>HS2是HiveServer2的简称</p>
<ul>
<li><p>HS2: Server端，默认端口10000</p>
<p>修改端口的方式是通过设置hive.server2.thrift.port的值</p>
</li>
<li><p>beeline: Client端</p>
<p>连接方式: <code>./beeline -u jdbc:hive2://hadoop001:10000/matestore -n hadoop</code></p>
</li>
</ul>
<h2 id="复杂数据结构"><a href="#复杂数据结构" class="headerlink" title="复杂数据结构"></a>复杂数据结构</h2><p>复杂数据类型有三种，分别是: array、map、structs</p>
<h3 id="array"><a href="#array" class="headerlink" title="array"></a>array</h3><p><code>array</code>的建表格式:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hive_array(<span class="keyword">name</span> <span class="keyword">string</span>, work_locations <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line">COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span>;</span><br></pre></td></tr></table></figure>

<p>注意: <code>COLLECTION ITEMS TERMINATED BY &#39;,&#39;</code>是指定array数组中的元素分隔符</p>
<h4 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h4><ol>
<li><p>第一步准备数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pk      beijing,shanghai,tianjin,hangzhou</span><br><span class="line">jepson  changchu,chengdu,wuhan,beijing</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> array_(</span><br><span class="line">	<span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">	address <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span> </span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第三步装载数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/hive_array.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> array_;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第四步查询数据是否加载成功</p>
<p><code>select * from array_;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name  |address                                    |</span><br><span class="line"><span class="comment">------|-------------------------------------------|</span></span><br><span class="line">pk    |["beijing","shanghai","tianjin","hangzhou"]|</span><br><span class="line">jepson|["changchu","chengdu","wuhan","beijing"]   |</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="对array数组的”取”"><a href="#对array数组的”取”" class="headerlink" title="对array数组的”取”"></a>对array数组的”取”</h4><ol>
<li><p>根据数组的下标取出指定元素，下标从0开始</p>
<p><code>select name,address[1] as address from array_;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name  |address |</span><br><span class="line"><span class="comment">------|--------|</span></span><br><span class="line">pk    |shanghai|</span><br><span class="line">jepson|chengdu |</span><br></pre></td></tr></table></figure>
</li>
<li><p>获取数组中元素的个数</p>
<p><code>select name,size(address) as size from array_;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name  |size|</span><br><span class="line"><span class="comment">------|----|</span></span><br><span class="line">pk    |   4|</span><br><span class="line">jepson|   4|</span><br></pre></td></tr></table></figure>
</li>
<li><p>获取数组中包含某元素的记录</p>
<p><code>select name,address from array_ where  array_contains(address,&quot;shanghai&quot;);</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name|address                                    |</span><br><span class="line"><span class="comment">----|-------------------------------------------|</span></span><br><span class="line">pk  |["beijing","shanghai","tianjin","hangzhou"]|</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><p><code>map</code>的建表格式:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hive_map(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>, members <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;, age <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line">COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'#'</span></span><br><span class="line"><span class="keyword">MAP</span> <span class="keyword">KEYS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">':'</span>;</span><br></pre></td></tr></table></figure>

<p>注意: <code>COLLECTION ITEMS TERMINATED BY &#39;#&#39;</code>是指定map键值对中的元素分隔符，<code>MAP KEYS TERMINATED BY &#39;:&#39;</code>是指定key和value的分隔符</p>
<h4 id="准备数据-1"><a href="#准备数据-1" class="headerlink" title="准备数据"></a>准备数据</h4><ol>
<li><p>第一步准备数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1,zhangsan,father:xiaoming#mother:xiaohuang#brother:xiaoxu,28</span><br><span class="line">2,lisi,father:mayun#mother:huangyi#brother:guanyu,22</span><br><span class="line">3,wangwu,father:wangjianlin#mother:ruhua#sister:jingtian,29</span><br><span class="line">4,mayun,father:mayongzhen#mother:angelababy,26</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> map_(</span><br><span class="line">	<span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">	<span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">	family <span class="keyword">map</span>&lt;<span class="keyword">String</span>,<span class="keyword">String</span>&gt;,</span><br><span class="line">	age <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"#"</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">":"</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第三步加载数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/hive_map.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> map_;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第四步查询数据是否加载成功</p>
<p><code>select * from map_;</code></p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">id|name    |family                                                       |age|</span><br><span class="line"><span class="comment">--|--------|-------------------------------------------------------------|---|</span></span><br><span class="line"> 1|zhangsan|&#123;"father":"xiaoming","mother":"xiaohuang","brother":"xiaoxu"&#125;| 28|</span><br><span class="line"> 2|lisi    |&#123;"father":"mayun","mother":"huangyi","brother":"guanyu"&#125;     | 22|</span><br><span class="line"> 3|wangwu  |&#123;"father":"wangjianlin","mother":"ruhua","sister":"jingtian"&#125;| 29|</span><br><span class="line"> 4|mayun   |&#123;"father":"mayongzhen","mother":"angelababy"&#125;                | 26|</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="对map键值对的”取”"><a href="#对map键值对的”取”" class="headerlink" title="对map键值对的”取”"></a>对map键值对的”取”</h4><ol>
<li><p>根据key取value</p>
<p><code>select name,family[&#39;father&#39;] as father,age from map_;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name    |father     |age|</span><br><span class="line"><span class="comment">--------|-----------|---|</span></span><br><span class="line">zhangsan|xiaoming   | 28|</span><br><span class="line">lisi    |mayun      | 22|</span><br><span class="line">wangwu  |wangjianlin| 29|</span><br><span class="line">mayun   |mayongzhen | 26|</span><br></pre></td></tr></table></figure>
</li>
<li><p>取出所有key集合</p>
<p><code>select name,map_keys(family) as map_keys,age from map_;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name    |map_keys                     |age|</span><br><span class="line"><span class="comment">--------|-----------------------------|---|</span></span><br><span class="line">zhangsan|["father","mother","brother"]| 28|</span><br><span class="line">lisi    |["father","mother","brother"]| 22|</span><br><span class="line">wangwu  |["father","mother","sister"] | 29|</span><br><span class="line">mayun   |["father","mother"]          | 26|</span><br></pre></td></tr></table></figure>
</li>
<li><p>取出所有的value集合</p>
<p><code>select name,map_values(family) as map_values,age from map_;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name    |map_values                        |age|</span><br><span class="line"><span class="comment">--------|----------------------------------|---|</span></span><br><span class="line">zhangsan|["xiaoming","xiaohuang","xiaoxu"] | 28|</span><br><span class="line">lisi    |["mayun","huangyi","guanyu"]      | 22|</span><br><span class="line">wangwu  |["wangjianlin","ruhua","jingtian"]| 29|</span><br><span class="line">mayun   |["mayongzhen","angelababy"]       | 26|</span><br></pre></td></tr></table></figure>
</li>
<li><p>求key的数量(对key集合求size)</p>
<p><code>select name,size(map_keys(family)) as key_size,age from map_;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name    |key_size|age|</span><br><span class="line"><span class="comment">--------|--------|---|</span></span><br><span class="line">zhangsan|       3| 28|</span><br><span class="line">lisi    |       3| 22|</span><br><span class="line">wangwu  |       3| 29|</span><br><span class="line">mayun   |       2| 26|</span><br></pre></td></tr></table></figure>
</li>
<li><p>求key是否包含某个元素(对key的集合求contains)</p>
<p><code>select name,family,age from map_ where array_contains((map_keys(family)),&quot;brother&quot;);</code></p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">name    |family                                                       |age|</span><br><span class="line"><span class="comment">--------|-------------------------------------------------------------|---|</span></span><br><span class="line">zhangsan|&#123;"father":"xiaoming","mother":"xiaohuang","brother":"xiaoxu"&#125;| 28|</span><br><span class="line">lisi    |&#123;"father":"mayun","mother":"huangyi","brother":"guanyu"&#125;     | 22|</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="structs"><a href="#structs" class="headerlink" title="structs"></a>structs</h3><p><code>structs</code>的建表格式:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hive_struct(</span><br><span class="line">ip <span class="keyword">string</span>, info <span class="keyword">struct</span>&lt;<span class="keyword">name</span>:<span class="keyword">string</span>,age:<span class="built_in">int</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'#'</span></span><br><span class="line">COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">':'</span>;</span><br></pre></td></tr></table></figure>

<p>注意: <code>COLLECTION ITEMS TERMINATED BY &#39;:&#39;</code>是指定每个元素之间的分隔符</p>
<h4 id="准备数据-2"><a href="#准备数据-2" class="headerlink" title="准备数据"></a>准备数据</h4><ol>
<li><p>第一步准备数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">192.168.1.1#zhangsan:40</span><br><span class="line">192.168.1.2#lisi:50</span><br><span class="line">192.168.1.3#wangwu:60</span><br><span class="line">192.168.1.4#zhaoliu:70</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> structs_(</span><br><span class="line">	address <span class="keyword">string</span>,</span><br><span class="line">	<span class="string">`user`</span> <span class="keyword">struct</span>&lt;<span class="keyword">name</span>:<span class="keyword">string</span>,age:<span class="built_in">int</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"#"</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">":"</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第三步加载数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/hive_struct.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> structs_;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第四步查询数据是否装载成功</p>
<p><code>select * from structs_;</code></p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">address    |user                          |</span><br><span class="line"><span class="comment">-----------|------------------------------|</span></span><br><span class="line">192.168.1.1|[&#123;"name":"zhangsan","age":40&#125;]|</span><br><span class="line">192.168.1.2|[&#123;"name":"lisi","age":50&#125;]    |</span><br><span class="line">192.168.1.3|[&#123;"name":"wangwu","age":60&#125;]  |</span><br><span class="line">192.168.1.4|[&#123;"name":"zhaoliu","age":70&#125;] |</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="对structs结构体的”取”"><a href="#对structs结构体的”取”" class="headerlink" title="对structs结构体的”取”"></a>对structs结构体的”取”</h4><ol>
<li><p>取出结构体中的元素</p>
<p><code>select address,user.name as name from structs_;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">address    |name    |</span><br><span class="line"><span class="comment">-----------|--------|</span></span><br><span class="line">192.168.1.1|zhangsan|</span><br><span class="line">192.168.1.2|lisi    |</span><br><span class="line">192.168.1.3|wangwu  |</span><br><span class="line">192.168.1.4|zhaoliu |</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="行列互转"><a href="#行列互转" class="headerlink" title="行列互转"></a>行列互转</h2><p>这是一个复杂数据类型的综合练习</p>
<h3 id="准备数据-3"><a href="#准备数据-3" class="headerlink" title="准备数据"></a>准备数据</h3><ol>
<li><p>第一步准备数据</p>
<p>数据1: session点击广告记录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">11      ad_101  2014-05-01 06:01:12.334+01</span><br><span class="line">22      ad_102  2014-05-01 07:28:12.342+01</span><br><span class="line">33      ad_103  2014-05-01 07:50:12.33+01</span><br><span class="line">11      ad_104  2014-05-01 09:27:12.33+01</span><br><span class="line">22      ad_103  2014-05-01 09:03:12.324+01</span><br><span class="line">33      ad_102  2014-05-02 19:10:12.343+01</span><br><span class="line">11      ad_101  2014-05-02 09:07:12.344+01</span><br><span class="line">35      ad_105  2014-05-03 11:07:12.339+01</span><br><span class="line">22      ad_104  2014-05-03 12:59:12.743+01</span><br><span class="line">77      ad_103  2014-05-03 18:04:12.355+01</span><br><span class="line">99      ad_102  2014-05-04 00:36:39.713+01</span><br><span class="line">33      ad_101  2014-05-04 19:10:12.343+01</span><br><span class="line">11      ad_101  2014-05-05 09:07:12.344+01</span><br><span class="line">35      ad_102  2014-05-05 11:07:12.339+01</span><br><span class="line">22      ad_103  2014-05-05 12:59:12.743+01</span><br><span class="line">77      ad_104  2014-05-05 18:04:12.355+01</span><br><span class="line">99      ad_105  2014-05-05 20:36:39.713+01</span><br></pre></td></tr></table></figure>

<p>数据2: 广告的详情</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ad_101  http://www.google.com   catalog8|catalog1</span><br><span class="line">ad_102  http://www.sohu.com     catalog6|catalog3</span><br><span class="line">ad_103  http://www.baidu.com    catalog7</span><br><span class="line">ad_104  http://www.qq.com       catalog5|catalog1|catalog4|catalog9</span><br><span class="line">ad_105  http://sina.com</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步创建表</p>
<p>表1: 动作表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> click_tbl(</span><br><span class="line">	cookie_id <span class="keyword">string</span>,  </span><br><span class="line">	ad_id <span class="built_in">int</span>,</span><br><span class="line">	<span class="built_in">time</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure>

<p>表2: 广告表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ad_tbl(</span><br><span class="line">	ad_id <span class="keyword">string</span>,</span><br><span class="line">	<span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">	catalogs <span class="built_in">array</span>&lt;<span class="keyword">String</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"|"</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>第三步加载数据</p>
<p>给动作表加载数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/click_log.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> click_tbl;</span><br></pre></td></tr></table></figure>

<p>给广告表加载数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/ad_list.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> ad_tbl;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第四步查询数据是否装载成功</p>
<p>动作表查询</p>
<p><code>select * from click_tbl;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id |time                      |</span><br><span class="line"><span class="comment">---------|------|--------------------------|</span></span><br><span class="line">       11|ad_101|2014-05-01 06:01:12.334+01|</span><br><span class="line">       22|ad_102|2014-05-01 07:28:12.342+01|</span><br><span class="line">       33|ad_103|2014-05-01 07:50:12.33+01 |</span><br><span class="line">       11|ad_104|2014-05-01 09:27:12.33+01 |</span><br><span class="line">       22|ad_103|2014-05-01 09:03:12.324+01|</span><br><span class="line">       33|ad_102|2014-05-02 19:10:12.343+01|</span><br><span class="line">       11|ad_101|2014-05-02 09:07:12.344+01|</span><br><span class="line">       35|ad_105|2014-05-03 11:07:12.339+01|</span><br><span class="line">       22|ad_104|2014-05-03 12:59:12.743+01|</span><br><span class="line">       77|ad_103|2014-05-03 18:04:12.355+01|</span><br><span class="line">       99|ad_102|2014-05-04 00:36:39.713+01|</span><br><span class="line">       33|ad_101|2014-05-04 19:10:12.343+01|</span><br><span class="line">       11|ad_101|2014-05-05 09:07:12.344+01|</span><br><span class="line">       35|ad_102|2014-05-05 11:07:12.339+01|</span><br><span class="line">       22|ad_103|2014-05-05 12:59:12.743+01|</span><br><span class="line">       77|ad_104|2014-05-05 18:04:12.355+01|</span><br><span class="line">       99|ad_105|2014-05-05 20:36:39.713+01|</span><br></pre></td></tr></table></figure>

<p>广告把查询</p>
<p><code>select * from ad_tbl;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ad_id |url                  |catalogs                               |</span><br><span class="line"><span class="comment">------|---------------------|---------------------------------------|</span></span><br><span class="line">ad_101|http://www.google.com|["catalog8|catalog1"]                  |</span><br><span class="line">ad_102|http://www.sohu.com  |["catalog6|catalog3"]                  |</span><br><span class="line">ad_103|http://www.baidu.com |["catalog7"]                           |</span><br><span class="line">ad_104|http://www.qq.com    |["catalog5|catalog1|catalog4|catalog9"]|</span><br><span class="line">ad_105|http://sina.com      |NULL                                   |</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><h4 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h4><p>查询每个人访问的广告</p>
<ol>
<li><p>去重(collect_set): </p>
<p><code>select cookie_id,collect_set(ad_id) ad_id from click_tbl group by cookie_id;</code></p>
</li>
</ol>
<ul>
<li><p>结果:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id                       |</span><br><span class="line"><span class="comment">---------|----------------------------|</span></span><br><span class="line">       11|["ad_101","ad_104"]         |</span><br><span class="line">       22|["ad_102","ad_103","ad_104"]|</span><br><span class="line">       33|["ad_103","ad_102","ad_101"]|</span><br><span class="line">       35|["ad_105","ad_102"]         |</span><br><span class="line">       77|["ad_103","ad_104"]         |</span><br><span class="line">       99|["ad_102","ad_105"]         |</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol start="2">
<li><p>不去重(collect_list):</p>
<p><code>select cookie_id,collect_list(ad_id) ad_id from click_tbl group by cookie_id;</code></p>
</li>
</ol>
<ul>
<li><p>结果:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id                                |</span><br><span class="line"><span class="comment">---------|-------------------------------------|</span></span><br><span class="line">       11|["ad_101","ad_104","ad_101","ad_101"]|</span><br><span class="line">       22|["ad_102","ad_103","ad_104","ad_103"]|</span><br><span class="line">       33|["ad_103","ad_102","ad_101"]         |</span><br><span class="line">       35|["ad_105","ad_102"]                  |</span><br><span class="line">       77|["ad_103","ad_104"]                  |</span><br><span class="line">       99|["ad_102","ad_105"]                  |</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="聚合查询"><a href="#聚合查询" class="headerlink" title="聚合查询"></a>聚合查询</h4><ol>
<li><p>查询每个人访问相同广告的次数</p>
<p><code>select cookie_id,ad_id,count(1) amount from click_tbl group by  cookie_id,ad_id;</code></p>
<p>结果:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id |amount|</span><br><span class="line"><span class="comment">---------|------|------|</span></span><br><span class="line">       11|ad_101|     3|</span><br><span class="line">       11|ad_104|     1|</span><br><span class="line">       22|ad_102|     1|</span><br><span class="line">       22|ad_103|     2|</span><br><span class="line">       22|ad_104|     1|</span><br><span class="line">       33|ad_101|     1|</span><br><span class="line">       33|ad_102|     1|</span><br><span class="line">       33|ad_103|     1|</span><br><span class="line">       35|ad_102|     1|</span><br><span class="line">       35|ad_105|     1|</span><br><span class="line">       77|ad_103|     1|</span><br><span class="line">       77|ad_104|     1|</span><br><span class="line">       99|ad_102|     1|</span><br><span class="line">       99|ad_105|     1|</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询每个人访问的广告详情</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	click_tmp.cookie_id,click_tmp.ad_id,ad_tbl.url,ad_tbl.catalogs </span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	ad_tbl</span><br><span class="line"><span class="keyword">JOIN</span></span><br><span class="line">	(<span class="keyword">select</span> cookie_id,ad_id,<span class="keyword">count</span>(<span class="number">1</span>) amount <span class="keyword">from</span> click_tbl <span class="keyword">group</span> <span class="keyword">by</span>  cookie_id,ad_id) click_tmp</span><br><span class="line"><span class="keyword">on</span> click_tmp.ad_id=ad_tbl.ad_id;</span><br></pre></td></tr></table></figure>

<p>结果:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id |url                  |catalogs                                     |</span><br><span class="line"><span class="comment">---------|------|---------------------|---------------------------------------------|</span></span><br><span class="line">       11|ad_101|http://www.google.com|["catalog8","catalog1"]                      |</span><br><span class="line">       11|ad_104|http://www.qq.com    |["catalog5","catalog1","catalog4","catalog9"]|</span><br><span class="line">       22|ad_102|http://www.sohu.com  |["catalog6","catalog3"]                      |</span><br><span class="line">       22|ad_103|http://www.baidu.com |["catalog7"]                                 |</span><br><span class="line">       22|ad_104|http://www.qq.com    |["catalog5","catalog1","catalog4","catalog9"]|</span><br><span class="line">       33|ad_101|http://www.google.com|["catalog8","catalog1"]                      |</span><br><span class="line">       33|ad_102|http://www.sohu.com  |["catalog6","catalog3"]                      |</span><br><span class="line">       33|ad_103|http://www.baidu.com |["catalog7"]                                 |</span><br><span class="line">       35|ad_102|http://www.sohu.com  |["catalog6","catalog3"]                      |</span><br><span class="line">       35|ad_105|http://sina.com      |NULL                                         |</span><br><span class="line">       77|ad_103|http://www.baidu.com |["catalog7"]                                 |</span><br><span class="line">       77|ad_104|http://www.qq.com    |["catalog5","catalog1","catalog4","catalog9"]|</span><br><span class="line">       99|ad_102|http://www.sohu.com  |["catalog6","catalog3"]                      |</span><br><span class="line">       99|ad_105|http://sina.com      |NULL                                         |</span><br></pre></td></tr></table></figure>
</li>
<li><p>把catalogs中的数据元素排序</p>
<p><code>select ad_id,url,sort_array(catalogs) as catalogs from ad_tbl;</code></p>
<p>结果:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ad_id |url                  |catalogs                                     |</span><br><span class="line"><span class="comment">------|---------------------|---------------------------------------------|</span></span><br><span class="line">ad_101|http://www.google.com|["catalog1","catalog8"]                      |</span><br><span class="line">ad_102|http://www.sohu.com  |["catalog3","catalog6"]                      |</span><br><span class="line">ad_103|http://www.baidu.com |["catalog7"]                                 |</span><br><span class="line">ad_104|http://www.qq.com    |["catalog1","catalog4","catalog5","catalog9"]|</span><br><span class="line">ad_105|http://sina.com      |NULL                                         |</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h4><p>查询每个广告详情</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	ad_id,<span class="keyword">catalog</span></span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	ad_tbl</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(catalogs) t <span class="keyword">as</span> <span class="keyword">catalog</span>;</span><br></pre></td></tr></table></figure>

<p>结果:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ad_id |catalog |</span><br><span class="line"><span class="comment">------|--------|</span></span><br><span class="line">ad_101|catalog8|</span><br><span class="line">ad_101|catalog1|</span><br><span class="line">ad_102|catalog6|</span><br><span class="line">ad_102|catalog3|</span><br><span class="line">ad_103|catalog7|</span><br><span class="line">ad_104|catalog5|</span><br><span class="line">ad_104|catalog1|</span><br><span class="line">ad_104|catalog4|</span><br><span class="line">ad_104|catalog9|</span><br></pre></td></tr></table></figure>

<p><code>注意:</code> 如果ad_tbl的catalogs字段是String类型的，那么在explode炸开的时候要转换成数组，也就是用split把字段元素按’|’切分开返回一个数组</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">lateral view outer explode(split(catalogs,'\\|')) t as catalog</span><br></pre></td></tr></table></figure>

<h2 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h2><p>查用函数的用法查询: <code>desc function extended 函数名;</code></p>
<h3 id="时间函数"><a href="#时间函数" class="headerlink" title="时间函数"></a>时间函数</h3><p><code>current_date:</code> 返回当前日期</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">current_date</span>		<span class="comment">--2019-12-21</span></span><br></pre></td></tr></table></figure>

<p><code>current_timestamp:</code> 返回当前时间</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">current_timestamp</span>	<span class="comment">--2019-12-21 02:23:44</span></span><br></pre></td></tr></table></figure>

<p><code>unix_timestamp:</code> 时间转秒</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">unix_timestamp</span>(<span class="string">'2019-08-15 16:40:00'</span>,<span class="string">'yyyy-MM-dd HH:mm:ss'</span>)   <span class="comment">--1565858400</span></span><br></pre></td></tr></table></figure>

<p><code>from_unixtime:</code> 秒转时间</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">1565858389</span>,<span class="string">'yyyy-MM-dd HH:mm:ss'</span>)  <span class="comment">--2019-08-15 16:39:49</span></span><br><span class="line"><span class="keyword">select</span> from_unixtime(<span class="keyword">cast</span>(<span class="keyword">substr</span>(<span class="number">1553184000488</span>,<span class="number">1</span>,<span class="number">10</span>) <span class="keyword">as</span> <span class="built_in">int</span>),<span class="string">'yyyy-MM-dd HH:mm:ss'</span>)  <span class="comment">--2019-03-22 00:00:00</span></span><br><span class="line"><span class="keyword">select</span> from_unixtime(<span class="keyword">unix_timestamp</span>(),<span class="string">'yyyy-MM-dd HH:mm:ss'</span>)   <span class="comment">-- 2019-08-15 17:18:55</span></span><br></pre></td></tr></table></figure>

<p><code>to_date:</code> 返回日期</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">to_date</span>(<span class="string">'2009-07-30 04:17:52'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;		<span class="comment">--2009-07-30</span></span><br></pre></td></tr></table></figure>

<p><code>year:</code> 返回年份</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">year</span>(<span class="string">'2009-07-30'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;		<span class="comment">--2009</span></span><br></pre></td></tr></table></figure>

<p><code>month:</code> 返回月份</p>
<p><code>day:</code> 返回日期</p>
<p><code>hour:</code> 返回小时</p>
<p><code>minute:</code> 返回分钟</p>
<p><code>second:</code> 返回毫秒</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">second</span>(<span class="string">'2009-07-30 12:58:59'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>; 		<span class="comment">--59</span></span><br></pre></td></tr></table></figure>

<p><code>date_add:</code> 增加指定时间</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">date_add</span>(<span class="string">'2009-07-30'</span>, <span class="number">1</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;		<span class="comment">--2009-07-31</span></span><br></pre></td></tr></table></figure>

<p><code>date_sub:</code> 减少指定时间</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">date_sub</span>(<span class="string">'2009-07-30'</span>, <span class="number">1</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;		<span class="comment">--2009-07-29</span></span><br></pre></td></tr></table></figure>

<h3 id="数值函数"><a href="#数值函数" class="headerlink" title="数值函数"></a>数值函数</h3><p><code>round:</code> 返回指定范围的数值</p>
<p><code>ceil:</code> 返回天花板，取最大的整数值</p>
<p><code>floor:</code> 返回地板，去最小的整数值</p>
<p><code>abs:</code> 返回绝对值</p>
<p><code>least:</code> 数列中最小值 ==&gt; min</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">least</span>(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;		<span class="comment">--1</span></span><br></pre></td></tr></table></figure>

<h3 id="字符串函数"><a href="#字符串函数" class="headerlink" title="字符串函数"></a>字符串函数</h3><p><code>substr:</code> 返回截取字符串</p>
<p><code>concat:</code> 返回连接字符串</p>
<p><code>concat_ws:</code> 根据特定格式组合字符串</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">concat_ws</span>(<span class="string">'.'</span>, <span class="string">'www'</span>, <span class="built_in">array</span>(<span class="string">'facebook'</span>, <span class="string">'com'</span>)) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;	<span class="comment">--www.facebook.com</span></span><br></pre></td></tr></table></figure>

<p><code>length:</code> 返回字符串的长度，整数值</p>
<p><code>split:</code> 返回切分后的字符串数组</p>
<p><code>upper:</code> 字符转大写</p>
<p><code>lower:</code> 字符转小写</p>
<h3 id="Json处理函数"><a href="#Json处理函数" class="headerlink" title="Json处理函数"></a>Json处理函数</h3><ol>
<li><p>第一步准备数据</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"1"</span>,<span class="attr">"rate"</span>:<span class="string">"5"</span>,<span class="attr">"time"</span>:<span class="string">"978300760"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"2"</span>,<span class="attr">"rate"</span>:<span class="string">"4"</span>,<span class="attr">"time"</span>:<span class="string">"978702109"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"3"</span>,<span class="attr">"rate"</span>:<span class="string">"3"</span>,<span class="attr">"time"</span>:<span class="string">"978401968"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"4"</span>,<span class="attr">"rate"</span>:<span class="string">"4"</span>,<span class="attr">"time"</span>:<span class="string">"978300275"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"5"</span>,<span class="attr">"rate"</span>:<span class="string">"3"</span>,<span class="attr">"time"</span>:<span class="string">"978801091"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> rating_json(</span><br><span class="line">	<span class="keyword">json</span> <span class="keyword">string</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li><p>第三步加载数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/rating.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> rating_json;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第四步查看数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">json                                                    |</span><br><span class="line"><span class="comment">--------------------------------------------------------|</span></span><br><span class="line">&#123;"movie":"1","rate":"5","time":"978300760","userid":"1"&#125;|</span><br><span class="line">&#123;"movie":"2","rate":"4","time":"978702109","userid":"1"&#125;|</span><br><span class="line">&#123;"movie":"3","rate":"3","time":"978401968","userid":"1"&#125;|</span><br><span class="line">&#123;"movie":"4","rate":"4","time":"978300275","userid":"1"&#125;|</span><br><span class="line">&#123;"movie":"5","rate":"3","time":"978801091","userid":"1"&#125;|</span><br></pre></td></tr></table></figure>
</li>
<li><p>对数据进行处理</p>
<p><code>json_tuple:</code> 返回指定json文件的指定字段，返回的是字符串</p>
<p><code>select json_tuple(json,&quot;movie&quot;,&quot;rate&quot;,&quot;time&quot;,&quot;userid&quot;) as (movie,rate,time,userid) from rating_json;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">movie|rate|time     |userid|</span><br><span class="line"><span class="comment">-----|----|---------|------|</span></span><br><span class="line">1    |5   |978300760|1     |</span><br><span class="line">2    |4   |978702109|1     |</span><br><span class="line">3    |3   |978401968|1     |</span><br><span class="line">4    |4   |978300275|1     |</span><br><span class="line">5    |3   |978801091|1     |</span><br></pre></td></tr></table></figure>

<p><code>parse_url_tuple:</code> 返回指定url的指定字段，返回的是字符串</p>
<p><code>select parse_url_tuple(&quot;https://www.baidu.com/bigdate/spark?cookie_id=10&quot;,&#39;HOST&#39;,&#39;PATH&#39;,&#39;QUERY&#39;,&#39;QUERY:cookie_id&#39;);</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">c0           |c1            |c2          |c3|</span><br><span class="line"><span class="comment">-------------|--------------|------------|--|</span></span><br><span class="line">www.baidu.com|/bigdate/spark|cookie_id=10|10|</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="Null值处理函数"><a href="#Null值处理函数" class="headerlink" title="Null值处理函数"></a>Null值处理函数</h3><p><code>isnull:</code> 指定字段的元素如果为null则返回true</p>
<p><code>isnotnull:</code> 指定字段的元素如果不为null则返回true</p>
<p><code>elt:</code> 指定返回的元素</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">elt</span>(<span class="number">1</span>, <span class="string">'face'</span>, <span class="string">'book'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;		<span class="comment">--face</span></span><br></pre></td></tr></table></figure>

<p><code>nvl:</code> 替换null值</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> nvl(<span class="literal">null</span>,<span class="string">'bla'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;	<span class="comment">--bla</span></span><br></pre></td></tr></table></figure>

<h3 id="其他函数"><a href="#其他函数" class="headerlink" title="其他函数"></a>其他函数</h3><p><code>cast:</code> 转换数据类型</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cast(time as bigint)	<span class="comment">--时间转数值</span></span><br><span class="line">cast(string as date)	<span class="comment">--字符串转日期</span></span><br></pre></td></tr></table></figure>

<p><code>注意:</code> binary只能转string</p>
<h2 id="静动态分区表"><a href="#静动态分区表" class="headerlink" title="静动态分区表"></a>静动态分区表</h2><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><ol>
<li><p>数据源</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">1,jack,shanghai,20190129</span><br><span class="line">2,kevin,beijing,20190130</span><br><span class="line">3,lucas,hangzhou,20190129</span><br><span class="line">4,lily,hangzhou,20190130</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建源数据表（外表）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> prit_tbl(</span><br><span class="line">	<span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">	<span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">	address <span class="keyword">string</span>,</span><br><span class="line">	<span class="keyword">day</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载数据：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/partition.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> prit_tbl;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建分区表（外表）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> prit_tbl(</span><br><span class="line">	<span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">	<span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">	address <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">day</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="静态分区加载数据"><a href="#静态分区加载数据" class="headerlink" title="静态分区加载数据"></a>静态分区加载数据</h3><ul>
<li><p>静态分区缺点：每次写入都要明确指定分区日期。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> prit_tbl <span class="keyword">partition</span>(<span class="keyword">day</span>=<span class="string">"20190129"</span>) <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span>,address <span class="keyword">from</span> test_tbl <span class="keyword">where</span> <span class="keyword">day</span> = <span class="string">"20190129"</span> ;</span><br></pre></td></tr></table></figure>

<p><code>注意:</code> 并且在查询处不能包含分区字段day，否则会报错</p>
</li>
</ul>
<h3 id="动态分区加载数据"><a href="#动态分区加载数据" class="headerlink" title="动态分区加载数据"></a>动态分区加载数据</h3><ul>
<li><p>查看表分区</p>
<p><code>show partitions prit_tbl;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">partition   |</span><br><span class="line"><span class="comment">------------|</span></span><br><span class="line">day=20190129|</span><br><span class="line">day=20190130|</span><br></pre></td></tr></table></figure>
</li>
<li><p>自动识别分区，不需要明确指定</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> prit_tbl <span class="keyword">partition</span>(<span class="keyword">day</span>) <span class="keyword">select</span> * <span class="keyword">from</span> test_tbl;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询验证：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> prit_tbl;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> prit_tbl <span class="keyword">where</span> <span class="keyword">day</span> = <span class="number">20190129</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> prit_tbl <span class="keyword">where</span> <span class="keyword">day</span> = <span class="number">20190130</span>;</span><br></pre></td></tr></table></figure>

<p>HDFS web界面验证</p>
</li>
</ul>
<h3 id="分区注意"><a href="#分区注意" class="headerlink" title="分区注意"></a>分区注意</h3><ol>
<li><p>尽量不要是用动态分区，因为动态分区的时候，将会为每一个分区分配reducer数量，当分区数量多的时候，reducer数量将会增加，对服务器是一种灾难。</p>
</li>
<li><p>动态分区和静态分区的区别: 静态分区不管有没有数据都将会创建该分区，动态分区是有结果集将创建，否则不创建。</p>
</li>
<li><p>hive动态分区的严格模式和hive提供的<code>hive.mapred.mode的</code>严格模式,为了阻止用户不小心提交恶意<code>hql</code>。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive.mapred.mode=nostrict : strict</span><br></pre></td></tr></table></figure>

<p>如果该模式值为strict，将会阻止以下三种查询：</p>
<ol>
<li>对分区表查询，where中过滤字段不是分区字段。</li>
<li>笛卡尔积join查询，join查询语句，不带on条件 或者 where条件。</li>
<li>对order by查询，有order by的查询不带limit语句。</li>
</ol>
</li>
</ol>
<h2 id="桶表"><a href="#桶表" class="headerlink" title="桶表"></a>桶表</h2><h3 id="分桶的概念"><a href="#分桶的概念" class="headerlink" title="分桶的概念"></a>分桶的概念</h3><p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。</p>
<p>分桶是将数据集分解成更容易管理的若干部分的另一个技术。</p>
<p><code>分区针对的是数据的存储路径；分桶针对的是数据文件。</code></p>
<h3 id="分桶的好处"><a href="#分桶的好处" class="headerlink" title="分桶的好处"></a>分桶的好处</h3><ul>
<li>分桶规则：对分桶字段值进行哈希，哈希值除以桶的个数求余，余数决定了该条记录在哪个桶中，也就是余数相同的在一个桶中。</li>
<li>优点：<ol>
<li>提高join查询效率 </li>
<li>提高抽样效率</li>
</ol>
</li>
</ul>
<h3 id="分桶实践"><a href="#分桶实践" class="headerlink" title="分桶实践"></a>分桶实践</h3><ol start="2">
<li><p>准备数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1,name1</span><br><span class="line">2,name2</span><br><span class="line">3,name3</span><br><span class="line">4,name4</span><br><span class="line">5,name5</span><br><span class="line">6,name6</span><br><span class="line">7,name7</span><br><span class="line">8,name8</span><br><span class="line">9,name9</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建桶表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> bucket_tbl(</span><br><span class="line">	<span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">	<span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>) </span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建中间表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> mid_tbl(</span><br><span class="line">	<span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">	<span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载数据到中间表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/bucket.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> mid_tbl;</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置强制分桶</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set hive.enforce.bucketing = true; --Hive 2.x 不需要这一步</span><br><span class="line">set mapreduce.job.reduces=-1;</span><br></pre></td></tr></table></figure>

<p>在 Hive 0.x and 1.x 版本，必须使用设置 <code>hive.enforce.bucketing = true</code>，表示强制分桶，允许程序根据表结构自动选择正确数量的 Reducer 和 cluster by column 来进行分桶。</p>
</li>
<li><p>插入数据到分桶表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> bucket_tbl <span class="keyword">select</span> * <span class="keyword">from</span> mid_tbl;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看结果</p>
<p>HDFS：<code>桶是以文件的形式存在的，而不是像分区那样以文件夹的形式存在。</code></p>
<p><img src="https://upload-images.jianshu.io/upload_images/12267859-d7e00bb44b332b5b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/format/webp" alt="分桶文件"></p>
</li>
</ol>
<h3 id="有序的分桶表"><a href="#有序的分桶表" class="headerlink" title="有序的分桶表"></a>有序的分桶表</h3><ul>
<li><p>如果要按id升序排序可以这样建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_bucket_sorted (</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'ID'</span>, </span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'名字'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">comment</span> <span class="string">'测试分桶'</span></span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>) </span><br><span class="line">sorted <span class="keyword">by</span> (<span class="keyword">id</span>) </span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span> ;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> bucket_tbl(</span><br><span class="line">	<span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">	<span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>) sorted <span class="keyword">by</span> (<span class="keyword">id</span>) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>

<p><code>注意:</code> 同样需要中间表insert插入数据</p>
</li>
<li><p>好处:</p>
<p>因为每个桶内的数据是排序的，这样每个桶进行连接时就变成了高效的归并排序</p>
</li>
</ul>
<h3 id="分桶抽样查询"><a href="#分桶抽样查询" class="headerlink" title="分桶抽样查询"></a>分桶抽样查询</h3><p>对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。</p>
<h4 id="SQL示例"><a href="#SQL示例" class="headerlink" title="SQL示例"></a>SQL示例</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from test_bucket tablesample (bucket 1 out of 2);</span><br><span class="line">OK</span><br><span class="line">8   name8</span><br><span class="line">4   name4</span><br><span class="line">2   name2</span><br><span class="line">6   name6</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from test tablesample (bucket 1 out of 2 on id);</span><br><span class="line">OK</span><br><span class="line">2   name2</span><br><span class="line">8   name8</span><br><span class="line">4   name4</span><br><span class="line">6   name6</span><br></pre></td></tr></table></figure>

<h4 id="区别"><a href="#区别" class="headerlink" title="区别:"></a>区别:</h4><ul>
<li>分桶表后面可以不带on 字段名，不带时默认的是按分桶字段,也可以带，而没有分桶的表则必须带</li>
<li>按分桶字段取样时，因为分桶表是直接去对应的桶中拿数据，在表比较大时会提高取样效率</li>
</ul>
<h4 id="语法"><a href="#语法" class="headerlink" title="语法:"></a>语法:</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">tablesample (bucket x out of y on id);</span><br></pre></td></tr></table></figure>

<p>x表示从哪个桶开始，y代表分几个桶，也可以理解分x为分子，y为分母，及将表分为y份（桶），取第x份（桶）</p>
<p>所以这时对于分桶表是有要求的，y为桶数的倍数或因子，</p>
<ul>
<li><p>x=1,y=2，取2(4/y)个bucket的数据，分别桶1和桶3(1+y)</p>
</li>
<li><p>x=1,y=4, 取1(4/y)个bucket的数据，即桶1</p>
</li>
<li><p>x=2,y=8, 取1/2(4/y)个bucket的数据，即桶1的一半</p>
<p> x的值必须小于等于y的值</p>
</li>
</ul>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/04/hive/4/">Hive数据类型&amp;DDL数据定义(增删查改)&amp;DML数据操作(导入导出)</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-04</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h2 id="Hive数据类型"><a href="#Hive数据类型" class="headerlink" title="Hive数据类型"></a>Hive数据类型</h2><h3 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h3><table>
<thead>
<tr>
<th>Hive数据类型</th>
<th>Java数据类型</th>
<th>长度</th>
<th>例子</th>
</tr>
</thead>
<tbody><tr>
<td>TINYINT</td>
<td>byte</td>
<td>1byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>SMALINT</td>
<td>short</td>
<td>2byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>INT</td>
<td>int</td>
<td>4byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BIGINT</td>
<td>long</td>
<td>8byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BOOLEAN</td>
<td>boolean</td>
<td>布尔类型，true或者false</td>
<td>TRUE FALSE</td>
</tr>
<tr>
<td>FLOAT</td>
<td>float</td>
<td>单精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>double</td>
<td>双精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>STRING</td>
<td>string</td>
<td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td>
<td>‘now is the time’ “for all good men”</td>
</tr>
<tr>
<td>TIMESTAMP</td>
<td></td>
<td>时间类型</td>
<td></td>
</tr>
<tr>
<td>BINARY</td>
<td></td>
<td>字节数组</td>
<td></td>
</tr>
</tbody></table>
<p>对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</p>
<h3 id="集合数据类型"><a href="#集合数据类型" class="headerlink" title="集合数据类型"></a>集合数据类型</h3><table>
<thead>
<tr>
<th>数据类型</th>
<th>描述</th>
<th>语法示例</th>
</tr>
</thead>
<tbody><tr>
<td>STRUCT</td>
<td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td>
<td>struct()  例如struct&lt;street:string,  city:string&gt;</td>
</tr>
<tr>
<td>MAP</td>
<td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td>
<td>map()  例如map&lt;string, int&gt;</td>
</tr>
<tr>
<td>ARRAY</td>
<td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’,  ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td>
<td>Array()  例如array<string></td>
</tr>
</tbody></table>
<p>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p>
<ul>
<li><p>案例实操</p>
<ol>
<li><p>假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问的格式为</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"name"</span>: <span class="string">"songsong"</span>,</span><br><span class="line">    <span class="attr">"friends"</span>: [<span class="string">"bingbing"</span> , <span class="string">"lili"</span>] ,       <span class="comment">//列表Array, </span></span><br><span class="line">    <span class="attr">"children"</span>: &#123;                      <span class="comment">//键值Map,</span></span><br><span class="line">        <span class="attr">"xiao song"</span>: <span class="number">18</span> ,</span><br><span class="line">        <span class="attr">"xiaoxiao song"</span>: <span class="number">19</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">"address"</span>: &#123;                      <span class="comment">//结构Struct,</span></span><br><span class="line">        <span class="attr">"street"</span>: <span class="string">"hui long guan"</span> ,</span><br><span class="line">        <span class="attr">"city"</span>: <span class="string">"beijing"</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>基于上述数据结构，我们在Hive里创建对应的表，并导入数据。 创建本地测试文件test.txt</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing</span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</span><br></pre></td></tr></table></figure>

<p>注意：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。</p>
</li>
<li><p>Hive上创建测试表test</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">test</span>(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">friends <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">children <span class="keyword">map</span>&lt;<span class="keyword">string</span>, <span class="built_in">int</span>&gt;,</span><br><span class="line">address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>, city:<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'_'</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">':'</span></span><br><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span>;	<span class="comment">#默认"\n"</span></span><br></pre></td></tr></table></figure>

<p>字段解释：</p>
<p><code>row format delimited fields terminated by &#39;,&#39;</code>      – 列分隔符</p>
<p><code>collection items terminated by &#39;_&#39;</code>                             – MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</p>
<p><code>map keys terminated by &#39;:&#39;</code>                                              – MAP中的key与value的分隔符</p>
<p><code>lines terminated by &#39;\n&#39;;</code>                                                – 行分隔符</p>
</li>
<li><p>导入文本数据到测试表</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath ‘/opt/module/datas/test.txt’into table test</span><br></pre></td></tr></table></figure>
</li>
<li><p>访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select friends[1],children['xiao song'],address.city from test</span><br><span class="line">where name="songsong";</span><br><span class="line">OK</span><br><span class="line">_c0     _c1     city</span><br><span class="line">lili    18      beijing</span><br><span class="line">Time taken: 0.076 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ul>
<h2 id="DDL数据定义"><a href="#DDL数据定义" class="headerlink" title="DDL数据定义"></a>DDL数据定义</h2><h3 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] database_name</span><br><span class="line">[<span class="keyword">COMMENT</span> database_comment]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[<span class="keyword">WITH</span> DBPROPERTIES (property_name=property_value, ...)];</span><br></pre></td></tr></table></figure>

<ul>
<li>创建一个数据库，数据库在HDFS上的默认存储路径是/user/hive/warehouse/*.db。</li>
</ul>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br></pre></td></tr></table></figure>

<ul>
<li>避免要创建的数据库已经存在错误，增加if not exists判断。（标准写法）</li>
</ul>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already exists</span><br><span class="line">hive (default)&gt; create database if not exists db_hive;</span><br></pre></td></tr></table></figure>

<ul>
<li>创建一个数据库，指定数据库在HDFS上存放的位置</li>
</ul>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive2 location '/db_hive2.db';</span><br></pre></td></tr></table></figure>

<h3 id="查询数据库"><a href="#查询数据库" class="headerlink" title="查询数据库"></a>查询数据库</h3><h4 id="显示数据库"><a href="#显示数据库" class="headerlink" title="显示数据库"></a>显示数据库</h4><ol>
<li><p>显示数据库</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure>
</li>
<li><p>过滤显示查询的数据库</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; show databases like 'db_hive*';</span><br><span class="line">OK</span><br><span class="line">db_hive</span><br><span class="line">db_hive_1</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="查看数据库详情"><a href="#查看数据库详情" class="headerlink" title="查看数据库详情"></a>查看数据库详情</h4><ol>
<li><p>显示数据库信息</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; desc database db_hive;</span><br><span class="line">OK</span><br><span class="line">db_hive		hdfs://aliyun:9000/user/hive/warehouse/db_hive.db	hadoopUSER</span><br></pre></td></tr></table></figure>
</li>
<li><p>显示数据库详细信息，extended</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br><span class="line">OK</span><br><span class="line">db_hive		hdfs://aliyun:9000/user/hive/warehouse/db_hive.db	hadoopUSER</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="切换当前数据库"><a href="#切换当前数据库" class="headerlink" title="切换当前数据库"></a>切换当前数据库</h4><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; use db_hive;</span><br></pre></td></tr></table></figure>

<h3 id="修改数据库"><a href="#修改数据库" class="headerlink" title="修改数据库"></a>修改数据库</h3><p>用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter database db_hive set dbproperties('createtime'='20170830');</span><br></pre></td></tr></table></figure>

<p>在hive中查看修改结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br><span class="line">db_name <span class="keyword">comment</span> location        owner_name      owner_type      <span class="keyword">parameters</span></span><br><span class="line">db_hive         hdfs://aliyun:<span class="number">8020</span>/<span class="keyword">user</span>/hive/warehouse/db_hive.db    hadoop <span class="keyword">USER</span>    &#123;createtime=<span class="number">20170830</span>&#125;</span><br></pre></td></tr></table></figure>

<h3 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h3><ul>
<li><p>删除空数据库</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt;drop database db_hive2;</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果删除的数据库不存在，最好采用 if exists判断数据库是否存在</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: SemanticException [Error 10072]: Database does not exist: db_hive</span><br><span class="line">hive&gt; drop database if exists db_hive2;</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果数据库不为空，可以采用cascade命令，强制删除</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database db_hive is not empty. One or more tables exist.)</span><br><span class="line">hive&gt; drop database db_hive cascade;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><ul>
<li><p>建表语法</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name </span><br><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment] </span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) </span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS] </span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] </span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name=property_value, ...)]</span><br><span class="line">[<span class="keyword">AS</span> select_statement]</span><br></pre></td></tr></table></figure>
</li>
<li><p>字段解释说明 </p>
<ol>
<li><p><code>CREATE TABLE</code> 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。</p>
</li>
<li><p><code>EXTERNAL</code>关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实际数据的路径（LOCATION），在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</p>
</li>
<li><p><code>COMMENT</code>为表和列添加注释。</p>
</li>
<li><p><code>PARTITIONED BY</code>创建分区表</p>
</li>
<li><p><code>CLUSTERED BY</code>创建分桶表</p>
</li>
<li><p><code>SORTED BY</code>不常用，对桶中的一个或多个列另外排序</p>
</li>
<li><p><code>ROW FORMAT</code> </p>
<p><code>DELIMITED [FIELDS TERMINATED BY char]</code> </p>
<p><code>[COLLECTION ITEMS TERMINATED BY char]</code></p>
<p><code>[MAP KEYS TERMINATED BY char]</code> </p>
<p><code>[LINES TERMINATED BY char]</code> </p>
<p><code>SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</code></p>
<p>用户在建表的时候可以自定义<code>SerDe</code>或者使用自带的<code>SerDe</code>。如果没有指定<code>ROW FORMAT</code> 或者<code>ROW FORMAT DELIMITED</code>，将会使用自带的<code>SerDe</code>。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的<code>SerDe</code>，Hive通过<code>SerDe</code>确定表的具体的列的数据。</p>
<p><code>SerDe</code>是<code>Serialize</code>/<code>Deserilize</code>的简称， hive使用<code>Serde</code>进行行对象的序列与反序列化。</p>
</li>
<li><p><code>STORED AS</code>指定存储文件类型</p>
<p>常用的存储文件类型：<code>SEQUENCEFILE</code>（二进制序列文件）、<code>TEXTFILE</code>（文本）、<code>RCFILE</code>（列式存储格式文件）</p>
<p>如果文件数据是纯文本，可以使用<code>STORED AS TEXTFILE</code>。如果数据需要压缩，使用 <code>STORED AS</code> <code>SEQUENCEFILE</code>。</p>
</li>
<li><p><code>LOCATION</code> ：指定表在HDFS上的存储位置。</p>
</li>
<li><p><code>AS</code>：后跟查询语句，根据查询结果创建表。</p>
</li>
<li><p><code>LIKE</code>允许用户复制现有的表结构，但是不复制数据。</p>
</li>
</ol>
</li>
</ul>
<h4 id="管理表"><a href="#管理表" class="headerlink" title="管理表"></a>管理表</h4><ul>
<li><p>理论</p>
<p>默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。 当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据。</p>
</li>
<li><p>案例实操</p>
<ol>
<li><p>普通创建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student2(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile</span><br><span class="line">location <span class="string">'/user/hive/warehouse/student2'</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据查询结果创建表（查询的结果会添加到新创建的表中）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student3 <span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据已经存在的表结构创建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student4 <span class="keyword">like</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询表的类型</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ul>
<h4 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h4><ul>
<li><p>理论</p>
<p>因为表是外部表，所以Hive并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。</p>
</li>
<li><p>管理表和外部表的使用场景</p>
<p>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p>
</li>
<li><p>案例实操</p>
<p>分别创建部门和员工外部表，并向表中导入数据。</p>
<ol>
<li><p>上传数据到HDFS</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir /student;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>建表语句</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create external table stu (</span><br><span class="line">id int, </span><br><span class="line">name string) </span><br><span class="line">row format delimited fields terminated by '\t' </span><br><span class="line">location '/student';</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看创建的表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from stu_external;</span><br><span class="line">OK</span><br><span class="line">stu_external.id stu_external.name</span><br><span class="line">1001    lisi</span><br><span class="line">1002    wangwu</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看表格式化数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted dept;</span><br><span class="line">Table Type:             EXTERNAL_TABLE</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除外部表</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; drop table stu_external;</span><br></pre></td></tr></table></figure>

<p>外部表删除后，hdfs中的数据还在，但是metadata中stu_external的元数据已被删除</p>
</li>
</ol>
</li>
</ul>
<h4 id="管理表与外部表的互相转换"><a href="#管理表与外部表的互相转换" class="headerlink" title="管理表与外部表的互相转换"></a>管理表与外部表的互相转换</h4><ol>
<li><p>查询表的类型</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改内部表student2为外部表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'TRUE'</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询表的类型</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             EXTERNAL_TABLE</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改外部表student2为内部表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'FALSE'</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询表的类型</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure>

<p>注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写！</p>
</li>
</ol>
<h3 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h3><p>分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。</p>
<h4 id="分区表基本操作"><a href="#分区表基本操作" class="headerlink" title="分区表基本操作"></a>分区表基本操作</h4><ol>
<li><p>引入分区表（需要根据日期对日志进行管理）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">/user/hive/warehouse/log_partition/20170702/20170702.log</span><br><span class="line">/user/hive/warehouse/log_partition/20170703/20170703.log</span><br><span class="line">/user/hive/warehouse/log_partition/20170704/20170704.log</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建分区表语法</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table dept (</span><br><span class="line">deptno int, </span><br><span class="line">dname string, </span><br><span class="line">loc string</span><br><span class="line">)</span><br><span class="line">partitioned by (month string)</span><br><span class="line">row format delimited fields terminated by '\t';</span><br></pre></td></tr></table></figure>

<p><code>注意：</code> 分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。</p>
</li>
<li><p>加载数据到分区表中</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept partition(month='201709');</span><br><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept partition(month='201708');</span><br><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept partition(month='201707’);</span><br></pre></td></tr></table></figure>

<p><code>注意：</code> 分区表加载数据时，必须指定分区</p>
</li>
<li><p>查询分区表中数据</p>
<ol>
<li><p>单分区查询</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month='201709';</span><br></pre></td></tr></table></figure>
</li>
<li><p>多分区联合查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month='201709'</span><br><span class="line">              union all</span><br><span class="line">              <span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201708'</span></span><br><span class="line">              <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">              <span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201707'</span>;</span><br><span class="line"></span><br><span class="line">_u3.deptno      _u3.dname       _u3.loc _u3.month</span><br><span class="line">10      ACCOUNTING      NEW YORK        201707</span><br><span class="line">10      ACCOUNTING      NEW YORK        201708</span><br><span class="line">10      ACCOUNTING      NEW YORK        201709</span><br><span class="line">20      RESEARCH        DALLAS  201707</span><br><span class="line">20      RESEARCH        DALLAS  201708</span><br><span class="line">20      RESEARCH        DALLAS  201709</span><br><span class="line">30      SALES   CHICAGO 201707</span><br><span class="line">30      SALES   CHICAGO 201708</span><br><span class="line">30      SALES   CHICAGO 201709</span><br><span class="line">40      OPERATIONS      BOSTON  201707</span><br><span class="line">40      OPERATIONS      BOSTON  201708</span><br><span class="line">40      OPERATIONS      BOSTON  201709</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>增加分区</p>
<ol>
<li><p>创建单个分区</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(month='201706') ;</span><br></pre></td></tr></table></figure>
</li>
<li><p>同时创建多个分区</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(month='201705') partition(month='201704');</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>删除分区</p>
<ol>
<li><p>删除单个分区</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (month='201704');</span><br></pre></td></tr></table></figure>
</li>
<li><p>同时删除多个分区</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (month='201705'), partition (month='201706');</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>查看分区表有多少分区</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show partitions dept_partition;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看分区表结构</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; desc formatted dept_partition;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Partition Information          </span></span><br><span class="line"><span class="comment"># col_name              data_type               comment             </span></span><br><span class="line">month                   string</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="分区表注意事项"><a href="#分区表注意事项" class="headerlink" title="分区表注意事项"></a>分区表注意事项</h4><ul>
<li><p>创建二级分区表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition2(</span><br><span class="line">               deptno int, </span><br><span class="line">               dname string, </span><br><span class="line">               loc string</span><br><span class="line">               )</span><br><span class="line">               partitioned by (month string, day string)</span><br><span class="line">               row format delimited fields terminated by '\t';</span><br></pre></td></tr></table></figure>
</li>
<li><p>正常的加载数据</p>
<ol>
<li><p>加载数据到二级分区表中</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table</span><br><span class="line"> default.dept_partition2 partition(month='201709', day='13');</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询分区数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='13';</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式</p>
<ol>
<li><p>方式一：上传数据后修复(<code>禁用</code>)</p>
<p>上传数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br></pre></td></tr></table></figure>

<p>查询数据（查询不到刚上传的数据）</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='12';</span><br></pre></td></tr></table></figure>

<p>执行修复命令</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; msck repair table dept_partition2;</span><br></pre></td></tr></table></figure>

<p>再次查询数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='12';</span><br></pre></td></tr></table></figure>
</li>
<li><p>方式二：上传数据后添加分区(<code>推介</code>)</p>
<p>上传数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br></pre></td></tr></table></figure>

<p>执行添加分区</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 add partition(month='201709',</span><br><span class="line"> day='11');</span><br></pre></td></tr></table></figure>

<p>查询数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='11';</span><br></pre></td></tr></table></figure>
</li>
<li><p>方式三：创建文件夹后load数据到分区</p>
<p>创建目录</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=10;</span><br></pre></td></tr></table></figure>

<p>上传数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table</span><br><span class="line"> dept_partition2 partition(month='201709',day='10');</span><br></pre></td></tr></table></figure>

<p>查询数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='10';</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ul>
<h3 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h3><h4 id="重命名表"><a href="#重命名表" class="headerlink" title="重命名表"></a>重命名表</h4><ol>
<li><p>语法</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">RENAME</span> <span class="keyword">TO</span> new_table_name</span><br></pre></td></tr></table></figure>
</li>
<li><p>实操案例</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 rename to dept_partition3;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="增加、修改和删除表分区"><a href="#增加、修改和删除表分区" class="headerlink" title="增加、修改和删除表分区"></a>增加、修改和删除表分区</h4><p>详见分区表基本操作。</p>
<h4 id="增加-修改-替换列信息"><a href="#增加-修改-替换列信息" class="headerlink" title="增加/修改/替换列信息"></a>增加/修改/替换列信息</h4><ul>
<li><p>语法</p>
<ol>
<li><p>更新列</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">CHANGE</span> [<span class="keyword">COLUMN</span>] col_old_name col_new_name column_type [<span class="keyword">COMMENT</span> col_comment] [<span class="keyword">FIRST</span>|<span class="keyword">AFTER</span> column_name]</span><br></pre></td></tr></table></figure>
</li>
<li><p>增加和替换列</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">ADD</span>|<span class="keyword">REPLACE</span> <span class="keyword">COLUMNS</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p><code>注：</code> ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。</p>
</li>
<li><p>实操案例</p>
<ol>
<li><p>查询表结构</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加列</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add columns(deptdesc string);</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询表结构</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure>
</li>
<li><p>更新列</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition change column deptdesc desc int;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询表结构</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure>
</li>
<li><p>替换列</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition replace columns(deptno string, dname</span><br><span class="line"> string, loc string);</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询表结构</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ul>
<h3 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; drop table dept_partition;</span><br></pre></td></tr></table></figure>

<h2 id="DML数据操作"><a href="#DML数据操作" class="headerlink" title="DML数据操作"></a>DML数据操作</h2><h3 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h3><h4 id="向表中装载数据（Load）"><a href="#向表中装载数据（Load）" class="headerlink" title="向表中装载数据（Load）"></a>向表中装载数据（Load）</h4><ul>
<li><p>语法</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; load data [local] inpath '/opt/module/datas/student.txt' [overwrite] into table student [partition (partcol1=val1,…)];</span><br></pre></td></tr></table></figure>

<p><code>load data:</code> 表示加载数据</p>
<p><code>local:</code> 表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</p>
<p><code>inpath:</code> 表示加载数据的路径</p>
<p><code>overwrite:</code> 表示覆盖表中已有数据，否则表示追加</p>
<p><code>into table:</code> 表示加载到哪张表</p>
<p><code>student:</code> 表示具体的表</p>
<p><code>partition:</code> 表示上传到指定分区</p>
</li>
<li><p>实操案例</p>
<ol>
<li><p>创建一张表</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table student(id string, name string) row format delimited fields terminated by '\t';</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载本地文件到hive</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/student.txt' into table default.student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载HDFS文件到hive中</p>
<p>上传文件到HDFS</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/hadoop/hive;</span><br></pre></td></tr></table></figure>

<p>加载HDFS上数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data inpath '/user/hadoop/hive/student.txt' into table default.student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载数据覆盖表中已有的数据</p>
<p>上传文件到HDFS</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/hadoop/hive;</span><br></pre></td></tr></table></figure>

<p>加载数据覆盖表中已有的数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data inpath '/user/hadoop/hive/student.txt' overwrite into table default.student;</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ul>
<h4 id="通过查询语句向表中插入数据（Insert）"><a href="#通过查询语句向表中插入数据（Insert）" class="headerlink" title="通过查询语句向表中插入数据（Insert）"></a>通过查询语句向表中插入数据（Insert）</h4><ol>
<li><p>创建一张分区表</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table student(id int, name string) partitioned by (month string) row format delimited fields terminated by '\t';</span><br></pre></td></tr></table></figure>
</li>
<li><p>基本插入数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert into table  student partition(month='201709') values(1,'wangwu'),(2,’zhaoliu’);</span><br></pre></td></tr></table></figure>
</li>
<li><p>基本模式插入（根据单张表查询结果）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite table student partition(month='201708')</span><br><span class="line">             <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br></pre></td></tr></table></figure>

<p><code>insert into：</code> 以追加数据的方式插入到表或分区，原有数据不会删除</p>
<p><code>insert overwrite：</code> 会覆盖表或分区中已存在的数据</p>
<p><code>注意：</code> insert不支持插入部分字段</p>
</li>
<li><p>多表（多分区）插入模式（根据多张表查询结果）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; from student</span><br><span class="line">              <span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201707'</span>)</span><br><span class="line">              <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span></span><br><span class="line">              <span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201706'</span>)</span><br><span class="line">              <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="查询语句中创建表并加载数据（CTAS）"><a href="#查询语句中创建表并加载数据（CTAS）" class="headerlink" title="查询语句中创建表并加载数据（CTAS）"></a>查询语句中创建表并加载数据（CTAS）</h4><p>详见创建表。</p>
<p>根据查询结果创建表（查询的结果会添加到新创建的表中）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student3 <span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>

<h4 id="创建表时通过Location指定加载数据路径"><a href="#创建表时通过Location指定加载数据路径" class="headerlink" title="创建表时通过Location指定加载数据路径"></a>创建表时通过Location指定加载数据路径</h4><ol>
<li><p>上传数据到hdfs上</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir /student;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建表，并指定在hdfs上的位置</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create external table if not exists student5(</span><br><span class="line">              id int, name string</span><br><span class="line">              )</span><br><span class="line">              row format delimited fields terminated by '\t'</span><br><span class="line">              location '/student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from student5;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="Import数据到指定Hive表中"><a href="#Import数据到指定Hive表中" class="headerlink" title="Import数据到指定Hive表中"></a>Import数据到指定Hive表中</h4><p>注意：先用export导出后，再将数据导入。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; import table student2 partition(month='201709') from</span><br><span class="line"> '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure>

<h3 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h3><h4 id="Insert导出"><a href="#Insert导出" class="headerlink" title="Insert导出"></a>Insert导出</h4><ol>
<li><p>将查询的结果导出到本地</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory '/opt/module/datas/export/student' select * from student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>将查询的结果格式化导出到本地</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive(default)&gt;insert overwrite local directory '/opt/module/datas/export/student1' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' select * from student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>将查询的结果导出到HDFS上(没有local)</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite directory '/user/hadoop/student2' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' select * from student;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="Hadoop命令导出到本地"><a href="#Hadoop命令导出到本地" class="headerlink" title="Hadoop命令导出到本地"></a>Hadoop命令导出到本地</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -get /user/hive/warehouse/student/month=201709/000000_0</span><br><span class="line">/opt/module/datas/export/student3.txt;</span><br></pre></td></tr></table></figure>

<h4 id="Hive-Shell-命令导出"><a href="#Hive-Shell-命令导出" class="headerlink" title="Hive Shell 命令导出"></a>Hive Shell 命令导出</h4><p>基本语法：（hive -f/-e 执行语句或者脚本 &gt; file）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop102 hive]$ bin/hive -e 'select * from default.student;' &gt;</span><br><span class="line"> /opt/module/datas/export/student4.txt;</span><br></pre></td></tr></table></figure>

<h4 id="Export导出到HDFS上"><a href="#Export导出到HDFS上" class="headerlink" title="Export导出到HDFS上"></a>Export导出到HDFS上</h4><figure class="highlight"><table><tr><td class="code"><pre><span class="line">(defahiveult)&gt; export table default.student to '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure>

<p>export和import主要用于两个Hadoop平台集群之间Hive表迁移。</p>
<h4 id="Sqoop导出"><a href="#Sqoop导出" class="headerlink" title="Sqoop导出"></a>Sqoop导出</h4><p>后续的博客讲解</p>
<h3 id="清除表中数据（Truncate）"><a href="#清除表中数据（Truncate）" class="headerlink" title="清除表中数据（Truncate）"></a>清除表中数据（Truncate）</h3><p>注意：Truncate只能删除管理表，不能删除外部表中数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; truncate table student;</span><br></pre></td></tr></table></figure>



</div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/8/">&lt;&lt;</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/10/">&gt;&gt;</a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>