<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="BigData Developer"><meta name="keywords" content="yerias,TUNANのBlog,BigData"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017 | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">133</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">30</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">26</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">TUNANのBlog</div><div id="site-sub-title">感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017</div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/07/hive/7/">创建伪表&amp;自定义UDF函数&amp;MR解决数据倾斜的问题&amp;行转列案例&amp;列转行案例&amp;使用hive实现wc&amp;修改hadoop的URI带来的hive数据库路径问题&amp;多文件多目录做wc或建表带来的问题</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-07</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>创建伪表</li>
<li>自定义UDF函数</li>
<li>MR解决数据倾斜的问题(引入)</li>
<li>行转列案例</li>
<li>列转行案例</li>
<li>使用hive实现wc</li>
<li>修改hadoop的URI带来的hive数据库路径问题</li>
<li>多文件多目录做wc或建表带来的问题</li>
</ol>
<h2 id="创建伪表"><a href="#创建伪表" class="headerlink" title="创建伪表"></a>创建伪表</h2><ol>
<li><p>创建表dual</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dual(a <span class="keyword">string</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建数据并导入到表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">touch dual.txt</span><br><span class="line">echo 'X' &gt;dual.txt</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/dual.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> dual;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="自定义UDF函数"><a href="#自定义UDF函数" class="headerlink" title="自定义UDF函数"></a>自定义UDF函数</h2><p>hive官网关于用户如何自定义UDF: <a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins1" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/HivePlugins1</a>. </p>
<p>hive查看函数: show functions;</p>
<p>hive查看jar包: list jars;</p>
<ol>
<li><p>首先，需要创建一个扩展UDF的新类，其中有一个或多个名为evaluate的方法</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">// 继承UDF</span><br><span class="line">public class UDFPrintf extends UDF &#123;</span><br><span class="line">    </span><br><span class="line">	//方法重载</span><br><span class="line">    public void evaluate()&#123;</span><br><span class="line">        System.out.println("你不要老婆吧");</span><br><span class="line">    &#125;</span><br><span class="line">    public void evaluate(String name)&#123;</span><br><span class="line">        System.out.println(name + ",你要老婆不要?");</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>对方法添加描述，先看系统的</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc function extended upper;</span><br><span class="line">upper(str) - Returns str <span class="keyword">with</span> <span class="keyword">all</span> <span class="keyword">characters</span> <span class="keyword">changed</span> <span class="keyword">to</span> uppercase</span><br><span class="line">Synonyms: <span class="keyword">ucase</span></span><br><span class="line">Example:</span><br><span class="line">  &gt; <span class="keyword">SELECT</span> <span class="keyword">upper</span>(<span class="string">'Facebook'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;</span><br><span class="line">  'FACEBOOK'</span><br><span class="line">=================================================================</span><br><span class="line">@Description(</span><br><span class="line">    name = "upper,ucase",</span><br><span class="line">    value = "_FUNC_(str) - Returns str <span class="keyword">with</span> <span class="keyword">all</span> <span class="keyword">characters</span> <span class="keyword">changed</span> <span class="keyword">to</span> uppercase<span class="string">",</span></span><br><span class="line"><span class="string">    extended = "</span>Example:\n  &gt; <span class="keyword">SELECT</span> _FUNC_(<span class="string">'Facebook'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;\n  		  </span><br><span class="line">    'FACEBOOK'"</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>我们仿照上面的给自定义的方法写个描述(Description)</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">@Description(</span><br><span class="line">    name = "my_printf,ucase",</span><br><span class="line">    value = "_FUNC_(str) - 返回一个名字加上字符串",</span><br><span class="line">    extended = "Example:\n  &gt; SELECT _FUNC_('老王') FROM src LIMIT 1;\n  </span><br><span class="line">    '老王,你要老婆不要?'"</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li><p>打包上传到Linux，启动hive，jar包添加到class path，创建<strong>临时UDF函数</strong>，只能在当前session中有效</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">add jar /home/hadoop/lib/hive-client-1.0.0.jar;</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">temporary</span> <span class="keyword">function</span> my_printf <span class="keyword">as</span> <span class="string">'org.hadoop.hive.UDF.UDFPrintf'</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>伪表使用自定义函数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> my_printf() <span class="keyword">from</span> dual;</span><br><span class="line">hello,小七</span><br><span class="line"><span class="keyword">select</span> my_printf(<span class="string">"老王"</span>) <span class="keyword">from</span> dual;</span><br><span class="line">hello,老王</span><br></pre></td></tr></table></figure>
</li>
<li><p>上面的方法只能创建临时函数，我们接下来将会创建永久函数，需要把jar把上传到hdfs</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hdfs dfs -mkdir /jar</span><br><span class="line">hdfs dfs -put /home/hadoop/lib/hive-client-1.0.0.jar /jar</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">FUNCTION</span> my_printf <span class="keyword">AS</span> <span class="string">'org.hadoop.hive.UDF.UDFPrintf'</span> <span class="keyword">USING</span> JAR <span class="string">'hdfs:///jar/hive-client-1.0.0.jar'</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>然后可以在多个窗口执行创建的永久函数</p>
</li>
</ol>
<h2 id="MR解决数据倾斜的思想"><a href="#MR解决数据倾斜的思想" class="headerlink" title="MR解决数据倾斜的思想"></a>MR解决数据倾斜的思想</h2><p>核心思想==&gt;先加盐(随机数)，再去盐(随机数)</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">public class skew &#123;</span><br><span class="line">    private List&lt;String&gt; newList = new ArrayList&lt;&gt;();</span><br><span class="line">    private Random r = new Random();</span><br><span class="line">    public void incRandom(List list)&#123;</span><br><span class="line">        newList.clear();</span><br><span class="line">        list.forEach( word -&gt;&#123;</span><br><span class="line">            int i = r.nextInt(10);</span><br><span class="line">            newList.add(i+"_"+word);</span><br><span class="line">        &#125;);</span><br><span class="line">        newList.forEach(System.out::println);</span><br><span class="line">    &#125;</span><br><span class="line">    public void decRandom()&#123;</span><br><span class="line">        newList.forEach(word-&gt;&#123;</span><br><span class="line">            System.out.println(word.substring(word.lastIndexOf("_")+1));</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        skew skew = new skew();</span><br><span class="line">        List&lt;String&gt; arr = new ArrayList&lt;&gt;();</span><br><span class="line">        arr.add("老王");</span><br><span class="line">        arr.add("狗子");</span><br><span class="line">        arr.add("张三");</span><br><span class="line">        skew.incRandom(arr);</span><br><span class="line">        System.out.println("<span class="comment">-------------------");</span></span><br><span class="line">        skew.decRandom();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">7_老王</span><br><span class="line">2_狗子</span><br><span class="line">4_张三</span><br><span class="line"><span class="comment">-------------------</span></span><br><span class="line">老王</span><br><span class="line">狗子</span><br><span class="line">张三</span><br></pre></td></tr></table></figure>

<h2 id="行转列案例"><a href="#行转列案例" class="headerlink" title="行转列案例"></a>行转列案例</h2><p>实现部门号的所有学生格式为</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">(dept01,A	laowang|wangwu)</span><br></pre></td></tr></table></figure>

<ol>
<li><p>数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">laowang	dept01	A</span><br><span class="line">zhangsan	dept02	A</span><br><span class="line">lisi	dept01	B</span><br><span class="line">wangwu	dept01	A</span><br><span class="line">zhaoliu	dept02	A</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建表结构</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> row2col(</span><br><span class="line">	<span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">	dept <span class="keyword">string</span>,</span><br><span class="line">	grade <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>导入数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/row2col.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> row2col;</span><br></pre></td></tr></table></figure>
</li>
<li><p>实现(dept01,A    laowang|wangwu)</p>
<ol>
<li><p>第一步组合dept和grade成dept_grade</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">concat_ws</span>(<span class="string">','</span>,dept,grade) dept_grade,<span class="keyword">name</span> <span class="keyword">from</span> row2col;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步按dept_grade分组求collect_set()将返回的数组使用concat_ws()函数转成字符串</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	t.dept_grade,<span class="keyword">concat_ws</span>(<span class="string">'|'</span>,collect_set(t.name)) <span class="keyword">names</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	(<span class="keyword">select</span> <span class="keyword">concat_ws</span>(<span class="string">','</span>,dept,grade) dept_grade,<span class="keyword">name</span> <span class="keyword">from</span> row2col) t</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">	t.dept_grade;</span><br></pre></td></tr></table></figure>

<p>collect_set()返回一个数组，concat_ws()返回一个指定字符切分数组的字符串</p>
</li>
<li><p>结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">dept_grade|names           |</span><br><span class="line"><span class="comment">----------|----------------|</span></span><br><span class="line">dept01,A  |laowang|wangwu  |</span><br><span class="line">dept01,B  |lisi            |</span><br><span class="line">dept02,A  |zhangsan|zhaoliu|</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h2 id="列转行案例"><a href="#列转行案例" class="headerlink" title="列转行案例"></a>列转行案例</h2><p>实现所有课程的格式为</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">1       zhangsan        化学</span><br><span class="line">1       zhangsan        物理</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<ol>
<li><p>数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">1,zhangsan,化学:物理:数学:语文</span><br><span class="line">2,lisi,化学:数学:生物:生理:卫生</span><br><span class="line">3,wangwu,化学:语文:英语:体育:生物</span><br></pre></td></tr></table></figure>
</li>
<li><p>建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> col2row(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">subjects <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">":"</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>导数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/col2row.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> col2row;</span><br></pre></td></tr></table></figure>
</li>
<li><p>实现</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	<span class="keyword">id</span>,<span class="keyword">name</span>,subject </span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	col2row</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(subjects) t <span class="keyword">as</span> subject;</span><br></pre></td></tr></table></figure>
</li>
<li><p>结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">id|name    |subject|</span><br><span class="line"><span class="comment">--|--------|-------|</span></span><br><span class="line"> 1|zhangsan|化学     |</span><br><span class="line"> 1|zhangsan|物理     |</span><br><span class="line"> 1|zhangsan|数学     |</span><br><span class="line"> 1|zhangsan|语文     |</span><br><span class="line"> 2|lisi    |化学     |</span><br><span class="line"> 2|lisi    |数学     |</span><br><span class="line"> 2|lisi    |生物     |</span><br><span class="line"> 2|lisi    |生理     |</span><br><span class="line"> 2|lisi    |卫生     |</span><br><span class="line"> 3|wangwu  |化学     |</span><br><span class="line"> 3|wangwu  |语文     |</span><br><span class="line"> 3|wangwu  |英语     |</span><br><span class="line"> 3|wangwu  |体育     |</span><br><span class="line"> 3|wangwu  |生物     |</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="使用hive实现wc"><a href="#使用hive实现wc" class="headerlink" title="使用hive实现wc"></a>使用hive实现wc</h2><ol>
<li><p>数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">化学:物理:数学:语文:英语</span><br><span class="line">化学:数学:生物:生理:卫生</span><br><span class="line">化学:语文:英语:体育:生物</span><br><span class="line">数学:语文:英语:体育:生物</span><br></pre></td></tr></table></figure>
</li>
<li><p>切分返回数组</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">split</span>(line,<span class="string">":"</span>) <span class="keyword">as</span> subjects <span class="keyword">from</span> wc</span><br></pre></td></tr></table></figure>
</li>
<li><p>炸开数组返回单词</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	words</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	(<span class="keyword">select</span> <span class="keyword">split</span>(line,<span class="string">":"</span>) subjects <span class="keyword">from</span> wc) <span class="keyword">as</span> t</span><br><span class="line"> <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(t.subjects) t1 <span class="keyword">as</span> words;</span><br></pre></td></tr></table></figure>
</li>
<li><p>每个单词分组求count()</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	t2.words,<span class="keyword">count</span>(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	(<span class="keyword">select</span></span><br><span class="line">	words</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	(<span class="keyword">select</span> <span class="keyword">split</span>(line,<span class="string">":"</span>) subjects <span class="keyword">from</span> wc) <span class="keyword">as</span> t</span><br><span class="line"> <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(t.subjects) t1 <span class="keyword">as</span> words) t2</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> t2.words;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="修改hadoop的URI带来的hive数据库路径问题"><a href="#修改hadoop的URI带来的hive数据库路径问题" class="headerlink" title="修改hadoop的URI带来的hive数据库路径问题"></a>修改hadoop的URI带来的hive数据库路径问题</h2><p>hive的数据保存在hadoop中，而hive的源数据保存在mysql中</p>
<p>这里就有一个问题，如果修改了hadoop的fs.defaultFS这个参数</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://aliyun:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>那么hive的元数据没有修改就会找mysql中保存的路径</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">*************************** 1. row ***************************</span><br><span class="line">          DB_ID: 1</span><br><span class="line">           DESC: Default Hive database</span><br><span class="line">DB_LOCATION_URI: hdfs://aliyun:9000/user/hive/warehouse</span><br><span class="line">           NAME: default</span><br><span class="line">     OWNER_NAME: public</span><br><span class="line">     OWNER_TYPE: ROLE</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>这时候需要使用到一个命令metatool来将mysql中的元数据修改为新的仓库路径</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">-updateLocation &lt;new-loc&gt; &lt;old-loc&gt;</span><br></pre></td></tr></table></figure>

<h2 id="多文件多目录做wc或建表带来的问题"><a href="#多文件多目录做wc或建表带来的问题" class="headerlink" title="多文件多目录做wc或建表带来的问题"></a>多文件多目录做wc或建表带来的问题</h2><p>我们查看一下hdfs中的目录结构</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ hdfs dfs -ls /mdir<span class="comment">/*</span></span><br><span class="line"><span class="comment">-rwxr-xr-x   1 hadoop supergroup         13 2020-02-05 00:59 /mdir/1.txt</span></span><br><span class="line"><span class="comment">-rwxr-xr-x   1 hadoop supergroup         14 2020-02-05 00:59 /mdir/2.txt</span></span><br><span class="line"><span class="comment">-rwxr-xr-x   1 hadoop supergroup         11 2020-02-05 00:59 /mdir/mdir2/3.txt</span></span><br></pre></td></tr></table></figure>

<p>存在嵌套目录，在做wc或者建表时，将会报错</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> dir (<span class="keyword">name</span> <span class="keyword">string</span>) location <span class="string">'/mdir'</span>;</span><br><span class="line">Failed <span class="keyword">with</span> <span class="keyword">exception</span> java.io.IOException:java.io.IOException: <span class="keyword">Not</span> a <span class="keyword">file</span>: hdfs://aliyun:<span class="number">9000</span>/mdir<span class="comment">/*</span></span><br></pre></td></tr></table></figure>

<p>通过设置参数<code>mapreduce.input.fileinputformat.input.dir.recursive</code>为<code>true</code>来解决这个问题，该参数默认为<code>false</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dir;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>可以直接在mapped-site.xml文件中直接配配置参数</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.input.fileinputformat.input.dir.recursive&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br></pre></td></tr></table></figure></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/06/hive/6/">Order By&amp;Sort By&amp;Distribute By&amp;Cluster By</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-06</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>全局排序（Order By）</li>
<li>Reduce内部排序（Sort By）</li>
<li>分区排序（Distribute By）</li>
<li>Cluster By</li>
</ol>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><ol>
<li><p>准备测试数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">7369	SMITH	CLERK	7902	1980-12-17	800.00		20</span><br><span class="line">7499	ALLEN	SALESMAN	7698	1981-2-20	1600.00	300.00	30</span><br><span class="line">7521	WARD	SALESMAN	7698	1981-2-22	1250.00	500.00	30</span><br><span class="line">7566	JONES	MANAGER	7839	1981-4-2	2975.00		20</span><br><span class="line">7654	MARTIN	SALESMAN	7698	1981-9-28	1250.00	1400.00	30</span><br><span class="line">7698	BLAKE	MANAGER	7839	1981-5-1	2850.00		30</span><br><span class="line">7782	CLARK	MANAGER	7839	1981-6-9	2450.00		10</span><br><span class="line">7788	SCOTT	ANALYST	7566	1987-4-19	3000.00		20</span><br><span class="line">7839	KING	PRESIDENT		1981-11-17	5000.00		10</span><br><span class="line">7844	TURNER	SALESMAN	7698	1981-9-8	1500.00	0.00	30</span><br><span class="line">7876	ADAMS	CLERK	7788	1987-5-23	1100.00		20</span><br><span class="line">7900	JAMES	CLERK	7698	1981-12-3	950.00		30</span><br><span class="line">7902	FORD	ANALYST	7566	1981-12-3	3000.00		20</span><br><span class="line">7934	MILLER	CLERK	7782	1982-1-23	1300.00		10</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建emp</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> emp(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">job <span class="keyword">string</span>,</span><br><span class="line">mgr <span class="built_in">int</span>,</span><br><span class="line">hiredate <span class="keyword">string</span>, </span><br><span class="line">sal <span class="keyword">double</span>, </span><br><span class="line">comm <span class="keyword">double</span>,</span><br><span class="line">deptno <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/emp.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> emp;</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证数据</p>
<p><code>select * from emp;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br></pre></td></tr></table></figure>


</li>
</ol>
<h2 id="全局排序（Order-By）-慎用"><a href="#全局排序（Order-By）-慎用" class="headerlink" title="全局排序（Order By）[慎用]"></a>全局排序（Order By）[慎用]</h2><p>语句格式:     <code>order by col1,col2 asc/desc</code></p>
<p>使用order by语句排序的是全局排序，只能有一个reduce作用来完成，与此对应的是sort by由多个reduce来完成</p>
<p>案例实操</p>
<ol>
<li><p>查询员工信息按工资升序排列</p>
<p><code>select * from emp order by sal;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询员工信息按工资降序排列(增加一个替换null值的功能)</p>
<p><code>select *,nvl(comm,&#39;-1&#39;) from emp order by sal desc;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|_c1   |</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|------|</span></span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|-1    |</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|-1    |</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|-1    |</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|-1    |</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|-1    |</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|-1    |</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|300.0 |</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|0.0   |</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|-1    |</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|1400.0|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|500.0 |</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|-1    |</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|-1    |</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|-1    |</span><br></pre></td></tr></table></figure>
</li>
<li><p>按照员工薪水的2倍排序</p>
<p><code>select *,sal+nvl(comm,0) as salandcomm from emp order by salandcomm;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|salandcomm|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|----------|</span></span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|       800|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|       950|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|      1100|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|      1300|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|      1500|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|      1750|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|      1900|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|      2450|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|      2650|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|      2850|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|      2975|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|      3000|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|      3000|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|      5000|</span><br></pre></td></tr></table></figure>
</li>
<li><p>按照部门和工资升序排序</p>
<p><code>select * from emp order by deptno,sal+nvl(comm,0);</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="Reduce内部排序（Sort-By）"><a href="#Reduce内部排序（Sort-By）" class="headerlink" title="Reduce内部排序（Sort By）"></a>Reduce内部排序（Sort By）</h2><p>对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用<strong>sort by</strong>。</p>
<p>Sort by为每个reducer产生一个排序文件。每个Reducer内部进行排序，对全局结果集来说不是排序。</p>
<ol>
<li><p>设置reduce个数，不然一个一个reduce没有效果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据部门编号降序查看员工信息</p>
<p><code>select * from emp order by deptno desc;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br></pre></td></tr></table></figure>
</li>
<li><p>将查询结果导入到文件中（按照部门编号降序排序）[指定了格式]</p>
<p><code>insert overwrite local directory &#39;/home/hadoop/emp&#39; row format delimited fields terminated by &quot;\t&quot; select * from emp order by deptno desc;</code></p>
<p>打开/home/hadoop/emp下的文件</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun emp.txt]$ cat 000000_0 </span><br><span class="line">7521	WARD	SALESMAN	7698	1981-2-22	1250.0	500.0	30</span><br><span class="line">7499	ALLEN	SALESMAN	7698	1981-2-20	1600.0	300.0	30</span><br><span class="line">7900	JAMES	CLERK	7698	1981-12-3	950.0	\N	30</span><br><span class="line">7844	TURNER	SALESMAN	7698	1981-9-8	1500.0	0.0	30</span><br><span class="line">7698	BLAKE	MANAGER	7839	1981-5-1	2850.0	\N	30</span><br><span class="line">7654	MARTIN	SALESMAN	7698	1981-9-28	1250.0	1400.0	30</span><br><span class="line">7876	ADAMS	CLERK	7788	1987-5-23	1100.0	\N	20</span><br><span class="line">7902	FORD	ANALYST	7566	1981-12-3	3000.0	\N	20</span><br><span class="line">7788	SCOTT	ANALYST	7566	1987-4-19	3000.0	\N	20</span><br><span class="line">7369	SMITH	CLERK	7902	1980-12-17	800.0	\N	20</span><br><span class="line">7566	JONES	MANAGER	7839	1981-4-2	2975.0	\N	20</span><br><span class="line">7934	MILLER	CLERK	7782	1982-1-23	1300.0	\N	10</span><br><span class="line">7839	KING	PRESIDENT	\N	1981-11-17	5000.0	\N	10</span><br><span class="line">7782	CLARK	MANAGER	7839	1981-6-9	2450.0	\N	10</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="发送分区排序（Distribute-By）"><a href="#发送分区排序（Distribute-By）" class="headerlink" title="发送分区排序（Distribute By）"></a>发送分区排序（Distribute By）</h2><p>在有些情况下，我们需要控制某个特定行应该到哪个reducer，通常是为了进行后续的聚集操作。<strong>distribute by</strong> 子句可以做这件事。<strong>distribute by</strong>类似MR中<strong>partition</strong>（自定义分区），进行分区，结合<strong>sort by</strong>使用。 </p>
<p>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p>
<ol>
<li><p>先按照部门编号分区，再按照员工编号降序排序。</p>
<p><code>select * from emp distribute by deptno sort by empno desc;</code> </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br></pre></td></tr></table></figure>

<p><code>注意:</code> </p>
<ol>
<li>distribute by的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。</li>
<li>Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。</li>
</ol>
</li>
</ol>
<h2 id="Cluster-By"><a href="#Cluster-By" class="headerlink" title="Cluster By"></a>Cluster By</h2><p>当<code>distribute by</code>和<code>sorts by</code>字段相同时，可以使用<code>cluster by</code>方式。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</p>
<p>当分区字段和排序字段都是部门编号的时候我们可以这么做</p>
<p><code>select * from emp cluster by deptno;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">empno|ename |job      |mgr |hiredate  |sal |comm|deptno|</span><br><span class="line"><span class="comment">-----|------|---------|----|----------|----|----|------|</span></span><br><span class="line"> 7654|MARTIN|SALESMAN |7698|1981-9-28 |1250|1400|    30|</span><br><span class="line"> 7900|JAMES |CLERK    |7698|1981-12-3 | 950|    |    30|</span><br><span class="line"> 7698|BLAKE |MANAGER  |7839|1981-5-1  |2850|    |    30|</span><br><span class="line"> 7521|WARD  |SALESMAN |7698|1981-2-22 |1250| 500|    30|</span><br><span class="line"> 7844|TURNER|SALESMAN |7698|1981-9-8  |1500|   0|    30|</span><br><span class="line"> 7499|ALLEN |SALESMAN |7698|1981-2-20 |1600| 300|    30|</span><br><span class="line"> 7934|MILLER|CLERK    |7782|1982-1-23 |1300|    |    10|</span><br><span class="line"> 7839|KING  |PRESIDENT|    |1981-11-17|5000|    |    10|</span><br><span class="line"> 7782|CLARK |MANAGER  |7839|1981-6-9  |2450|    |    10|</span><br><span class="line"> 7788|SCOTT |ANALYST  |7566|1987-4-19 |3000|    |    20|</span><br><span class="line"> 7566|JONES |MANAGER  |7839|1981-4-2  |2975|    |    20|</span><br><span class="line"> 7876|ADAMS |CLERK    |7788|1987-5-23 |1100|    |    20|</span><br><span class="line"> 7902|FORD  |ANALYST  |7566|1981-12-3 |3000|    |    20|</span><br><span class="line"> 7369|SMITH |CLERK    |7902|1980-12-17| 800|    |    20|</span><br></pre></td></tr></table></figure>

<p><code>注意:</code> 注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/05/hive/5/">HS2&amp;Hive的复杂数据结构&amp;行列互转&amp;常用函数&amp;静动态分区表&amp;桶表</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-05</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>HS2</li>
<li>复杂数据结构</li>
<li>行列互转</li>
<li>常用函数</li>
<li>静动态分区表</li>
<li>桶表</li>
</ol>
<h2 id="SH2"><a href="#SH2" class="headerlink" title="SH2"></a>SH2</h2><p>HS2是HiveServer2的简称</p>
<ul>
<li><p>HS2: Server端，默认端口10000</p>
<p>修改端口的方式是通过设置hive.server2.thrift.port的值</p>
</li>
<li><p>beeline: Client端</p>
<p>连接方式: <code>./beeline -u jdbc:hive2://hadoop001:10000/matestore -n hadoop</code></p>
</li>
</ul>
<h2 id="复杂数据结构"><a href="#复杂数据结构" class="headerlink" title="复杂数据结构"></a>复杂数据结构</h2><p>复杂数据类型有三种，分别是: array、map、structs</p>
<h3 id="array"><a href="#array" class="headerlink" title="array"></a>array</h3><p><code>array</code>的建表格式:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hive_array(<span class="keyword">name</span> <span class="keyword">string</span>, work_locations <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line">COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span>;</span><br></pre></td></tr></table></figure>

<p>注意: <code>COLLECTION ITEMS TERMINATED BY &#39;,&#39;</code>是指定array数组中的元素分隔符</p>
<h4 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h4><ol>
<li><p>第一步准备数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pk      beijing,shanghai,tianjin,hangzhou</span><br><span class="line">jepson  changchu,chengdu,wuhan,beijing</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> array_(</span><br><span class="line">	<span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">	address <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span> </span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第三步装载数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/hive_array.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> array_;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第四步查询数据是否加载成功</p>
<p><code>select * from array_;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name  |address                                    |</span><br><span class="line"><span class="comment">------|-------------------------------------------|</span></span><br><span class="line">pk    |["beijing","shanghai","tianjin","hangzhou"]|</span><br><span class="line">jepson|["changchu","chengdu","wuhan","beijing"]   |</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="对array数组的”取”"><a href="#对array数组的”取”" class="headerlink" title="对array数组的”取”"></a>对array数组的”取”</h4><ol>
<li><p>根据数组的下标取出指定元素，下标从0开始</p>
<p><code>select name,address[1] as address from array_;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name  |address |</span><br><span class="line"><span class="comment">------|--------|</span></span><br><span class="line">pk    |shanghai|</span><br><span class="line">jepson|chengdu |</span><br></pre></td></tr></table></figure>
</li>
<li><p>获取数组中元素的个数</p>
<p><code>select name,size(address) as size from array_;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name  |size|</span><br><span class="line"><span class="comment">------|----|</span></span><br><span class="line">pk    |   4|</span><br><span class="line">jepson|   4|</span><br></pre></td></tr></table></figure>
</li>
<li><p>获取数组中包含某元素的记录</p>
<p><code>select name,address from array_ where  array_contains(address,&quot;shanghai&quot;);</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name|address                                    |</span><br><span class="line"><span class="comment">----|-------------------------------------------|</span></span><br><span class="line">pk  |["beijing","shanghai","tianjin","hangzhou"]|</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><p><code>map</code>的建表格式:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hive_map(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>, members <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;, age <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line">COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'#'</span></span><br><span class="line"><span class="keyword">MAP</span> <span class="keyword">KEYS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">':'</span>;</span><br></pre></td></tr></table></figure>

<p>注意: <code>COLLECTION ITEMS TERMINATED BY &#39;#&#39;</code>是指定map键值对中的元素分隔符，<code>MAP KEYS TERMINATED BY &#39;:&#39;</code>是指定key和value的分隔符</p>
<h4 id="准备数据-1"><a href="#准备数据-1" class="headerlink" title="准备数据"></a>准备数据</h4><ol>
<li><p>第一步准备数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1,zhangsan,father:xiaoming#mother:xiaohuang#brother:xiaoxu,28</span><br><span class="line">2,lisi,father:mayun#mother:huangyi#brother:guanyu,22</span><br><span class="line">3,wangwu,father:wangjianlin#mother:ruhua#sister:jingtian,29</span><br><span class="line">4,mayun,father:mayongzhen#mother:angelababy,26</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> map_(</span><br><span class="line">	<span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">	<span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">	family <span class="keyword">map</span>&lt;<span class="keyword">String</span>,<span class="keyword">String</span>&gt;,</span><br><span class="line">	age <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"#"</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">":"</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第三步加载数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/hive_map.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> map_;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第四步查询数据是否加载成功</p>
<p><code>select * from map_;</code></p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">id|name    |family                                                       |age|</span><br><span class="line"><span class="comment">--|--------|-------------------------------------------------------------|---|</span></span><br><span class="line"> 1|zhangsan|&#123;"father":"xiaoming","mother":"xiaohuang","brother":"xiaoxu"&#125;| 28|</span><br><span class="line"> 2|lisi    |&#123;"father":"mayun","mother":"huangyi","brother":"guanyu"&#125;     | 22|</span><br><span class="line"> 3|wangwu  |&#123;"father":"wangjianlin","mother":"ruhua","sister":"jingtian"&#125;| 29|</span><br><span class="line"> 4|mayun   |&#123;"father":"mayongzhen","mother":"angelababy"&#125;                | 26|</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="对map键值对的”取”"><a href="#对map键值对的”取”" class="headerlink" title="对map键值对的”取”"></a>对map键值对的”取”</h4><ol>
<li><p>根据key取value</p>
<p><code>select name,family[&#39;father&#39;] as father,age from map_;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name    |father     |age|</span><br><span class="line"><span class="comment">--------|-----------|---|</span></span><br><span class="line">zhangsan|xiaoming   | 28|</span><br><span class="line">lisi    |mayun      | 22|</span><br><span class="line">wangwu  |wangjianlin| 29|</span><br><span class="line">mayun   |mayongzhen | 26|</span><br></pre></td></tr></table></figure>
</li>
<li><p>取出所有key集合</p>
<p><code>select name,map_keys(family) as map_keys,age from map_;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name    |map_keys                     |age|</span><br><span class="line"><span class="comment">--------|-----------------------------|---|</span></span><br><span class="line">zhangsan|["father","mother","brother"]| 28|</span><br><span class="line">lisi    |["father","mother","brother"]| 22|</span><br><span class="line">wangwu  |["father","mother","sister"] | 29|</span><br><span class="line">mayun   |["father","mother"]          | 26|</span><br></pre></td></tr></table></figure>
</li>
<li><p>取出所有的value集合</p>
<p><code>select name,map_values(family) as map_values,age from map_;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name    |map_values                        |age|</span><br><span class="line"><span class="comment">--------|----------------------------------|---|</span></span><br><span class="line">zhangsan|["xiaoming","xiaohuang","xiaoxu"] | 28|</span><br><span class="line">lisi    |["mayun","huangyi","guanyu"]      | 22|</span><br><span class="line">wangwu  |["wangjianlin","ruhua","jingtian"]| 29|</span><br><span class="line">mayun   |["mayongzhen","angelababy"]       | 26|</span><br></pre></td></tr></table></figure>
</li>
<li><p>求key的数量(对key集合求size)</p>
<p><code>select name,size(map_keys(family)) as key_size,age from map_;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">name    |key_size|age|</span><br><span class="line"><span class="comment">--------|--------|---|</span></span><br><span class="line">zhangsan|       3| 28|</span><br><span class="line">lisi    |       3| 22|</span><br><span class="line">wangwu  |       3| 29|</span><br><span class="line">mayun   |       2| 26|</span><br></pre></td></tr></table></figure>
</li>
<li><p>求key是否包含某个元素(对key的集合求contains)</p>
<p><code>select name,family,age from map_ where array_contains((map_keys(family)),&quot;brother&quot;);</code></p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">name    |family                                                       |age|</span><br><span class="line"><span class="comment">--------|-------------------------------------------------------------|---|</span></span><br><span class="line">zhangsan|&#123;"father":"xiaoming","mother":"xiaohuang","brother":"xiaoxu"&#125;| 28|</span><br><span class="line">lisi    |&#123;"father":"mayun","mother":"huangyi","brother":"guanyu"&#125;     | 22|</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="structs"><a href="#structs" class="headerlink" title="structs"></a>structs</h3><p><code>structs</code>的建表格式:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hive_struct(</span><br><span class="line">ip <span class="keyword">string</span>, info <span class="keyword">struct</span>&lt;<span class="keyword">name</span>:<span class="keyword">string</span>,age:<span class="built_in">int</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'#'</span></span><br><span class="line">COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">':'</span>;</span><br></pre></td></tr></table></figure>

<p>注意: <code>COLLECTION ITEMS TERMINATED BY &#39;:&#39;</code>是指定每个元素之间的分隔符</p>
<h4 id="准备数据-2"><a href="#准备数据-2" class="headerlink" title="准备数据"></a>准备数据</h4><ol>
<li><p>第一步准备数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">192.168.1.1#zhangsan:40</span><br><span class="line">192.168.1.2#lisi:50</span><br><span class="line">192.168.1.3#wangwu:60</span><br><span class="line">192.168.1.4#zhaoliu:70</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> structs_(</span><br><span class="line">	address <span class="keyword">string</span>,</span><br><span class="line">	<span class="string">`user`</span> <span class="keyword">struct</span>&lt;<span class="keyword">name</span>:<span class="keyword">string</span>,age:<span class="built_in">int</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"#"</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">":"</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第三步加载数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/hive_struct.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> structs_;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第四步查询数据是否装载成功</p>
<p><code>select * from structs_;</code></p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">address    |user                          |</span><br><span class="line"><span class="comment">-----------|------------------------------|</span></span><br><span class="line">192.168.1.1|[&#123;"name":"zhangsan","age":40&#125;]|</span><br><span class="line">192.168.1.2|[&#123;"name":"lisi","age":50&#125;]    |</span><br><span class="line">192.168.1.3|[&#123;"name":"wangwu","age":60&#125;]  |</span><br><span class="line">192.168.1.4|[&#123;"name":"zhaoliu","age":70&#125;] |</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="对structs结构体的”取”"><a href="#对structs结构体的”取”" class="headerlink" title="对structs结构体的”取”"></a>对structs结构体的”取”</h4><ol>
<li><p>取出结构体中的元素</p>
<p><code>select address,user.name as name from structs_;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">address    |name    |</span><br><span class="line"><span class="comment">-----------|--------|</span></span><br><span class="line">192.168.1.1|zhangsan|</span><br><span class="line">192.168.1.2|lisi    |</span><br><span class="line">192.168.1.3|wangwu  |</span><br><span class="line">192.168.1.4|zhaoliu |</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="行列互转"><a href="#行列互转" class="headerlink" title="行列互转"></a>行列互转</h2><p>这是一个复杂数据类型的综合练习</p>
<h3 id="准备数据-3"><a href="#准备数据-3" class="headerlink" title="准备数据"></a>准备数据</h3><ol>
<li><p>第一步准备数据</p>
<p>数据1: session点击广告记录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">11      ad_101  2014-05-01 06:01:12.334+01</span><br><span class="line">22      ad_102  2014-05-01 07:28:12.342+01</span><br><span class="line">33      ad_103  2014-05-01 07:50:12.33+01</span><br><span class="line">11      ad_104  2014-05-01 09:27:12.33+01</span><br><span class="line">22      ad_103  2014-05-01 09:03:12.324+01</span><br><span class="line">33      ad_102  2014-05-02 19:10:12.343+01</span><br><span class="line">11      ad_101  2014-05-02 09:07:12.344+01</span><br><span class="line">35      ad_105  2014-05-03 11:07:12.339+01</span><br><span class="line">22      ad_104  2014-05-03 12:59:12.743+01</span><br><span class="line">77      ad_103  2014-05-03 18:04:12.355+01</span><br><span class="line">99      ad_102  2014-05-04 00:36:39.713+01</span><br><span class="line">33      ad_101  2014-05-04 19:10:12.343+01</span><br><span class="line">11      ad_101  2014-05-05 09:07:12.344+01</span><br><span class="line">35      ad_102  2014-05-05 11:07:12.339+01</span><br><span class="line">22      ad_103  2014-05-05 12:59:12.743+01</span><br><span class="line">77      ad_104  2014-05-05 18:04:12.355+01</span><br><span class="line">99      ad_105  2014-05-05 20:36:39.713+01</span><br></pre></td></tr></table></figure>

<p>数据2: 广告的详情</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ad_101  http://www.google.com   catalog8|catalog1</span><br><span class="line">ad_102  http://www.sohu.com     catalog6|catalog3</span><br><span class="line">ad_103  http://www.baidu.com    catalog7</span><br><span class="line">ad_104  http://www.qq.com       catalog5|catalog1|catalog4|catalog9</span><br><span class="line">ad_105  http://sina.com</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步创建表</p>
<p>表1: 动作表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> click_tbl(</span><br><span class="line">	cookie_id <span class="keyword">string</span>,  </span><br><span class="line">	ad_id <span class="built_in">int</span>,</span><br><span class="line">	<span class="built_in">time</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure>

<p>表2: 广告表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ad_tbl(</span><br><span class="line">	ad_id <span class="keyword">string</span>,</span><br><span class="line">	<span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">	catalogs <span class="built_in">array</span>&lt;<span class="keyword">String</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"|"</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>第三步加载数据</p>
<p>给动作表加载数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/click_log.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> click_tbl;</span><br></pre></td></tr></table></figure>

<p>给广告表加载数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/ad_list.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> ad_tbl;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第四步查询数据是否装载成功</p>
<p>动作表查询</p>
<p><code>select * from click_tbl;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id |time                      |</span><br><span class="line"><span class="comment">---------|------|--------------------------|</span></span><br><span class="line">       11|ad_101|2014-05-01 06:01:12.334+01|</span><br><span class="line">       22|ad_102|2014-05-01 07:28:12.342+01|</span><br><span class="line">       33|ad_103|2014-05-01 07:50:12.33+01 |</span><br><span class="line">       11|ad_104|2014-05-01 09:27:12.33+01 |</span><br><span class="line">       22|ad_103|2014-05-01 09:03:12.324+01|</span><br><span class="line">       33|ad_102|2014-05-02 19:10:12.343+01|</span><br><span class="line">       11|ad_101|2014-05-02 09:07:12.344+01|</span><br><span class="line">       35|ad_105|2014-05-03 11:07:12.339+01|</span><br><span class="line">       22|ad_104|2014-05-03 12:59:12.743+01|</span><br><span class="line">       77|ad_103|2014-05-03 18:04:12.355+01|</span><br><span class="line">       99|ad_102|2014-05-04 00:36:39.713+01|</span><br><span class="line">       33|ad_101|2014-05-04 19:10:12.343+01|</span><br><span class="line">       11|ad_101|2014-05-05 09:07:12.344+01|</span><br><span class="line">       35|ad_102|2014-05-05 11:07:12.339+01|</span><br><span class="line">       22|ad_103|2014-05-05 12:59:12.743+01|</span><br><span class="line">       77|ad_104|2014-05-05 18:04:12.355+01|</span><br><span class="line">       99|ad_105|2014-05-05 20:36:39.713+01|</span><br></pre></td></tr></table></figure>

<p>广告把查询</p>
<p><code>select * from ad_tbl;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ad_id |url                  |catalogs                               |</span><br><span class="line"><span class="comment">------|---------------------|---------------------------------------|</span></span><br><span class="line">ad_101|http://www.google.com|["catalog8|catalog1"]                  |</span><br><span class="line">ad_102|http://www.sohu.com  |["catalog6|catalog3"]                  |</span><br><span class="line">ad_103|http://www.baidu.com |["catalog7"]                           |</span><br><span class="line">ad_104|http://www.qq.com    |["catalog5|catalog1|catalog4|catalog9"]|</span><br><span class="line">ad_105|http://sina.com      |NULL                                   |</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><h4 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h4><p>查询每个人访问的广告</p>
<ol>
<li><p>去重(collect_set): </p>
<p><code>select cookie_id,collect_set(ad_id) ad_id from click_tbl group by cookie_id;</code></p>
</li>
</ol>
<ul>
<li><p>结果:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id                       |</span><br><span class="line"><span class="comment">---------|----------------------------|</span></span><br><span class="line">       11|["ad_101","ad_104"]         |</span><br><span class="line">       22|["ad_102","ad_103","ad_104"]|</span><br><span class="line">       33|["ad_103","ad_102","ad_101"]|</span><br><span class="line">       35|["ad_105","ad_102"]         |</span><br><span class="line">       77|["ad_103","ad_104"]         |</span><br><span class="line">       99|["ad_102","ad_105"]         |</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol start="2">
<li><p>不去重(collect_list):</p>
<p><code>select cookie_id,collect_list(ad_id) ad_id from click_tbl group by cookie_id;</code></p>
</li>
</ol>
<ul>
<li><p>结果:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id                                |</span><br><span class="line"><span class="comment">---------|-------------------------------------|</span></span><br><span class="line">       11|["ad_101","ad_104","ad_101","ad_101"]|</span><br><span class="line">       22|["ad_102","ad_103","ad_104","ad_103"]|</span><br><span class="line">       33|["ad_103","ad_102","ad_101"]         |</span><br><span class="line">       35|["ad_105","ad_102"]                  |</span><br><span class="line">       77|["ad_103","ad_104"]                  |</span><br><span class="line">       99|["ad_102","ad_105"]                  |</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="聚合查询"><a href="#聚合查询" class="headerlink" title="聚合查询"></a>聚合查询</h4><ol>
<li><p>查询每个人访问相同广告的次数</p>
<p><code>select cookie_id,ad_id,count(1) amount from click_tbl group by  cookie_id,ad_id;</code></p>
<p>结果:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id |amount|</span><br><span class="line"><span class="comment">---------|------|------|</span></span><br><span class="line">       11|ad_101|     3|</span><br><span class="line">       11|ad_104|     1|</span><br><span class="line">       22|ad_102|     1|</span><br><span class="line">       22|ad_103|     2|</span><br><span class="line">       22|ad_104|     1|</span><br><span class="line">       33|ad_101|     1|</span><br><span class="line">       33|ad_102|     1|</span><br><span class="line">       33|ad_103|     1|</span><br><span class="line">       35|ad_102|     1|</span><br><span class="line">       35|ad_105|     1|</span><br><span class="line">       77|ad_103|     1|</span><br><span class="line">       77|ad_104|     1|</span><br><span class="line">       99|ad_102|     1|</span><br><span class="line">       99|ad_105|     1|</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询每个人访问的广告详情</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">	click_tmp.cookie_id,click_tmp.ad_id,ad_tbl.url,ad_tbl.catalogs </span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	ad_tbl</span><br><span class="line"><span class="keyword">JOIN</span></span><br><span class="line">	(<span class="keyword">select</span> cookie_id,ad_id,<span class="keyword">count</span>(<span class="number">1</span>) amount <span class="keyword">from</span> click_tbl <span class="keyword">group</span> <span class="keyword">by</span>  cookie_id,ad_id) click_tmp</span><br><span class="line"><span class="keyword">on</span> click_tmp.ad_id=ad_tbl.ad_id;</span><br></pre></td></tr></table></figure>

<p>结果:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cookie_id|ad_id |url                  |catalogs                                     |</span><br><span class="line"><span class="comment">---------|------|---------------------|---------------------------------------------|</span></span><br><span class="line">       11|ad_101|http://www.google.com|["catalog8","catalog1"]                      |</span><br><span class="line">       11|ad_104|http://www.qq.com    |["catalog5","catalog1","catalog4","catalog9"]|</span><br><span class="line">       22|ad_102|http://www.sohu.com  |["catalog6","catalog3"]                      |</span><br><span class="line">       22|ad_103|http://www.baidu.com |["catalog7"]                                 |</span><br><span class="line">       22|ad_104|http://www.qq.com    |["catalog5","catalog1","catalog4","catalog9"]|</span><br><span class="line">       33|ad_101|http://www.google.com|["catalog8","catalog1"]                      |</span><br><span class="line">       33|ad_102|http://www.sohu.com  |["catalog6","catalog3"]                      |</span><br><span class="line">       33|ad_103|http://www.baidu.com |["catalog7"]                                 |</span><br><span class="line">       35|ad_102|http://www.sohu.com  |["catalog6","catalog3"]                      |</span><br><span class="line">       35|ad_105|http://sina.com      |NULL                                         |</span><br><span class="line">       77|ad_103|http://www.baidu.com |["catalog7"]                                 |</span><br><span class="line">       77|ad_104|http://www.qq.com    |["catalog5","catalog1","catalog4","catalog9"]|</span><br><span class="line">       99|ad_102|http://www.sohu.com  |["catalog6","catalog3"]                      |</span><br><span class="line">       99|ad_105|http://sina.com      |NULL                                         |</span><br></pre></td></tr></table></figure>
</li>
<li><p>把catalogs中的数据元素排序</p>
<p><code>select ad_id,url,sort_array(catalogs) as catalogs from ad_tbl;</code></p>
<p>结果:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ad_id |url                  |catalogs                                     |</span><br><span class="line"><span class="comment">------|---------------------|---------------------------------------------|</span></span><br><span class="line">ad_101|http://www.google.com|["catalog1","catalog8"]                      |</span><br><span class="line">ad_102|http://www.sohu.com  |["catalog3","catalog6"]                      |</span><br><span class="line">ad_103|http://www.baidu.com |["catalog7"]                                 |</span><br><span class="line">ad_104|http://www.qq.com    |["catalog1","catalog4","catalog5","catalog9"]|</span><br><span class="line">ad_105|http://sina.com      |NULL                                         |</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h4><p>查询每个广告详情</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	ad_id,<span class="keyword">catalog</span></span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">	ad_tbl</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(catalogs) t <span class="keyword">as</span> <span class="keyword">catalog</span>;</span><br></pre></td></tr></table></figure>

<p>结果:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ad_id |catalog |</span><br><span class="line"><span class="comment">------|--------|</span></span><br><span class="line">ad_101|catalog8|</span><br><span class="line">ad_101|catalog1|</span><br><span class="line">ad_102|catalog6|</span><br><span class="line">ad_102|catalog3|</span><br><span class="line">ad_103|catalog7|</span><br><span class="line">ad_104|catalog5|</span><br><span class="line">ad_104|catalog1|</span><br><span class="line">ad_104|catalog4|</span><br><span class="line">ad_104|catalog9|</span><br></pre></td></tr></table></figure>

<p><code>注意:</code> 如果ad_tbl的catalogs字段是String类型的，那么在explode炸开的时候要转换成数组，也就是用split把字段元素按’|’切分开返回一个数组</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">lateral view outer explode(split(catalogs,'\\|')) t as catalog</span><br></pre></td></tr></table></figure>

<h2 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h2><p>查用函数的用法查询: <code>desc function extended 函数名;</code></p>
<h3 id="时间函数"><a href="#时间函数" class="headerlink" title="时间函数"></a>时间函数</h3><p><code>current_date:</code> 返回当前日期</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">current_date</span>		<span class="comment">--2019-12-21</span></span><br></pre></td></tr></table></figure>

<p><code>current_timestamp:</code> 返回当前时间</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">current_timestamp</span>	<span class="comment">--2019-12-21 02:23:44</span></span><br></pre></td></tr></table></figure>

<p><code>unix_timestamp:</code> 时间转秒</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">unix_timestamp</span>(<span class="string">'2019-08-15 16:40:00'</span>,<span class="string">'yyyy-MM-dd HH:mm:ss'</span>)   <span class="comment">--1565858400</span></span><br></pre></td></tr></table></figure>

<p><code>from_unixtime:</code> 秒转时间</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">1565858389</span>,<span class="string">'yyyy-MM-dd HH:mm:ss'</span>)  <span class="comment">--2019-08-15 16:39:49</span></span><br><span class="line"><span class="keyword">select</span> from_unixtime(<span class="keyword">cast</span>(<span class="keyword">substr</span>(<span class="number">1553184000488</span>,<span class="number">1</span>,<span class="number">10</span>) <span class="keyword">as</span> <span class="built_in">int</span>),<span class="string">'yyyy-MM-dd HH:mm:ss'</span>)  <span class="comment">--2019-03-22 00:00:00</span></span><br><span class="line"><span class="keyword">select</span> from_unixtime(<span class="keyword">unix_timestamp</span>(),<span class="string">'yyyy-MM-dd HH:mm:ss'</span>)   <span class="comment">-- 2019-08-15 17:18:55</span></span><br></pre></td></tr></table></figure>

<p><code>to_date:</code> 返回日期</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">to_date</span>(<span class="string">'2009-07-30 04:17:52'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;		<span class="comment">--2009-07-30</span></span><br></pre></td></tr></table></figure>

<p><code>year:</code> 返回年份</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">year</span>(<span class="string">'2009-07-30'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;		<span class="comment">--2009</span></span><br></pre></td></tr></table></figure>

<p><code>month:</code> 返回月份</p>
<p><code>day:</code> 返回日期</p>
<p><code>hour:</code> 返回小时</p>
<p><code>minute:</code> 返回分钟</p>
<p><code>second:</code> 返回毫秒</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">second</span>(<span class="string">'2009-07-30 12:58:59'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>; 		<span class="comment">--59</span></span><br></pre></td></tr></table></figure>

<p><code>date_add:</code> 增加指定时间</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">date_add</span>(<span class="string">'2009-07-30'</span>, <span class="number">1</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;		<span class="comment">--2009-07-31</span></span><br></pre></td></tr></table></figure>

<p><code>date_sub:</code> 减少指定时间</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">date_sub</span>(<span class="string">'2009-07-30'</span>, <span class="number">1</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;		<span class="comment">--2009-07-29</span></span><br></pre></td></tr></table></figure>

<h3 id="数值函数"><a href="#数值函数" class="headerlink" title="数值函数"></a>数值函数</h3><p><code>round:</code> 返回指定范围的数值</p>
<p><code>ceil:</code> 返回天花板，取最大的整数值</p>
<p><code>floor:</code> 返回地板，去最小的整数值</p>
<p><code>abs:</code> 返回绝对值</p>
<p><code>least:</code> 数列中最小值 ==&gt; min</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">least</span>(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;		<span class="comment">--1</span></span><br></pre></td></tr></table></figure>

<h3 id="字符串函数"><a href="#字符串函数" class="headerlink" title="字符串函数"></a>字符串函数</h3><p><code>substr:</code> 返回截取字符串</p>
<p><code>concat:</code> 返回连接字符串</p>
<p><code>concat_ws:</code> 根据特定格式组合字符串</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">concat_ws</span>(<span class="string">'.'</span>, <span class="string">'www'</span>, <span class="built_in">array</span>(<span class="string">'facebook'</span>, <span class="string">'com'</span>)) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;	<span class="comment">--www.facebook.com</span></span><br></pre></td></tr></table></figure>

<p><code>length:</code> 返回字符串的长度，整数值</p>
<p><code>split:</code> 返回切分后的字符串数组</p>
<p><code>upper:</code> 字符转大写</p>
<p><code>lower:</code> 字符转小写</p>
<h3 id="Json处理函数"><a href="#Json处理函数" class="headerlink" title="Json处理函数"></a>Json处理函数</h3><ol>
<li><p>第一步准备数据</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"1"</span>,<span class="attr">"rate"</span>:<span class="string">"5"</span>,<span class="attr">"time"</span>:<span class="string">"978300760"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"2"</span>,<span class="attr">"rate"</span>:<span class="string">"4"</span>,<span class="attr">"time"</span>:<span class="string">"978702109"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"3"</span>,<span class="attr">"rate"</span>:<span class="string">"3"</span>,<span class="attr">"time"</span>:<span class="string">"978401968"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"4"</span>,<span class="attr">"rate"</span>:<span class="string">"4"</span>,<span class="attr">"time"</span>:<span class="string">"978300275"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"5"</span>,<span class="attr">"rate"</span>:<span class="string">"3"</span>,<span class="attr">"time"</span>:<span class="string">"978801091"</span>,<span class="attr">"userid"</span>:<span class="string">"1"</span>&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> rating_json(</span><br><span class="line">	<span class="keyword">json</span> <span class="keyword">string</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li><p>第三步加载数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/rating.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> rating_json;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第四步查看数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">json                                                    |</span><br><span class="line"><span class="comment">--------------------------------------------------------|</span></span><br><span class="line">&#123;"movie":"1","rate":"5","time":"978300760","userid":"1"&#125;|</span><br><span class="line">&#123;"movie":"2","rate":"4","time":"978702109","userid":"1"&#125;|</span><br><span class="line">&#123;"movie":"3","rate":"3","time":"978401968","userid":"1"&#125;|</span><br><span class="line">&#123;"movie":"4","rate":"4","time":"978300275","userid":"1"&#125;|</span><br><span class="line">&#123;"movie":"5","rate":"3","time":"978801091","userid":"1"&#125;|</span><br></pre></td></tr></table></figure>
</li>
<li><p>对数据进行处理</p>
<p><code>json_tuple:</code> 返回指定json文件的指定字段，返回的是字符串</p>
<p><code>select json_tuple(json,&quot;movie&quot;,&quot;rate&quot;,&quot;time&quot;,&quot;userid&quot;) as (movie,rate,time,userid) from rating_json;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">movie|rate|time     |userid|</span><br><span class="line"><span class="comment">-----|----|---------|------|</span></span><br><span class="line">1    |5   |978300760|1     |</span><br><span class="line">2    |4   |978702109|1     |</span><br><span class="line">3    |3   |978401968|1     |</span><br><span class="line">4    |4   |978300275|1     |</span><br><span class="line">5    |3   |978801091|1     |</span><br></pre></td></tr></table></figure>

<p><code>parse_url_tuple:</code> 返回指定url的指定字段，返回的是字符串</p>
<p><code>select parse_url_tuple(&quot;https://www.baidu.com/bigdate/spark?cookie_id=10&quot;,&#39;HOST&#39;,&#39;PATH&#39;,&#39;QUERY&#39;,&#39;QUERY:cookie_id&#39;);</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">c0           |c1            |c2          |c3|</span><br><span class="line"><span class="comment">-------------|--------------|------------|--|</span></span><br><span class="line">www.baidu.com|/bigdate/spark|cookie_id=10|10|</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="Null值处理函数"><a href="#Null值处理函数" class="headerlink" title="Null值处理函数"></a>Null值处理函数</h3><p><code>isnull:</code> 指定字段的元素如果为null则返回true</p>
<p><code>isnotnull:</code> 指定字段的元素如果不为null则返回true</p>
<p><code>elt:</code> 指定返回的元素</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">elt</span>(<span class="number">1</span>, <span class="string">'face'</span>, <span class="string">'book'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;		<span class="comment">--face</span></span><br></pre></td></tr></table></figure>

<p><code>nvl:</code> 替换null值</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> nvl(<span class="literal">null</span>,<span class="string">'bla'</span>) <span class="keyword">FROM</span> src <span class="keyword">LIMIT</span> <span class="number">1</span>;	<span class="comment">--bla</span></span><br></pre></td></tr></table></figure>

<h3 id="其他函数"><a href="#其他函数" class="headerlink" title="其他函数"></a>其他函数</h3><p><code>cast:</code> 转换数据类型</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cast(time as bigint)	<span class="comment">--时间转数值</span></span><br><span class="line">cast(string as date)	<span class="comment">--字符串转日期</span></span><br></pre></td></tr></table></figure>

<p><code>注意:</code> binary只能转string</p>
<h2 id="静动态分区表"><a href="#静动态分区表" class="headerlink" title="静动态分区表"></a>静动态分区表</h2><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><ol>
<li><p>数据源</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">1,jack,shanghai,20190129</span><br><span class="line">2,kevin,beijing,20190130</span><br><span class="line">3,lucas,hangzhou,20190129</span><br><span class="line">4,lily,hangzhou,20190130</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建源数据表（外表）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> prit_tbl(</span><br><span class="line">	<span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">	<span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">	address <span class="keyword">string</span>,</span><br><span class="line">	<span class="keyword">day</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载数据：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/partition.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> prit_tbl;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建分区表（外表）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> prit_tbl(</span><br><span class="line">	<span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">	<span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">	address <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">day</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="静态分区加载数据"><a href="#静态分区加载数据" class="headerlink" title="静态分区加载数据"></a>静态分区加载数据</h3><ul>
<li><p>静态分区缺点：每次写入都要明确指定分区日期。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> prit_tbl <span class="keyword">partition</span>(<span class="keyword">day</span>=<span class="string">"20190129"</span>) <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span>,address <span class="keyword">from</span> test_tbl <span class="keyword">where</span> <span class="keyword">day</span> = <span class="string">"20190129"</span> ;</span><br></pre></td></tr></table></figure>

<p><code>注意:</code> 并且在查询处不能包含分区字段day，否则会报错</p>
</li>
</ul>
<h3 id="动态分区加载数据"><a href="#动态分区加载数据" class="headerlink" title="动态分区加载数据"></a>动态分区加载数据</h3><ul>
<li><p>查看表分区</p>
<p><code>show partitions prit_tbl;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">partition   |</span><br><span class="line"><span class="comment">------------|</span></span><br><span class="line">day=20190129|</span><br><span class="line">day=20190130|</span><br></pre></td></tr></table></figure>
</li>
<li><p>自动识别分区，不需要明确指定</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> prit_tbl <span class="keyword">partition</span>(<span class="keyword">day</span>) <span class="keyword">select</span> * <span class="keyword">from</span> test_tbl;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询验证：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> prit_tbl;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> prit_tbl <span class="keyword">where</span> <span class="keyword">day</span> = <span class="number">20190129</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> prit_tbl <span class="keyword">where</span> <span class="keyword">day</span> = <span class="number">20190130</span>;</span><br></pre></td></tr></table></figure>

<p>HDFS web界面验证</p>
</li>
</ul>
<h3 id="分区注意"><a href="#分区注意" class="headerlink" title="分区注意"></a>分区注意</h3><ol>
<li><p>尽量不要是用动态分区，因为动态分区的时候，将会为每一个分区分配reducer数量，当分区数量多的时候，reducer数量将会增加，对服务器是一种灾难。</p>
</li>
<li><p>动态分区和静态分区的区别: 静态分区不管有没有数据都将会创建该分区，动态分区是有结果集将创建，否则不创建。</p>
</li>
<li><p>hive动态分区的严格模式和hive提供的<code>hive.mapred.mode的</code>严格模式,为了阻止用户不小心提交恶意<code>hql</code>。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive.mapred.mode=nostrict : strict</span><br></pre></td></tr></table></figure>

<p>如果该模式值为strict，将会阻止以下三种查询：</p>
<ol>
<li>对分区表查询，where中过滤字段不是分区字段。</li>
<li>笛卡尔积join查询，join查询语句，不带on条件 或者 where条件。</li>
<li>对order by查询，有order by的查询不带limit语句。</li>
</ol>
</li>
</ol>
<h2 id="桶表"><a href="#桶表" class="headerlink" title="桶表"></a>桶表</h2><h3 id="分桶的概念"><a href="#分桶的概念" class="headerlink" title="分桶的概念"></a>分桶的概念</h3><p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。</p>
<p>分桶是将数据集分解成更容易管理的若干部分的另一个技术。</p>
<p><code>分区针对的是数据的存储路径；分桶针对的是数据文件。</code></p>
<h3 id="分桶的好处"><a href="#分桶的好处" class="headerlink" title="分桶的好处"></a>分桶的好处</h3><ul>
<li>分桶规则：对分桶字段值进行哈希，哈希值除以桶的个数求余，余数决定了该条记录在哪个桶中，也就是余数相同的在一个桶中。</li>
<li>优点：<ol>
<li>提高join查询效率 </li>
<li>提高抽样效率</li>
</ol>
</li>
</ul>
<h3 id="分桶实践"><a href="#分桶实践" class="headerlink" title="分桶实践"></a>分桶实践</h3><ol start="2">
<li><p>准备数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1,name1</span><br><span class="line">2,name2</span><br><span class="line">3,name3</span><br><span class="line">4,name4</span><br><span class="line">5,name5</span><br><span class="line">6,name6</span><br><span class="line">7,name7</span><br><span class="line">8,name8</span><br><span class="line">9,name9</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建桶表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> bucket_tbl(</span><br><span class="line">	<span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">	<span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>) </span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建中间表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> mid_tbl(</span><br><span class="line">	<span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">	<span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载数据到中间表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/data/bucket.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> mid_tbl;</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置强制分桶</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set hive.enforce.bucketing = true; --Hive 2.x 不需要这一步</span><br><span class="line">set mapreduce.job.reduces=-1;</span><br></pre></td></tr></table></figure>

<p>在 Hive 0.x and 1.x 版本，必须使用设置 <code>hive.enforce.bucketing = true</code>，表示强制分桶，允许程序根据表结构自动选择正确数量的 Reducer 和 cluster by column 来进行分桶。</p>
</li>
<li><p>插入数据到分桶表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> bucket_tbl <span class="keyword">select</span> * <span class="keyword">from</span> mid_tbl;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看结果</p>
<p>HDFS：<code>桶是以文件的形式存在的，而不是像分区那样以文件夹的形式存在。</code></p>
<p><img src="https://upload-images.jianshu.io/upload_images/12267859-d7e00bb44b332b5b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/format/webp" alt="分桶文件"></p>
</li>
</ol>
<h3 id="有序的分桶表"><a href="#有序的分桶表" class="headerlink" title="有序的分桶表"></a>有序的分桶表</h3><ul>
<li><p>如果要按id升序排序可以这样建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_bucket_sorted (</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'ID'</span>, </span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'名字'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">comment</span> <span class="string">'测试分桶'</span></span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>) </span><br><span class="line">sorted <span class="keyword">by</span> (<span class="keyword">id</span>) </span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span> ;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> bucket_tbl(</span><br><span class="line">	<span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">	<span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>) sorted <span class="keyword">by</span> (<span class="keyword">id</span>) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure>

<p><code>注意:</code> 同样需要中间表insert插入数据</p>
</li>
<li><p>好处:</p>
<p>因为每个桶内的数据是排序的，这样每个桶进行连接时就变成了高效的归并排序</p>
</li>
</ul>
<h3 id="分桶抽样查询"><a href="#分桶抽样查询" class="headerlink" title="分桶抽样查询"></a>分桶抽样查询</h3><p>对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。</p>
<h4 id="SQL示例"><a href="#SQL示例" class="headerlink" title="SQL示例"></a>SQL示例</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from test_bucket tablesample (bucket 1 out of 2);</span><br><span class="line">OK</span><br><span class="line">8   name8</span><br><span class="line">4   name4</span><br><span class="line">2   name2</span><br><span class="line">6   name6</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from test tablesample (bucket 1 out of 2 on id);</span><br><span class="line">OK</span><br><span class="line">2   name2</span><br><span class="line">8   name8</span><br><span class="line">4   name4</span><br><span class="line">6   name6</span><br></pre></td></tr></table></figure>

<h4 id="区别"><a href="#区别" class="headerlink" title="区别:"></a>区别:</h4><ul>
<li>分桶表后面可以不带on 字段名，不带时默认的是按分桶字段,也可以带，而没有分桶的表则必须带</li>
<li>按分桶字段取样时，因为分桶表是直接去对应的桶中拿数据，在表比较大时会提高取样效率</li>
</ul>
<h4 id="语法"><a href="#语法" class="headerlink" title="语法:"></a>语法:</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">tablesample (bucket x out of y on id);</span><br></pre></td></tr></table></figure>

<p>x表示从哪个桶开始，y代表分几个桶，也可以理解分x为分子，y为分母，及将表分为y份（桶），取第x份（桶）</p>
<p>所以这时对于分桶表是有要求的，y为桶数的倍数或因子，</p>
<ul>
<li><p>x=1,y=2，取2(4/y)个bucket的数据，分别桶1和桶3(1+y)</p>
</li>
<li><p>x=1,y=4, 取1(4/y)个bucket的数据，即桶1</p>
</li>
<li><p>x=2,y=8, 取1/2(4/y)个bucket的数据，即桶1的一半</p>
<p> x的值必须小于等于y的值</p>
</li>
</ul>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/04/hive/4/">Hive数据类型&amp;DDL数据定义(增删查改)&amp;DML数据操作(导入导出)</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-04</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h2 id="Hive数据类型"><a href="#Hive数据类型" class="headerlink" title="Hive数据类型"></a>Hive数据类型</h2><h3 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h3><table>
<thead>
<tr>
<th>Hive数据类型</th>
<th>Java数据类型</th>
<th>长度</th>
<th>例子</th>
</tr>
</thead>
<tbody><tr>
<td>TINYINT</td>
<td>byte</td>
<td>1byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>SMALINT</td>
<td>short</td>
<td>2byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>INT</td>
<td>int</td>
<td>4byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BIGINT</td>
<td>long</td>
<td>8byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BOOLEAN</td>
<td>boolean</td>
<td>布尔类型，true或者false</td>
<td>TRUE FALSE</td>
</tr>
<tr>
<td>FLOAT</td>
<td>float</td>
<td>单精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>double</td>
<td>双精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>STRING</td>
<td>string</td>
<td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td>
<td>‘now is the time’ “for all good men”</td>
</tr>
<tr>
<td>TIMESTAMP</td>
<td></td>
<td>时间类型</td>
<td></td>
</tr>
<tr>
<td>BINARY</td>
<td></td>
<td>字节数组</td>
<td></td>
</tr>
</tbody></table>
<p>对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</p>
<h3 id="集合数据类型"><a href="#集合数据类型" class="headerlink" title="集合数据类型"></a>集合数据类型</h3><table>
<thead>
<tr>
<th>数据类型</th>
<th>描述</th>
<th>语法示例</th>
</tr>
</thead>
<tbody><tr>
<td>STRUCT</td>
<td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td>
<td>struct()  例如struct&lt;street:string,  city:string&gt;</td>
</tr>
<tr>
<td>MAP</td>
<td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td>
<td>map()  例如map&lt;string, int&gt;</td>
</tr>
<tr>
<td>ARRAY</td>
<td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’,  ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td>
<td>Array()  例如array<string></td>
</tr>
</tbody></table>
<p>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p>
<ul>
<li><p>案例实操</p>
<ol>
<li><p>假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问的格式为</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"name"</span>: <span class="string">"songsong"</span>,</span><br><span class="line">    <span class="attr">"friends"</span>: [<span class="string">"bingbing"</span> , <span class="string">"lili"</span>] ,       <span class="comment">//列表Array, </span></span><br><span class="line">    <span class="attr">"children"</span>: &#123;                      <span class="comment">//键值Map,</span></span><br><span class="line">        <span class="attr">"xiao song"</span>: <span class="number">18</span> ,</span><br><span class="line">        <span class="attr">"xiaoxiao song"</span>: <span class="number">19</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">"address"</span>: &#123;                      <span class="comment">//结构Struct,</span></span><br><span class="line">        <span class="attr">"street"</span>: <span class="string">"hui long guan"</span> ,</span><br><span class="line">        <span class="attr">"city"</span>: <span class="string">"beijing"</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>基于上述数据结构，我们在Hive里创建对应的表，并导入数据。 创建本地测试文件test.txt</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing</span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</span><br></pre></td></tr></table></figure>

<p>注意：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。</p>
</li>
<li><p>Hive上创建测试表test</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">test</span>(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">friends <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">children <span class="keyword">map</span>&lt;<span class="keyword">string</span>, <span class="built_in">int</span>&gt;,</span><br><span class="line">address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>, city:<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'_'</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">':'</span></span><br><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span>;	<span class="comment">#默认"\n"</span></span><br></pre></td></tr></table></figure>

<p>字段解释：</p>
<p><code>row format delimited fields terminated by &#39;,&#39;</code>      – 列分隔符</p>
<p><code>collection items terminated by &#39;_&#39;</code>                             – MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</p>
<p><code>map keys terminated by &#39;:&#39;</code>                                              – MAP中的key与value的分隔符</p>
<p><code>lines terminated by &#39;\n&#39;;</code>                                                – 行分隔符</p>
</li>
<li><p>导入文本数据到测试表</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath ‘/opt/module/datas/test.txt’into table test</span><br></pre></td></tr></table></figure>
</li>
<li><p>访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select friends[1],children['xiao song'],address.city from test</span><br><span class="line">where name="songsong";</span><br><span class="line">OK</span><br><span class="line">_c0     _c1     city</span><br><span class="line">lili    18      beijing</span><br><span class="line">Time taken: 0.076 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ul>
<h2 id="DDL数据定义"><a href="#DDL数据定义" class="headerlink" title="DDL数据定义"></a>DDL数据定义</h2><h3 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] database_name</span><br><span class="line">[<span class="keyword">COMMENT</span> database_comment]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[<span class="keyword">WITH</span> DBPROPERTIES (property_name=property_value, ...)];</span><br></pre></td></tr></table></figure>

<ul>
<li>创建一个数据库，数据库在HDFS上的默认存储路径是/user/hive/warehouse/*.db。</li>
</ul>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br></pre></td></tr></table></figure>

<ul>
<li>避免要创建的数据库已经存在错误，增加if not exists判断。（标准写法）</li>
</ul>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already exists</span><br><span class="line">hive (default)&gt; create database if not exists db_hive;</span><br></pre></td></tr></table></figure>

<ul>
<li>创建一个数据库，指定数据库在HDFS上存放的位置</li>
</ul>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive2 location '/db_hive2.db';</span><br></pre></td></tr></table></figure>

<h3 id="查询数据库"><a href="#查询数据库" class="headerlink" title="查询数据库"></a>查询数据库</h3><h4 id="显示数据库"><a href="#显示数据库" class="headerlink" title="显示数据库"></a>显示数据库</h4><ol>
<li><p>显示数据库</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure>
</li>
<li><p>过滤显示查询的数据库</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; show databases like 'db_hive*';</span><br><span class="line">OK</span><br><span class="line">db_hive</span><br><span class="line">db_hive_1</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="查看数据库详情"><a href="#查看数据库详情" class="headerlink" title="查看数据库详情"></a>查看数据库详情</h4><ol>
<li><p>显示数据库信息</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; desc database db_hive;</span><br><span class="line">OK</span><br><span class="line">db_hive		hdfs://aliyun:9000/user/hive/warehouse/db_hive.db	hadoopUSER</span><br></pre></td></tr></table></figure>
</li>
<li><p>显示数据库详细信息，extended</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br><span class="line">OK</span><br><span class="line">db_hive		hdfs://aliyun:9000/user/hive/warehouse/db_hive.db	hadoopUSER</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="切换当前数据库"><a href="#切换当前数据库" class="headerlink" title="切换当前数据库"></a>切换当前数据库</h4><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; use db_hive;</span><br></pre></td></tr></table></figure>

<h3 id="修改数据库"><a href="#修改数据库" class="headerlink" title="修改数据库"></a>修改数据库</h3><p>用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter database db_hive set dbproperties('createtime'='20170830');</span><br></pre></td></tr></table></figure>

<p>在hive中查看修改结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br><span class="line">db_name <span class="keyword">comment</span> location        owner_name      owner_type      <span class="keyword">parameters</span></span><br><span class="line">db_hive         hdfs://aliyun:<span class="number">8020</span>/<span class="keyword">user</span>/hive/warehouse/db_hive.db    hadoop <span class="keyword">USER</span>    &#123;createtime=<span class="number">20170830</span>&#125;</span><br></pre></td></tr></table></figure>

<h3 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h3><ul>
<li><p>删除空数据库</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt;drop database db_hive2;</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果删除的数据库不存在，最好采用 if exists判断数据库是否存在</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: SemanticException [Error 10072]: Database does not exist: db_hive</span><br><span class="line">hive&gt; drop database if exists db_hive2;</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果数据库不为空，可以采用cascade命令，强制删除</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database db_hive is not empty. One or more tables exist.)</span><br><span class="line">hive&gt; drop database db_hive cascade;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><ul>
<li><p>建表语法</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name </span><br><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment] </span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) </span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS] </span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] </span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name=property_value, ...)]</span><br><span class="line">[<span class="keyword">AS</span> select_statement]</span><br></pre></td></tr></table></figure>
</li>
<li><p>字段解释说明 </p>
<ol>
<li><p><code>CREATE TABLE</code> 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。</p>
</li>
<li><p><code>EXTERNAL</code>关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实际数据的路径（LOCATION），在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</p>
</li>
<li><p><code>COMMENT</code>为表和列添加注释。</p>
</li>
<li><p><code>PARTITIONED BY</code>创建分区表</p>
</li>
<li><p><code>CLUSTERED BY</code>创建分桶表</p>
</li>
<li><p><code>SORTED BY</code>不常用，对桶中的一个或多个列另外排序</p>
</li>
<li><p><code>ROW FORMAT</code> </p>
<p><code>DELIMITED [FIELDS TERMINATED BY char]</code> </p>
<p><code>[COLLECTION ITEMS TERMINATED BY char]</code></p>
<p><code>[MAP KEYS TERMINATED BY char]</code> </p>
<p><code>[LINES TERMINATED BY char]</code> </p>
<p><code>SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</code></p>
<p>用户在建表的时候可以自定义<code>SerDe</code>或者使用自带的<code>SerDe</code>。如果没有指定<code>ROW FORMAT</code> 或者<code>ROW FORMAT DELIMITED</code>，将会使用自带的<code>SerDe</code>。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的<code>SerDe</code>，Hive通过<code>SerDe</code>确定表的具体的列的数据。</p>
<p><code>SerDe</code>是<code>Serialize</code>/<code>Deserilize</code>的简称， hive使用<code>Serde</code>进行行对象的序列与反序列化。</p>
</li>
<li><p><code>STORED AS</code>指定存储文件类型</p>
<p>常用的存储文件类型：<code>SEQUENCEFILE</code>（二进制序列文件）、<code>TEXTFILE</code>（文本）、<code>RCFILE</code>（列式存储格式文件）</p>
<p>如果文件数据是纯文本，可以使用<code>STORED AS TEXTFILE</code>。如果数据需要压缩，使用 <code>STORED AS</code> <code>SEQUENCEFILE</code>。</p>
</li>
<li><p><code>LOCATION</code> ：指定表在HDFS上的存储位置。</p>
</li>
<li><p><code>AS</code>：后跟查询语句，根据查询结果创建表。</p>
</li>
<li><p><code>LIKE</code>允许用户复制现有的表结构，但是不复制数据。</p>
</li>
</ol>
</li>
</ul>
<h4 id="管理表"><a href="#管理表" class="headerlink" title="管理表"></a>管理表</h4><ul>
<li><p>理论</p>
<p>默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。 当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据。</p>
</li>
<li><p>案例实操</p>
<ol>
<li><p>普通创建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student2(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile</span><br><span class="line">location <span class="string">'/user/hive/warehouse/student2'</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据查询结果创建表（查询的结果会添加到新创建的表中）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student3 <span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据已经存在的表结构创建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student4 <span class="keyword">like</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询表的类型</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ul>
<h4 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h4><ul>
<li><p>理论</p>
<p>因为表是外部表，所以Hive并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。</p>
</li>
<li><p>管理表和外部表的使用场景</p>
<p>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p>
</li>
<li><p>案例实操</p>
<p>分别创建部门和员工外部表，并向表中导入数据。</p>
<ol>
<li><p>上传数据到HDFS</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir /student;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>建表语句</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create external table stu (</span><br><span class="line">id int, </span><br><span class="line">name string) </span><br><span class="line">row format delimited fields terminated by '\t' </span><br><span class="line">location '/student';</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看创建的表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from stu_external;</span><br><span class="line">OK</span><br><span class="line">stu_external.id stu_external.name</span><br><span class="line">1001    lisi</span><br><span class="line">1002    wangwu</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看表格式化数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted dept;</span><br><span class="line">Table Type:             EXTERNAL_TABLE</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除外部表</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; drop table stu_external;</span><br></pre></td></tr></table></figure>

<p>外部表删除后，hdfs中的数据还在，但是metadata中stu_external的元数据已被删除</p>
</li>
</ol>
</li>
</ul>
<h4 id="管理表与外部表的互相转换"><a href="#管理表与外部表的互相转换" class="headerlink" title="管理表与外部表的互相转换"></a>管理表与外部表的互相转换</h4><ol>
<li><p>查询表的类型</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改内部表student2为外部表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'TRUE'</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询表的类型</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             EXTERNAL_TABLE</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改外部表student2为内部表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'FALSE'</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询表的类型</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure>

<p>注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写！</p>
</li>
</ol>
<h3 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h3><p>分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。</p>
<h4 id="分区表基本操作"><a href="#分区表基本操作" class="headerlink" title="分区表基本操作"></a>分区表基本操作</h4><ol>
<li><p>引入分区表（需要根据日期对日志进行管理）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">/user/hive/warehouse/log_partition/20170702/20170702.log</span><br><span class="line">/user/hive/warehouse/log_partition/20170703/20170703.log</span><br><span class="line">/user/hive/warehouse/log_partition/20170704/20170704.log</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建分区表语法</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table dept (</span><br><span class="line">deptno int, </span><br><span class="line">dname string, </span><br><span class="line">loc string</span><br><span class="line">)</span><br><span class="line">partitioned by (month string)</span><br><span class="line">row format delimited fields terminated by '\t';</span><br></pre></td></tr></table></figure>

<p><code>注意：</code> 分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。</p>
</li>
<li><p>加载数据到分区表中</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept partition(month='201709');</span><br><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept partition(month='201708');</span><br><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept partition(month='201707’);</span><br></pre></td></tr></table></figure>

<p><code>注意：</code> 分区表加载数据时，必须指定分区</p>
</li>
<li><p>查询分区表中数据</p>
<ol>
<li><p>单分区查询</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month='201709';</span><br></pre></td></tr></table></figure>
</li>
<li><p>多分区联合查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month='201709'</span><br><span class="line">              union all</span><br><span class="line">              <span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201708'</span></span><br><span class="line">              <span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">              <span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201707'</span>;</span><br><span class="line"></span><br><span class="line">_u3.deptno      _u3.dname       _u3.loc _u3.month</span><br><span class="line">10      ACCOUNTING      NEW YORK        201707</span><br><span class="line">10      ACCOUNTING      NEW YORK        201708</span><br><span class="line">10      ACCOUNTING      NEW YORK        201709</span><br><span class="line">20      RESEARCH        DALLAS  201707</span><br><span class="line">20      RESEARCH        DALLAS  201708</span><br><span class="line">20      RESEARCH        DALLAS  201709</span><br><span class="line">30      SALES   CHICAGO 201707</span><br><span class="line">30      SALES   CHICAGO 201708</span><br><span class="line">30      SALES   CHICAGO 201709</span><br><span class="line">40      OPERATIONS      BOSTON  201707</span><br><span class="line">40      OPERATIONS      BOSTON  201708</span><br><span class="line">40      OPERATIONS      BOSTON  201709</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>增加分区</p>
<ol>
<li><p>创建单个分区</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(month='201706') ;</span><br></pre></td></tr></table></figure>
</li>
<li><p>同时创建多个分区</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(month='201705') partition(month='201704');</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>删除分区</p>
<ol>
<li><p>删除单个分区</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (month='201704');</span><br></pre></td></tr></table></figure>
</li>
<li><p>同时删除多个分区</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (month='201705'), partition (month='201706');</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>查看分区表有多少分区</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show partitions dept_partition;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看分区表结构</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; desc formatted dept_partition;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Partition Information          </span></span><br><span class="line"><span class="comment"># col_name              data_type               comment             </span></span><br><span class="line">month                   string</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="分区表注意事项"><a href="#分区表注意事项" class="headerlink" title="分区表注意事项"></a>分区表注意事项</h4><ul>
<li><p>创建二级分区表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition2(</span><br><span class="line">               deptno int, </span><br><span class="line">               dname string, </span><br><span class="line">               loc string</span><br><span class="line">               )</span><br><span class="line">               partitioned by (month string, day string)</span><br><span class="line">               row format delimited fields terminated by '\t';</span><br></pre></td></tr></table></figure>
</li>
<li><p>正常的加载数据</p>
<ol>
<li><p>加载数据到二级分区表中</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table</span><br><span class="line"> default.dept_partition2 partition(month='201709', day='13');</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询分区数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='13';</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式</p>
<ol>
<li><p>方式一：上传数据后修复(<code>禁用</code>)</p>
<p>上传数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br></pre></td></tr></table></figure>

<p>查询数据（查询不到刚上传的数据）</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='12';</span><br></pre></td></tr></table></figure>

<p>执行修复命令</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; msck repair table dept_partition2;</span><br></pre></td></tr></table></figure>

<p>再次查询数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='12';</span><br></pre></td></tr></table></figure>
</li>
<li><p>方式二：上传数据后添加分区(<code>推介</code>)</p>
<p>上传数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=11;</span><br></pre></td></tr></table></figure>

<p>执行添加分区</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 add partition(month='201709',</span><br><span class="line"> day='11');</span><br></pre></td></tr></table></figure>

<p>查询数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='11';</span><br></pre></td></tr></table></figure>
</li>
<li><p>方式三：创建文件夹后load数据到分区</p>
<p>创建目录</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p</span><br><span class="line"> /user/hive/warehouse/dept_partition2/month=201709/day=10;</span><br></pre></td></tr></table></figure>

<p>上传数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table</span><br><span class="line"> dept_partition2 partition(month='201709',day='10');</span><br></pre></td></tr></table></figure>

<p>查询数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month='201709' and day='10';</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ul>
<h3 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h3><h4 id="重命名表"><a href="#重命名表" class="headerlink" title="重命名表"></a>重命名表</h4><ol>
<li><p>语法</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">RENAME</span> <span class="keyword">TO</span> new_table_name</span><br></pre></td></tr></table></figure>
</li>
<li><p>实操案例</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 rename to dept_partition3;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="增加、修改和删除表分区"><a href="#增加、修改和删除表分区" class="headerlink" title="增加、修改和删除表分区"></a>增加、修改和删除表分区</h4><p>详见分区表基本操作。</p>
<h4 id="增加-修改-替换列信息"><a href="#增加-修改-替换列信息" class="headerlink" title="增加/修改/替换列信息"></a>增加/修改/替换列信息</h4><ul>
<li><p>语法</p>
<ol>
<li><p>更新列</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">CHANGE</span> [<span class="keyword">COLUMN</span>] col_old_name col_new_name column_type [<span class="keyword">COMMENT</span> col_comment] [<span class="keyword">FIRST</span>|<span class="keyword">AFTER</span> column_name]</span><br></pre></td></tr></table></figure>
</li>
<li><p>增加和替换列</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">ADD</span>|<span class="keyword">REPLACE</span> <span class="keyword">COLUMNS</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p><code>注：</code> ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。</p>
</li>
<li><p>实操案例</p>
<ol>
<li><p>查询表结构</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加列</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add columns(deptdesc string);</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询表结构</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure>
</li>
<li><p>更新列</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition change column deptdesc desc int;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询表结构</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure>
</li>
<li><p>替换列</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition replace columns(deptno string, dname</span><br><span class="line"> string, loc string);</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询表结构</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc dept_partition;</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ul>
<h3 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; drop table dept_partition;</span><br></pre></td></tr></table></figure>

<h2 id="DML数据操作"><a href="#DML数据操作" class="headerlink" title="DML数据操作"></a>DML数据操作</h2><h3 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h3><h4 id="向表中装载数据（Load）"><a href="#向表中装载数据（Load）" class="headerlink" title="向表中装载数据（Load）"></a>向表中装载数据（Load）</h4><ul>
<li><p>语法</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; load data [local] inpath '/opt/module/datas/student.txt' [overwrite] into table student [partition (partcol1=val1,…)];</span><br></pre></td></tr></table></figure>

<p><code>load data:</code> 表示加载数据</p>
<p><code>local:</code> 表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</p>
<p><code>inpath:</code> 表示加载数据的路径</p>
<p><code>overwrite:</code> 表示覆盖表中已有数据，否则表示追加</p>
<p><code>into table:</code> 表示加载到哪张表</p>
<p><code>student:</code> 表示具体的表</p>
<p><code>partition:</code> 表示上传到指定分区</p>
</li>
<li><p>实操案例</p>
<ol>
<li><p>创建一张表</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table student(id string, name string) row format delimited fields terminated by '\t';</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载本地文件到hive</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/opt/module/datas/student.txt' into table default.student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载HDFS文件到hive中</p>
<p>上传文件到HDFS</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/hadoop/hive;</span><br></pre></td></tr></table></figure>

<p>加载HDFS上数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data inpath '/user/hadoop/hive/student.txt' into table default.student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载数据覆盖表中已有的数据</p>
<p>上传文件到HDFS</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/hadoop/hive;</span><br></pre></td></tr></table></figure>

<p>加载数据覆盖表中已有的数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data inpath '/user/hadoop/hive/student.txt' overwrite into table default.student;</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ul>
<h4 id="通过查询语句向表中插入数据（Insert）"><a href="#通过查询语句向表中插入数据（Insert）" class="headerlink" title="通过查询语句向表中插入数据（Insert）"></a>通过查询语句向表中插入数据（Insert）</h4><ol>
<li><p>创建一张分区表</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table student(id int, name string) partitioned by (month string) row format delimited fields terminated by '\t';</span><br></pre></td></tr></table></figure>
</li>
<li><p>基本插入数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert into table  student partition(month='201709') values(1,'wangwu'),(2,’zhaoliu’);</span><br></pre></td></tr></table></figure>
</li>
<li><p>基本模式插入（根据单张表查询结果）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite table student partition(month='201708')</span><br><span class="line">             <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br></pre></td></tr></table></figure>

<p><code>insert into：</code> 以追加数据的方式插入到表或分区，原有数据不会删除</p>
<p><code>insert overwrite：</code> 会覆盖表或分区中已存在的数据</p>
<p><code>注意：</code> insert不支持插入部分字段</p>
</li>
<li><p>多表（多分区）插入模式（根据多张表查询结果）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; from student</span><br><span class="line">              <span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201707'</span>)</span><br><span class="line">              <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span></span><br><span class="line">              <span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201706'</span>)</span><br><span class="line">              <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="查询语句中创建表并加载数据（CTAS）"><a href="#查询语句中创建表并加载数据（CTAS）" class="headerlink" title="查询语句中创建表并加载数据（CTAS）"></a>查询语句中创建表并加载数据（CTAS）</h4><p>详见创建表。</p>
<p>根据查询结果创建表（查询的结果会添加到新创建的表中）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student3 <span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>

<h4 id="创建表时通过Location指定加载数据路径"><a href="#创建表时通过Location指定加载数据路径" class="headerlink" title="创建表时通过Location指定加载数据路径"></a>创建表时通过Location指定加载数据路径</h4><ol>
<li><p>上传数据到hdfs上</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir /student;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建表，并指定在hdfs上的位置</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create external table if not exists student5(</span><br><span class="line">              id int, name string</span><br><span class="line">              )</span><br><span class="line">              row format delimited fields terminated by '\t'</span><br><span class="line">              location '/student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from student5;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="Import数据到指定Hive表中"><a href="#Import数据到指定Hive表中" class="headerlink" title="Import数据到指定Hive表中"></a>Import数据到指定Hive表中</h4><p>注意：先用export导出后，再将数据导入。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; import table student2 partition(month='201709') from</span><br><span class="line"> '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure>

<h3 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h3><h4 id="Insert导出"><a href="#Insert导出" class="headerlink" title="Insert导出"></a>Insert导出</h4><ol>
<li><p>将查询的结果导出到本地</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory '/opt/module/datas/export/student' select * from student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>将查询的结果格式化导出到本地</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive(default)&gt;insert overwrite local directory '/opt/module/datas/export/student1' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' select * from student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>将查询的结果导出到HDFS上(没有local)</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite directory '/user/hadoop/student2' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' select * from student;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="Hadoop命令导出到本地"><a href="#Hadoop命令导出到本地" class="headerlink" title="Hadoop命令导出到本地"></a>Hadoop命令导出到本地</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -get /user/hive/warehouse/student/month=201709/000000_0</span><br><span class="line">/opt/module/datas/export/student3.txt;</span><br></pre></td></tr></table></figure>

<h4 id="Hive-Shell-命令导出"><a href="#Hive-Shell-命令导出" class="headerlink" title="Hive Shell 命令导出"></a>Hive Shell 命令导出</h4><p>基本语法：（hive -f/-e 执行语句或者脚本 &gt; file）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop102 hive]$ bin/hive -e 'select * from default.student;' &gt;</span><br><span class="line"> /opt/module/datas/export/student4.txt;</span><br></pre></td></tr></table></figure>

<h4 id="Export导出到HDFS上"><a href="#Export导出到HDFS上" class="headerlink" title="Export导出到HDFS上"></a>Export导出到HDFS上</h4><figure class="highlight"><table><tr><td class="code"><pre><span class="line">(defahiveult)&gt; export table default.student to '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure>

<p>export和import主要用于两个Hadoop平台集群之间Hive表迁移。</p>
<h4 id="Sqoop导出"><a href="#Sqoop导出" class="headerlink" title="Sqoop导出"></a>Sqoop导出</h4><p>后续的博客讲解</p>
<h3 id="清除表中数据（Truncate）"><a href="#清除表中数据（Truncate）" class="headerlink" title="清除表中数据（Truncate）"></a>清除表中数据（Truncate）</h3><p>注意：Truncate只能删除管理表，不能删除外部表中数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; truncate table student;</span><br></pre></td></tr></table></figure>



</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/03/hive/3/">Hive的部署和初始化工作&amp;验证Hive部署成功</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-03</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h3 id="Hive安装部署"><a href="#Hive安装部署" class="headerlink" title="Hive安装部署"></a>Hive安装部署</h3><h4 id="Hive安装及配置"><a href="#Hive安装及配置" class="headerlink" title="Hive安装及配置"></a>Hive安装及配置</h4><ol>
<li><p>把apache-hive-1.2.1-bin.tar.gz上传到linux的/opt/software目录下</p>
</li>
<li><p>解压apache-hive-1.2.1-bin.tar.gz到/opt/module/目录下面</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun software]$ tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改apache-hive-1.2.1-bin.tar.gz的名称为hive</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun module]$ mv apache-hive-1.2.1-bin/ hive</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改/opt/module/hive/conf目录下的hive-env.sh.template名称为hive-env.sh</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun conf]$ mv hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置hive-env.sh文件</p>
<ol>
<li><p>配置HADOOP_HOME路径</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置HIVE_CONF_DIR路径</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HIVE_CONF_DIR=/opt/module/hive/conf</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h4 id="Hadoop集群配置"><a href="#Hadoop集群配置" class="headerlink" title="Hadoop集群配置"></a>Hadoop集群配置</h4><ol>
<li><p>必须启动hdfs和yarn</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[hadoop@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>在HDFS上创建/tmp和/user/hive/warehouse两个目录并修改他们的同组权限可写</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop-2.7.2]$ bin/hadoop fs -mkdir /tmp</span><br><span class="line">[hadoop@aliyun hadoop-2.7.2]$ bin/hadoop fs -mkdir -p /user/hive/warehouse</span><br><span class="line">[hadoop@aliyun hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /tmp</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="Hive基本操作"><a href="#Hive基本操作" class="headerlink" title="Hive基本操作"></a>Hive基本操作</h4><ol>
<li><p>启动hive</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看数据库</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure>
</li>
<li><p>打开默认数据库</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; use default;</span><br></pre></td></tr></table></figure>
</li>
<li><p>显示default数据库中的表</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建一张表</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; create table student(id int, name string);</span><br></pre></td></tr></table></figure>
</li>
<li><p>显示数据库中有几张表</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看表的结构</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>向表中插入数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; insert into student values(1000,"ss");</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询表中数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>退出hive</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; quit;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ul>
<li>说明：<br><code>数据库：</code>在hdfs中表现为${hive.metastore.warehouse.dir}目录下一个文件夹<br><code>表：</code>在hdfs中表现所属db目录下一个文件夹，文件夹中存放该表中的具体数据</li>
</ul>
<h3 id="MySql安装"><a href="#MySql安装" class="headerlink" title="MySql安装"></a>MySql安装</h3><ul>
<li>安装过程请看我的另外一个博客: <a href="https://yerias.github.io/2018/10/01/DataWarehouse/mysql/1/">https://yerias.github.io/2018/10/01/DataWarehouse/mysql/1/</a></li>
</ul>
<h3 id="Hive元数据配置到MySql"><a href="#Hive元数据配置到MySql" class="headerlink" title="Hive元数据配置到MySql"></a>Hive元数据配置到MySql</h3><h4 id="驱动拷贝"><a href="#驱动拷贝" class="headerlink" title="驱动拷贝"></a>驱动拷贝</h4><ol>
<li><p>在/opt/software/mysql-libs目录下解压mysql-connector-java-5.1.27.tar.gz驱动包</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun mysql-libs]# tar -zxvf mysql-connector-java-5.1.27.tar.gz</span><br></pre></td></tr></table></figure>
</li>
<li><p>拷贝/opt/software/mysql-libs/mysql-connector-java-5.1.27目录下的mysql-connector-java-5.1.27-bin.jar到/opt/module/hive/lib/</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun mysql-connector-java-5.1.27]# cp mysql-connector-java-5.1.27-bin.jar</span><br><span class="line">/opt/module/hive/lib/</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="配置Metastore到MySql"><a href="#配置Metastore到MySql" class="headerlink" title="配置Metastore到MySql"></a>配置Metastore到MySql</h4><ol>
<li><p>在/opt/module/hive/conf目录下创建一个hive-site.xml</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun conf]$ touch hive-site.xml</span><br><span class="line">[hadoop@aliyun conf]$ vi hive-site.xml</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据官方文档配置参数，拷贝数据到hive-site.xml文件中<br><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;?xml version="1.0"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;jdbc:mysql://aliyun:3306/metastore?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">	&lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">	&lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;root&lt;/value&gt;</span><br><span class="line">	&lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;000000&lt;/value&gt;</span><br><span class="line">	&lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="多窗口启动Hive测试"><a href="#多窗口启动Hive测试" class="headerlink" title="多窗口启动Hive测试"></a>多窗口启动Hive测试</h4><ol>
<li><p>先启动MySQL</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun mysql-libs]$ mysql -uroot -p000000</span><br></pre></td></tr></table></figure>

<p>查看有几个数据库</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; show databases;</span><br><span class="line">   +<span class="comment">--------------------+</span></span><br><span class="line">   | Database |</span><br><span class="line">   +<span class="comment">--------------------+</span></span><br><span class="line">   | information_schema |</span><br><span class="line">   | mysql |</span><br><span class="line">   | performance_schema |</span><br><span class="line">   | test |</span><br><span class="line">   +<span class="comment">--------------------+</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>再次打开多个窗口，分别启动hive</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动hive后，回到MySQL窗口查看数据库，显示增加了metastore数据库</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; show databases;</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line">| Database |</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line">| information_schema |</span><br><span class="line">| metastore |</span><br><span class="line">| mysql |</span><br><span class="line">| performance_schema |</span><br><span class="line">| test |</span><br><span class="line">+<span class="comment">--------------------+</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="HiveJDBC访问"><a href="#HiveJDBC访问" class="headerlink" title="HiveJDBC访问"></a>HiveJDBC访问</h4><ol>
<li><p>启动hiveserver2服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hiveserver2</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动beeline</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/beeline</span><br><span class="line">Beeline version 1.2.1 by Apache Hive</span><br><span class="line"><span class="meta">beeline&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>连接hiveserver2</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">beeline&gt;</span><span class="bash"> !connect jdbc:hive2://aliyun:10000（回车）</span></span><br><span class="line">Connecting to jdbc:hive2://aliyun:10000</span><br><span class="line">Enter username for jdbc:hive2://aliyun:10000: hadoop（回车）</span><br><span class="line">Enter password for jdbc:hive2://aliyun:10000: （直接回车）</span><br><span class="line">Connected to: Apache Hive (version 1.2.1)</span><br><span class="line">Driver: Hive JDBC (version 1.2.1)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">0: jdbc:hive2://aliyun:10000&gt; show databases;</span><br><span class="line">+----------------+--+</span><br><span class="line">| database_name |</span><br><span class="line">+----------------+--+</span><br><span class="line">| default |</span><br><span class="line">| hive_db2 |</span><br><span class="line">+----------------+--+</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="Hive常用交互命令"><a href="#Hive常用交互命令" class="headerlink" title="Hive常用交互命令"></a>Hive常用交互命令</h4><ol>
<li><p>Hive命令帮助</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive -help</span><br><span class="line">usage: hive</span><br><span class="line">-d,--define &lt;key=value&gt; Variable subsitution to apply to hive</span><br><span class="line">commands. e.g. -d A=B or --define A=B</span><br><span class="line">--database &lt;databasename&gt; Specify the database to use</span><br><span class="line">-e &lt;quoted-query-string&gt; SQL from command line</span><br><span class="line">-f &lt;filename&gt; SQL from files</span><br><span class="line">-H,--help Print help information</span><br><span class="line">--hiveconf &lt;property=value&gt; Use value for given property</span><br><span class="line">--hivevar &lt;key=value&gt; Variable subsitution to apply to hive</span><br><span class="line">commands. e.g. --hivevar A=B</span><br><span class="line">-i &lt;filename&gt; Initialization SQL file</span><br><span class="line">-S,--silent Silent mode in interactive shell</span><br><span class="line">-v,--verbose Verbose mode (echo executed SQL to the console)</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>&quot;-e&quot;</code>不进入hive的交互窗口执行sql语句</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive -e "select id from student;"</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>&quot;-f&quot;</code>执行脚本中sql语句</p>
<ol>
<li><p>在/opt/module/datas目录下创建hivef.sql文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun datas]$ touch hivef.sql</span><br></pre></td></tr></table></figure>

<p>文件中写入正确的sql语句</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">select * from student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行文件中的sql语句</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive -f /opt/module/datas/hivef.sql</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行文件中的sql语句并将结果写入文件中</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive -f /opt/module/datas/hivef.sql &gt; /opt/module/datas/hive_result.txt</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h4 id="Hive其他命令操作"><a href="#Hive其他命令操作" class="headerlink" title="Hive其他命令操作"></a>Hive其他命令操作</h4><ol>
<li><p>退出hive窗口：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash"><span class="built_in">exit</span>;</span></span><br><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">quit;</span></span><br></pre></td></tr></table></figure>

<p>在新版的hive中没区别了，在以前的版本是有的：<br>exit:先隐性提交数据，再退出；<br>quit:不提交数据，退出；</p>
</li>
<li><p>在hive cli命令窗口中如何查看hdfs文件系统</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">dfs -ls /;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>在hive cli命令窗口中如何查看本地文件系统</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">! ls /opt/module/datas;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>查看在hive中输入的所有历史命令</p>
<ol>
<li><p>进入到当前用户的根目录/root或/home/hadoop</p>
</li>
<li><p>查看. hivehistory文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ cat .hivehistory</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h3 id="Hive常见属性配置"><a href="#Hive常见属性配置" class="headerlink" title="Hive常见属性配置"></a>Hive常见属性配置</h3><h4 id="Hive数据仓库位置配置"><a href="#Hive数据仓库位置配置" class="headerlink" title="Hive数据仓库位置配置"></a>Hive数据仓库位置配置</h4><ol>
<li><p>Default数据仓库的最原始位置是在hdfs上的：/user/hive/warehouse路径下。</p>
</li>
<li><p>在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹。</p>
</li>
<li><p>修改default数据仓库原始位置（将hive-default.xml.template如下配置信息拷贝到hive-site.xml文件中）。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">	&lt;description&gt;location of default database for the warehouse&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>配置同组用户有执行权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/hdfs dfs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="查询后信息显示配置"><a href="#查询后信息显示配置" class="headerlink" title="查询后信息显示配置"></a>查询后信息显示配置</h4><ol>
<li><p>在hive-site.xml文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;hive.cli.print.header&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;hive.cli.print.current.db&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>重新启动hive，对比配置前后差异。</p>
</li>
</ol>
<h4 id="Hive运行日志信息配置"><a href="#Hive运行日志信息配置" class="headerlink" title="Hive运行日志信息配置"></a>Hive运行日志信息配置</h4><ol>
<li><p>Hive的log默认存放在/tmp/hadoop/hive.log目录下（当前用户名下）</p>
</li>
<li><p>修改hive的log存放日志到/opt/module/hive/logs</p>
<ol>
<li><p>修改/opt/module/hive/conf/hive-log4j.properties.template文件名称为hive-log4j.properties</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun conf]$ pwd</span><br><span class="line">/opt/module/hive/conf</span><br><span class="line">[hadoop@aliyun conf]$ mv hive-log4j.properties.template hive-log4j.properties</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>在hive-log4j.properties文件中修改log存放位置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive.log.dir=/opt/module/hive/logs</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="参数配置方式"><a href="#参数配置方式" class="headerlink" title="参数配置方式"></a>参数配置方式</h4><ol>
<li><p>查看当前所有的配置信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"><span class="built_in">set</span>;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>参数的配置三种方式</p>
<ol>
<li><p>配置文件方式<br>默认配置文件：hive-default.xml<br>用户自定义配置文件：hive-site.xml</p>
<p><code>注意：</code>用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。</p>
</li>
<li><p>命令行参数方式<br>启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。<br>例如：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10;</span><br></pre></td></tr></table></figure>

<p>注意：仅对本次hive启动有效<br>查看参数设置：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure>
</li>
<li><p>参数声明方式<br>可以在HQL中使用SET关键字设定参数<br>例如：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks=100;</span><br></pre></td></tr></table></figure>

<p>注意：仅对本次hive启动有效。<br>查看参数设置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</p>
</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/02/hive/2/">谷粒影音8道SQL题(各种Top N)</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-02</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h4 id="data表字段"><a href="#data表字段" class="headerlink" title="data表字段"></a>data表字段</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">videoId string 				<span class="keyword">comment</span> <span class="string">"视频唯一id"</span>, </span><br><span class="line">uploader <span class="keyword">string</span> 			<span class="keyword">comment</span> <span class="string">"视频上传者"</span>,</span><br><span class="line">age <span class="built_in">int</span> 					<span class="keyword">comment</span> <span class="string">"视频年龄"</span>,</span><br><span class="line"><span class="keyword">category</span> <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;		<span class="keyword">comment</span> <span class="string">"视频类别"</span>,</span><br><span class="line"><span class="keyword">length</span> <span class="built_in">int</span> 					<span class="keyword">comment</span> <span class="string">"视频长度"</span>,</span><br><span class="line">views <span class="built_in">int</span> 					<span class="keyword">comment</span> <span class="string">"观看次数"</span>,</span><br><span class="line">rate <span class="built_in">float</span> 					<span class="keyword">comment</span> <span class="string">"视频评分"</span>,</span><br><span class="line">ratings <span class="built_in">int</span> 				<span class="keyword">comment</span> <span class="string">"流量"</span>,</span><br><span class="line">comments <span class="built_in">int</span> 				<span class="keyword">comment</span> <span class="string">"评论数"</span>,</span><br><span class="line">relatedId <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;		<span class="keyword">comment</span> <span class="string">"相关视频id"</span></span><br></pre></td></tr></table></figure>

<h4 id="user表字段"><a href="#user表字段" class="headerlink" title="user表字段"></a>user表字段</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">uploader String				<span class="keyword">comment</span> <span class="string">"上传者用户名"</span>,</span><br><span class="line">videos <span class="built_in">int</span>					<span class="keyword">comment</span> <span class="string">"上传视频数"</span>,</span><br><span class="line">friends	 <span class="built_in">int</span>				<span class="keyword">comment</span> <span class="string">"朋友数量"</span>,</span><br></pre></td></tr></table></figure>

<h4 id="8道题目-思路"><a href="#8道题目-思路" class="headerlink" title="8道题目(思路)"></a>8道题目(思路)</h4><p>统计硅谷影音视频网站的常规指标，各种TopN指标：</p>
<ol>
<li>统计视频观看数Top10</li>
<li>统计视频类别热度Top10</li>
<li>统计视频观看数Top20所属类别</li>
<li>统计视频观看数Top50所关联视频的所属类别Rank</li>
<li>统计每个类别中的视频热度Top10</li>
<li>统计每个类别中视频流量Top10</li>
<li>统计上传视频最多的用户Top10以及他们上传的视频</li>
<li>统计每个类别视频观看数Top10</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/01/hive/1/">Hive中的字符集编码若干问题</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-01</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h3 id="个人初始开发环境的基本情况以及Hive元数据库说明"><a href="#个人初始开发环境的基本情况以及Hive元数据库说明" class="headerlink" title="个人初始开发环境的基本情况以及Hive元数据库说明"></a>个人初始开发环境的基本情况以及Hive元数据库说明</h3><ol>
<li><p>hive的元数据库改成了mysql(安装完mysql之后也没有进行其它别的设置)</p>
</li>
<li><p>hive-site.xml中设置元数据库对应的配置为  <code>jdbc:mysql://ip:3306/metastore?createDatabaseIfNotExist=true</code></p>
</li>
<li><p>普通情况下咱们的mysql默认编码是latin1,但是我们在日常开发中大多数情况下需要用到utf-8编码,如果是默认latin1的话,咱们的中文存储进去容易乱码,所以说大家在遇到一些数据乱码的情况话,最好把mysql的编码改成utf-8.</p>
</li>
</ol>
<p><code>注意:</code> 但是在这里要非常严重强调的一点:hive的元数据metastore在mysql的数据库,不管是数据库本身,还是里面的表编码都必须是latin1(CHARACTER SET latin1 COLLATE latin1_bin)!!!!!</p>
<h4 id="验证方式"><a href="#验证方式" class="headerlink" title="验证方式:"></a>验证方式:</h4><p>可以通过客户端软件在数据库上右键属性查看,也可以通过命令查看</p>
<p>mysql&gt; show create database hive_cz3q;</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">-----------+-----------------------------------------------------------------------------------------+</span></span><br><span class="line">| Database  | <span class="keyword">Create</span> <span class="keyword">Database</span>                                                                         |</span><br><span class="line">+<span class="comment">-----------+-----------------------------------------------------------------------------------------+</span></span><br><span class="line">| hive_cz3q | <span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> <span class="string">`hive_cz3q`</span> <span class="comment">/*!40100 DEFAULT CHARACTER SET latin1 COLLATE latin1_bin */</span> |</span><br><span class="line">+<span class="comment">-----------+-----------------------------------------------------------------------------------------+</span></span><br></pre></td></tr></table></figure>

<p>不然会有类似如下的错误:</p>
<p> <img src="https://yerias.github.io/hive_img/610238-20170903131046858-396716990.png" alt="图片1"></p>
<p>那么怎么修改mysql的编码为utf8呢?这里提供了在线安装修改和离线方式安装下的修改方式供大家选择!</p>
<h3 id="乱码的情况"><a href="#乱码的情况" class="headerlink" title="乱码的情况:"></a>乱码的情况:</h3><p> 向hive的表中 创建表,表语句部分如下:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ods.ods_order</span><br><span class="line">(</span><br><span class="line">   ORDER_ID             <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'订单ID'</span>,</span><br><span class="line">   ORDER_NO             <span class="built_in">varchar</span>(<span class="number">30</span>)  <span class="keyword">comment</span> <span class="string">'订单编号（唯一字段），</span></span><br><span class="line"><span class="string">   DEALER_ID            int comment '</span>门店<span class="keyword">ID</span><span class="string">',</span></span><br><span class="line"><span class="string">   CUST_ID              int comment '</span>客户<span class="keyword">ID</span><span class="string">',</span></span><br></pre></td></tr></table></figure>

<p>在创建表的时候，字段可以有 comment，但是 comment 建议不要用中文说明，因为我们说过，hive 的 metastore 支持的字符集是 latin1，所以中文写入的时候会有编码问题，如下图！ </p>
<p>然后通过desc ods_order 查看 对应的comment中是中文的地方,通过Xshell显示全部都是 “?” 问号.  同时确认了Xshell支持显示中文(排除Xshell的问题).</p>
<p><code>以上就是说Hive在字段定义时的Comment中文乱码问题.</code></p>
<p><img src="https://yerias.github.io/hive_img/610238-20170903134309687-1604377616.png" alt="图片2"></p>
<p>有了上述的问题，那么我们该如何去解决注释中文乱码问题呢？ </p>
<h3 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h3><h4 id="首先进行Mysql的编码设置"><a href="#首先进行Mysql的编码设置" class="headerlink" title="首先进行Mysql的编码设置"></a>首先进行Mysql的编码设置</h4><h5 id="离线安装mysql的修改方式"><a href="#离线安装mysql的修改方式" class="headerlink" title="离线安装mysql的修改方式"></a>离线安装mysql的修改方式</h5><ol>
<li><p>修改编码,设置为utf8</p>
<p>拷贝 mysql 的配置文件/usr/share/mysql/my-small.cnf 到/etc/my.cnf </p>
<p>在mysql 配置文件/etc/my.cnf 中增加以下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[client]下面增加</span><br><span class="line">default-character-set=utf8</span><br><span class="line">在[mysqld]下面增加</span><br><span class="line">default-character-set=utf8</span><br><span class="line">init_connect='SET NAMES utf8'</span><br></pre></td></tr></table></figure>

<p><em>2020/03/09更新</em>：<code>default-character-set=utf8</code>，如果这样改会导致5.7版本mysql无法打开所以要改为 <code>character-set-server=utf8</code>，(可选)改完后，要删除数据库中所有数据，才能使用。``````````</p>
</li>
<li><p>重启mysql 服务(这样确保缺省编码是utf8)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">service mysqld restart</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证编码是否改成了utf8:</p>
<p>输入命令 “\s”</p>
<p><img src="https://yerias.github.io/hive_img/610238-20170903132119671-1333763157.png" alt="图片3"></p>
<p>输入命令:show variables like ‘char%’</p>
<p><img src="https://yerias.github.io/hive_img/610238-20170903132210124-717565896.png" alt="图片4"></p>
<p>输入命令:show variables like “colla%”;</p>
<p> <img src="https://yerias.github.io/hive_img/610238-20170903132350968-1341436179.png" alt="图片5"></p>
<p>OK修改成功!</p>
</li>
<li><p>这样在启动hive,向hive中插入的表中comment等有汉字的情况,就可以正常的显示(如下为本人测试的部分显示结果):</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://localhost:10000/ods&gt; desc ods_order;</span><br><span class="line">+<span class="comment">--------------------------+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--+</span></span><br><span class="line">|         col_name         |       data_type       |                                                                   <span class="keyword">comment</span>                                                                   |</span><br><span class="line">+<span class="comment">--------------------------+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--+</span></span><br><span class="line">| order_id                 | <span class="built_in">int</span>                   | 订单<span class="keyword">ID</span>                                                                                                                                        |</span><br><span class="line">| order_no                 | <span class="built_in">varchar</span>(<span class="number">30</span>)           | 订单编号（唯一字段），前缀字符表示订单来源：a，Andriod；b，微博；c，WEB；e，饿了么；i，Iphone；m，Mobile；x，微信； z，中粮我买网；l，其它。 接着<span class="number">3</span>位数字代表订单城市编号；接着字符z与后面的真正订单编号分隔。这套机制从<span class="number">2014</span>年<span class="number">12</span>月开始实施。</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h5 id="在线安装mysql的修改方式"><a href="#在线安装mysql的修改方式" class="headerlink" title="在线安装mysql的修改方式"></a>在线安装mysql的修改方式</h5><ol>
<li><p>修改编码,设置为utf-8</p>
<p> 在 mysql 配置文件/etc/my.cnf（不需要拷贝）中[mysqld]的下面增加以下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">init_connect='SET collation_connection = utf8_unicode_ci'</span><br><span class="line">init_connect='SET NAMES utf8'</span><br><span class="line">character-set-server=utf8</span><br><span class="line">collation-server=utf8_unicode_ci</span><br><span class="line">skip-character-set-client-handshake</span><br></pre></td></tr></table></figure>

<p> <img src="https://yerias.github.io/hive_img/610238-20170903133604796-492434925.png" alt="图片6"></p>
</li>
<li><p>重启mysqld服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">service mysqld restart</span><br></pre></td></tr></table></figure>
</li>
<li><p>和离线方式一样验证编码是否确实修改;</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">show variables like 'char%';</span><br></pre></td></tr></table></figure>

<p> <img src="https://yerias.github.io/hive_img/610238-20170903133820296-974086333.png" alt="图片7"></p>
</li>
</ol>
<h4 id="针对元数据库metastore中的表-分区-视图的编码设置"><a href="#针对元数据库metastore中的表-分区-视图的编码设置" class="headerlink" title="针对元数据库metastore中的表,分区,视图的编码设置"></a>针对元数据库metastore中的表,分区,视图的编码设置</h4><p>因为我们知道 metastore 支持数据库级别，表级别的字符集是 latin1，那么我们<code>只需要把相应注释的地方的字符集由 latin1 改成 utf-8</code>，就可以了。用到注释的就三个地方，表、分区、视图。如下修改分为两个步骤：</p>
<ol>
<li><p>进入数据库 Metastore 中执行以下 5 条 SQL 语句 </p>
<ol>
<li>修改表字段注解和表注解<br>alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8<br>alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8</li>
<li>修改分区字段注解：<br>alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8 ;<br>alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8</li>
<li>修改索引注解：<br>alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</li>
</ol>
</li>
<li><p>修改 metastore 的连接 URL</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;jdbc:mysql://IP:3306/db_name?createDatabaseIfNotExist=true&amp;amp;useUnicode=true&amp;amp;characterEncoding=UTF-8&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p><strong>测试结果：</strong></p>
<p><img src="https://yerias.github.io/hive_img/610238-20170903134604093-93033588.png" alt="图片8"></p>
<p>以上就能完美的解决这个问题.</p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>有时候在使用xml作为配置文件的时候，应该要使用xml的编码规则来进行适当的设置。如<code>&amp;</code>在xml文件中应该写为<code>&amp;amp;</code></p>
<p>下面给出xml中一些特殊符号的编码转换：</p>
<table>
<thead>
<tr>
<th>代码</th>
<th>符号</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td><code>&amp;lt;</code></td>
<td>&lt;</td>
<td>小于号</td>
</tr>
<tr>
<td><code>&amp;gt;</code></td>
<td>&gt;</td>
<td>大于号</td>
</tr>
<tr>
<td><code>&amp;amp;</code></td>
<td>&amp;</td>
<td>and字符</td>
</tr>
<tr>
<td><code>&amp;apos;</code></td>
<td>‘</td>
<td>单引号</td>
</tr>
<tr>
<td><code>&amp;quot;</code></td>
<td>“</td>
<td>双引号</td>
</tr>
</tbody></table>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/16/hadoop/12/">MR调优之压缩</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-16</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/MapReduce/">MapReduce</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/MapReduce/">MapReduce</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>什么是压缩</li>
<li>压缩的好处与坏处</li>
<li>常见的压缩格式</li>
<li>优缺点比较</li>
<li>如何选择压缩格式</li>
<li>MR配置文件压缩格式</li>
<li>Hive配置文件压缩格式</li>
</ol>
<h2 id="什么是压缩"><a href="#什么是压缩" class="headerlink" title="什么是压缩"></a>什么是压缩</h2><p>压缩就是通过某种技术（算法）把原始文件变小，相应的解压就是把压缩后的文件变成原始文件。嘿嘿是不是又可以变大又可以变小。</p>
<h2 id="压缩的好处与坏处"><a href="#压缩的好处与坏处" class="headerlink" title="压缩的好处与坏处"></a>压缩的好处与坏处</h2><p><strong>好处</strong></p>
<ul>
<li>减少存储磁盘空间</li>
<li>降低IO(网络的IO和磁盘的IO)</li>
<li>加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度</li>
</ul>
<p><strong>坏处</strong></p>
<ul>
<li>由于使用数据时，需要先将数据解压，加重<strong>CPU</strong>负荷</li>
</ul>
<h2 id="常见的压缩格式"><a href="#常见的压缩格式" class="headerlink" title="常见的压缩格式"></a>常见的压缩格式</h2><table>
<thead>
<tr>
<th align="left">格式</th>
<th align="left">可分割</th>
<th align="left">平均压缩速度</th>
<th align="left">文本文件压缩效率</th>
<th align="left">Hadoop压缩编解码器</th>
<th align="center">纯Java实现</th>
<th align="left">原生</th>
<th align="left">备注</th>
</tr>
</thead>
<tbody><tr>
<td align="left">gzip</td>
<td align="left">否</td>
<td align="left">快</td>
<td align="left">高</td>
<td align="left">org.apache.hadoop.io.compress.GzipCodec</td>
<td align="center">是</td>
<td align="left">是</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">lzo</td>
<td align="left">是（取决于所使用的库）</td>
<td align="left">非常快</td>
<td align="left">中等</td>
<td align="left">com.hadoop.compression.lzo.LzoCodec</td>
<td align="center">是</td>
<td align="left">是</td>
<td align="left">需要在每个节点上安装LZO</td>
</tr>
<tr>
<td align="left">bzip2</td>
<td align="left">是</td>
<td align="left">慢</td>
<td align="left">非常高</td>
<td align="left">org.apache.hadoop.io.compress.Bzip2Codec</td>
<td align="center">是</td>
<td align="left">是</td>
<td align="left">为可分割版本使用纯Java</td>
</tr>
<tr>
<td align="left">zlib</td>
<td align="left">否</td>
<td align="left">慢</td>
<td align="left">中等</td>
<td align="left">org.apache.hadoop.io.compress.DefaultCodec</td>
<td align="center">是</td>
<td align="left">是</td>
<td align="left">Hadoop 的默认压缩编解码器</td>
</tr>
<tr>
<td align="left">Snappy</td>
<td align="left">否</td>
<td align="left">非常快</td>
<td align="left">低</td>
<td align="left">org.apache.hadoop.io.compress.SnappyCodec</td>
<td align="center">否</td>
<td align="left">是</td>
<td align="left">Snappy 有纯Java的移植版，但是在Spark/Hadoop中不能用</td>
</tr>
</tbody></table>
<p>一个简单的案例对于集中压缩方式之间的压缩大小和压缩时间进行一个感观性的认识</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">测试环境:</span><br><span class="line">8 core i7 cpu </span><br><span class="line">8GB memory</span><br><span class="line">64 bit CentOS</span><br><span class="line">1.4GB Wikipedia Corpus 2-gram text input</span><br></pre></td></tr></table></figure>

<p>压缩比</p>
<p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E6%AF%94.png" alt="压缩比"></p>
<p>压缩时间比</p>
<p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E6%97%B6%E9%97%B4.png" alt="压缩时间"></p>
<p>可以看出，压缩比越高，压缩时间越长</p>
<h2 id="优缺点比较"><a href="#优缺点比较" class="headerlink" title="优缺点比较"></a>优缺点比较</h2><table>
<thead>
<tr>
<th align="left">压缩格式</th>
<th align="left">优点</th>
<th align="left">缺点</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>gzip</strong></td>
<td align="left">压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便</td>
<td align="left"><strong>不支持split</strong></td>
<td></td>
</tr>
<tr>
<td align="left"><strong>lzo</strong></td>
<td align="left">压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便</td>
<td align="left">压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）</td>
<td></td>
</tr>
<tr>
<td align="left"><strong>snappy</strong></td>
<td align="left">压缩速度快；支持hadoop native库</td>
<td align="left"><strong>不支持split</strong>；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令d. bzip2</td>
<td></td>
</tr>
<tr>
<td align="left"><strong>bzip2</strong></td>
<td align="left">支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便</td>
<td align="left">压缩/解压速度慢；不支持native</td>
<td></td>
</tr>
</tbody></table>
<h2 id="如何选择压缩格式"><a href="#如何选择压缩格式" class="headerlink" title="如何选择压缩格式"></a>如何选择压缩格式</h2><p>从两方面考虑：Storage + Compute；</p>
<ol>
<li>Storage ：基于HDFS考虑，减少了存储文件所占空间，提升了数据传输速率；如gzip、bzip2。</li>
<li>Compute：基于YARN上的计算(MapReduce/Hive/Spark/….)速度的提升；如lzo、lz4、snappy。</li>
</ol>
<p>不同的场景选择不同的压缩方式，肯定没有一个一劳永逸的方法，如果选择高压缩比，那么对于cpu的性能要求要高，同时压缩、解压时间耗费也多；选择压缩比低的，对于磁盘io、网络io的时间要多，空间占据要多；<strong>对于支持分割的，可以实现并行处理</strong>。</p>
<ol>
<li>IO密集型：使用压缩</li>
<li>运算密集型：慎用压缩</li>
</ol>
<h2 id="压缩在MapReduce中的应用场景"><a href="#压缩在MapReduce中的应用场景" class="headerlink" title="压缩在MapReduce中的应用场景"></a>压缩在MapReduce中的应用场景</h2><p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E5%9C%A8MR%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF.png" alt="压缩在MR的应用场景"></p>
<p>压缩在hadoop中的应用场景总结在三方面：<strong>输入</strong>，<strong>中间</strong>，<strong>输出</strong>。</p>
<p>整体思路：hdfs ==&gt; map ==&gt; shuffle ==&gt; reduce</p>
<ol>
<li>Use Compressd Map Input: 从HDFS中读取文件进行Mapreuce作业，如果数据很大，可以使用压缩并且<strong>选择支持分片的压缩方式（Bzip2,LZO）</strong>，可以实现并行处理，提高效率，减少磁盘读取时间，同时选择合适的存储格式例如Sequence Files，RC,ORC等；</li>
<li>Compress Intermediate Data: Map输出作为Reducer的输入，需要经过shuffle这一过程，需要把数据读取到一个环形缓冲区，然后读取到本地磁盘，所以选择压缩可以减少了存储文件所占空间，提升了数据传输速率，建议<strong>使用压缩速度快的压缩方式，例如Snappy和LZO.</strong></li>
<li>Compress Reducer Output: 进行归档处理或者链式Mapreduce的工作（该作业的输出作为下个作业的输入），压缩可以减少了存储文件所占空间，提升了数据传输速率，如果作为归档处理，可以<strong>采用高的压缩比（Gzip,Bzip2）</strong>，如果作为下个作业的输入，考虑<strong>是否要分片</strong>进行选择。</li>
</ol>
<h2 id="MR配置文件压缩格式"><a href="#MR配置文件压缩格式" class="headerlink" title="MR配置文件压缩格式"></a>MR配置文件压缩格式</h2><p>hadoop自带不支持split的gzip和支持split的bzip2，我们还手动安装了lzo的压缩方式</p>
<ol>
<li><p>修改<code>core-site.xml</code>文件</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">    org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">    org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">    org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">    com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">    com.hadoop.compression.lzo.LzopCodec,</span><br><span class="line">    org.apache.hadoop.io.compress.Lz4Codec,</span><br><span class="line">    org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">    <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>修改<code>mapred-site.xml</code>文件</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">#开启支持压缩</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.output.fileoutputformat.compress<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">#压缩方式/最终输出的压缩方式</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.output.fileoutputformat.compress.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.compress.BZip2Codec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line">#中间压缩(可选，Snappy需要手动安装)</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.map.output.compression.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.compress.SnappyCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>中间压缩</strong>：中间压缩就是处理作业map任务和reduce任务之间的数据，对于中间压缩，最好选择一个节省CPU耗时的压缩方式（快）</p>
<p><strong>最终压缩</strong>：可以选择高压缩比，减少了存储文件所占空间，提升了数据传输速率<br><code>mapred-site.xml</code> 中设置</p>
</li>
<li><p>验证，跑个wc看最终输出文件的后缀</p>
</li>
</ol>
<p><strong>更换压缩方式只需要修改中间输出或者最终输出的压缩类即可</strong></p>
<h2 id="Hive配置文件压缩格式"><a href="#Hive配置文件压缩格式" class="headerlink" title="Hive配置文件压缩格式"></a>Hive配置文件压缩格式</h2><ol>
<li><p>配置压缩功能</p>
<p>hive配置文件压缩格式只需要配置两个参数</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">//开启压缩功能</span><br><span class="line">SET hive.exec.compress.output=true; </span><br><span class="line">//设置最终以bz2格式存储</span><br><span class="line">SET mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Code;</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>：不建议再配置文件中设置</p>
</li>
<li><p>使用压缩</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">#创建表：</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载数据：</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/page_views.dat"</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> page_views;  </span><br><span class="line"></span><br><span class="line"><span class="comment">#配置压缩格式</span></span><br><span class="line">hive：</span><br><span class="line"><span class="keyword">SET</span> hive.exec.compress.output=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">SET</span> mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建压缩表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_bzip2 <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span> <span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views;</span><br></pre></td></tr></table></figure></li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/15/hadoop/13/">HADOOP安装LZO压缩</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a></span><div class="content"><h3 id="编译安装lzo与lzop"><a href="#编译安装lzo与lzop" class="headerlink" title="编译安装lzo与lzop"></a>编译安装lzo与lzop</h3><p> <strong>在集群的每一台主机上都需要编译安装！！！</strong></p>
<ol>
<li><p>下载编译安装lzo文件，<a href="http://www.oberhumer.com/opensource/lzo/download" target="_blank" rel="noopener"><strong>版本可以下载最新的</strong></a> </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget http://www.oberhumer.com/opensource/lzo/download/lzo-2.10.tar.gz</span><br></pre></td></tr></table></figure>
</li>
<li><p>编译安装(保证主机上有gcc与g++)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xvzf   lzo-2.10.tar.gz</span><br><span class="line">cd lzo-2.10</span><br><span class="line">./configure --enable-shared</span><br><span class="line">make -j 10</span><br><span class="line">make install</span><br><span class="line">cp /usr/local/lib/*lzo* /usr/lib</span><br></pre></td></tr></table></figure>

<p><strong>安装完成后需要将 cp部分文件到/usr/lib中，这个步骤不做会抛</strong>   lzop: error while loading shared libraries: liblzo2.so.2: cannot open shared object file: No such file or directory</p>
</li>
<li><p>下载编译lzop，<a href="http://www.lzop.org/download/" target="_blank" rel="noopener"><strong>最新版选择</strong></a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget http://www.lzop.org/download/lzop-1.04.tar.gz</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xvzf lzop-1.04.tar.gz </span><br><span class="line">cd lzop-1.04 </span><br><span class="line">./configure </span><br><span class="line">make -j 10 </span><br><span class="line">make install</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="安装、编译hadoop-lzo-master"><a href="#安装、编译hadoop-lzo-master" class="headerlink" title="安装、编译hadoop-lzo-master"></a>安装、编译hadoop-lzo-master</h3><p>需在linux环境中安装,在windows上编译不过</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget https://github.com/twitter/hadoop-lzo/archive/master.zip</span><br></pre></td></tr></table></figure>

<ol>
<li><p>解压</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">unzip master.zip </span><br><span class="line">cd hadoop-lzo-master/</span><br></pre></td></tr></table></figure>
</li>
<li><p>编辑<em>pom.xm</em>l修改hadoop的版本号与你集群中hadoop版本一致   </p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">hadoop.current.version</span>&gt;</span>2.6.0-cdh5.16.2<span class="tag">&lt;/<span class="name">hadoop.current.version</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>pom文件增加cloudera的仓库地址</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--允许发布版本，禁止快照版--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">releases</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">releases</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>false<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>检查所在主机是否有maven,如果没有需要安装,如下(安装了maven即可跳过):</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget http://mirrors.hust.edu.cn/apache/maven/maven-3/3.5.4/binaries/apache-maven-3.5.4-bin.tar.gz</span><br><span class="line"></span><br><span class="line">tar -zxvf apache-maven-3.5.4-bin.tar.gz</span><br><span class="line"></span><br><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line">添加环境变量:</span><br><span class="line">MAVEN_HOME=/usr/local/apache-maven-3.5.4</span><br><span class="line">export MAVEN_HOME</span><br><span class="line">export PATH=$&#123;PATH&#125;:$&#123;MAVEN_HOME&#125;/bin</span><br><span class="line"></span><br><span class="line">保存退出profile</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>
</li>
<li><p>在maven中配置阿里云和cloudera的仓库</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>nexus-aliyun<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>*,!cloudera<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Nexus aliyun<span class="tag">&lt;/<span class="name">name</span>&gt;</span>                     </span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span></span><br><span class="line">        http://maven.aliyun.com/nexus/content/groups/public</span><br><span class="line">    <span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>导入hadoop-lzo编译时需要路径信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export CFLAGS=-m64</span><br><span class="line">export CXXFLAGS=-m64</span><br><span class="line">export C_INCLUDE_PATH=/home/hadoop/app/hadoop/lzo/include</span><br><span class="line">export LIBRARY_PATH=/home/hadoop/app/hadoop/lzo/lib</span><br></pre></td></tr></table></figure>
</li>
<li><p>maven编译安装</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mvn clean package -Dmaven.test.skip=true</span><br></pre></td></tr></table></figure>

<p><strong>编译安装没有异常结束后往下继续   PS:如果在mvn这里出现异常，请解决后再继续，注意权限问题</strong></p>
</li>
<li><p>编译成功后会有target文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd target/native/Linux-amd64-64/</span><br><span class="line">mkdir ~/hadoop-lzo-files</span><br><span class="line">tar -cBf - -C lib . | tar -xBvf - -C ~/hadoop-lzo-files</span><br></pre></td></tr></table></figure>
</li>
<li><p>在 ~/hadoop-lzo-files 目录下产生几个文件,执行cp</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp ~/hadoop-lzo-files/libgplcompression*  $HADOOP_HOME/lib/native/</span><br></pre></td></tr></table></figure>

<p><strong>注意！！！上面这一步的cp文件也要同步到集群其他主机的hadoop的对应目录下</strong></p>
</li>
<li><p>cp  hadoop-lzo的jar包到hadoop目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp target/hadoop-lzo-0.4.21-SNAPSHOT.jar $HADOOP_HOME/share/hadoop/common/</span><br></pre></td></tr></table></figure>

<p><strong>注意！！！上面这一步的cp文件也要同步到集群其他主机的hadoop的对应目录下</strong></p>
</li>
</ol>
<h3 id="配置hadoop配置文件"><a href="#配置hadoop配置文件" class="headerlink" title="配置hadoop配置文件"></a>配置hadoop配置文件</h3><ol>
<li><p>修改<strong>core-site.xml</strong>(<em>如果配置过了不需要配置</em>)</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">        org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">        com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzopCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.Lz4Codec,</span><br><span class="line">        org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">    <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>修改<strong>mapred-site.xml</strong>中的压缩方式</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">#开启压缩</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.compress.map.output<span class="tag">&lt;/<span class="name">name</span>&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span>    </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">#配置压缩方式</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.map.output.compression.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span>    </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>跑个wc验证输出文件是否压缩</p>
</li>
<li><p>创建索引</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop jar $HADOOP_HOME/share/hadoop/common/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer /data/lzo-index/large_wc.txt.lzo</span><br></pre></td></tr></table></figure>

</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/15/hadoop/14/">MapReduce使用压缩以及在MR中的通用做法</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a></span><div class="content"><p>上一步中我们在Hadoop中安装了lzo的压缩方式，现在将测试如何在MapReduce程序中使用压缩</p>
<ol>
<li><p>在MapReduce中使用压缩，要注意三个位置，分别是map输入文件的压缩格式，map输出的压缩格式，和reduce最终输出的压缩格式</p>
<ul>
<li><p>首先配置使用压缩</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">mapreduce.output.fileoutputformat.compress=<span class="keyword">true</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步配置输入文件的压缩格式(如lzo):</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">mapreduce.job.inputformat<span class="class">.<span class="keyword">class</span></span>=com.hadoop.mapreduce.LzoTextInputFormat</span><br></pre></td></tr></table></figure>
</li>
<li><p>第三步配置map输出的文件压缩格式(如snappy):</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">mapred.map.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec</span><br></pre></td></tr></table></figure>
</li>
<li><p>第四步配置reduce输出的文件压缩格式(可不配置，这里配置lzo):</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Codec</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>我们随意找个表，查看表结构(这里只拿出来了我们想看的)</p>
<p>desc formatted emp;</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Storage Information	 	 </span><br><span class="line">SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 </span><br><span class="line">InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 </span><br><span class="line">OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</span><br></pre></td></tr></table></figure>

<p>由于hive底层也是跑的MapReduce，现在我们就能知道为什么要设置InputFormat和OutputFormat了。</p>
</li>
<li><p>一般不固定写在配置文件中，而是提交作业的时候手动指定，通过-D 指定参数</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop jar hadoop-mapreduce-examples-<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.16</span><span class="number">.2</span>.jar wordcount \</span><br><span class="line"> -D mapreduce.output.fileoutputformat.compress=<span class="keyword">true</span> \</span><br><span class="line"> -D io.compression.codec.lzo<span class="class">.<span class="keyword">class</span></span>=com.hadoop.compression.lzo.LzoCodec \</span><br><span class="line"> -D mapreduce.job.inputformat<span class="class">.<span class="keyword">class</span></span>=com.hadoop.mapreduce.LzoTextInputFormat \</span><br><span class="line"> -D mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzopCodec \</span><br><span class="line"> /data/lzo-index/  /out</span><br></pre></td></tr></table></figure>
</li>
<li><p>给.lzo文件创建索引</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop jar $HADOOP_HOME/share/hadoop/common/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer /data/lzo-index/large_wc.txt.lzo</span><br></pre></td></tr></table></figure>


</li>
</ol>
</div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/9/">&lt;&lt;</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/11/">&gt;&gt;</a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>