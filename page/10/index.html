<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="BigData Developer"><meta name="keywords" content="yerias,TUNANのBlog,BigData"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017 | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">129</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">30</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">26</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">TUNANのBlog</div><div id="site-sub-title">感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017</div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/03/hive/3/">Hive的部署和初始化工作&amp;验证Hive部署成功</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-03</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h3 id="Hive安装部署"><a href="#Hive安装部署" class="headerlink" title="Hive安装部署"></a>Hive安装部署</h3><h4 id="Hive安装及配置"><a href="#Hive安装及配置" class="headerlink" title="Hive安装及配置"></a>Hive安装及配置</h4><ol>
<li><p>把apache-hive-1.2.1-bin.tar.gz上传到linux的/opt/software目录下</p>
</li>
<li><p>解压apache-hive-1.2.1-bin.tar.gz到/opt/module/目录下面</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun software]$ tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改apache-hive-1.2.1-bin.tar.gz的名称为hive</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun module]$ mv apache-hive-1.2.1-bin/ hive</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改/opt/module/hive/conf目录下的hive-env.sh.template名称为hive-env.sh</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun conf]$ mv hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置hive-env.sh文件</p>
<ol>
<li><p>配置HADOOP_HOME路径</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置HIVE_CONF_DIR路径</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HIVE_CONF_DIR=/opt/module/hive/conf</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h4 id="Hadoop集群配置"><a href="#Hadoop集群配置" class="headerlink" title="Hadoop集群配置"></a>Hadoop集群配置</h4><ol>
<li><p>必须启动hdfs和yarn</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[hadoop@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>在HDFS上创建/tmp和/user/hive/warehouse两个目录并修改他们的同组权限可写</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hadoop-2.7.2]$ bin/hadoop fs -mkdir /tmp</span><br><span class="line">[hadoop@aliyun hadoop-2.7.2]$ bin/hadoop fs -mkdir -p /user/hive/warehouse</span><br><span class="line">[hadoop@aliyun hadoop-2.7.2]$ bin/hadoop fs -chmod g+w /tmp</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="Hive基本操作"><a href="#Hive基本操作" class="headerlink" title="Hive基本操作"></a>Hive基本操作</h4><ol>
<li><p>启动hive</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看数据库</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure>
</li>
<li><p>打开默认数据库</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; use default;</span><br></pre></td></tr></table></figure>
</li>
<li><p>显示default数据库中的表</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建一张表</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; create table student(id int, name string);</span><br></pre></td></tr></table></figure>
</li>
<li><p>显示数据库中有几张表</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看表的结构</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; desc student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>向表中插入数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; insert into student values(1000,"ss");</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询表中数据</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>退出hive</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; quit;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ul>
<li>说明：<br><code>数据库：</code>在hdfs中表现为${hive.metastore.warehouse.dir}目录下一个文件夹<br><code>表：</code>在hdfs中表现所属db目录下一个文件夹，文件夹中存放该表中的具体数据</li>
</ul>
<h3 id="MySql安装"><a href="#MySql安装" class="headerlink" title="MySql安装"></a>MySql安装</h3><ul>
<li>安装过程请看我的另外一个博客: <a href="https://yerias.github.io/2018/10/01/DataWarehouse/mysql/1/">https://yerias.github.io/2018/10/01/DataWarehouse/mysql/1/</a></li>
</ul>
<h3 id="Hive元数据配置到MySql"><a href="#Hive元数据配置到MySql" class="headerlink" title="Hive元数据配置到MySql"></a>Hive元数据配置到MySql</h3><h4 id="驱动拷贝"><a href="#驱动拷贝" class="headerlink" title="驱动拷贝"></a>驱动拷贝</h4><ol>
<li><p>在/opt/software/mysql-libs目录下解压mysql-connector-java-5.1.27.tar.gz驱动包</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun mysql-libs]# tar -zxvf mysql-connector-java-5.1.27.tar.gz</span><br></pre></td></tr></table></figure>
</li>
<li><p>拷贝/opt/software/mysql-libs/mysql-connector-java-5.1.27目录下的mysql-connector-java-5.1.27-bin.jar到/opt/module/hive/lib/</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@aliyun mysql-connector-java-5.1.27]# cp mysql-connector-java-5.1.27-bin.jar</span><br><span class="line">/opt/module/hive/lib/</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="配置Metastore到MySql"><a href="#配置Metastore到MySql" class="headerlink" title="配置Metastore到MySql"></a>配置Metastore到MySql</h4><ol>
<li><p>在/opt/module/hive/conf目录下创建一个hive-site.xml</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun conf]$ touch hive-site.xml</span><br><span class="line">[hadoop@aliyun conf]$ vi hive-site.xml</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据官方文档配置参数，拷贝数据到hive-site.xml文件中<br><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;?xml version="1.0"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;jdbc:mysql://aliyun:3306/metastore?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">	&lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">	&lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;root&lt;/value&gt;</span><br><span class="line">	&lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;000000&lt;/value&gt;</span><br><span class="line">	&lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="多窗口启动Hive测试"><a href="#多窗口启动Hive测试" class="headerlink" title="多窗口启动Hive测试"></a>多窗口启动Hive测试</h4><ol>
<li><p>先启动MySQL</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun mysql-libs]$ mysql -uroot -p000000</span><br></pre></td></tr></table></figure>

<p>查看有几个数据库</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; show databases;</span><br><span class="line">   +<span class="comment">--------------------+</span></span><br><span class="line">   | Database |</span><br><span class="line">   +<span class="comment">--------------------+</span></span><br><span class="line">   | information_schema |</span><br><span class="line">   | mysql |</span><br><span class="line">   | performance_schema |</span><br><span class="line">   | test |</span><br><span class="line">   +<span class="comment">--------------------+</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>再次打开多个窗口，分别启动hive</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动hive后，回到MySQL窗口查看数据库，显示增加了metastore数据库</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; show databases;</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line">| Database |</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line">| information_schema |</span><br><span class="line">| metastore |</span><br><span class="line">| mysql |</span><br><span class="line">| performance_schema |</span><br><span class="line">| test |</span><br><span class="line">+<span class="comment">--------------------+</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="HiveJDBC访问"><a href="#HiveJDBC访问" class="headerlink" title="HiveJDBC访问"></a>HiveJDBC访问</h4><ol>
<li><p>启动hiveserver2服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hiveserver2</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动beeline</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/beeline</span><br><span class="line">Beeline version 1.2.1 by Apache Hive</span><br><span class="line"><span class="meta">beeline&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>连接hiveserver2</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">beeline&gt;</span><span class="bash"> !connect jdbc:hive2://aliyun:10000（回车）</span></span><br><span class="line">Connecting to jdbc:hive2://aliyun:10000</span><br><span class="line">Enter username for jdbc:hive2://aliyun:10000: hadoop（回车）</span><br><span class="line">Enter password for jdbc:hive2://aliyun:10000: （直接回车）</span><br><span class="line">Connected to: Apache Hive (version 1.2.1)</span><br><span class="line">Driver: Hive JDBC (version 1.2.1)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">0: jdbc:hive2://aliyun:10000&gt; show databases;</span><br><span class="line">+----------------+--+</span><br><span class="line">| database_name |</span><br><span class="line">+----------------+--+</span><br><span class="line">| default |</span><br><span class="line">| hive_db2 |</span><br><span class="line">+----------------+--+</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="Hive常用交互命令"><a href="#Hive常用交互命令" class="headerlink" title="Hive常用交互命令"></a>Hive常用交互命令</h4><ol>
<li><p>Hive命令帮助</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive -help</span><br><span class="line">usage: hive</span><br><span class="line">-d,--define &lt;key=value&gt; Variable subsitution to apply to hive</span><br><span class="line">commands. e.g. -d A=B or --define A=B</span><br><span class="line">--database &lt;databasename&gt; Specify the database to use</span><br><span class="line">-e &lt;quoted-query-string&gt; SQL from command line</span><br><span class="line">-f &lt;filename&gt; SQL from files</span><br><span class="line">-H,--help Print help information</span><br><span class="line">--hiveconf &lt;property=value&gt; Use value for given property</span><br><span class="line">--hivevar &lt;key=value&gt; Variable subsitution to apply to hive</span><br><span class="line">commands. e.g. --hivevar A=B</span><br><span class="line">-i &lt;filename&gt; Initialization SQL file</span><br><span class="line">-S,--silent Silent mode in interactive shell</span><br><span class="line">-v,--verbose Verbose mode (echo executed SQL to the console)</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>&quot;-e&quot;</code>不进入hive的交互窗口执行sql语句</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive -e "select id from student;"</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>&quot;-f&quot;</code>执行脚本中sql语句</p>
<ol>
<li><p>在/opt/module/datas目录下创建hivef.sql文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun datas]$ touch hivef.sql</span><br></pre></td></tr></table></figure>

<p>文件中写入正确的sql语句</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">select * from student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行文件中的sql语句</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive -f /opt/module/datas/hivef.sql</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行文件中的sql语句并将结果写入文件中</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun hive]$ bin/hive -f /opt/module/datas/hivef.sql &gt; /opt/module/datas/hive_result.txt</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h4 id="Hive其他命令操作"><a href="#Hive其他命令操作" class="headerlink" title="Hive其他命令操作"></a>Hive其他命令操作</h4><ol>
<li><p>退出hive窗口：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash"><span class="built_in">exit</span>;</span></span><br><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">quit;</span></span><br></pre></td></tr></table></figure>

<p>在新版的hive中没区别了，在以前的版本是有的：<br>exit:先隐性提交数据，再退出；<br>quit:不提交数据，退出；</p>
</li>
<li><p>在hive cli命令窗口中如何查看hdfs文件系统</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">dfs -ls /;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>在hive cli命令窗口中如何查看本地文件系统</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">! ls /opt/module/datas;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>查看在hive中输入的所有历史命令</p>
<ol>
<li><p>进入到当前用户的根目录/root或/home/hadoop</p>
</li>
<li><p>查看. hivehistory文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun ~]$ cat .hivehistory</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
</ol>
<h3 id="Hive常见属性配置"><a href="#Hive常见属性配置" class="headerlink" title="Hive常见属性配置"></a>Hive常见属性配置</h3><h4 id="Hive数据仓库位置配置"><a href="#Hive数据仓库位置配置" class="headerlink" title="Hive数据仓库位置配置"></a>Hive数据仓库位置配置</h4><ol>
<li><p>Default数据仓库的最原始位置是在hdfs上的：/user/hive/warehouse路径下。</p>
</li>
<li><p>在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹。</p>
</li>
<li><p>修改default数据仓库原始位置（将hive-default.xml.template如下配置信息拷贝到hive-site.xml文件中）。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">	&lt;description&gt;location of default database for the warehouse&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>配置同组用户有执行权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/hdfs dfs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="查询后信息显示配置"><a href="#查询后信息显示配置" class="headerlink" title="查询后信息显示配置"></a>查询后信息显示配置</h4><ol>
<li><p>在hive-site.xml文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;hive.cli.print.header&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;hive.cli.print.current.db&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>重新启动hive，对比配置前后差异。</p>
</li>
</ol>
<h4 id="Hive运行日志信息配置"><a href="#Hive运行日志信息配置" class="headerlink" title="Hive运行日志信息配置"></a>Hive运行日志信息配置</h4><ol>
<li><p>Hive的log默认存放在/tmp/hadoop/hive.log目录下（当前用户名下）</p>
</li>
<li><p>修改hive的log存放日志到/opt/module/hive/logs</p>
<ol>
<li><p>修改/opt/module/hive/conf/hive-log4j.properties.template文件名称为hive-log4j.properties</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@aliyun conf]$ pwd</span><br><span class="line">/opt/module/hive/conf</span><br><span class="line">[hadoop@aliyun conf]$ mv hive-log4j.properties.template hive-log4j.properties</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>在hive-log4j.properties文件中修改log存放位置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive.log.dir=/opt/module/hive/logs</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="参数配置方式"><a href="#参数配置方式" class="headerlink" title="参数配置方式"></a>参数配置方式</h4><ol>
<li><p>查看当前所有的配置信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"><span class="built_in">set</span>;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>参数的配置三种方式</p>
<ol>
<li><p>配置文件方式<br>默认配置文件：hive-default.xml<br>用户自定义配置文件：hive-site.xml</p>
<p><code>注意：</code>用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。</p>
</li>
<li><p>命令行参数方式<br>启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。<br>例如：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10;</span><br></pre></td></tr></table></figure>

<p>注意：仅对本次hive启动有效<br>查看参数设置：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure>
</li>
<li><p>参数声明方式<br>可以在HQL中使用SET关键字设定参数<br>例如：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks=100;</span><br></pre></td></tr></table></figure>

<p>注意：仅对本次hive启动有效。<br>查看参数设置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</p>
</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/02/hive/2/">谷粒影音8道SQL题(各种Top N)</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-02</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h4 id="data表字段"><a href="#data表字段" class="headerlink" title="data表字段"></a>data表字段</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">videoId string 				<span class="keyword">comment</span> <span class="string">"视频唯一id"</span>, </span><br><span class="line">uploader <span class="keyword">string</span> 			<span class="keyword">comment</span> <span class="string">"视频上传者"</span>,</span><br><span class="line">age <span class="built_in">int</span> 					<span class="keyword">comment</span> <span class="string">"视频年龄"</span>,</span><br><span class="line"><span class="keyword">category</span> <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;		<span class="keyword">comment</span> <span class="string">"视频类别"</span>,</span><br><span class="line"><span class="keyword">length</span> <span class="built_in">int</span> 					<span class="keyword">comment</span> <span class="string">"视频长度"</span>,</span><br><span class="line">views <span class="built_in">int</span> 					<span class="keyword">comment</span> <span class="string">"观看次数"</span>,</span><br><span class="line">rate <span class="built_in">float</span> 					<span class="keyword">comment</span> <span class="string">"视频评分"</span>,</span><br><span class="line">ratings <span class="built_in">int</span> 				<span class="keyword">comment</span> <span class="string">"流量"</span>,</span><br><span class="line">comments <span class="built_in">int</span> 				<span class="keyword">comment</span> <span class="string">"评论数"</span>,</span><br><span class="line">relatedId <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;		<span class="keyword">comment</span> <span class="string">"相关视频id"</span></span><br></pre></td></tr></table></figure>

<h4 id="user表字段"><a href="#user表字段" class="headerlink" title="user表字段"></a>user表字段</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">uploader String				<span class="keyword">comment</span> <span class="string">"上传者用户名"</span>,</span><br><span class="line">videos <span class="built_in">int</span>					<span class="keyword">comment</span> <span class="string">"上传视频数"</span>,</span><br><span class="line">friends	 <span class="built_in">int</span>				<span class="keyword">comment</span> <span class="string">"朋友数量"</span>,</span><br></pre></td></tr></table></figure>

<h4 id="8道题目-思路"><a href="#8道题目-思路" class="headerlink" title="8道题目(思路)"></a>8道题目(思路)</h4><p>统计硅谷影音视频网站的常规指标，各种TopN指标：</p>
<ol>
<li>统计视频观看数Top10</li>
<li>统计视频类别热度Top10</li>
<li>统计视频观看数Top20所属类别</li>
<li>统计视频观看数Top50所关联视频的所属类别Rank</li>
<li>统计每个类别中的视频热度Top10</li>
<li>统计每个类别中视频流量Top10</li>
<li>统计上传视频最多的用户Top10以及他们上传的视频</li>
<li>统计每个类别视频观看数Top10</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/11/01/hive/1/">Hive中的字符集编码若干问题</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-01</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hive/">Hive</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hive/">Hive</a></span><div class="content"><h3 id="个人初始开发环境的基本情况以及Hive元数据库说明"><a href="#个人初始开发环境的基本情况以及Hive元数据库说明" class="headerlink" title="个人初始开发环境的基本情况以及Hive元数据库说明"></a>个人初始开发环境的基本情况以及Hive元数据库说明</h3><ol>
<li><p>hive的元数据库改成了mysql(安装完mysql之后也没有进行其它别的设置)</p>
</li>
<li><p>hive-site.xml中设置元数据库对应的配置为  <code>jdbc:mysql://ip:3306/metastore?createDatabaseIfNotExist=true</code></p>
</li>
<li><p>普通情况下咱们的mysql默认编码是latin1,但是我们在日常开发中大多数情况下需要用到utf-8编码,如果是默认latin1的话,咱们的中文存储进去容易乱码,所以说大家在遇到一些数据乱码的情况话,最好把mysql的编码改成utf-8.</p>
</li>
</ol>
<p><code>注意:</code> 但是在这里要非常严重强调的一点:hive的元数据metastore在mysql的数据库,不管是数据库本身,还是里面的表编码都必须是latin1(CHARACTER SET latin1 COLLATE latin1_bin)!!!!!</p>
<h4 id="验证方式"><a href="#验证方式" class="headerlink" title="验证方式:"></a>验证方式:</h4><p>可以通过客户端软件在数据库上右键属性查看,也可以通过命令查看</p>
<p>mysql&gt; show create database hive_cz3q;</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">-----------+-----------------------------------------------------------------------------------------+</span></span><br><span class="line">| Database  | <span class="keyword">Create</span> <span class="keyword">Database</span>                                                                         |</span><br><span class="line">+<span class="comment">-----------+-----------------------------------------------------------------------------------------+</span></span><br><span class="line">| hive_cz3q | <span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> <span class="string">`hive_cz3q`</span> <span class="comment">/*!40100 DEFAULT CHARACTER SET latin1 COLLATE latin1_bin */</span> |</span><br><span class="line">+<span class="comment">-----------+-----------------------------------------------------------------------------------------+</span></span><br></pre></td></tr></table></figure>

<p>不然会有类似如下的错误:</p>
<p> <img src="https://yerias.github.io/hive_img/610238-20170903131046858-396716990.png" alt="图片1"></p>
<p>那么怎么修改mysql的编码为utf8呢?这里提供了在线安装修改和离线方式安装下的修改方式供大家选择!</p>
<h3 id="乱码的情况"><a href="#乱码的情况" class="headerlink" title="乱码的情况:"></a>乱码的情况:</h3><p> 向hive的表中 创建表,表语句部分如下:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ods.ods_order</span><br><span class="line">(</span><br><span class="line">   ORDER_ID             <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'订单ID'</span>,</span><br><span class="line">   ORDER_NO             <span class="built_in">varchar</span>(<span class="number">30</span>)  <span class="keyword">comment</span> <span class="string">'订单编号（唯一字段），</span></span><br><span class="line"><span class="string">   DEALER_ID            int comment '</span>门店<span class="keyword">ID</span><span class="string">',</span></span><br><span class="line"><span class="string">   CUST_ID              int comment '</span>客户<span class="keyword">ID</span><span class="string">',</span></span><br></pre></td></tr></table></figure>

<p>在创建表的时候，字段可以有 comment，但是 comment 建议不要用中文说明，因为我们说过，hive 的 metastore 支持的字符集是 latin1，所以中文写入的时候会有编码问题，如下图！ </p>
<p>然后通过desc ods_order 查看 对应的comment中是中文的地方,通过Xshell显示全部都是 “?” 问号.  同时确认了Xshell支持显示中文(排除Xshell的问题).</p>
<p><code>以上就是说Hive在字段定义时的Comment中文乱码问题.</code></p>
<p><img src="https://yerias.github.io/hive_img/610238-20170903134309687-1604377616.png" alt="图片2"></p>
<p>有了上述的问题，那么我们该如何去解决注释中文乱码问题呢？ </p>
<h3 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h3><h4 id="首先进行Mysql的编码设置"><a href="#首先进行Mysql的编码设置" class="headerlink" title="首先进行Mysql的编码设置"></a>首先进行Mysql的编码设置</h4><h5 id="离线安装mysql的修改方式"><a href="#离线安装mysql的修改方式" class="headerlink" title="离线安装mysql的修改方式"></a>离线安装mysql的修改方式</h5><ol>
<li><p>修改编码,设置为utf8</p>
<p>拷贝 mysql 的配置文件/usr/share/mysql/my-small.cnf 到/etc/my.cnf </p>
<p>在mysql 配置文件/etc/my.cnf 中增加以下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[client]下面增加</span><br><span class="line">default-character-set=utf8</span><br><span class="line">在[mysqld]下面增加</span><br><span class="line">default-character-set=utf8</span><br><span class="line">init_connect='SET NAMES utf8'</span><br></pre></td></tr></table></figure>

<p><em>2020/03/09更新</em>：<code>default-character-set=utf8</code>，如果这样改会导致5.7版本mysql无法打开所以要改为 <code>character-set-server=utf8</code>，(可选)改完后，要删除数据库中所有数据，才能使用。``````````</p>
</li>
<li><p>重启mysql 服务(这样确保缺省编码是utf8)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">service mysqld restart</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证编码是否改成了utf8:</p>
<p>输入命令 “\s”</p>
<p><img src="https://yerias.github.io/hive_img/610238-20170903132119671-1333763157.png" alt="图片3"></p>
<p>输入命令:show variables like ‘char%’</p>
<p><img src="https://yerias.github.io/hive_img/610238-20170903132210124-717565896.png" alt="图片4"></p>
<p>输入命令:show variables like “colla%”;</p>
<p> <img src="https://yerias.github.io/hive_img/610238-20170903132350968-1341436179.png" alt="图片5"></p>
<p>OK修改成功!</p>
</li>
<li><p>这样在启动hive,向hive中插入的表中comment等有汉字的情况,就可以正常的显示(如下为本人测试的部分显示结果):</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://localhost:10000/ods&gt; desc ods_order;</span><br><span class="line">+<span class="comment">--------------------------+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--+</span></span><br><span class="line">|         col_name         |       data_type       |                                                                   <span class="keyword">comment</span>                                                                   |</span><br><span class="line">+<span class="comment">--------------------------+-----------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--+</span></span><br><span class="line">| order_id                 | <span class="built_in">int</span>                   | 订单<span class="keyword">ID</span>                                                                                                                                        |</span><br><span class="line">| order_no                 | <span class="built_in">varchar</span>(<span class="number">30</span>)           | 订单编号（唯一字段），前缀字符表示订单来源：a，Andriod；b，微博；c，WEB；e，饿了么；i，Iphone；m，Mobile；x，微信； z，中粮我买网；l，其它。 接着<span class="number">3</span>位数字代表订单城市编号；接着字符z与后面的真正订单编号分隔。这套机制从<span class="number">2014</span>年<span class="number">12</span>月开始实施。</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h5 id="在线安装mysql的修改方式"><a href="#在线安装mysql的修改方式" class="headerlink" title="在线安装mysql的修改方式"></a>在线安装mysql的修改方式</h5><ol>
<li><p>修改编码,设置为utf-8</p>
<p> 在 mysql 配置文件/etc/my.cnf（不需要拷贝）中[mysqld]的下面增加以下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">init_connect='SET collation_connection = utf8_unicode_ci'</span><br><span class="line">init_connect='SET NAMES utf8'</span><br><span class="line">character-set-server=utf8</span><br><span class="line">collation-server=utf8_unicode_ci</span><br><span class="line">skip-character-set-client-handshake</span><br></pre></td></tr></table></figure>

<p> <img src="https://yerias.github.io/hive_img/610238-20170903133604796-492434925.png" alt="图片6"></p>
</li>
<li><p>重启mysqld服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">service mysqld restart</span><br></pre></td></tr></table></figure>
</li>
<li><p>和离线方式一样验证编码是否确实修改;</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">show variables like 'char%';</span><br></pre></td></tr></table></figure>

<p> <img src="https://yerias.github.io/hive_img/610238-20170903133820296-974086333.png" alt="图片7"></p>
</li>
</ol>
<h4 id="针对元数据库metastore中的表-分区-视图的编码设置"><a href="#针对元数据库metastore中的表-分区-视图的编码设置" class="headerlink" title="针对元数据库metastore中的表,分区,视图的编码设置"></a>针对元数据库metastore中的表,分区,视图的编码设置</h4><p>因为我们知道 metastore 支持数据库级别，表级别的字符集是 latin1，那么我们<code>只需要把相应注释的地方的字符集由 latin1 改成 utf-8</code>，就可以了。用到注释的就三个地方，表、分区、视图。如下修改分为两个步骤：</p>
<ol>
<li><p>进入数据库 Metastore 中执行以下 5 条 SQL 语句 </p>
<ol>
<li>修改表字段注解和表注解<br>alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8<br>alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8</li>
<li>修改分区字段注解：<br>alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8 ;<br>alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8</li>
<li>修改索引注解：<br>alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</li>
</ol>
</li>
<li><p>修改 metastore 的连接 URL</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;jdbc:mysql://IP:3306/db_name?createDatabaseIfNotExist=true&amp;amp;useUnicode=true&amp;amp;characterEncoding=UTF-8&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p><strong>测试结果：</strong></p>
<p><img src="https://yerias.github.io/hive_img/610238-20170903134604093-93033588.png" alt="图片8"></p>
<p>以上就能完美的解决这个问题.</p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>有时候在使用xml作为配置文件的时候，应该要使用xml的编码规则来进行适当的设置。如<code>&amp;</code>在xml文件中应该写为<code>&amp;amp;</code></p>
<p>下面给出xml中一些特殊符号的编码转换：</p>
<table>
<thead>
<tr>
<th>代码</th>
<th>符号</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td><code>&amp;lt;</code></td>
<td>&lt;</td>
<td>小于号</td>
</tr>
<tr>
<td><code>&amp;gt;</code></td>
<td>&gt;</td>
<td>大于号</td>
</tr>
<tr>
<td><code>&amp;amp;</code></td>
<td>&amp;</td>
<td>and字符</td>
</tr>
<tr>
<td><code>&amp;apos;</code></td>
<td>‘</td>
<td>单引号</td>
</tr>
<tr>
<td><code>&amp;quot;</code></td>
<td>“</td>
<td>双引号</td>
</tr>
</tbody></table>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/16/hadoop/12/">MR调优之压缩</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-16</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/MapReduce/">MapReduce</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/MapReduce/">MapReduce</a></span><div class="content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>什么是压缩</li>
<li>压缩的好处与坏处</li>
<li>常见的压缩格式</li>
<li>优缺点比较</li>
<li>如何选择压缩格式</li>
<li>MR配置文件压缩格式</li>
<li>Hive配置文件压缩格式</li>
</ol>
<h2 id="什么是压缩"><a href="#什么是压缩" class="headerlink" title="什么是压缩"></a>什么是压缩</h2><p>压缩就是通过某种技术（算法）把原始文件变小，相应的解压就是把压缩后的文件变成原始文件。嘿嘿是不是又可以变大又可以变小。</p>
<h2 id="压缩的好处与坏处"><a href="#压缩的好处与坏处" class="headerlink" title="压缩的好处与坏处"></a>压缩的好处与坏处</h2><p><strong>好处</strong></p>
<ul>
<li>减少存储磁盘空间</li>
<li>降低IO(网络的IO和磁盘的IO)</li>
<li>加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度</li>
</ul>
<p><strong>坏处</strong></p>
<ul>
<li>由于使用数据时，需要先将数据解压，加重<strong>CPU</strong>负荷</li>
</ul>
<h2 id="常见的压缩格式"><a href="#常见的压缩格式" class="headerlink" title="常见的压缩格式"></a>常见的压缩格式</h2><table>
<thead>
<tr>
<th align="left">格式</th>
<th align="left">可分割</th>
<th align="left">平均压缩速度</th>
<th align="left">文本文件压缩效率</th>
<th align="left">Hadoop压缩编解码器</th>
<th align="center">纯Java实现</th>
<th align="left">原生</th>
<th align="left">备注</th>
</tr>
</thead>
<tbody><tr>
<td align="left">gzip</td>
<td align="left">否</td>
<td align="left">快</td>
<td align="left">高</td>
<td align="left">org.apache.hadoop.io.compress.GzipCodec</td>
<td align="center">是</td>
<td align="left">是</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">lzo</td>
<td align="left">是（取决于所使用的库）</td>
<td align="left">非常快</td>
<td align="left">中等</td>
<td align="left">com.hadoop.compression.lzo.LzoCodec</td>
<td align="center">是</td>
<td align="left">是</td>
<td align="left">需要在每个节点上安装LZO</td>
</tr>
<tr>
<td align="left">bzip2</td>
<td align="left">是</td>
<td align="left">慢</td>
<td align="left">非常高</td>
<td align="left">org.apache.hadoop.io.compress.Bzip2Codec</td>
<td align="center">是</td>
<td align="left">是</td>
<td align="left">为可分割版本使用纯Java</td>
</tr>
<tr>
<td align="left">zlib</td>
<td align="left">否</td>
<td align="left">慢</td>
<td align="left">中等</td>
<td align="left">org.apache.hadoop.io.compress.DefaultCodec</td>
<td align="center">是</td>
<td align="left">是</td>
<td align="left">Hadoop 的默认压缩编解码器</td>
</tr>
<tr>
<td align="left">Snappy</td>
<td align="left">否</td>
<td align="left">非常快</td>
<td align="left">低</td>
<td align="left">org.apache.hadoop.io.compress.SnappyCodec</td>
<td align="center">否</td>
<td align="left">是</td>
<td align="left">Snappy 有纯Java的移植版，但是在Spark/Hadoop中不能用</td>
</tr>
</tbody></table>
<p>一个简单的案例对于集中压缩方式之间的压缩大小和压缩时间进行一个感观性的认识</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">测试环境:</span><br><span class="line">8 core i7 cpu </span><br><span class="line">8GB memory</span><br><span class="line">64 bit CentOS</span><br><span class="line">1.4GB Wikipedia Corpus 2-gram text input</span><br></pre></td></tr></table></figure>

<p>压缩比</p>
<p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E6%AF%94.png" alt="压缩比"></p>
<p>压缩时间比</p>
<p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E6%97%B6%E9%97%B4.png" alt="压缩时间"></p>
<p>可以看出，压缩比越高，压缩时间越长</p>
<h2 id="优缺点比较"><a href="#优缺点比较" class="headerlink" title="优缺点比较"></a>优缺点比较</h2><table>
<thead>
<tr>
<th align="left">压缩格式</th>
<th align="left">优点</th>
<th align="left">缺点</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>gzip</strong></td>
<td align="left">压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便</td>
<td align="left"><strong>不支持split</strong></td>
<td></td>
</tr>
<tr>
<td align="left"><strong>lzo</strong></td>
<td align="left">压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便</td>
<td align="left">压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）</td>
<td></td>
</tr>
<tr>
<td align="left"><strong>snappy</strong></td>
<td align="left">压缩速度快；支持hadoop native库</td>
<td align="left"><strong>不支持split</strong>；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令d. bzip2</td>
<td></td>
</tr>
<tr>
<td align="left"><strong>bzip2</strong></td>
<td align="left">支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便</td>
<td align="left">压缩/解压速度慢；不支持native</td>
<td></td>
</tr>
</tbody></table>
<h2 id="如何选择压缩格式"><a href="#如何选择压缩格式" class="headerlink" title="如何选择压缩格式"></a>如何选择压缩格式</h2><p>从两方面考虑：Storage + Compute；</p>
<ol>
<li>Storage ：基于HDFS考虑，减少了存储文件所占空间，提升了数据传输速率；如gzip、bzip2。</li>
<li>Compute：基于YARN上的计算(MapReduce/Hive/Spark/….)速度的提升；如lzo、lz4、snappy。</li>
</ol>
<p>不同的场景选择不同的压缩方式，肯定没有一个一劳永逸的方法，如果选择高压缩比，那么对于cpu的性能要求要高，同时压缩、解压时间耗费也多；选择压缩比低的，对于磁盘io、网络io的时间要多，空间占据要多；<strong>对于支持分割的，可以实现并行处理</strong>。</p>
<ol>
<li>IO密集型：使用压缩</li>
<li>运算密集型：慎用压缩</li>
</ol>
<h2 id="压缩在MapReduce中的应用场景"><a href="#压缩在MapReduce中的应用场景" class="headerlink" title="压缩在MapReduce中的应用场景"></a>压缩在MapReduce中的应用场景</h2><p><img src="https://yerias.github.io/hadoop_img/%E5%8E%8B%E7%BC%A9%E5%9C%A8MR%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF.png" alt="压缩在MR的应用场景"></p>
<p>压缩在hadoop中的应用场景总结在三方面：<strong>输入</strong>，<strong>中间</strong>，<strong>输出</strong>。</p>
<p>整体思路：hdfs ==&gt; map ==&gt; shuffle ==&gt; reduce</p>
<ol>
<li>Use Compressd Map Input: 从HDFS中读取文件进行Mapreuce作业，如果数据很大，可以使用压缩并且<strong>选择支持分片的压缩方式（Bzip2,LZO）</strong>，可以实现并行处理，提高效率，减少磁盘读取时间，同时选择合适的存储格式例如Sequence Files，RC,ORC等；</li>
<li>Compress Intermediate Data: Map输出作为Reducer的输入，需要经过shuffle这一过程，需要把数据读取到一个环形缓冲区，然后读取到本地磁盘，所以选择压缩可以减少了存储文件所占空间，提升了数据传输速率，建议<strong>使用压缩速度快的压缩方式，例如Snappy和LZO.</strong></li>
<li>Compress Reducer Output: 进行归档处理或者链式Mapreduce的工作（该作业的输出作为下个作业的输入），压缩可以减少了存储文件所占空间，提升了数据传输速率，如果作为归档处理，可以<strong>采用高的压缩比（Gzip,Bzip2）</strong>，如果作为下个作业的输入，考虑<strong>是否要分片</strong>进行选择。</li>
</ol>
<h2 id="MR配置文件压缩格式"><a href="#MR配置文件压缩格式" class="headerlink" title="MR配置文件压缩格式"></a>MR配置文件压缩格式</h2><p>hadoop自带不支持split的gzip和支持split的bzip2，我们还手动安装了lzo的压缩方式</p>
<ol>
<li><p>修改<code>core-site.xml</code>文件</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">    org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">    org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">    org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">    com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">    com.hadoop.compression.lzo.LzopCodec,</span><br><span class="line">    org.apache.hadoop.io.compress.Lz4Codec,</span><br><span class="line">    org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">    <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>修改<code>mapred-site.xml</code>文件</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">#开启支持压缩</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.output.fileoutputformat.compress<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">#压缩方式/最终输出的压缩方式</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.output.fileoutputformat.compress.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.compress.BZip2Codec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line">#中间压缩(可选，Snappy需要手动安装)</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.map.output.compression.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.compress.SnappyCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>中间压缩</strong>：中间压缩就是处理作业map任务和reduce任务之间的数据，对于中间压缩，最好选择一个节省CPU耗时的压缩方式（快）</p>
<p><strong>最终压缩</strong>：可以选择高压缩比，减少了存储文件所占空间，提升了数据传输速率<br><code>mapred-site.xml</code> 中设置</p>
</li>
<li><p>验证，跑个wc看最终输出文件的后缀</p>
</li>
</ol>
<p><strong>更换压缩方式只需要修改中间输出或者最终输出的压缩类即可</strong></p>
<h2 id="Hive配置文件压缩格式"><a href="#Hive配置文件压缩格式" class="headerlink" title="Hive配置文件压缩格式"></a>Hive配置文件压缩格式</h2><ol>
<li><p>配置压缩功能</p>
<p>hive配置文件压缩格式只需要配置两个参数</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">//开启压缩功能</span><br><span class="line">SET hive.exec.compress.output=true; </span><br><span class="line">//设置最终以bz2格式存储</span><br><span class="line">SET mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Code;</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>：不建议再配置文件中设置</p>
</li>
<li><p>使用压缩</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">#创建表：</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载数据：</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/data/page_views.dat"</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> page_views;  </span><br><span class="line"></span><br><span class="line"><span class="comment">#配置压缩格式</span></span><br><span class="line">hive：</span><br><span class="line"><span class="keyword">SET</span> hive.exec.compress.output=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">SET</span> mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建压缩表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> page_views_bzip2 <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span> <span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> page_views;</span><br></pre></td></tr></table></figure></li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/15/hadoop/13/">HADOOP安装LZO压缩</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a></span><div class="content"><h3 id="编译安装lzo与lzop"><a href="#编译安装lzo与lzop" class="headerlink" title="编译安装lzo与lzop"></a>编译安装lzo与lzop</h3><p> <strong>在集群的每一台主机上都需要编译安装！！！</strong></p>
<ol>
<li><p>下载编译安装lzo文件，<a href="http://www.oberhumer.com/opensource/lzo/download" target="_blank" rel="noopener"><strong>版本可以下载最新的</strong></a> </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget http://www.oberhumer.com/opensource/lzo/download/lzo-2.10.tar.gz</span><br></pre></td></tr></table></figure>
</li>
<li><p>编译安装(保证主机上有gcc与g++)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xvzf   lzo-2.10.tar.gz</span><br><span class="line">cd lzo-2.10</span><br><span class="line">./configure --enable-shared</span><br><span class="line">make -j 10</span><br><span class="line">make install</span><br><span class="line">cp /usr/local/lib/*lzo* /usr/lib</span><br></pre></td></tr></table></figure>

<p><strong>安装完成后需要将 cp部分文件到/usr/lib中，这个步骤不做会抛</strong>   lzop: error while loading shared libraries: liblzo2.so.2: cannot open shared object file: No such file or directory</p>
</li>
<li><p>下载编译lzop，<a href="http://www.lzop.org/download/" target="_blank" rel="noopener"><strong>最新版选择</strong></a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget http://www.lzop.org/download/lzop-1.04.tar.gz</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xvzf lzop-1.04.tar.gz </span><br><span class="line">cd lzop-1.04 </span><br><span class="line">./configure </span><br><span class="line">make -j 10 </span><br><span class="line">make install</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="安装、编译hadoop-lzo-master"><a href="#安装、编译hadoop-lzo-master" class="headerlink" title="安装、编译hadoop-lzo-master"></a>安装、编译hadoop-lzo-master</h3><p>需在linux环境中安装,在windows上编译不过</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget https://github.com/twitter/hadoop-lzo/archive/master.zip</span><br></pre></td></tr></table></figure>

<ol>
<li><p>解压</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">unzip master.zip </span><br><span class="line">cd hadoop-lzo-master/</span><br></pre></td></tr></table></figure>
</li>
<li><p>编辑<em>pom.xm</em>l修改hadoop的版本号与你集群中hadoop版本一致   </p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">hadoop.current.version</span>&gt;</span>2.6.0-cdh5.16.2<span class="tag">&lt;/<span class="name">hadoop.current.version</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>pom文件增加cloudera的仓库地址</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--允许发布版本，禁止快照版--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">releases</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">releases</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>false<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>检查所在主机是否有maven,如果没有需要安装,如下(安装了maven即可跳过):</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget http://mirrors.hust.edu.cn/apache/maven/maven-3/3.5.4/binaries/apache-maven-3.5.4-bin.tar.gz</span><br><span class="line"></span><br><span class="line">tar -zxvf apache-maven-3.5.4-bin.tar.gz</span><br><span class="line"></span><br><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line">添加环境变量:</span><br><span class="line">MAVEN_HOME=/usr/local/apache-maven-3.5.4</span><br><span class="line">export MAVEN_HOME</span><br><span class="line">export PATH=$&#123;PATH&#125;:$&#123;MAVEN_HOME&#125;/bin</span><br><span class="line"></span><br><span class="line">保存退出profile</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>
</li>
<li><p>在maven中配置阿里云和cloudera的仓库</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>nexus-aliyun<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>*,!cloudera<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Nexus aliyun<span class="tag">&lt;/<span class="name">name</span>&gt;</span>                     </span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span></span><br><span class="line">        http://maven.aliyun.com/nexus/content/groups/public</span><br><span class="line">    <span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>导入hadoop-lzo编译时需要路径信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export CFLAGS=-m64</span><br><span class="line">export CXXFLAGS=-m64</span><br><span class="line">export C_INCLUDE_PATH=/home/hadoop/app/hadoop/lzo/include</span><br><span class="line">export LIBRARY_PATH=/home/hadoop/app/hadoop/lzo/lib</span><br></pre></td></tr></table></figure>
</li>
<li><p>maven编译安装</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mvn clean package -Dmaven.test.skip=true</span><br></pre></td></tr></table></figure>

<p><strong>编译安装没有异常结束后往下继续   PS:如果在mvn这里出现异常，请解决后再继续，注意权限问题</strong></p>
</li>
<li><p>编译成功后会有target文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd target/native/Linux-amd64-64/</span><br><span class="line">mkdir ~/hadoop-lzo-files</span><br><span class="line">tar -cBf - -C lib . | tar -xBvf - -C ~/hadoop-lzo-files</span><br></pre></td></tr></table></figure>
</li>
<li><p>在 ~/hadoop-lzo-files 目录下产生几个文件,执行cp</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp ~/hadoop-lzo-files/libgplcompression*  $HADOOP_HOME/lib/native/</span><br></pre></td></tr></table></figure>

<p><strong>注意！！！上面这一步的cp文件也要同步到集群其他主机的hadoop的对应目录下</strong></p>
</li>
<li><p>cp  hadoop-lzo的jar包到hadoop目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp target/hadoop-lzo-0.4.21-SNAPSHOT.jar $HADOOP_HOME/share/hadoop/common/</span><br></pre></td></tr></table></figure>

<p><strong>注意！！！上面这一步的cp文件也要同步到集群其他主机的hadoop的对应目录下</strong></p>
</li>
</ol>
<h3 id="配置hadoop配置文件"><a href="#配置hadoop配置文件" class="headerlink" title="配置hadoop配置文件"></a>配置hadoop配置文件</h3><ol>
<li><p>修改<strong>core-site.xml</strong>(<em>如果配置过了不需要配置</em>)</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">        org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">        com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzopCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.Lz4Codec,</span><br><span class="line">        org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">    <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>修改<strong>mapred-site.xml</strong>中的压缩方式</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">#开启压缩</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.compress.map.output<span class="tag">&lt;/<span class="name">name</span>&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span>    </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">#配置压缩方式</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.map.output.compression.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span>    </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>跑个wc验证输出文件是否压缩</p>
</li>
<li><p>创建索引</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop jar $HADOOP_HOME/share/hadoop/common/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer /data/lzo-index/large_wc.txt.lzo</span><br></pre></td></tr></table></figure>

</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/15/hadoop/14/">MapReduce使用压缩以及在MR中的通用做法</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a></span><div class="content"><p>上一步中我们在Hadoop中安装了lzo的压缩方式，现在将测试如何在MapReduce程序中使用压缩</p>
<ol>
<li><p>在MapReduce中使用压缩，要注意三个位置，分别是map输入文件的压缩格式，map输出的压缩格式，和reduce最终输出的压缩格式</p>
<ul>
<li><p>首先配置使用压缩</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">mapreduce.output.fileoutputformat.compress=<span class="keyword">true</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>第二步配置输入文件的压缩格式(如lzo):</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">mapreduce.job.inputformat<span class="class">.<span class="keyword">class</span></span>=com.hadoop.mapreduce.LzoTextInputFormat</span><br></pre></td></tr></table></figure>
</li>
<li><p>第三步配置map输出的文件压缩格式(如snappy):</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">mapred.map.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec</span><br></pre></td></tr></table></figure>
</li>
<li><p>第四步配置reduce输出的文件压缩格式(可不配置，这里配置lzo):</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Codec</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>我们随意找个表，查看表结构(这里只拿出来了我们想看的)</p>
<p>desc formatted emp;</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Storage Information	 	 </span><br><span class="line">SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 </span><br><span class="line">InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 </span><br><span class="line">OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</span><br></pre></td></tr></table></figure>

<p>由于hive底层也是跑的MapReduce，现在我们就能知道为什么要设置InputFormat和OutputFormat了。</p>
</li>
<li><p>一般不固定写在配置文件中，而是提交作业的时候手动指定，通过-D 指定参数</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop jar hadoop-mapreduce-examples-<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.16</span><span class="number">.2</span>.jar wordcount \</span><br><span class="line"> -D mapreduce.output.fileoutputformat.compress=<span class="keyword">true</span> \</span><br><span class="line"> -D io.compression.codec.lzo<span class="class">.<span class="keyword">class</span></span>=com.hadoop.compression.lzo.LzoCodec \</span><br><span class="line"> -D mapreduce.job.inputformat<span class="class">.<span class="keyword">class</span></span>=com.hadoop.mapreduce.LzoTextInputFormat \</span><br><span class="line"> -D mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzopCodec \</span><br><span class="line"> /data/lzo-index/  /out</span><br></pre></td></tr></table></figure>
</li>
<li><p>给.lzo文件创建索引</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop jar $HADOOP_HOME/share/hadoop/common/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer /data/lzo-index/large_wc.txt.lzo</span><br></pre></td></tr></table></figure>


</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/14/hadoop/11/">数据倾斜&amp;MRchain解决数据倾斜&amp;大小表Reduce Join&amp;大小表Map Join&amp;SQL的执行计划</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-14</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/MapReduce/">MapReduce</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/MapReduce/">MapReduce</a></span><div class="content"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>数据倾斜</li>
<li>MRchain解决数据倾斜</li>
<li>大小表Reduce Join</li>
<li>大小表Map Join</li>
<li>SQL的执行计划</li>
</ol>
<h2 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h2><ol>
<li><p>数据倾斜怎么造成的</p>
<p>mapreduce计算是将map相同的key丢到reduce，在reduce中进行聚合操作,在map和reduce中间有个shuffle操作，shuffle会将map阶段相同的key划分到reduce阶段中的一个reduce中去，数据倾斜就是数据的key 的分化严重不均，造成一部分数据很多，一部分数据很少的局面。</p>
</li>
<li><p>数据倾斜产生的问题</p>
<ul>
<li><p>有一个或多个reduce卡住</p>
</li>
<li><p>各种container报错OOM</p>
</li>
<li><p>读写的数据量极大，至少远远超过其它正常的reduce</p>
</li>
<li><p>伴随着数据倾斜，会出现任务被kill等各种诡异的表现。</p>
</li>
</ul>
</li>
<li><p>原因和解决方法</p>
<p>原因:</p>
<ul>
<li>单个值有大量记录(1.内存的限制存在，2.可能会对集群其他任务的运行产生不稳定的影响)</li>
<li>唯一值较多(单个唯一值的记录数占用内存不会超过分配给reduce的内存)</li>
</ul>
<p>解决办法:</p>
<ul>
<li><p>增加reduce个数</p>
</li>
<li><p>使用自定义partitioner</p>
</li>
<li><p>增加reduce 的jvm内存（效果不好）</p>
</li>
<li><p>map 阶段将造成倾斜的key 先分成多组加随机数并且在reduce阶段去除随机数</p>
</li>
<li><p>从业务和数据上解决数据倾斜</p>
<p>我们通过设计的角度尝试解决它</p>
<ul>
<li>数据预处理，过滤掉异常值</li>
<li>将数据打散让它的并行度变大，再汇集</li>
</ul>
</li>
<li><p>平台的优化方法</p>
<ul>
<li>join 操作中，使用 map join 在 map 端就先进行 join ，免得到reduce 时卡住</li>
<li>能先进行 group 操作的时候先进行 group 操作，把 key 先进行一次 reduce,之后再进行 count 或者 distinct count 操作</li>
<li>设置map端输出、中间结果压缩</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="MRchain解决数据倾斜"><a href="#MRchain解决数据倾斜" class="headerlink" title="MRchain解决数据倾斜"></a>MRchain解决数据倾斜</h2><p>核心思想: 第一个mapredue把具有数据倾斜特性的数据加盐(随机数)，进行聚合；第二个mapreduce把第一个mapreduce的加盐结果进行去盐，再聚合，问题是两个MR IO高。</p>
<p>参考代码:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.item;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.utils.FileUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.chain.ChainMapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.chain.ChainReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.Random;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-02-04 14:50</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ChainMRDriver</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Random r;</span><br><span class="line">    String in = <span class="string">"data/skew/access.txt"</span>;</span><br><span class="line">    String out1 = <span class="string">"out/mr1"</span>;</span><br><span class="line">    String out2 = <span class="string">"out/mr2"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(conf, <span class="keyword">new</span> ChainMRDriver(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">super</span>.getConf();</span><br><span class="line">        FileUtil.checkFileIsExists(conf, out1);</span><br><span class="line">        FileUtil.checkFileIsExists(conf, out2);</span><br><span class="line"></span><br><span class="line">        Job job1 = Job.getInstance(conf);</span><br><span class="line">        Job job2 = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        job1.setJarByClass(ChainMRDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job2.setJarByClass(ChainMRDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        ChainMapper.addMapper(job1, ChainMRDriver.incRanDomMapper<span class="class">.<span class="keyword">class</span>, <span class="title">LongWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line">        ChainReducer.setReducer(job1, ChainMRDriver.incRanDomReduver<span class="class">.<span class="keyword">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line"></span><br><span class="line">        ChainMapper.addMapper(job2, ChainMRDriver.decRanDomMapper<span class="class">.<span class="keyword">class</span>, <span class="title">LongWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line">        ChainReducer.setReducer(job2, ChainMRDriver.decRanDomReduver<span class="class">.<span class="keyword">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job1, <span class="keyword">new</span> Path(in));</span><br><span class="line">        FileOutputFormat.setOutputPath(job1, <span class="keyword">new</span> Path(out1));</span><br><span class="line">        FileInputFormat.setInputPaths(job2, <span class="keyword">new</span> Path(out1));</span><br><span class="line">        FileOutputFormat.setOutputPath(job2, <span class="keyword">new</span> Path(out2));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//提交job1和job2 job1--&gt;job2 必须按照顺序提交</span></span><br><span class="line">        System.out.println(<span class="string">"=============第一阶段=============="</span>);</span><br><span class="line">        <span class="keyword">boolean</span> b = job1.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">if</span> (b) &#123;</span><br><span class="line">            System.out.println(<span class="string">"=============第二阶段=============="</span>);</span><br><span class="line">            <span class="keyword">boolean</span> b1 = job2.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">            <span class="keyword">return</span> b1 ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">incRanDomMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//创建对象</span></span><br><span class="line">            r = <span class="keyword">new</span> Random();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//把数据读出来，加盐  www.baidu.com   2</span></span><br><span class="line">            String[] line = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">            String incR = r.nextInt(<span class="number">10</span>) +<span class="string">"_"</span>+line[<span class="number">0</span>];</span><br><span class="line">            <span class="keyword">int</span> number = Integer.parseInt(line[<span class="number">1</span>]);</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(incR), <span class="keyword">new</span> IntWritable(number));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (r != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">//回收对象</span></span><br><span class="line">                r = <span class="keyword">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">incRanDomReduver</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">int</span> sums = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (IntWritable sum : values) &#123;</span><br><span class="line">                sums += sum.get();</span><br><span class="line">            &#125;</span><br><span class="line">            context.write(key, <span class="keyword">new</span> IntWritable(sums));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">decRanDomMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="comment">//去盐 聚合</span></span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            String[] line = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">            String decWord = line[<span class="number">0</span>].split(<span class="string">"_"</span>)[<span class="number">1</span>];</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(decWord), <span class="keyword">new</span> IntWritable(Integer.parseInt(line[<span class="number">1</span>])));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">decRanDomReduver</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">int</span> sums = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (IntWritable sum : values) &#123;</span><br><span class="line">                sums += sum.get();</span><br><span class="line">            &#125;</span><br><span class="line">            context.write(key, <span class="keyword">new</span> IntWritable(sums));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="SQL的执行计划"><a href="#SQL的执行计划" class="headerlink" title="SQL的执行计划"></a>SQL的执行计划</h2><p>如何运行SQL的执行计划</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">EXPLAIN</span> [<span class="keyword">EXTENDED</span>] Syntax</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">EXPLAIN</span> <span class="keyword">select</span> e.empno,e.ename,d.deptno,d.dname <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure>

<p>解析这句SQL的执行计划</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">+-----------------------------------------------------------------+--+</span><br><span class="line">                      		Explain                       </span><br><span class="line">+-----------------------------------------------------------------+--+</span><br><span class="line"> STAGE DEPENDENCIES:     <span class="comment">//阶段性依赖                           </span></span><br><span class="line">   Stage-<span class="number">4</span> is a root stage   <span class="comment">//这是一个根依赖                       </span></span><br><span class="line">   Stage-<span class="number">3</span> depends on stages: Stage-<span class="number">4</span>    <span class="comment">//Stage-3依赖Stage-4           </span></span><br><span class="line">   Stage-<span class="number">0</span> depends on stages: Stage-<span class="number">3</span>    <span class="comment">//Stage-0依赖Stage-3           </span></span><br><span class="line">                                                    </span><br><span class="line"> STAGE PLANS:   <span class="comment">// 阶段性计划                                    </span></span><br><span class="line">   Stage: Stage-<span class="number">4</span>     <span class="comment">//阶段4                              </span></span><br><span class="line">     Map Reduce Local Work   <span class="comment">//这是一个本地作业                    </span></span><br><span class="line">       Alias -&gt; Map Local Tables:    <span class="comment">// Map本地表的别名为d 即表dept              </span></span><br><span class="line">         d                                          </span><br><span class="line">           Fetch Operator   <span class="comment">//抓取                        </span></span><br><span class="line">             limit: -<span class="number">1</span>    <span class="comment">//limit为-1，即把数据全部读出来了                   </span></span><br><span class="line">       Alias -&gt; Map Local Operator Tree:  <span class="comment">//Map本地操作树          </span></span><br><span class="line">         d                                          </span><br><span class="line">           TableScan     <span class="comment">//表扫描                           </span></span><br><span class="line">             alias: d    <span class="comment">//别名d                           </span></span><br><span class="line">             Statistics: Num rows: <span class="number">2</span> Data size: <span class="number">284</span> Basic stats: PARTIAL Column stats: NONE 	<span class="comment">//统计 </span></span><br><span class="line">             Filter Operator  <span class="comment">//过滤                      </span></span><br><span class="line">               predicate: <span class="function">deptno is not <span class="title">null</span> <span class="params">(type: <span class="keyword">boolean</span>)</span>  <span class="comment">//断定：deptno字段 不为空</span></span></span><br><span class="line"><span class="function">               Statistics: Num rows: 1 Data size: 142 Basic stats: COMPLETE Column stats: NONE 	   <span class="comment">//统计</span></span></span><br><span class="line"><span class="function">               HashTable Sink Operator  <span class="comment">//输出类型为HashTable           </span></span></span><br><span class="line"><span class="function">                 keys:                              </span></span><br><span class="line"><span class="function">                   0 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                   1 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                                                    </span></span><br><span class="line"><span class="function">   Stage: Stage-3   <span class="comment">//阶段3                                </span></span></span><br><span class="line"><span class="function">     Map Reduce                                     </span></span><br><span class="line"><span class="function">       Map Operator Tree:    <span class="comment">//Map操作树                       </span></span></span><br><span class="line"><span class="function">           TableScan   <span class="comment">//表扫描                             </span></span></span><br><span class="line"><span class="function">             alias: e  <span class="comment">//e表 即emp表                             </span></span></span><br><span class="line"><span class="function">             Statistics: Num rows: 6 Data size: 657 Basic stats: COMPLETE Column stats: NONE  	<span class="comment">//统计		</span></span></span><br><span class="line"><span class="function">             Filter Operator    <span class="comment">//过滤                    </span></span></span><br><span class="line"><span class="function">               predicate: deptno is not <span class="title">null</span> <span class="params">(type: <span class="keyword">boolean</span>)</span> 	<span class="comment">//断定：deptno字段 不为空</span></span></span><br><span class="line"><span class="function">               Statistics: Num rows: 3 Data size: 328 Basic stats: COMPLETE Column stats: NONE 	  <span class="comment">//统计</span></span></span><br><span class="line"><span class="function">               Map Join Operator    <span class="comment">// Map Join  操作               </span></span></span><br><span class="line"><span class="function">                 condition map:   <span class="comment">//Map条件                 </span></span></span><br><span class="line"><span class="function">                      Inner Join 0 to 1             </span></span><br><span class="line"><span class="function">                 keys:                              </span></span><br><span class="line"><span class="function">                   0 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                   1 <span class="title">deptno</span> <span class="params">(type: <span class="keyword">int</span>)</span>             </span></span><br><span class="line"><span class="function">                 outputColumnNames: _col0, _col1, _col11, _col12  <span class="comment">//数值字段在表中的位置</span></span></span><br><span class="line"><span class="function">                 Statistics: Num rows: 3 Data size: 360 Basic stats: COMPLETE Column stats: NONE	<span class="comment">//统计 </span></span></span><br><span class="line"><span class="function">                 Select Operator   <span class="comment">//Select操作                 </span></span></span><br><span class="line"><span class="function">                   expressions: <span class="title">_col0</span> <span class="params">(type: <span class="keyword">int</span>)</span>, <span class="title">_col1</span> <span class="params">(type: string)</span>, <span class="title">_col11</span> <span class="params">(type: <span class="keyword">int</span>)</span>, <span class="title">_col12</span> <span class="params">(type: string)</span> 	<span class="comment">//表达式</span></span></span><br><span class="line"><span class="function">                   outputColumnNames: _col0, _col1, _col2, _col3 	<span class="comment">//数值字段在表中的位置</span></span></span><br><span class="line"><span class="function">                   Statistics: Num rows: 3 Data size: 360 Basic stats: COMPLETE Column stats: NONE 	  <span class="comment">//统计</span></span></span><br><span class="line"><span class="function">                   File Output Operator   <span class="comment">//文件输出操作          </span></span></span><br><span class="line"><span class="function">                     compressed: <span class="keyword">false</span>    <span class="comment">//是否压缩：否          </span></span></span><br><span class="line"><span class="function">                     Statistics: Num rows: 3 Data size: 360 Basic stats: COMPLETE Column stats: NONE	<span class="comment">//统计 </span></span></span><br><span class="line"><span class="function">                     table:   <span class="comment">//表文件的输入、输出、序列化类型                      </span></span></span><br><span class="line"><span class="function">                         input format: org.apache.hadoop.mapred.TextInputFormat <span class="comment">//文件输入格式</span></span></span><br><span class="line"><span class="function">                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat <span class="comment">//文件输出格式</span></span></span><br><span class="line"><span class="function">                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe <span class="comment">//反序列化</span></span></span><br><span class="line"><span class="function">       Local Work:                                  </span></span><br><span class="line"><span class="function">         Map Reduce Local Work                      </span></span><br><span class="line"><span class="function">                                                    </span></span><br><span class="line"><span class="function">   Stage: Stage-0   <span class="comment">//阶段0                                </span></span></span><br><span class="line"><span class="function">     Fetch Operator                                 </span></span><br><span class="line"><span class="function">       limit: -1     <span class="comment">// limit设置的值                               </span></span></span><br><span class="line"><span class="function">       Processor Tree:                              </span></span><br><span class="line"><span class="function">         ListSink                                   </span></span><br><span class="line"><span class="function">                                                    </span></span><br><span class="line"><span class="function">+-----------------------------------------------------------------+--+</span></span><br></pre></td></tr></table></figure>

<p>从执行计划得知，hive中执行SQL语句底层执行的是MapReduce。</p>
<p>我们在SQL中关联了两张表分别是emp dept，并从两张表中取出某些字段，在SQL执行计划中共分为三个阶段，分别是stage4、stage3、stage0。</p>
<p>stage4是根stage，stage3依赖stage4，同时stage0依赖stage3。</p>
<p>stage4是一个本地作业，读取的是dept表，输出一个Map类型的hashTable，关联的key是两张表的deptno，在执行计划中表现为0 deptno和 1 deptno，即执行的MapReduce中的Key是deptno字段。</p>
<p>stage3是MapReduce中的Map阶段，扫描emp表，执行一个Map Join操作，条件是两张表的dept字段相等(内连接)，现在我们得到的是一张包含所有字段的大表，得到需要的字段的对应位置，并且匹配字段的类型，在输出的时候检查是否需要压缩，以及输入、输出、和序列化类型</p>
<p>Stage-0阶段取出limit中指定的记录数</p>
<p>总结: 我们发现执行该SQL没有Reduce阶段，在现有的版本中默认设置<code>hive.auto.convert.join</code>(是否自动转换为mapjoin)为true，该参数配合<code>hive.mapjoin.smalltable.filesize</code>参数(小表的最大文件大小)默认为25M。即小于25M的表为小表，自动转为mapjoin，小表上传到hadoop缓存，提供给各个大表join使用。大表和小表根据关联的key形成一张大表，取出select需要的字段，最后根据limit设置的值取出对应的记录数。</p>
<p>参考参数：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--是否自动转换为mapjoin</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">true</span>;</span><br><span class="line"><span class="comment">--小表的最大文件大小，默认为25000000，即25M</span></span><br><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize = <span class="number">25000000</span>;</span><br><span class="line"><span class="comment">--是否将多个mapjoin合并为一个</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask = <span class="literal">true</span>;</span><br><span class="line"><span class="comment">--多个mapjoin转换为1个时，所有小表的文件大小总和的最大值。</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask.size = <span class="number">10000000</span>;</span><br></pre></td></tr></table></figure>

<h2 id="大小表Reduce-Join-emp、dept"><a href="#大小表Reduce-Join-emp、dept" class="headerlink" title="大小表Reduce Join(emp、dept)"></a>大小表Reduce Join(emp、dept)</h2><p>Reduce Join的核心思路是定义输出字段作为一个实体类，用来作为输出，实体类中定义一个标志用来区分表的来源</p>
<ol>
<li><p>将大小两个表在SQL中join的字段作为MapReduce中的key，原因是MapReduce中的key具有排序和分区的作用</p>
</li>
<li><p>Map中获取context中切片所在的文件名，按行获取文件中的数据并且根据获取的文件名分别将数据set到对象中，并写出Map。</p>
</li>
<li><p>Reduce中每次获取key相同的一组value值数据，这组value值既有dept中的数据，也</p>
</li>
</ol>
<p>   有emp中的数据，只要他们有相同的key，就会在shuffle中丢到一个reduce，这时候获取这组数据的值，根据flag来判断来自哪个表，如果是dept表则将数据设置到新new出来的对象中，添加到List列表中，同时创建一个保存emp表中数据的变量，由于emp表是小表，emp表中需要的数据对应dept/emp中的key的字段是唯一的，所以只需要把value中所有的对象都遍历循环出来，dept表数据添加到了List列表，emp表的数据添加到了变量中，最后循环List列表把变量set到每一个对象中，即完成了全部对象的全部成员属性。最后输出即可。</p>
<p>   参考代码:</p>
   <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.hadoop.join;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.pojo.JoinMain;</span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.utils.FileUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-01-29 16:39</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReduceJoinDriver</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    String in = <span class="string">"data/join/"</span>;</span><br><span class="line">    String out = <span class="string">"out/"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(conf, <span class="keyword">new</span> ReduceJoinDriver(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//获得configuration</span></span><br><span class="line">        Configuration conf = <span class="keyword">super</span>.getConf();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//检查文件夹</span></span><br><span class="line">        FileUtil.checkFileIsExists(conf, out);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用新方法这里怎么操作?</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置驱动类</span></span><br><span class="line">        job.setJarByClass(ReduceJoinDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置Map/Reducer类</span></span><br><span class="line">        job.setMapperClass(JoinMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(JoinReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置Map参数类型</span></span><br><span class="line">        job.setMapOutputKeyClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(JoinMain<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//job.setNumReduceTasks(3);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置Reducer参数类型</span></span><br><span class="line">        job.setOutputKeyClass(JoinMain<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置文件的输入输出</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(in));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(out));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//提交任务</span></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> b ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">JoinMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">JoinMain</span>&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            FileSplit fileSplit = (FileSplit) context.getInputSplit();</span><br><span class="line">            name = fileSplit.getPath().getName();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//做一个传入的表的判断</span></span><br><span class="line">            <span class="keyword">if</span> (name.contains(<span class="string">"emp"</span>))&#123;  <span class="comment">//emp</span></span><br><span class="line">                <span class="comment">//读取数据</span></span><br><span class="line">                String[] lines = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">                <span class="keyword">if</span> (lines.length ==<span class="number">8</span>)&#123;</span><br><span class="line">                    <span class="comment">//细粒度划分</span></span><br><span class="line">                    Integer empno = Integer.parseInt(lines[<span class="number">0</span>].trim());</span><br><span class="line">                    String ename = lines[<span class="number">1</span>];</span><br><span class="line">                    Integer deptno = Integer.parseInt(lines[lines.length-<span class="number">1</span>].trim());</span><br><span class="line">                    <span class="comment">//写入数据</span></span><br><span class="line">                    context.write(<span class="keyword">new</span> IntWritable(deptno),<span class="keyword">new</span> JoinMain(empno,ename,deptno,<span class="string">""</span>,<span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;      <span class="comment">//dept</span></span><br><span class="line">                <span class="comment">//读取数据</span></span><br><span class="line">                String[] lines = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">                <span class="keyword">if</span> (lines.length ==<span class="number">3</span>)&#123;</span><br><span class="line">                    <span class="keyword">int</span> deptno = Integer.parseInt(lines[<span class="number">0</span>].trim());</span><br><span class="line">                    String dname = lines[<span class="number">1</span>];</span><br><span class="line">                    context.write(<span class="keyword">new</span> IntWritable(deptno),<span class="keyword">new</span> JoinMain(<span class="number">0</span>, <span class="string">""</span>, deptno, dname, <span class="number">2</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">super</span>.cleanup(context);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">JoinReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">IntWritable</span>, <span class="title">JoinMain</span>,<span class="title">JoinMain</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="comment">//核心思路在 每个deptno组 进一次reduce ，前提是map中的key是deptno</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(IntWritable key, Iterable&lt;JoinMain&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            List&lt;JoinMain&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            String dname=<span class="string">""</span>;</span><br><span class="line">            <span class="comment">// 1.取出map中每行数据，判断flag值</span></span><br><span class="line">            <span class="comment">// 2.取出所有的emp中数据放入list中</span></span><br><span class="line">            <span class="comment">// 3.取出dept中的dname赋值给变量</span></span><br><span class="line">            <span class="comment">// 4.取出属于这个deptno中的所有数据，并给dname赋值</span></span><br><span class="line">            <span class="comment">// 5.每条赋值dname的数据写入reduce</span></span><br><span class="line">            <span class="keyword">for</span> (JoinMain main : values)&#123;</span><br><span class="line">                <span class="comment">// emp表</span></span><br><span class="line">                <span class="keyword">if</span> (main.getFlag() == <span class="number">1</span>)&#123;</span><br><span class="line">                    <span class="comment">//给emp表全部行重新赋值</span></span><br><span class="line">                    JoinMain m = <span class="keyword">new</span> JoinMain();</span><br><span class="line">                    m.setDeptno(main.getDeptno());</span><br><span class="line">                    m.setEmpno(main.getEmpno());</span><br><span class="line">                    m.setEname(main.getEname());</span><br><span class="line">                    <span class="comment">//写出到list</span></span><br><span class="line">                    list.add(m);</span><br><span class="line">                &#125;<span class="keyword">else</span> <span class="keyword">if</span> (main.getFlag() ==<span class="number">2</span> )&#123; <span class="comment">//dept</span></span><br><span class="line">                    <span class="comment">//拿到dept表中的dname</span></span><br><span class="line">                dname = main.getDname();</span><br><span class="line">            &#125;&#125;</span><br><span class="line">            <span class="comment">//循环赋值</span></span><br><span class="line">            <span class="keyword">for</span> (JoinMain bean : list) &#123;</span><br><span class="line">                bean.setDname(dname);</span><br><span class="line">                context.write(bean,NullWritable.get());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="大小表Map-Join-emp、dept"><a href="#大小表Map-Join-emp、dept" class="headerlink" title="大小表Map Join(emp、dept)"></a>大小表Map Join(emp、dept)</h2><p>Map Join的核心思想是把小表添加到缓存中(Map中)，在map中读取大表每行数据时set到对象值时取出小表(Map)对应key的值即可</p>
<ol>
<li><p>setup中，通过context获取小表文件切片的路径，然后通过读取流的方式读取为字符，按行获取到字符后切分，使用HashMap结构设置key和value分别为map方法中大表join时需要的键和值。</p>
</li>
<li><p>在map方法中读取文件数据，并且根据key取出HashMap(小表)中的value，一起set到对象中即可，最后写出，写出时，可以把value设置为NullWritable。</p>
<p>参考代码:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.tunan.hadoop.join;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.pojo.JoinMain;</span><br><span class="line"><span class="keyword">import</span> com.tunan.hadoop.utils.FileUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"><span class="keyword">import</span> org.codehaus.groovy.runtime.wrappers.LongWrapper;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: tunan</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-02-01 23:10</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@since</span>: 1.0.0</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapJoinDriver</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String in = <span class="string">"data/join/emp.txt"</span>;</span><br><span class="line">    <span class="keyword">private</span> String out = <span class="string">"out"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(conf, <span class="keyword">new</span> MapJoinDriver(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> strings</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> : int</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@describe</span> : 设置配置文件，不用设置reduce</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@author</span> : tunan</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@date</span> : 2020/2/1 23:14</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">super</span>.getConf();</span><br><span class="line">        FileUtil.checkFileIsExists(conf,out);</span><br><span class="line"></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(MapJoinDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"data/join/dept.txt"</span>));</span><br><span class="line"></span><br><span class="line">        job.setMapperClass(MapperJoin<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(JoinMain<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(in));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(out));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> b ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MapperJoin</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">JoinMain</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> HashMap&lt;Integer, String&gt; hashMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">//得到缓存文件路径</span></span><br><span class="line">            <span class="comment">//String path = context.getCacheFiles()[0].getPath().toString();</span></span><br><span class="line">            String path = context.getCacheFiles()[<span class="number">0</span>].getPath();</span><br><span class="line">            <span class="comment">/*URI[] files = context.getCacheFiles();   //URI 通过getPath()解码 没有toString()方法</span></span><br><span class="line"><span class="comment">            String s = files[0].getPath();*/</span></span><br><span class="line">            <span class="comment">//得到文件</span></span><br><span class="line">            <span class="comment">//File file = new File(cacheFiles[0]);</span></span><br><span class="line">            <span class="comment">//String path = file.getPath();</span></span><br><span class="line">            <span class="comment">//得到文件的流        InputStreamReader将字节转换为字符</span></span><br><span class="line">            BufferedReader br = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(<span class="keyword">new</span> FileInputStream(path)));</span><br><span class="line">            <span class="comment">//读取文件为字符串</span></span><br><span class="line">            String line ;</span><br><span class="line">            <span class="keyword">while</span>(StringUtils.isNotEmpty(line=br.readLine()))&#123;</span><br><span class="line">                <span class="comment">//切分字符串得到字符串数组</span></span><br><span class="line">                String[] split = line.split(<span class="string">"\t"</span>);</span><br><span class="line">                hashMap.put(Integer.parseInt(split[<span class="number">0</span>]),split[<span class="number">1</span>]);</span><br><span class="line">            &#125;</span><br><span class="line">            IOUtils.closeStream(br);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@describe</span> :</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@author</span> : tunan</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> key</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@return</span> : void</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@date</span> : 2020/2/1 23:38</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            String[] line = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">            <span class="keyword">if</span> (line.length &gt;= <span class="number">8</span>)&#123;</span><br><span class="line">                Integer empno = Integer.parseInt(line[<span class="number">0</span>].trim());</span><br><span class="line">                String ename = line[<span class="number">1</span>];</span><br><span class="line">                Integer deptno = Integer.parseInt(line[line.length-<span class="number">1</span>].trim());</span><br><span class="line">                String dname = hashMap.get(deptno);</span><br><span class="line">                context.write(<span class="keyword">new</span> JoinMain(empno,ename,deptno,dname),NullWritable.get());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">super</span>.cleanup(context);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/13/hadoop/10/">InputFormat&amp;Partitioner&amp;Conbiner&amp;Sort&amp;OutputFormat</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-13</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/MapReduce/">MapReduce</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/MapReduce/">MapReduce</a></span><div class="content"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>InputFormat</li>
<li>Partitioner</li>
<li>Conbiner</li>
<li>Sort</li>
<li>OutputFormat</li>
</ol>
<h2 id="InputFormat"><a href="#InputFormat" class="headerlink" title="InputFormat"></a>InputFormat</h2><p>在数据进入map之前，会进过一系列的格式化操作</p>
<ol>
<li>在客户端submitJob()方法提交作业前，会获取配置信息，形成一个任务分配的规划</li>
<li>提交文件分片(文件夹)和应用程序jar包</li>
<li>MR运行MapTask根据InputFormat读取文件，这里将详细介绍InputFormat</li>
</ol>
<p>InputFormat是一个抽象类，MR默认是用TextInputFormat方法读取文件</p>
<p>TextInputFormat是按行读取文件中的数据，实际上TextInputFormat中只实现了createRecordReader()和isSplitable()两个方法，它的具体实现在FileInputFormat中就已经实现的，FileInputFormat也是一个抽象类。</p>
<p>NLineInputFormat也继承于FileInputFormat，它的特点是按特定的行数读取数据</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 设置指定的InputFormat(重点)</span></span><br><span class="line">NLineInputFormat.setNumLinesPerSplit(job, <span class="number">3</span>);</span><br><span class="line">job.setInputFormatClass(NLineInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>

<p>DBInputFormat继承于DBInputFormat的同时实现了InputFormat，这个方法可以从数据库中读取数据，写入HDFS，类似于Sqoop，需要注意的是它的实体类要同时继承DBWritable和Writable，提交到HDFS上执行的时候需要指定jdbc的jar包(不推介使用)。</p>
<h2 id="Partitioner"><a href="#Partitioner" class="headerlink" title="Partitioner"></a>Partitioner</h2><p>MR的默认分区规则是按照key分区，相同的key到一个reduce方法中去，<strong>Partitoner可以自定义分区规则</strong>，自定义类继承Partitioner&lt;Text, Flow&gt;，泛型是map输出的key和value类型</p>
<p>参考代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Partitoner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">Flow</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, Flow flow, <span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">        String phone = text.toString();</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"137"</span>.equals(phone.substring(<span class="number">0</span>,<span class="number">3</span>)))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"136"</span>.equals(phone.substring(<span class="number">0</span>,<span class="number">3</span>)))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>要在Driver中指定Partitioner的类，并且指定reduce的个数，这里的<strong>reduce设置的个数一定要和Partitoner分区中返回的分区个数相同</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 设置Partitoner</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">3</span>);</span><br><span class="line">job.setPartitionerClass(Partitoner<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>

<p>如果reduce设置的数量大于分区个数，则会产生空的输出文件，即空的reduce。</p>
<p>如果reduce设置的个数小于分区个数，则会报错，表示多余的数据没有分区可去。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">java.io.IOException: Illegal partition <span class="keyword">for</span> <span class="number">13826544101</span> (<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Conbiner"><a href="#Conbiner" class="headerlink" title="Conbiner"></a>Conbiner</h2><p>Conbiner是合并，即是map阶段的reduce，可以自定义，也可以直接使用reduce方法，需要在Driver中指定，需要注意的是不能改变业务逻辑(不适用于乘积)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//设置conbiner</span></span><br><span class="line">job.setCombinerClass(FlowReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>

<h2 id="Sort"><a href="#Sort" class="headerlink" title="Sort"></a>Sort</h2><p>Sort是分区排序，需要知道的是MR的key默认是有序的，如果要自定义排序规则需要将实体类实现<code>WritableComparable&lt;FlowSort&gt;</code>接口，泛型就传入实体类的类名，WritableComparable实际上是继承了Writable和Comparable<T>，Writable是Hadoop自己实现的，Comparable是Java中的类</p>
<p>实体类参考代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowSort o)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Long.compare(o.sum, <span class="keyword">this</span>.sum);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>需要注意的是使用自定义排序的实体类要放到mapreduce方法key的位置，使之有序。</p>
<p>主要注意的是Sort是每个reduce中有序，如果设置了多个reduce，则只能保证每个reduce内部有序</p>
<h2 id="OutputFormat"><a href="#OutputFormat" class="headerlink" title="OutputFormat"></a>OutputFormat</h2><p>有一类很常见的需求：按照一定的规则把数据给我写到某个文件中去</p>
<p>OutputFormat是一个接口，实现它的类有FileOutputFormat和DBOutputFormat，使用和InputFormat差不多，用的不多，不写了</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/12/hadoop/9/">Spark、IDEA和Maven的环境准备&amp;Hadoop的依赖以及常用API&amp;WordCount Debug流程&amp;map、reduce方法的参数类型和作用&amp;瘦包在服务器上的jar包依赖</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-12</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/MapReduce/">MapReduce</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/MapReduce/">MapReduce</a></span><div class="content"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>Spark、IDEA和Maven的环境准备</li>
<li>hadoop的依赖以及常用API</li>
<li>WordCount Debug流程</li>
<li>map、reduce方法的参数类型和作用</li>
<li>Writable和WritableComparable的作用</li>
<li>瘦包在服务器上的jar包依赖</li>
</ol>
<h2 id="Spark、IDEA和Maven的环境准备"><a href="#Spark、IDEA和Maven的环境准备" class="headerlink" title="Spark、IDEA和Maven的环境准备"></a>Spark、IDEA和Maven的环境准备</h2><p>环境:</p>
<ol>
<li>Spark3.0</li>
<li>IDEA19.3</li>
<li>Maven3.6.3(安装配置阿里云的镜像)</li>
</ol>
<h2 id="Hadoop的依赖以及常用API"><a href="#Hadoop的依赖以及常用API" class="headerlink" title="Hadoop的依赖以及常用API"></a>Hadoop的依赖以及常用API</h2><p>依赖:</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.6.0-cdh5.16.2&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p>常用API:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">FileSystem fileSystem; <span class="comment">//核心</span></span><br><span class="line">open()	<span class="comment">//打开文件返回流</span></span><br><span class="line">mkdirs()	<span class="comment">//创建目录</span></span><br><span class="line">create()	<span class="comment">//创建文件</span></span><br><span class="line">copyFromLocalFile()	<span class="comment">//从本地复制文件到hdfs，类似于get</span></span><br><span class="line">copyToLocalFile()	<span class="comment">//从hdfs复制文件到本地，类似于put</span></span><br><span class="line">listFiles()	<span class="comment">//列出目录下的所有文件，可以迭代</span></span><br><span class="line">delete()	<span class="comment">//删除文件，不存在报错</span></span><br><span class="line">deleteOnExit()	<span class="comment">//删除存在的文件,不存在不报错</span></span><br></pre></td></tr></table></figure>

<h2 id="WordCount-Debug流程"><a href="#WordCount-Debug流程" class="headerlink" title="WordCount Debug流程"></a>WordCount Debug流程</h2><ol>
<li><p>编译</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">job.waitForCompletion(<span class="keyword">true</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>提交</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (state == JobState.DEFINE) &#123;</span><br><span class="line">   submit();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>兼容新老API</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">setUseNewAPI();</span><br></pre></td></tr></table></figure>
</li>
<li><p>本地连接/服务器连接</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">connect();</span><br></pre></td></tr></table></figure>
</li>
<li><p>检查配置、输出路径等</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> JobSubmitter submitter = getJobSubmitter(cluster.getFileSystem(),cluster.getClient());</span><br><span class="line"><span class="function"><span class="keyword">public</span> JobStatus <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException,ClassNotFoundException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster);</span><br><span class="line">&#125;</span><br><span class="line">checkSpecs(job);	<span class="comment">//validate the jobs output specs</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>把该作业的配置信息加到分布式缓存中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Configuration conf = job.getConfiguration();</span><br><span class="line">addMRFrameworkToDistributedCache(conf);</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建该Job对应的存放目录</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br></pre></td></tr></table></figure>
</li>
<li><p>拿到该Job对应的ID(local/application)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">JobID jobId = submitClient.getNewJobID();</span><br></pre></td></tr></table></figure>
</li>
<li><p>jobStagingArea/jobid</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Path submitJobDir = <span class="keyword">new</span> Path(jobStagingArea, jobId.toString());</span><br></pre></td></tr></table></figure>
</li>
<li><p>拷贝job对应的信息到jobStagingArea/jobid</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">copyAndConfigureFiles(job, submitJobDir);</span><br></pre></td></tr></table></figure>
</li>
<li><p>完成我们输入数据的切片(默认128MB，预留10%浮动空间)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> maps = writeSplits(job, submitJobDir);</span><br></pre></td></tr></table></figure>
</li>
<li><p>作业文件提交到指定目录</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">writeConf(conf, submitJobFile);</span><br></pre></td></tr></table></figure>
</li>
<li><p>提交作业</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="map、reduce方法的参数类型和作用"><a href="#map、reduce方法的参数类型和作用" class="headerlink" title="map、reduce方法的参数类型和作用"></a>map、reduce方法的参数类型和作用</h2><ul>
<li><p>继承Mapper后实现map方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br></pre></td></tr></table></figure>

<p>该方法中的参数分别是<code>LongWritable key, Text value, Context context</code></p>
<p>前两个参数是map方法中输入的键和值，输入的键和值必须是LongWritable类型和Text类型，因为是按offset读取每行本文数据，最后一个参数context，它是MapReduce的上下文。什么都可以拿出来。</p>
</li>
<li><p>继承Reducer后实现reduce方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span></span></span><br></pre></td></tr></table></figure>

<p>该方法中的参数分别是<code>Text key, Iterable&lt;IntWritable&gt; values, Context context</code></p>
<p>前两个参数是reduce方法中输入的键和值，输入的键和值对应map中输出的键值类型，并且值是一个Iterable类型，因为在shuffle阶段相同key的value分到了一起，是一个可迭代的参数。因为是按offset读取每行本文数据，最后一个参数context，它是MapReduce的上下文。什么都可以拿出来。</p>
</li>
</ul>
<h2 id="Writable和WritableComparable的作用"><a href="#Writable和WritableComparable的作用" class="headerlink" title="Writable和WritableComparable的作用"></a>Writable和WritableComparable的作用</h2><p>Writable是hadoop中的序列化接口，是一个接口，只定义了两个方法，分别是<code>write()</code>和<code>readFields()</code>方法，用于hadoop序列化时的读和写；<code>WritableComparable</code>也是一个序列化接口，只是在序列化的同时同时实现了java中的<code>Comparable&lt;T&gt;</code>接口，具有排序的特性。</p>
<p>hadoop是java写的，那么为什么hadoop要实现自己的序列化接口</p>
<ul>
<li>java序列化数据结果比较大、传输效率比较低、不能跨语言对接</li>
</ul>
<p>hadoop使用的是RPC协议传送数据，且hadoop是应用在大集群上，所以hadoop的序列化必须做到</p>
<ul>
<li>占用空间更小</li>
<li>传输速度更快</li>
<li>扩展性更强，支持多种格式的序列化</li>
<li>兼容性更好，需要支持多种语言，如java、scala、python等</li>
</ul>
<p>所以hadoop实现了自己的序列化接口Writable：<code>压缩</code>、<code>速度</code>、<code>扩展性</code>、<code>兼容性</code>都比java更优秀</p>
<p>另外:</p>
<ol>
<li><em>序列化的对象，他们超越了JVM的生死，不顾生他们的母亲，化作永恒。</em>static和transient修饰的属性除外，因为static修饰的属性是在编译时静态生成的，而对象是动态生成的，又因为transient修饰的属性禁止了属性的序列化。</li>
<li><em>把“活的”对象序列化，就是把“活的”对象转化成一串字节，而“反序列化”，就是从一串字节里解析出“活的”对象。</em></li>
</ol>
<h2 id="瘦包在服务器上的jar包依赖"><a href="#瘦包在服务器上的jar包依赖" class="headerlink" title="瘦包在服务器上的jar包依赖"></a>瘦包在服务器上的jar包依赖</h2><p>打包好的mapreduce程序上传到云主机，由于是瘦包，缺少某些依赖，比如连接mysql的的jar包，现在我们就解决缺少依赖的问题</p>
<ol>
<li><p>将下载好的jar包上传到云主机上</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">cp mysql-connector-java-<span class="number">5.1</span><span class="number">.27</span>-bin.jar ~/lib/</span><br></pre></td></tr></table></figure>
</li>
<li><p>将jar包加载到hadoop的classpath中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:~/lib/mysql-connector-java-<span class="number">5.1</span><span class="number">.27</span>-bin.jar</span><br></pre></td></tr></table></figure>
</li>
<li><p>用hadoop jar 执行jar文件时，加上-libjars参数</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop jar hadoop-mapreduce-examples-<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.16</span><span class="number">.2</span>.jar wordcount -libjars /home/hadoop/lib/mysql-connector-java-<span class="number">5.1</span><span class="number">.27</span>-bin.jar  /<span class="number">1</span>.txt /out</span><br></pre></td></tr></table></figure>


</li>
</ol>
<p>如果上诉方法有问题可以使用hadoop的分布式缓存</p>
<ol>
<li><p>把jar包传到集群上，命令如下</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop fs -put mysql-connector-java-<span class="number">5.1</span><span class="number">.27</span>.jar /lib</span><br></pre></td></tr></table></figure>
</li>
<li><p>在mr程序提交job前，添加一下语句：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">job.addArchiveToClassPath(<span class="keyword">new</span> Path(<span class="string">"hdfs://aliyun:9000/lib/mysql-connector-java-5.1.27.jar"</span>));</span><br></pre></td></tr></table></figure>

</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/11/hadoop/8/">YARN的调优&amp;YARN的三种调度器</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-11</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/">Hadoop</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Hadoop/Yarn/">Yarn</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Hadoop/">Hadoop</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Yarn/">Yarn</a></span><div class="content"><h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><ol>
<li>梳理YARN资源调优参数</li>
<li>调度器整理三种，区别是什么，CDH默认是什么</li>
</ol>
<h3 id="YARN的资源调优"><a href="#YARN的资源调优" class="headerlink" title="YARN的资源调优"></a>YARN的资源调优</h3><p><code>背景:</code> 假设每台服务器拥有内存128G 16物理core，怎么分配？</p>
<ol>
<li><p>装完CentOS，消耗内存1G</p>
</li>
<li><p>系统预览15%-20%内存(包含1.1)，以防全部使用导致系统夯住 和 oom机制事件，或者给未来部署组件预览点空间(<code>128*20%=25.6G==26G</code>)</p>
</li>
<li><p>假设只有DN NM节点，余下内存: <code>128-26=102G</code></p>
<ol>
<li><p>给DN进程(自身)2G，给NM进程(自身)4G，剩余<code>102-2-4=96G</code></p>
</li>
<li><p>container内存的分配共96G</p>
<ul>
<li><p><code>yarn.nodemanager.resource.memory-mb</code>     共 96G</p>
</li>
<li><p><code>yarn.scheduler.minimum-allocation-mb</code>    最少1G    极限情况下，只有96个container 内存1G</p>
</li>
<li><p><code>yarn.scheduler.maximum-allocation-mb</code>    最多96G  极限情况下，只有1个container 内存96G</p>
<p>container的内存会自动增加，默认1G递增，那么container的个数的范围: 1-96个</p>
</li>
</ul>
</li>
<li><p>container物理核分配 (物理核:虚拟核 =1:2 ==&gt;16:32)</p>
<ul>
<li><p><code>yarn.nodemanager.resource.cpu-vcores</code>            共 32个</p>
</li>
<li><p><code>yarn.scheduler.minimum-allocation-vcores</code>     最少1个   极限情况下，只有32个container    </p>
</li>
<li><p><code>yarn.scheduler.maximum-allocation-vcores</code>     最多32个 极限情况下，只有1个container</p>
<p>container的物理核会自动增加，默认1个递增，那么container的个数的范围: :1-32个</p>
</li>
</ul>
</li>
<li><p><code>关键:</code> cloudera公司推荐，一个container的vcore最好不要超过5，那么我们设置4</p>
<ul>
<li><p><code>yarn.scheduler.maximum-allocation-vcores</code>    4   </p>
<p>目前为止，极限情况下，共有8个container (32/4)</p>
</li>
</ul>
</li>
<li><p>综合memory+vcore的分配</p>
<ol>
<li><p>一共有32个vcore，一个container的vcore是4个，那么分配container一共有8个</p>
</li>
<li><p>重新分配核</p>
<ul>
<li><p><code>yarn.nodemanager.resource.cpu-vcores</code>            共32个</p>
</li>
<li><p><code>yarn.scheduler.minimum-allocation-vcores</code>    最少4个    </p>
</li>
<li><p><code>yarn.scheduler.maximum-allocation-vcores</code>    最多4个   极限container 8个</p>
</li>
</ul>
</li>
<li><p>根据物理核重新分配内存</p>
<ul>
<li><p><code>yarn.nodemanager.resource.memory-mb</code>      共96G</p>
</li>
<li><p><code>yarn.scheduler.minimum-allocation-mb</code>    最少12G  </p>
</li>
<li><p><code>yarn.scheduler.maximum-allocation-mb</code>    最多12G   (极限container 8个)</p>
</li>
</ul>
</li>
<li><p>分配后的每个container的物理核数是4个，内存大小是12G，当然spark计算时内存不够大，这个参数肯定要调大，那么这种理想化的设置个数必然要打破，以memory为主</p>
</li>
</ol>
</li>
<li><p>假如 256G内存 56core，请问参数如何设置</p>
<ol>
<li><p>首先减去系统内存开销和其他进程开销，</p>
<ul>
<li><p>系统开销: 256*0.2=52G</p>
</li>
<li><p>DN开销: 2G</p>
</li>
<li><p>NM开销: 4G</p>
</li>
<li><p>Hbase开销: 暂无</p>
</li>
<li><p>升序内存容量: 256-52-2-4=198G</p>
</li>
</ul>
</li>
<li><p>确定每个container的物理核数量是4个，56/4=14个container容器</p>
</li>
<li><p>确定了最多分配14个container容器，每个容器的内存应该分配的容量是: 198/14==&gt;14G</p>
<p><strong>那么每个container的最大核数设置4，最大内存数设置14G</strong></p>
</li>
</ol>
</li>
<li><p>假如该节点还有组件，比如hbase regionserver进程，那么该如何设置？</p>
<p>总容量减就完事了。    </p>
</li>
</ol>
<p>所有的配置信息在<code>yarn-default.xm</code>l文件中</p>
<h4 id="内存参数默认值"><a href="#内存参数默认值" class="headerlink" title="内存参数默认值:"></a>内存参数默认值:</h4><table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>yarn.nodemanager.resource.memory-mb</td>
<td>-1</td>
<td>可以分配给容器的物理内存总量(以MB为单位)。</td>
</tr>
<tr>
<td>yarn.scheduler.minimum-allocation-mb</td>
<td>1024</td>
<td>RM上每个容器请求的最小分配</td>
</tr>
<tr>
<td>yarn.scheduler.maximum-allocation-mb</td>
<td>8192</td>
<td>RM上每个容器请求的最大分配</td>
</tr>
</tbody></table>
<h4 id="核数参数默认值"><a href="#核数参数默认值" class="headerlink" title="核数参数默认值:"></a>核数参数默认值:</h4><table>
<thead>
<tr>
<th>KEY</th>
<th>VALUE</th>
<th>DESC</th>
</tr>
</thead>
<tbody><tr>
<td>yarn.nodemanager.resource.cpu-vcores</td>
<td>-1</td>
<td>可以为容器分配的vcore总数量。</td>
</tr>
<tr>
<td>yarn.scheduler.minimum-allocation-vcores</td>
<td>1</td>
<td>RM上每个容器请求的最小虚拟CPU核心分配。</td>
</tr>
<tr>
<td>yarn.scheduler.maximum-allocation-vcores</td>
<td>4</td>
<td>RM上每个容器请求的最大虚拟CPU核心分配。</td>
</tr>
</tbody></table>
</li>
</ol>
<h3 id="Yarn的三种调度器"><a href="#Yarn的三种调度器" class="headerlink" title="Yarn的三种调度器"></a>Yarn的三种调度器</h3><ul>
<li><p>Apache hadoop2.x的默认调度器是Capacity Scheduler(计算调度器)</p>
</li>
<li><p>CDH的默认调度器是Fair Scheduler(公平调度器)</p>
</li>
</ul>
<h4 id="Yarn三种调度策略对比"><a href="#Yarn三种调度策略对比" class="headerlink" title="Yarn三种调度策略对比"></a>Yarn三种调度策略对比</h4><p>在Yarn中有三种调度器可以选择：FIFO Scheduler ，Capacity Scheduler，FairScheduler。</p>
<ol>
<li><p>FIFO Scheduler</p>
<p><img src="https://img-blog.csdnimg.cn/20181101090612286.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MzgyNDUz,size_16,color_FFFFFF,t_70" alt="队列调度器"></p>
<p>FIFO Scheduler把应用按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的应用进行分配资源，待最头上的应用需求满足后再给下一个分配，以此类推。</p>
<p>FIFO Scheduler是最简单也是最容易理解的调度器，也不需要任何配置，但它并不适用于共享集群。大的应用可能会占用所有集群资源，这就导致其它应用被阻塞。在共享集群中，更适合采用Capacity Scheduler或Fair Scheduler，这两个调度器都允许大任务和小任务在提交的同时获得一定的系统资源。从图中可以看出，在FIFO 调度器中，小任务会被大任务阻塞。</p>
</li>
<li><p>Capacity Scheduler</p>
<p><img src="https://img-blog.csdnimg.cn/20181101091012607.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MzgyNDUz,size_16,color_FFFFFF,t_70" alt="计算调度器"></p>
<p>而对于Capacity调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间。</p>
</li>
<li><p>Fair Scheduler</p>
<p><img src="https://img-blog.csdnimg.cn/20181101091843173.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MzgyNDUz,size_16,color_FFFFFF,t_70" alt="公平调度器"></p>
<p>在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。如上图所示，当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。</p>
<p>需要注意的是，在上图Fair调度器中，从第二个任务提交到获得资源会有一定的延迟，因为它需要等待第一个任务释放占用的Container。小任务执行完成之后也会释放自己占用的资源，大任务又获得了全部的系统资源。最终的效果就是Fair调度器即得到了高的资源利用率又能保证小任务及时完成。</p>
</li>
</ol>
<h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn application -kill &lt;Application ID&gt;	#杀死进程</span><br></pre></td></tr></table></figure>

</div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/9/">&lt;&lt;</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/11/">&gt;&gt;</a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>