<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="BigData Developer"><meta name="keywords" content="yerias,TUNANのBlog,BigData"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017 | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">133</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">30</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">26</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">TUNANのBlog</div><div id="site-sub-title">感谢若老、J哥、师兄、前辈、同学、朋友、陌生人，在我行走在大数据道路上给我的谆谆教诲，同时此博客仅作为学习笔记存在，严禁任何人以何种理由商用，作者QQ: 971118017</div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2020/02/20/azkaban/1/">Azkaban的安装&amp;使用&amp;坑</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-02-20</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Azkaban/">Azkaban</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Azkaban/">Azkaban</a></span><div class="content"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>Azkaban的安装</li>
<li>Azkaban的使用</li>
<li>Git的安装</li>
</ol>
<h2 id="Git的安装"><a href="#Git的安装" class="headerlink" title="Git的安装"></a>Git的安装</h2><p>在安装Azkaban之前要安装Git</p>
<ol>
<li><p>获取github最新的Git安装包下载链接，进入Linux服务器，执行下载，命令为： </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https://github.com/git/git/archive/v2.17.0.tar.gz</span><br></pre></td></tr></table></figure>
</li>
<li><p>压缩包解压，命令为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -zxvf v2.17.0.tar.gz</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装编译源码所需依赖，命令为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel gcc perl-ExtUtils-MakeMaker</span><br></pre></td></tr></table></figure>

<p>耐心等待安装，出现提示输入y即可；</p>
</li>
<li><p>安装依赖时，yum自动安装了Git，需要卸载旧版本Git，命令为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum remove git -y</span><br></pre></td></tr></table></figure>
</li>
<li><p>进入解压后的文件夹，命令 cd git-2.17.0 ，然后执行编译，命令为:  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">make prefix=/usr/local/git all</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装Git至/usr/local/git路径，命令为:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">make prefix=/usr/local/git install</span><br></pre></td></tr></table></figure>
</li>
<li><p>打开环境变量配置文件，命令 vim /etc/profile ，在底部加上Git相关配置信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export GIT_HOME=/usr/local/git/bin</span><br><span class="line">export PATH=$GIT_HOME/bin/:$PATH</span><br></pre></td></tr></table></figure>
</li>
<li><p>输入命令 <code>git --version</code> ，查看安装的git版本，校验通过，安装成功。</p>
</li>
</ol>
<h2 id="Azkaban的安装"><a href="#Azkaban的安装" class="headerlink" title="Azkaban的安装"></a>Azkaban的安装</h2><ol>
<li><p>下载<a href="https://github.com/azkaban/azkaban/releases" target="_blank" rel="noopener">Azkaban</a></p>
</li>
<li><p>解压</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xzvf 3.81.0.tar.gz -C ../app/</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装<a href="https://azkaban.readthedocs.io/en/latest/getStarted.html" target="_blank" rel="noopener">编译文档</a></p>
<p>在开始编译之前要下载名为<a href="https://services.gradle.org/distributions/gradle-4.6-all.zip" target="_blank" rel="noopener">gradle-4.6-all.zip</a>的包，如果直接让系统下载超级慢，下载好了放在同目录下，并且在<code>/azkaban/gradle/wrapper/gradle-wrapper.properties</code> 中注释下载路径，并指定已经下载好的路径</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">distributionUrl=gradle-4.6-all.zip</span><br><span class="line"><span class="meta">#</span><span class="bash">distributionUrl=https\://services.gradle.org/distributions/gradle-4.6-all.zip</span></span><br></pre></td></tr></table></figure>

<p>还需要安装gcc环境:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install -y gcc-c++*</span><br></pre></td></tr></table></figure>

<p>修改maven仓库为阿里云镜像</p>
<p><code>vim azkaban-3.81.0/build.gradle 54行左右</code> </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">maven &#123;</span><br><span class="line">	//这里是阿里云</span><br><span class="line">	url &apos;http://maven.aliyun.com/nexus/content/repositories/central/&apos;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



</li>
</ol>
<p>   正式编译</p>
   <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./gradlew build installDist -x test</span><br></pre></td></tr></table></figure>

<p>   编译后生成三个文件</p>
   <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">azkaban-exec-server</span><br><span class="line">azkaban-solo-server</span><br><span class="line">azkaban-web-server</span><br></pre></td></tr></table></figure>

<ol start="4">
<li><p>执行<code>azkaban-solo-server</code></p>
<ol>
<li><p>解压<code>azkaban-solo-server</code>到<code>app</code>目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xzvf azkaban-solo-server/build/distributionsa/zkaban-solo-server-0.1.0-SNAPSHOT.tar.gz ~/app</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动: <code>bin/start-solo.sh</code>，生成<code>AzkabanSingleServer</code>进程</p>
</li>
<li><p>修改配置(时区、用户)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">时区: default.timezone.id=Asia/Shanghai</span><br><span class="line">用户: &lt;user password="tunan" roles="admin" username="tunan"/&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>web端登录，端口8081</p>
</li>
</ol>
</li>
<li><p>azkaban需要至少3G内存，如果内存不足3G，需要设置不检查内存</p>
<p>azkaban-web-server-2.7.0/plugins/jobtypes/commonprivate.properties</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">memCheck.enabled=false</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="Azkaban的使用"><a href="#Azkaban的使用" class="headerlink" title="Azkaban的使用"></a>Azkaban的使用</h2><h3 id="单节点部署"><a href="#单节点部署" class="headerlink" title="单节点部署"></a>单节点部署</h3><p><a href="https://azkaban.readthedocs.io/en/latest/createFlows.html" target="_blank" rel="noopener">官方文档</a></p>
<ol>
<li><p>创建flow20.projec文件，写入信息: </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">azkaban-flow-version: 2.0</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建basic.flow文件，写入信息:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: jobA</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: echo "This is an echoed text."</span><br></pre></td></tr></table></figure>
</li>
<li><p>压缩Archive.zip文件，在web页面中提交</p>
<p><code>坑</code>: 提交的用户和hadoop上的用户不同</p>
</li>
<li><p>多依赖案例</p>
<p>flow20.projec文件相同，basic.flow文件内容不同，文件名可不同</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: jobC</span><br><span class="line">    type: noop</span><br><span class="line">    # jobC depends on jobA and jobB</span><br><span class="line">    dependsOn:</span><br><span class="line">      - jobA</span><br><span class="line">      - jobB</span><br><span class="line"></span><br><span class="line">  - name: jobA</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: echo "This is an echoed text."</span><br><span class="line"></span><br><span class="line">  - name: jobB</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: pwd</span><br></pre></td></tr></table></figure>
</li>
<li><p>wc案例</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: tunan-wordcount</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: hadoop jar /home/hadoop/app/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar wordcount /azkaban/data/wc.txt /out</span><br></pre></td></tr></table></figure>

<ol>
<li>直接点击程序修改输出文件参数</li>
<li>使用Flow Parameters修改输出文件参数</li>
</ol>
</li>
<li><p>hive案例</p>
<p>除了固定的flow20.project文件，需要修改hive.flow文件和添加stat.sh文件</p>
<p>hive.flow</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nodes:</span><br><span class="line">  - name: exe hive</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: sh stat.sh</span><br></pre></td></tr></table></figure>

<p>stat.sh</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line">hive -e "select * from offline_dw.dws_country_traffic"</span><br></pre></td></tr></table></figure>

<p>上传相同的文件，新版本会覆盖旧的版本</p>
</li>
<li><p>调度执行</p>
<p>在执行页面的左下角点击Schedule即可，并可在Scheduling中查看调度信息，然后在History中查看历史执行信息</p>
</li>
</ol>
<h3 id="多节点部署"><a href="#多节点部署" class="headerlink" title="多节点部署"></a>多节点部署</h3><ol>
<li><p>在mysql中创建数据库</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE DATABASE azkaban;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建用户</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE USER 'tunan'@'%' IDENTIFIED BY 'tunan';</span><br></pre></td></tr></table></figure>
</li>
<li><p>赋权给用户</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span>,<span class="keyword">INSERT</span>,<span class="keyword">UPDATE</span>,<span class="keyword">DELETE</span> <span class="keyword">ON</span> azkaban.* <span class="keyword">to</span> <span class="string">'tunan'</span>@<span class="string">'%'</span> <span class="keyword">WITH</span> <span class="keyword">GRANT</span> <span class="keyword">OPTION</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在mysql中生成需要的表</p>
<p>这里的source可能会遇到权限问题，解决的办法很多，如移动其他位置</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">use</span> azkaban;</span><br><span class="line">source /home/hadoop/app/azkaban/azkaban-db/build/<span class="keyword">install</span>/azkaban-db/<span class="keyword">create</span>-<span class="keyword">all</span>-<span class="keyword">sql</span><span class="number">-0.1</span><span class="number">.0</span>-SNAPSHOT.sql;</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行SQL获取动态端口号</p>
<p><code>mysql&gt; select * from executors ;</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">+<span class="comment">----+--------+-------+--------+</span></span><br><span class="line">| id | host   | port  | active |</span><br><span class="line">+<span class="comment">----+--------+-------+--------+</span></span><br><span class="line">|  1 | aliyun | 46418 |      0 |</span><br><span class="line">+<span class="comment">----+--------+-------+--------+</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>激活执行器</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -G "http://aliyun:46418/executor?action=activate"</span><br></pre></td></tr></table></figure>
</li>
<li><p>把<code>azkaban-exec-server</code>和<code>azkaban-web-server</code>解压，并启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -xzvf azkaban-exec-server/build/distributionsa/azkaban-exec-server-0.1.0-SNAPSHOT.tar.gz ~/app</span><br><span class="line">tar -xzvf azkaban-web-server/build/distributionsa/azkaban-web-server-0.1.0-SNAPSHOT.tar.gz ~/app</span><br></pre></td></tr></table></figure>
</li>
<li><p>登录web使用，使用方式和单节点一样</p>
</li>
</ol>
<h3 id="二次开发"><a href="#二次开发" class="headerlink" title="二次开发"></a>二次开发</h3><p>azkaban的二次开发需要调用az已经做好的接口</p>
<p><a href="https://azkaban.readthedocs.io/en/latest/ajaxApi.html#request-parameters-1" target="_blank" rel="noopener">官方文档</a></p>
<p>命令行获取session.id</p>
<p><code>curl -k -X POST --data &quot;action=login&amp;username=azkaban&amp;password=azkaban&quot; http://aliyun:8081</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "session.id" : "f1725f59-9d20-4fe2-b89e-3715ef85****",</span><br><span class="line">  "status" : "success"</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="常用功能"><a href="#常用功能" class="headerlink" title="常用功能"></a>常用功能</h2><ol>
<li>配置代理用户</li>
<li>配置hadoop.home</li>
<li>配置Spark作业提交</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/02/20/offlinedw/7.%E7%BB%93%E6%9E%9C%E6%95%B0%E6%8D%AE%E7%9A%84%E5%B1%95%E7%A4%BA/">结果数据的展示</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-02-20</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/OfflineDW/">OfflineDW</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/OfflineDW/">OfflineDW</a></span><div class="content"><p>由于时间忙不过来，这篇暂时不更新，仅作为维护项目的完整性存在。。。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/02/19/offlinedw/6.%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%8A%BD%E5%8F%96/">业务数据的抽取</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-02-19</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/OfflineDW/">OfflineDW</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/OfflineDW/">OfflineDW</a></span><div class="content"><p>由于时间忙不过来，这篇暂时不更新，仅作为项目的完整性存在。。。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/02/18/offlinedw/5.%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E5%88%86%E5%B1%82/">数据仓库的分层</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-02-18</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/OfflineDW/">OfflineDW</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/OfflineDW/">OfflineDW</a></span><div class="content"><p>系统架构图:</p>
<p><img src="https://yerias.github.io/offlinedw/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E9%A1%B9%E7%9B%AE%E5%9B%BE_3.jpg" alt="数据仓库架构项目图"></p>
<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>数据仓库为什么要分层</li>
<li>数据仓库如何分层</li>
<li>使用脚本将数据执行分层</li>
<li>数据分析案例</li>
<li>使用crontab调度脚本(临时)</li>
</ol>
<h2 id="数据仓库为什么要分层"><a href="#数据仓库为什么要分层" class="headerlink" title="数据仓库为什么要分层"></a>数据仓库为什么要分层</h2><p>分层的主要原因是在管理数据的时候，能对数据有一个更加清晰的掌控，详细来讲，主要有下面几个原因：</p>
<ol>
<li><code>清晰数据结构</code>，每一个数据分层都有它的作用域，这样我们在使用表的时候能更方便地定位和理解。</li>
<li><code>数据血缘追踪</code>，简单来说，我们最终给业务呈现的是一个能直接使用业务表，但是它的来源有很多，如果有一张来源表出问题了，我们希望能够快速准确地定位到问题，并清楚它的危害范围。</li>
<li><code>减少重复开发</code>，规范数据分层，开发一些通用的中间层数据，能够减少极大的重复计算。</li>
<li><code>把复杂问题简单化</code>，将一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。</li>
<li><code>屏蔽原始数据的异常</code></li>
<li><code>屏蔽业务的影响</code>，不必改一次业务就需要重新接入数据</li>
</ol>
<h2 id="数据仓库如何分层"><a href="#数据仓库如何分层" class="headerlink" title="数据仓库如何分层"></a>数据仓库如何分层</h2><p>数据仓库的分层标准: 不为分层而分层</p>
<h3 id="标准的分层"><a href="#标准的分层" class="headerlink" title="标准的分层"></a>标准的分层</h3><ul>
<li>ODS：<code>数据原始层</code>: 直接加载原始数据，不做任何处理(不做ETL)</li>
<li>DWD：<code>数据明细层</code>，对ODS层进行清洗(ETL)</li>
<li>DWS：<code>数据服务层</code>，基于DWD做统计分析</li>
<li>ADS：<code>数据应用层</code>，为各种统计报表提供数据</li>
</ul>
<h3 id="我们的分层"><a href="#我们的分层" class="headerlink" title="我们的分层"></a>我们的分层</h3><ul>
<li>ODS(operate data store)：<code>数据原始层</code>，最接近数据源中数据的一层，数据源中的数据，经过抽取、洗净、传输，也就说传说中的ETL之后，装入本层</li>
<li>DWS(data warehouse server)：<code>数据明细层</code>，从ODS层中获得的数据按照主题建立各种数据模型。在这里，我们需要了解四个概念：维（dimension）、事实（Fact）、指标（Index）和粒度（ Granularity）。</li>
<li>ADS：<code>数据应用层</code>，该层主要是提供数据产品和数据分析使用的数据。 比如我们经常说的报表数据，或者说那种大宽表，一般就放在这里。</li>
</ul>
<h2 id="使用脚本将数据执行分层"><a href="#使用脚本将数据执行分层" class="headerlink" title="使用脚本将数据执行分层"></a>使用脚本将数据执行分层</h2><p>现在我们知道了数据需要在Hive中分成ODS层、DWS层和ADS层，每一层的建表标准是<code>ods_</code>、<code>dws_</code>、<code>ads_</code></p>
<ol>
<li><p>建库</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> offline_dw;</span><br></pre></td></tr></table></figure>
</li>
<li><p>建ODS层外部分区表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> ods_access(</span><br><span class="line"><span class="string">`year`</span>	<span class="keyword">String</span>,</span><br><span class="line"><span class="string">`month`</span>	<span class="keyword">String</span>,</span><br><span class="line"><span class="string">`day`</span>	<span class="keyword">String</span>,</span><br><span class="line">country	<span class="keyword">String</span>,</span><br><span class="line">province	<span class="keyword">String</span>,</span><br><span class="line">city	<span class="keyword">String</span>,</span><br><span class="line">area	<span class="keyword">String</span>,</span><br><span class="line">proxyIp	<span class="keyword">String</span>,</span><br><span class="line">responseTime	<span class="built_in">BigInt</span>,</span><br><span class="line">referer	<span class="keyword">String</span>,</span><br><span class="line"><span class="string">`method`</span>	<span class="keyword">String</span>,</span><br><span class="line"><span class="keyword">http</span>	<span class="keyword">String</span>,</span><br><span class="line"><span class="string">`domain`</span>	<span class="keyword">String</span>,</span><br><span class="line"><span class="string">`path`</span>	<span class="keyword">String</span>,</span><br><span class="line">httpCode	<span class="keyword">String</span>,</span><br><span class="line">requestSize	<span class="built_in">BigInt</span>,</span><br><span class="line">responseSize	<span class="built_in">bigInt</span>,</span><br><span class="line"><span class="keyword">cache</span>	<span class="keyword">String</span>,</span><br><span class="line">userId <span class="built_in">BigInt</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span>(d <span class="keyword">String</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span></span><br><span class="line">location <span class="string">"/item/offline-dw/ods/access/"</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>写脚本(etl.sh)</p>
<ol>
<li>使用MR通过ETL把源数据写入ODS临时目录</li>
<li>把ODS临时目录中<code>part*</code>开头的数据移动到ODS分区目录下，并指定分区</li>
<li>接着Hive执行命令新建分区</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line"></span><br><span class="line">if [ $# -eq 1 ];then</span><br><span class="line">        time=$1</span><br><span class="line">else</span><br><span class="line">        time=`date --date "1 days ago" +%Y%m%d`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">MR做ETL 注意输入和输出(带时间)</span></span><br><span class="line">hadoop jar /home/hadoop/lib/hadoop-client-1.0.0.jar com.tunan.item.ETLDriver -libjars $LIBJARS /item/offline-dw/raw/access/$time /item/offline-dw/ods_tmp/access/$time</span><br><span class="line"><span class="meta">#</span><span class="bash">删除分区目录(d=时间)</span></span><br><span class="line">hdfs dfs -rm -r -f  /item/offline-dw/ods/access/d=$time</span><br><span class="line"><span class="meta">#</span><span class="bash">创建分区目录(d=时间)</span></span><br><span class="line">hdfs dfs -mkdir -p /item/offline-dw/ods/access/d=$time</span><br><span class="line"><span class="meta">#</span><span class="bash">ods_tmp移动数据到ods((时间目录下的part* d=时间))  </span></span><br><span class="line">hdfs dfs -mv /item/offline-dw/ods_tmp/access/$time/part* /item/offline-dw/ods/access/d=$time</span><br><span class="line"><span class="meta">#</span><span class="bash">删除ods_tmp(带时间)</span></span><br><span class="line">hdfs dfs -rm -r -f /item/offline-dw/ods_tmp/access/$time</span><br><span class="line"><span class="meta">#</span><span class="bash">hive命令行刷新分区 (判断表是否存在，partition(d=<span class="string">"<span class="variable">$time</span>"</span>))</span></span><br><span class="line">hive -e "alter table offline_dw.ods_access add if not exists partition(d=$time)"</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="数据分析案例-DWS-ADS层"><a href="#数据分析案例-DWS-ADS层" class="headerlink" title="数据分析案例(DWS/ADS层)"></a>数据分析案例(DWS/ADS层)</h2><p>这层的数据可以作为DWS层，我们要什么数据就建什么表(分区)</p>
<ul>
<li><p>统计国家流量</p>
<p>思路: 建分区表，字段分别是国家和流量，最后分组查询，插入表中</p>
<ol>
<li><p>建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> dws_country_traffic(</span><br><span class="line">	country <span class="keyword">String</span>,</span><br><span class="line">	traffic <span class="built_in">BigInt</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span>(d <span class="keyword">String</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询数据插入表中</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> offline_dw.dws_country_traffic <span class="keyword">partition</span>(d=<span class="string">'$time'</span>) </span><br><span class="line"><span class="keyword">select</span> country,<span class="keyword">sum</span>(responseSize) <span class="keyword">from</span> offline_dw.ods_access <span class="keyword">where</span> d = <span class="string">'$time'</span><span class="keyword">group</span> <span class="keyword">by</span> country;</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>统计域名流量</p>
<p>思路: 建分区表，字段分别是域名和流量，最后分组查询，插入表中</p>
<ol>
<li><p>建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> dws_domain_traffic(</span><br><span class="line">	<span class="keyword">domain</span> <span class="keyword">String</span>,</span><br><span class="line">	traffic <span class="built_in">BigInt</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span>(d <span class="keyword">String</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询数据插入表中</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> offline_dw.dws_domain_traffic <span class="keyword">partition</span>(d=<span class="string">'$time'</span>) </span><br><span class="line"><span class="keyword">select</span> <span class="keyword">domain</span>,<span class="keyword">sum</span>(responseSize) <span class="keyword">from</span> offline_dw.ods_access <span class="keyword">where</span> d=<span class="string">'$time'</span><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">domain</span>;</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>写成脚本方便调度</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line"></span><br><span class="line">if [ $# -eq 1 ];then</span><br><span class="line">        time=$1</span><br><span class="line">else</span><br><span class="line">        time=`date --date '1 days ago' +%Y%m%d`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive -e "insert overwrite table offline_dw.dws_country_traffic partition(d='$time') </span><br><span class="line">select country,sum(responseSize) from offline_dw.ods_access where d='$time' group by country;"</span><br><span class="line"></span><br><span class="line">hive -e "insert overwrite table offline_dw.dws_domain_traffic partition(d='$time') </span><br><span class="line">select domain,sum(responseSize)  from offline_dw.ods_access where d='$time' group by domain;"</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="使用crontab调度脚本-临时"><a href="#使用crontab调度脚本-临时" class="headerlink" title="使用crontab调度脚本(临时)"></a>使用crontab调度脚本(临时)</h2><p>每天凌晨一点执行etl.sh，每天凌晨两点执行stats.sh</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">crontab -e</span><br><span class="line"></span><br><span class="line">0 1 * * * /home/hadoop/offline_dw/script/etl.sh</span><br><span class="line">0 2 * * * /home/hadoop/offline_dw/script/stats.sh</span><br></pre></td></tr></table></figure>

</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/02/17/offlinedw/4.%E9%A1%B9%E7%9B%AE%E6%95%B0%E6%8D%AE%E7%9A%84ETL/">项目数据的ETL</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-02-17</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/OfflineDW/">OfflineDW</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/OfflineDW/">OfflineDW</a></span><div class="content"><p>系统架构图:</p>
<p><img src="https://yerias.github.io/offlinedw/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E9%A1%B9%E7%9B%AE%E5%9B%BE_2.jpg" alt="数据仓库架构项目图"></p>
<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li><p>为什么要进行ETL</p>
</li>
<li><p>什么是ETL</p>
</li>
<li><p>ETL该怎么做</p>
</li>
<li><p>ETL在服务器上运行需要解决的问题</p>
</li>
</ol>
<h2 id="为什么要进行ETL"><a href="#为什么要进行ETL" class="headerlink" title="为什么要进行ETL"></a>为什么要进行ETL</h2><p>在上一步我们使用Flume采集数据到HDFS，从系统架构图来看现在要进行数据的ETL操作，ETL进程对数据进行规范化、验证、清洗，并最终装载进入数据仓库</p>
<h2 id="什么是ETL"><a href="#什么是ETL" class="headerlink" title="什么是ETL"></a>什么是ETL</h2><p>ETL 即 Extract Transform Load的首字母 ==&gt; 抽取、转换、加载</p>
<h2 id="ETL该怎么做"><a href="#ETL该怎么做" class="headerlink" title="ETL该怎么做"></a>ETL该怎么做</h2><p>数据采集到HDFS上指定的目录下，通过MR写入数据，进行ETL操作，并写出到指定的目录下，ETL操作包括定义数据字段的序列化类，把时间解析出年月日，把URL解析为http、domain和path、对异常值进行处理(try/catch)，使用计数器。</p>
<p>需要注意:</p>
<ol>
<li><p>时间解析参考代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//时间</span></span><br><span class="line">String time = split[<span class="number">0</span>];</span><br><span class="line">SimpleDateFormat format = <span class="keyword">new</span> SimpleDateFormat(<span class="string">"[dd/MM/yyyy:HH:mm:ss +0800]"</span>);</span><br><span class="line">Date date = format.parse(split[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">Calendar calendar = Calendar.getInstance();</span><br><span class="line">calendar.setTime(date);</span><br><span class="line"></span><br><span class="line"><span class="comment">//year</span></span><br><span class="line">String year =String.valueOf(calendar.get(Calendar.YEAR));</span><br><span class="line">access.setYear(year);</span><br><span class="line"></span><br><span class="line"><span class="comment">//month</span></span><br><span class="line"><span class="keyword">int</span>  month = calendar.get(Calendar.MONTH)+<span class="number">1</span>;</span><br><span class="line">access.setMont(month &lt; <span class="number">10</span> ? <span class="string">"0"</span>+month:String.valueOf(month))</span><br><span class="line"></span><br><span class="line"><span class="comment">//day</span></span><br><span class="line"><span class="keyword">int</span> day = calendar.get(Calendar.DAY_OF_MONTH);</span><br><span class="line">access.setDay(day &lt; <span class="number">10</span> ? <span class="string">"0"</span>+day:String.valueOf(day));</span><br></pre></td></tr></table></figure>
</li>
<li><p>异常值是舍去还是保留，这跟try/catch如何操作有关系，参考代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//要数据,设默认值为0</span></span><br><span class="line"><span class="keyword">long</span> responseSize = <span class="number">0L</span>;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    responseSize = Long.parseLong(split[<span class="number">9</span>].trim());</span><br><span class="line">    access.setResponseSize(responseSize);</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">    access.setResponseSize(responseSize);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 不要数据，产生异常直接返回</span></span><br><span class="line">Long responseSize = Long.parseLong(split[<span class="number">9</span>].trim());</span><br><span class="line">access.setResponseSize(responseSize);</span><br></pre></td></tr></table></figure>
</li>
<li><p>计数器mapper中的参考代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">context.getCounter(<span class="string">"ETL"</span>,<span class="string">"SUCCEED"</span>).increment(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>计数器Driver中的参考代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//通过迭代器获取mapper中的计数器</span></span><br><span class="line">CounterGroup group = job.getCounters().getGroup(<span class="string">"ETL"</span>);</span><br><span class="line">Iterator&lt;Counter&gt; iterator = group.iterator();</span><br><span class="line"><span class="keyword">while</span>(iterator.hasNext())&#123;</span><br><span class="line">Counter counter = iterator.next();</span><br><span class="line">     System.out.println(counter.getName() + <span class="string">"==&gt;"</span> + counter.getValue());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意: 这里可以通过jdbc将计数器结果根据批次写到mysql数据库中</p>
</li>
</ol>
<h2 id="ETL在服务器上运行需要解决的问题"><a href="#ETL在服务器上运行需要解决的问题" class="headerlink" title="ETL在服务器上运行需要解决的问题"></a>ETL在服务器上运行需要解决的问题</h2><p>在本地测试好代码后，上传Jar包到服务器上，跑HDFS上的数据</p>
<p>首先创建三个文件夹lib、data、script放ETL相关的文件，运行脚本的shell文件就在script目录下</p>
<p>由于我们把ETL打的瘦包，所以很多数据需要的依赖Jar包得不到，还有ip解析库的数据库也需要上传到本地文件下</p>
<p>思路是:</p>
<ol>
<li><p>把ip解析库放到项目的resources目录下</p>
</li>
<li><p>把需要的依赖上传到lib目录下</p>
</li>
<li><p>在<code>~/.bashrc</code>文件下导入LIBJARS路径用来指向lib目录下的依赖</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">export LIBJARS=/home/hadoop/lib/LIBJARS</span><br><span class="line">export LIBJARS=$LIBJARS/commons-lang3-<span class="number">3.4</span>.jar,$LIBJARS/qqwry.dat,$LIBJARS/mysql-connector-java-<span class="number">5.1</span><span class="number">.27</span>-bin.jar</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行jar包命令写进脚本，执行脚本即可</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">time=<span class="number">20200217</span></span><br><span class="line">hadoop jar hadoop-client-<span class="number">1.0</span><span class="number">.0</span>.jar  com.tunan.ip.ipParseDriver  -libjars $LIBJARS  /item/offline-dw/raw/access/$time /item/offline-dw/tmp/access/$time/</span><br></pre></td></tr></table></figure>

<p><code>-libjars</code>用来指定外部依赖，<code>$LIBJARS</code>指向<code>~/.bashrc</code>文件中的路径</p>
<p><code>/item/offline-dw/raw/access/$time</code>是源数据，这个数据一般保存7天后即可删除</p>
<p><code>/item/offline-dw/tmp/access/$time</code>/是ETL后的数据</p>
<p><code>$LIBJARS/qqwry.dat</code>是ip解析库的路径</p>
</li>
<li><p>还可以把ip解析库在服务器上的路径写死在代码中，就不用手动指定ip解析库的路径了</p>
</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/02/16/offlinedw/3.%E9%A1%B9%E7%9B%AE%E6%95%B0%E6%8D%AE%E7%9A%84%E9%87%87%E9%9B%86/">项目数据的采集</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-02-16</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/OfflineDW/">OfflineDW</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/OfflineDW/">OfflineDW</a></span><div class="content"><p>系统架构图:</p>
<p><img src="https://yerias.github.io/offlinedw/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E9%A1%B9%E7%9B%AE%E5%9B%BE_1.jpg" alt="数据仓库架构项目图"></p>
<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>Flume是用来做什么的</p>
<p>为什么要使用Flume</p>
<p>Flume具体怎么用</p>
<p>java客户端上传消息到Flume写到HDFS</p>
<p>解决Flume采集数据时生成大量小文件的问题</p>
<h2 id="为什么要使用Flume"><a href="#为什么要使用Flume" class="headerlink" title="为什么要使用Flume"></a>为什么要使用Flume</h2><p>在开源框架的选择中，因为Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. 所以我们选择了Flume作为数据的采集工具</p>
<h2 id="Flume是用来做什么的"><a href="#Flume是用来做什么的" class="headerlink" title="Flume是用来做什么的"></a>Flume是用来做什么的</h2><p>从系统架构图上来看，用户只要产生行为，那么日志就会在Nginx服务器中保存，所以我们现在要做的就是把数据从Nginx服务器中使用Flume采集到HDFS上</p>
<h2 id="Flume具体怎么用"><a href="#Flume具体怎么用" class="headerlink" title="Flume具体怎么用"></a>Flume具体怎么用</h2><h3 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h3><p>Flume就是一个针对日志数据进行采集和汇总的一个框架</p>
<p>Flume的进程叫做Agent，每个Agent中有Srouce、Channel、Sink</p>
<p>Flume从使用层面来讲就是写配置文件，其实就是配置我们的Agent，只要学会从官网查配置就行了</p>
<p><a href="http://flume.apache.org" target="_blank" rel="noopener">Flume官网</a></p>
<h3 id="Agent"><a href="#Agent" class="headerlink" title="Agent"></a>Agent</h3><p>Source中的常用方式有 avro、exec、spooling、taildir、kafka</p>
<p>Channel中的常用方式有 memory、kafka、file</p>
<p>Sink中的常用方式有 hdfs、logger、avro、kafka</p>
<h3 id="示例配置"><a href="#示例配置" class="headerlink" title="示例配置"></a>示例配置</h3><p>它描述了一个单节点Flume部署。该配置允许用户生成事件，然后将它们记录到控制台。以后写配置只需要在这个配置文件的基础上做出选择性的修改即可</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>有了这个配置文件，我们可以按如下方式启动Flume:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf $FLUME_HOME/conf \</span><br><span class="line">--conf-file $FLUME_HOME/script/example.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>输入端:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">telnet localhost 44444</span><br></pre></td></tr></table></figure>

<h3 id="TAILDIR"><a href="#TAILDIR" class="headerlink" title="TAILDIR"></a>TAILDIR</h3><p>taildir是Source端可以选择的一个类型，它可以同时支持目录和文件，并且支持offset，Flume挂了重启后可以接着上次消费的地方继续消费，下面提供一个配置taildir的文件</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = taildir</span><br><span class="line">a1.sources.r1.positionFile = /home/hadoop/taildir_position.json	#这个目录的上一级目录不能存在</span><br><span class="line">a1.sources.r1.filegroups = f1 f2</span><br><span class="line">a1.sources.r1.filegroups.f1 = /home/hadoop/data/flume/example</span><br><span class="line">a1.sources.r1.headers.f1.headerKey1 = value1</span><br><span class="line">a1.sources.r1.filegroups.f2 = /home/hadoop/data/flume/.*log.*</span><br><span class="line">a1.sources.r1.headers.f2.headerKey1 = value2</span><br><span class="line">a1.sources.r1.headers.f2.headerKey2 = value2-<span class="number">2</span></span><br><span class="line">a1.sources.r1.fileHeader = <span class="keyword">true</span></span><br><span class="line">a1.sources.ri.maxBatchCount = <span class="number">1000</span></span><br></pre></td></tr></table></figure>

<p>其中<code>taildir_position.json</code>文件是用来记录文件读取offset的位置，方便下次继续从offset位置读取</p>
<p>注意<code>taildir_position.json</code>文件不能存在上级目录，不然会报错</p>
<h2 id="java客户端上传消息到Flume写到HDFS"><a href="#java客户端上传消息到Flume写到HDFS" class="headerlink" title="java客户端上传消息到Flume写到HDFS"></a>java客户端上传消息到Flume写到HDFS</h2><p>参考博客: <a href="https://blog.csdn.net/lbship/article/details/85336555" target="_blank" rel="noopener">https://blog.csdn.net/lbship/article/details/85336555</a></p>
<ol>
<li><p>在java客户端编写代码生成logger数据</p>
<p>添加依赖</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">   &lt;groupId&gt;org.apache.flume.flume-ng-clients&lt;/groupId&gt;</span><br><span class="line">   &lt;artifactId&gt;flume-ng-log4jappender&lt;/artifactId&gt;</span><br><span class="line">   &lt;version&gt;1.6.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.slf4j&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.7.21&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flume.flume-ng-clients&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flume-ng-log4jappender&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.6.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p>生产日志</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> Logger logger = Logger.getLogger(LoggerGenerator<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(i&lt;<span class="number">99</span>)&#123;</span><br><span class="line">        Thread.sleep(<span class="number">500</span>);</span><br><span class="line">        logger.info(<span class="string">"now is : "</span>+i);</span><br><span class="line">        i++;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在log4j.properties文件中添加代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">log4j.rootCategory=info,console,flume  <span class="comment">//rootCategory需要指定flume</span></span><br><span class="line">    </span><br><span class="line">log4j.appender.flume=org.apache.flume.clients.log4jappender.Log4jAppender</span><br><span class="line">log4j.appender.flume.Hostname=<span class="number">121.196</span><span class="number">.220</span><span class="number">.143</span></span><br><span class="line">log4j.appender.flume.Port=<span class="number">41414</span></span><br><span class="line">log4j.appender.flume.UnsafeMode=<span class="keyword">true</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>在Hostname服务端接收log4j.properties文件中指定的端口即可，注意Source类型需要使用avro(序列化)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">a1.sources.r1.port = <span class="number">41414</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>写出到HDFS，这里需要注意解决时间戳和乱码问题</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = /flume/%y-%m-%d/%H%M/%S	#时间需要根据实际情况修改</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = events-</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp=true		#解决时间戳问题</span><br><span class="line">a1.sinks.k1.hdfs.fileType=DataStream 		#解决乱码</span><br></pre></td></tr></table></figure>

<p>使用<code>useLocalTimeStamp=true</code>参数解决时间戳异常，使用<code>fileType=DataStream</code>解决文件内容乱码问题</p>
</li>
</ol>
<h2 id="解决Flume采集数据时生成大量小文件的问题"><a href="#解决Flume采集数据时生成大量小文件的问题" class="headerlink" title="解决Flume采集数据时生成大量小文件的问题"></a>解决Flume采集数据时生成大量小文件的问题</h2><p>在使用Flume采集数据时，由于默认参数的影响会生产大量小文件，我们先看默认参数</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hdfs.rollInterval	<span class="number">30</span>	滚动当前文件之前要等待的秒数</span><br><span class="line">hdfs.rollSize	<span class="number">1024</span>	触发滚动当前文件的大小，单位bytes(B)</span><br><span class="line">hdfs.rollCount	<span class="number">10</span>		触发滚动当前文件的events数量</span><br></pre></td></tr></table></figure>

<p>我们看到默认生成文件有三个条件，每30秒、每1M、每10个events，这样的配置会生成大量的小文件，所以我们要对这三个文件进行修改</p>
<p>最终生成的文件必须综合时间、文件大小、event数量来决定，时间太长或者文件太大都不利于最终生成的文件。该时间还需要配合<code>hdfs.path</code>参数指定的生成文件时间。</p>
<p>注意<code>sink.type</code>如果是<code>memory</code>模式，注意文件的大小，防止内存不足，太大可以设置<code>sink.type = file</code></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/02/15/offlinedw/2.%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E6%B5%81%E7%A8%8B/">项目开发的流程</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-02-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/OfflineDW/">OfflineDW</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/OfflineDW/">OfflineDW</a></span><div class="content"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li>为什么是离线数据仓库</li>
<li>采集什么日志</li>
<li>技术实现流程</li>
</ol>
<h2 id="为什么是离线数据仓库"><a href="#为什么是离线数据仓库" class="headerlink" title="为什么是离线数据仓库"></a>为什么是离线数据仓库</h2><h3 id="什么是数据仓库"><a href="#什么是数据仓库" class="headerlink" title="什么是数据仓库"></a>什么是数据仓库</h3><p>将多个数据源的数据经过ETL之后，按照一定的主题继承，提供 <code>决策支持</code> 和 <code>联机分析应用</code> 的结构化数据环境</p>
<h3 id="为什么要建数据仓库"><a href="#为什么要建数据仓库" class="headerlink" title="为什么要建数据仓库"></a>为什么要建数据仓库</h3><p>摆脱多种不同数据源、异构数据库、不同数据格式等等带来的问题</p>
<h2 id="采集用户行为日志"><a href="#采集用户行为日志" class="headerlink" title="采集用户行为日志"></a>采集用户行为日志</h2><p>既然要建数据仓库，那么第一步需要考虑的是我们的数据从哪里来？来的什么数据？这些数据是做什么的？</p>
<h3 id="数据是从哪里来的？"><a href="#数据是从哪里来的？" class="headerlink" title="数据是从哪里来的？"></a>数据是从哪里来的？</h3><p><code>数据</code>通过<code>采集团队</code>从<code>ngnix</code>服务器(请求携带的数据经过nginx服务器)、<code>埋点日志</code>(用户行为过程及结果的记录)和<code>SDK</code>(软件开发工具包)通过<code>flume</code>采集产生的数据到HDFS上</p>
<h3 id="来的是什么数据？"><a href="#来的是什么数据？" class="headerlink" title="来的是什么数据？"></a>来的是什么数据？</h3><p>采集到的是用户的行为数据，包括日志和业务数据，只要是用户访问、搜索、点击、收藏都会产生数据通过flume采集到了HDFS上</p>
<h3 id="数据可以用来做什么的？"><a href="#数据可以用来做什么的？" class="headerlink" title="数据可以用来做什么的？"></a>数据可以用来做什么的？</h3><p>数据运用在数据仓库中会进行一系列的ETL、调度、建模，最终用来可视化分析。</p>
<h2 id="技术实现流程"><a href="#技术实现流程" class="headerlink" title="技术实现流程"></a>技术实现流程</h2><h3 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h3><p>从输入==&gt;到输出</p>
<p>可以使用的工具:</p>
<ol>
<li>SDK</li>
<li>Flume(推介)</li>
<li>Sqoop</li>
<li>DataX</li>
</ol>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>数据预处理也就是对数据进行ETL/清洗操作</p>
<ol>
<li>格式处理：时间、IP、URL</li>
<li>数据拆分：1 col ==&gt; n cols  (URL、UA)</li>
<li>数据补充：1 col ==&gt; n cols</li>
</ol>
<h3 id="数据入库"><a href="#数据入库" class="headerlink" title="数据入库"></a>数据入库</h3><p>数据清洗后的数据导入到你HIVE中的库/表中(大宽表/N多列)</p>
<ol>
<li>分析的维度：day or hour</li>
<li>表：分区外部表</li>
</ol>
<h3 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h3><p>根据不同业务的执行SQL并且通过SQOOP存到RDBMS/NoSQL的表中</p>
<h3 id="数据展示"><a href="#数据展示" class="headerlink" title="数据展示"></a>数据展示</h3><ol>
<li>WEB ==&gt; RDBMS</li>
<li>Echarts</li>
<li>d3</li>
<li>HUE</li>
<li>Zeppelin</li>
</ol>
<h3 id="项目架构"><a href="#项目架构" class="headerlink" title="项目架构"></a>项目架构</h3><p><img src="https://yerias.github.io/offlinedw/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E9%A1%B9%E7%9B%AE%E5%9B%BE.jpg" alt="数据仓库架构项目图"></p>
<h3 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h3><ol>
<li><p>评估你这个业务线需要多少资源，数据量的计算: 一条数据的大小==&gt;*用户数*每个人一天产生的数据量==&gt;*副本数*天数(365)==&gt;*每年30%的上升空间</p>
</li>
<li><p>每个节点磁盘多少？内存多少？物理核多少？通过计算得出需要的节点数</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">core：<span class="number">32</span>/<span class="number">64</span> 决定了应用程序的快慢</span><br><span class="line">memory：<span class="number">256</span>/<span class="number">512</span> 决定了应用程序的生死</span><br><span class="line">disk：<span class="number">10</span>T*数量</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h2><p>对数据进行ETL操作时，N个业务：至少是3*N个SQL(3层)，并且尽可能把一些麻烦的/繁琐的/join等SQL操作提前进行</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/02/14/offlinedw/1.%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E5%87%86%E5%A4%87/">项目开发的准备</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-02-14</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/OfflineDW/">OfflineDW</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Flume/">Flume</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/OfflineDW/">OfflineDW</a></span><div class="content"><hr>
<p>涉及到具体的文档，这里只描述流程</p>
<hr>
<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol>
<li><p>项目调研</p>
</li>
<li><p>需求分析</p>
</li>
<li><p>方案设计</p>
</li>
</ol>
<h2 id="项目调研"><a href="#项目调研" class="headerlink" title="项目调研"></a>项目调研</h2><p>是什么行业? </p>
<p>关于什么业务？</p>
<p>调研人员(资深的产品经理/业务分析人员)</p>
<h2 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h2><p>要做什么?</p>
<p>做成啥样?</p>
<ol>
<li>需求(表层的需求、隐藏的需求、售前团队的需求)</li>
<li>产出(需求规格说明书、进度规划：甘特图)</li>
<li>人员(项目经理、产品、leader)</li>
</ol>
<h2 id="方案设计"><a href="#方案设计" class="headerlink" title="方案设计"></a>方案设计</h2><h3 id="概要设计"><a href="#概要设计" class="headerlink" title="概要设计"></a>概要设计</h3><ol>
<li>技术架构(框架的调研、”糙”点的测试报告)</li>
<li>模块</li>
<li>功能点</li>
<li>和甲方确认过程</li>
</ol>
<h3 id="详细设计"><a href="#详细设计" class="headerlink" title="详细设计"></a>详细设计</h3><p>详细设计是最复杂的、篇幅最大的，针对具体功能的实现</p>
<p>基本要求(类、方法(入参、出参)、UML、图表)</p>
<p>系统要求/非功能需求</p>
<ol>
<li>扩展性</li>
<li>容错性</li>
<li>是否支持定制化：Azkaban</li>
<li>监控/告警(发生后)/预警(发生前)</li>
</ol>
<h3 id="功能开发"><a href="#功能开发" class="headerlink" title="功能开发"></a>功能开发</h3><p>码农对着文档把方法实现了而已，CICD(持续集成(CI)、持续交付(CD))</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><ol>
<li>测试覆盖率 95%</li>
<li>场景测试(案例)</li>
<li>功能测试</li>
<li>联调测试(一个完整流程)</li>
<li>性能/压力测试</li>
<li>用户测试</li>
</ol>
<h3 id="部署上线"><a href="#部署上线" class="headerlink" title="部署上线"></a>部署上线</h3><ol>
<li>试运行(“双活”系统做DIFF)</li>
<li>正式上线</li>
</ol>
<h3 id="后期维护"><a href="#后期维护" class="headerlink" title="后期维护"></a>后期维护</h3><p>后期维护的费用并不低，包括开发新功能，修复老bug</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/04/09/cdh/9/">CentOS7安装CDH 第九章：CDH中安装Kafka</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-04-09</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/CDH/">CDH</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/CDH/">CDH</a></span><div class="content"><h2 id="CDH官网Kafka的安装教程网址"><a href="#CDH官网Kafka的安装教程网址" class="headerlink" title="CDH官网Kafka的安装教程网址"></a>CDH官网Kafka的安装教程网址</h2><p><a href="https://www.cloudera.com/documentation/kafka/latest/topics/kafka_installing.html#concept_m2t_d45_4r" target="_blank" rel="noopener">点击进入官网</a></p>
<p><img src="https://yerias.github.io/cdh/kafka-1.png" alt=""></p>
<h2 id="下载对应的Kafka版本"><a href="#下载对应的Kafka版本" class="headerlink" title="下载对应的Kafka版本"></a>下载对应的Kafka版本</h2><ol>
<li><p>查看CDH和Kafka的版本对应列表：</p>
<p><a href="https://www.cloudera.com/documentation/enterprise/release-notes/topics/rn_consolidated_pcm.html#pcm_kafka" target="_blank" rel="noopener">点击进入官网</a></p>
</li>
</ol>
<p><img src="https://yerias.github.io/cdh/kafka-2.png" alt=""></p>
<ol start="2">
<li><p>因为安装的CDH版本为5.10或5.12，故选择的Kafka版本为2.2.x和0.10.2，此时去网站找到对应的Kafka版本：</p>
<p><a href="https://www.cloudera.com/documentation/kafka/latest/topics/kafka_packaging.html#concept_fzg_phl_br" target="_blank" rel="noopener">点击进入官网</a></p>
</li>
</ol>
<p><img src="https://yerias.github.io/cdh/kafka-3.png" alt=""></p>
<ol start="3">
<li>点击对应的下载地址，下载该Kafka的parcel包（需更改sha1的后缀名）：</li>
</ol>
<p><img src="https://yerias.github.io/cdh/kafka-4.png" alt=""></p>
<ol start="4">
<li>最终的主节点上的kafka_parcel包(包的位置任意，最终都要移动到/var/www/html/kafka_parcel目录下)</li>
</ol>
<p><img src="https://yerias.github.io/cdh/kafka-4-2.jpg" alt=""></p>
<h2 id="安装Kafka服务"><a href="#安装Kafka服务" class="headerlink" title="安装Kafka服务"></a>安装Kafka服务</h2><ol>
<li><p>将主节点上Kafka的parcel包（3个文件）上传到/var/www/html/kafka_parcel目录下，需配置好https服务，请参考上述CDH安装时的方法配置，在浏览器上能访问到如下场景即可：</p>
<p>安装httpd: <code>yum install -y httpd</code></p>
</li>
</ol>
<p><img src="https://yerias.github.io/cdh/kafka-5.png" alt=""></p>
<ol start="2">
<li>点击CDH主页面中的主机下面的Parcel按钮：</li>
</ol>
<p><img src="https://yerias.github.io/cdh/kafka-6.png" alt=""></p>
<ol start="3">
<li>点击Parcel界面的配置按钮，配置Kafka的地址，该地址默认是官网地址，但在CDH的离线安装时已将所有的在线地址删除，所以在这加上Kafka的Parcel包的离线地址即可：</li>
</ol>
<p><img src="https://yerias.github.io/cdh/kafka-7.png" alt=""></p>
<ol start="4">
<li>在Parcel界面，点击Kafka的下载按钮：</li>
</ol>
<p><img src="https://yerias.github.io/cdh/kafka-8.png" alt=""></p>
<ol start="5">
<li>依次执行Kafka的分配和激活：</li>
</ol>
<p><img src="https://yerias.github.io/cdh/kafka-9.png" alt=""></p>
<p><img src="https://yerias.github.io/cdh/kafka-9-2.png" alt="kafka-9-2"></p>
<h2 id="将Kafka服务添加到CDH中"><a href="#将Kafka服务添加到CDH中" class="headerlink" title="将Kafka服务添加到CDH中"></a>将Kafka服务添加到CDH中</h2><ol>
<li>在CDH的主界面点击添加服务按钮，并选择Kafka服务：</li>
</ol>
<p><img src="https://yerias.github.io/cdh/kafka-10.png" alt=""></p>
<ol start="2">
<li>给Kafka分配节点（Kafka后面2个服务一般情况下不选）：</li>
</ol>
<p><img src="https://yerias.github.io/cdh/kafka-11.png" alt=""></p>
<p><img src="https://yerias.github.io/cdh/kafka-11-2.png" alt=""></p>
<ol start="3">
<li>Kafka的配置文件进行配置：</li>
</ol>
<p>配置Kafka的文件存放目录，因为Kafka是依赖Zookeeper的，所以Kafka的文件也是存放在Zookeeper的目录中，如果要卸载Kafka时，需要将这些Kafka的文件也删除，所以可以把Kafka的文件存放在一个目录中：</p>
<p>Kafka的文件存放目录：</p>
<p><img src="https://yerias.github.io/cdh/kafka-12.png" alt=""></p>
<p>进入Zookeeper的文件管理界面（命令行）：</p>
<p><img src="https://yerias.github.io/cdh/kafka-13.png" alt=""></p>
<p><img src="https://yerias.github.io/cdh/kafka-14.png" alt=""></p>
<p>因为Kafka是一个消息中间键，有将生产者生产的信息进行缓存的操作，所以在配置Kafka的数据存储目录时需要注意，将数据存放到一个比较大的磁盘中，该数据存放的目录如下配置所示：</p>
<p><img src="https://yerias.github.io/cdh/kafka-15.png" alt=""></p>
<p>在卸载重装Kafka时，需要将Zookeeper目录下的Kafka文件，以及Kafka数据存放的目录都清空，请注意是每个节点都要清空，否则不能重装。</p>
<ol start="5">
<li>启动Kafka服务，会发现Kafka服务不能成功启动，报错如下：</li>
</ol>
<p><img src="https://yerias.github.io/cdh/kafka-16.png" alt=""></p>
<p>查看日志</p>
<p><img src="https://yerias.github.io/cdh/kafka-17.png" alt=""></p>
<p>此时为主机的内存不足，返回Kafka配置文件界面，修改memory中的Java Heap Size of Broker值为512M（如果机器内存充足，可以再大一些），如下：</p>
<p><img src="https://yerias.github.io/cdh/kafka-18.png" alt=""></p>
<p>修改之后去CDH的主界面重启Kafka，启动成功，如下所示：</p>
<p><img src="https://yerias.github.io/cdh/kafka-19.png" alt=""></p>
<h2 id="测试Kafka"><a href="#测试Kafka" class="headerlink" title="测试Kafka"></a>测试Kafka</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建topic</span></span><br><span class="line">./kafka-topics.sh \</span><br><span class="line">--create \</span><br><span class="line">--zookeeper cdh001:2181,cdh002:2181,cdh003:2181/kafka \</span><br><span class="line">--replication-factor 2 --partitions 3 --topic cloudera</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动生产者</span></span><br><span class="line">./kafka-console-producer.sh \</span><br><span class="line">--broker-list cdh001:9092 ,cdh002:9092 ,cdh003:9092  \</span><br><span class="line">--topic cloudera</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动消费者</span></span><br><span class="line">./kafka-console-consumer.sh \</span><br><span class="line">--bootstrap-server cdh001:9092 ,cdh002:9092 ,cdh003:9092  \</span><br><span class="line">--from-beginning \</span><br><span class="line">--topic cloudera</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除topic</span></span><br><span class="line">./kafka-topics.sh  \</span><br><span class="line">--delete \</span><br><span class="line">--zookeeper  cdh001:2181,cdh002:2181,cdh003:2181/kafka  \</span><br><span class="line">--topic cloudera</span><br></pre></td></tr></table></figure>

<h2 id="重装Kafka"><a href="#重装Kafka" class="headerlink" title="重装Kafka"></a>重装Kafka</h2><p>除了上文提到的要删除zookeeper中的kafka目录，还要删除Kafka的数据存储目录，即<code>/var/local/kafka/data</code></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/02/01/git/1/">GIT的常用操作&amp;GITHUB的常用操作&amp;在IDEA中使用GIT操作GITHUB</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-02-01</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Git/">Git</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Git/">Git</a></span><div class="content"><h2 id="GIT实战操作"><a href="#GIT实战操作" class="headerlink" title="GIT实战操作"></a>GIT实战操作</h2><ol>
<li><p>创建版本库</p>
<p>在项目文件夹内，执行: git init</p>
</li>
<li><p>提交文件</p>
<p>新建文件后，通过git status 进行查看文件状态(可选)</p>
<p>将文件添加到暂存区  git add 文件名</p>
<p>或者也可以git commit –m “注释内容”, 直接带注释提交</p>
</li>
<li><p>查看文件提交记录</p>
<p>git log –pretty=oneline 文件名   进行查看历史记录</p>
</li>
<li><p>回退历史</p>
<p>git reset –hard HEAD~n 回退n次操作</p>
</li>
<li><p>版本穿越</p>
<p>进行查看历史记录的版本号，执行 git reflog 文件名</p>
<p>执行 git reset –hard 版本号</p>
</li>
<li><p>还原文件</p>
<p>git checkout – 文件名 </p>
</li>
<li><p>删除某个文件</p>
<p>先删除文件 git rm 文件名</p>
<p>再git add 再提交</p>
</li>
<li><p>创建分支</p>
<p>git branch &lt;分支名&gt;</p>
<p>git branch –v 查看分支</p>
</li>
<li><p>切换分支</p>
<p>git checkout –b &lt;分支名&gt;</p>
</li>
<li><p>合并分支</p>
<p>先切换到主干  git checkout master</p>
<p>git merge &lt;分支名&gt;</p>
</li>
<li><p>合并时冲突</p>
<p>程序合并时发生冲突系统会提示CONFLICT关键字，命令行后缀会进入MERGING状态，表示此时是解决冲突的状态。</p>
<p>然后修改冲突文件的内容，再次git add <file> 和git commit 提交后，后缀MERGING消失，说明冲突解决完成。</p>
</li>
</ol>
<h2 id="GITHUB实战操作"><a href="#GITHUB实战操作" class="headerlink" title="GITHUB实战操作"></a>GITHUB实战操作</h2><ol>
<li><p>搭建代码库</p>
<ul>
<li><p>git init</p>
</li>
<li><p>git config </p>
<ul>
<li>git config –global(全局) user.email “<a href="mailto:you@example.com" target="_blank" rel="noopener">you@example.com</a>”</li>
<li>git config –global(全局) user.name “Your Name”</li>
</ul>
</li>
</ul>
</li>
<li><p>提交代码到本地仓库</p>
<ul>
<li><p>git add 文件名</p>
</li>
<li><p>git commit –m “注释内容”</p>
</li>
</ul>
</li>
<li><p>GitHub准备工作：</p>
<ul>
<li><p>注册GitHub账号</p>
</li>
<li><p>在GitHub搭建项目</p>
</li>
</ul>
</li>
<li><p>推送代码到远端</p>
<ul>
<li><p>git remote add origin <url>(仓库地址)</p>
</li>
<li><p>git push origin master</p>
</li>
</ul>
</li>
<li><p>其他用户克隆</p>
<p>git clone <url></p>
</li>
<li><p>其他用户提交代码到本地仓库</p>
<ul>
<li><p>git add 文件名</p>
</li>
<li><p>git commit –m “注释内容”</p>
</li>
</ul>
</li>
<li><p>其他用户推送到远端仓库</p>
<ul>
<li>git push origin master</li>
</ul>
</li>
<li><p>其他用户拉取代码</p>
<ul>
<li>git pull origin master</li>
</ul>
</li>
<li><p>增加远程地址</p>
<ul>
<li>git remote add &lt;远端代号(origin)&gt;  &lt;远端地址&gt;</li>
</ul>
</li>
<li><p>推送到远程库</p>
<ul>
<li>git push  &lt;远端代号&gt;  &lt;本地分支名称&gt;</li>
</ul>
</li>
<li><p>合作开发权限</p>
<p><img src="https://yerias.github.io/git/%E6%B7%BB%E5%8A%A0%E5%90%88%E4%BD%9C%E4%BC%99%E4%BC%B41.jpg" alt="添加合作伙伴1"></p>
<p><img src="https://yerias.github.io/git/%E6%B7%BB%E5%8A%A0%E5%90%88%E4%BD%9C%E4%BC%99%E4%BC%B42.jpg" alt="添加合作伙伴2"></p>
</li>
<li><p>协作冲突</p>
<p>在上传或同步代码时，由于你和他人都改了同一文件的同一位置的代码，版本管理软件无法判断究竟以谁为准，就会报告冲突,需要程序员手工解决。</p>
<ul>
<li><p>修改合并</p>
</li>
<li><p>git add 文件名</p>
</li>
<li><p>git commit –m “注释内容”</p>
</li>
<li><p>git push origin master</p>
</li>
</ul>
</li>
</ol>
<h2 id="在IDEA中使用GIT"><a href="#在IDEA中使用GIT" class="headerlink" title="在IDEA中使用GIT"></a>在IDEA中使用GIT</h2><ol>
<li><p>配置</p>
<p>setting配置GIT</p>
<p><img src="https://yerias.github.io/git/%E9%85%8D%E7%BD%AEgit%E6%89%A7%E8%A1%8C%E7%A8%8B%E5%BA%8F.jpg" alt="配置git执行程序"></p>
</li>
<li><p>创建仓库</p>
<p>VCS配置账户密码创建仓库</p>
<p><img src="https://yerias.github.io/git/%E5%88%9B%E5%BB%BAgithub%E4%BB%93%E5%BA%93.jpg" alt="创建github仓库"></p>
</li>
<li><p>提交代码</p>
<p><img src="https://yerias.github.io/git/%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%811.jpg" alt="提交代码1"></p>
<p><img src="https://yerias.github.io/git/%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%812.jpg" alt="提交代码2"></p>
<p><img src="https://yerias.github.io/git/%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%813.jpg" alt="提交代码3"></p>
</li>
<li><p>同步代码</p>
<p><img src="https://yerias.github.io/git/%E5%90%8C%E6%AD%A5%E4%BB%A3%E7%A0%81.jpg" alt="同步代码"></p>
</li>
<li><p>克隆项目</p>
<p><img src="https://yerias.github.io/git/%E5%85%8B%E9%9A%86%E4%BB%A3%E7%A0%811.jpg" alt="克隆代码1"></p>
<p><img src="https://yerias.github.io/git/%E5%85%8B%E9%9A%86%E4%BB%A3%E7%A0%812.jpg" alt="克隆代码2"></p>
</li>
<li><p>解决版本冲突</p>
<p>代码添加到公共区间再次提交</p>
<p><img src="https://yerias.github.io/git/%E7%89%88%E6%9C%AC%E5%86%B2%E7%AA%81.jpg" alt="版本冲突"></p>
</li>
</ol>
</div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/6/">&lt;&lt;</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/8/">&gt;&gt;</a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>