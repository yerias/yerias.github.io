<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="MongoDB + Spark: 完整的大数据解决方案(转载)"><meta name="keywords" content="MongoDB"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>MongoDB + Spark: 完整的大数据解决方案(转载) | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark介绍"><span class="toc-number">1.</span> <span class="toc-text">Spark介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#我们能用它做什么"><span class="toc-number">1.1.</span> <span class="toc-text">我们能用它做什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark生态系统"><span class="toc-number">1.2.</span> <span class="toc-text">Spark生态系统</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS-vs-MongoDB"><span class="toc-number">1.3.</span> <span class="toc-text">HDFS vs. MongoDB</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#一个日志的例子"><span class="toc-number">1.4.</span> <span class="toc-text">一个日志的例子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-MongoDB"><span class="toc-number">2.</span> <span class="toc-text">Spark + MongoDB</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Mongo-Spark-Connector-连接器"><span class="toc-number">2.1.</span> <span class="toc-text">Mongo Spark Connector 连接器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-MongoDB-成功案例"><span class="toc-number">3.</span> <span class="toc-text">Spark + MongoDB 成功案例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#东方航空的挑战"><span class="toc-number">3.1.</span> <span class="toc-text">东方航空的挑战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#思路：空间换时间"><span class="toc-number">3.2.</span> <span class="toc-text">思路：空间换时间</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-MongoDB-方案"><span class="toc-number">3.3.</span> <span class="toc-text">Spark + MongoDB 方案</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#批处理计算流程"><span class="toc-number">3.4.</span> <span class="toc-text">批处理计算流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-任务入口程序"><span class="toc-number">3.5.</span> <span class="toc-text">Spark 任务入口程序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#处理能力和响应时间比较"><span class="toc-number">3.6.</span> <span class="toc-text">处理能力和响应时间比较</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-＋-MongoDB-演示"><span class="toc-number">4.</span> <span class="toc-text">Spark ＋ MongoDB 演示</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#安装-Spark"><span class="toc-number">4.1.</span> <span class="toc-text">安装 Spark</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#测试连接器"><span class="toc-number">4.2.</span> <span class="toc-text">测试连接器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#简单分组统计"><span class="toc-number">4.3.</span> <span class="toc-text">简单分组统计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#简单分组统计加条件过滤"><span class="toc-number">4.4.</span> <span class="toc-text">简单分组统计加条件过滤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#性能优化事项"><span class="toc-number">4.5.</span> <span class="toc-text">性能优化事项</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">5.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/xiaoqi.jpg"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><div class="follow-button"><a href="#">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">186</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">37</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">33</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">HADOOP</a><a class="author-info-links__name text-center" href="https://cwiki.apache.org/confluence/display/Hive/" target="_blank" rel="noopener">HIVE</a><a class="author-info-links__name text-center" href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">CDH</a><a class="author-info-links__name text-center" href="http://flume.apache.org/" target="_blank" rel="noopener">FLUME</a><a class="author-info-links__name text-center" href="https://azkaban.github.io/" target="_blank" rel="noopener">AZKABAN</a><a class="author-info-links__name text-center" href="https://www.maxinhong.com/" target="_blank" rel="noopener">叶梨子</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/qingyunzong" target="_blank" rel="noopener">老铁</a><a class="author-info-links__name text-center" href="https://blog.csdn.net/jim8973/" target="_blank" rel="noopener">飞哥</a><a class="author-info-links__name text-center" href="https://vinxikk.github.io/" target="_blank" rel="noopener">vinx</a><a class="author-info-links__name text-center" href="http://dongxicheng.org/" target="_blank" rel="noopener">懂西成(Hadoop技术内幕作者)</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/xing901022/" target="_blank" rel="noopener">xingoo</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/itboys/tag/" target="_blank" rel="noopener">大葱拌豆腐</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/shishanyuan" target="_blank" rel="noopener">郭景瞻(图解Spark作者)</a><a class="author-info-links__name text-center" href="https://segmentfault.com/u/wishdaren_5c243b920a3eb" target="_blank" rel="noopener">Wish大人</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/huxi2b/" target="_blank" rel="noopener">huxi_2b(kafka)</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">MongoDB + Spark: 完整的大数据解决方案(转载)</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-07-20</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/MongoDB/">MongoDB</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">5k</span><span class="post-meta__separator">|</span><span>Reading time: 15 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="Spark介绍"><a href="#Spark介绍" class="headerlink" title="Spark介绍"></a>Spark介绍</h2><p>按照官方的定义，<a href="http://lib.csdn.net/base/spark" target="_blank" rel="noopener">Spark</a> 是一个通用，快速，适用于大规模数据的处理引擎。</p>
<ul>
<li>通用性：我们可以使用Spark SQL来执行常规分析， Spark Streaming 来流数据处理， 以及用Mlib来执行机器学习等。Java，python，scala及R语言的支持也是其通用性的表现之一。</li>
<li>快速： 这个可能是Spark成功的最初原因之一，主要归功于其基于内存的运算方式。当需要处理的数据需要反复迭代时，Spark可以直接在内存中暂存数据，而无需像Map Reduce一样需要把数据写回磁盘。官方的数据表明：它可以比传统的Map Reduce快上100倍。</li>
<li>大规模：原生支持HDFS，并且其计算节点支持弹性扩展，利用大量廉价计算资源并发的特点来支持大规模数据处理。</li>
</ul>
<h3 id="我们能用它做什么"><a href="#我们能用它做什么" class="headerlink" title="我们能用它做什么"></a>我们能用它做什么</h3><p>那我们能用Spark来做什么呢？ 场景数不胜数。</p>
<p>最简单的可以只是统计一下某一个页面多少点击量，复杂的可以通过<a href="http://lib.csdn.net/base/machinelearning" target="_blank" rel="noopener">机器学习</a>来预测。</p>
<p><em>个性化</em> 是一个常见的案例，比如说，Yahoo的网站首页使用Spark来实现快速的用户兴趣分析。应该在首页显示什么新闻？原始的做法是让用户选择分类；聪明的做法就是在用户交互的过程中揣摩用户可能喜欢的文章。另一方面就是要在新闻进来时候进行分析并确定什么样的用户是可能的受众。新闻的时效性非常高，按照常规的MapReduce做法，对于Yahoo几亿用户及海量的文章，可能需要计算一天才能得出结果。Spark的高效运算可以在这里得到充分的运用，来保证新闻推荐在数十分钟或更短时间内完成。另外，如美国最大的有线电视商Comcast用它来做节目推荐，最近刚和滴滴联姻的uber用它实时订单分析，优酷则在Spark上实现了商业<a href="http://lib.csdn.net/base/aiplanning" target="_blank" rel="noopener">智能</a>的升级版。</p>
<h3 id="Spark生态系统"><a href="#Spark生态系统" class="headerlink" title="Spark生态系统"></a>Spark生态系统</h3><p>在我们开始谈<a href="http://lib.csdn.net/base/mongodb" target="_blank" rel="noopener">MongoDB</a> 和Spark 之前，我们首先来了解一下Spark的生态系统。 Spark 作为一个大型分布式计算框架，需要和其他组件一起协同工作。</p>
<p><img src="https://yerias.github.io/mongodb/mongo-1.png" alt=""></p>
<p>在Hdaoop里面，HDFS是其核心，作为一个数据层。</p>
<p>Spark是<a href="http://lib.csdn.net/base/hadoop" target="_blank" rel="noopener">Hadoop</a>生态系统的一颗新星，原生就支持HDFS。大家知道HDFS是用来管理大规模非结构化数据的存储系统，具有高可用和巨大的横向扩展能力。</p>
<p>而作为一个横向扩展的分布式集群，资源管理是其核心必备的能力，Spark 可以通过YARN或者MESOS来负责资源（CPU）分配和任务调度。如果你不需要管理节点的高可用需求，你也可以直接使用Spark standalone。</p>
<p>在有了数据层和资源管理层后， 接下来就是我们真正的计算引擎。</p>
<p><a href="http://lib.csdn.net/base/hadoop" target="_blank" rel="noopener">hadoop</a>技术的两大基石之一的MapReduce就是用来实现集群大规模并行计算。而现在就多了一个选项：Spark。 Map Reduce的特点是，用4个字来概括，简单粗暴。采用divide &amp; conquer战术，我们可以用Map Reduce来处理PB级的数据。 而Spark 作为打了鸡血的Map Reduce增强版，利用了内存价格大量下降的时代因素，充分把计算所用变量和中间结果放到内存里，并且提供了一整套机器学习的分析<a href="http://lib.csdn.net/base/datastructure" target="_blank" rel="noopener">算法</a>，在加上很多语言的支持，使之成为一个较之于Map Reduce更加优秀的选择。</p>
<p>由于Map Reduce 是一个相对并不直观的程序接口，所以为了方便使用，一系列的高层接口如<a href="http://lib.csdn.net/base/hive" target="_blank" rel="noopener">Hive</a>或者Pig应运而生。 <a href="http://lib.csdn.net/base/hive" target="_blank" rel="noopener">hive</a>可以让我们使用非常熟悉的SQL语句的方式来做一些常见的统计分析工作。同理，在Spark 引擎层也有类似的封装，如Spark SQL、RDD以及2.0版本新推出的Dataframe等。</p>
<p>所以一个完整的<a href="http://lib.csdn.net/base/hadoop" target="_blank" rel="noopener">大数据</a>解决方案，包含了存储，资源管理，计算引擎及接口层。 那么问题来了：我们画了这么大这么圆的大饼，<a href="http://lib.csdn.net/base/mongodb" target="_blank" rel="noopener">mongodb</a>可以吃哪一块呢？</p>
<p><img src="https://yerias.github.io/mongodb/mongo-2.png" alt=""></p>
<p>MongoDB是个什么？是个database。 所以自然而然，MongoDB可以担任的角色，就是数据存储的这一部分。在和 Spark一起使用的时候，MongoDB就可以扮演HDFS的角色来为Spark提供计算的原始数据，以及用来持久化分析计算的结果。</p>
<h3 id="HDFS-vs-MongoDB"><a href="#HDFS-vs-MongoDB" class="headerlink" title="HDFS vs. MongoDB"></a>HDFS vs. MongoDB</h3><p>既然我们说MongoDB可以用在HDFS的地方，那我们来详细看看两者之间的差异性。</p>
<p>在说区别之前，其实我们可以先来注意一下两者的共同点。HDFS和MongoDB都是基于廉价x86服务器的横向扩展<a href="http://lib.csdn.net/base/architecture" target="_blank" rel="noopener">架构</a>，都能支持到TB到PB级的数据量。数据会在多节点自动备份，来保证数据的高可用和冗余。两者都支持非结构化数据的存储，等等。</p>
<p>但是，HDFS和MongoDB更多的是差异点：</p>
<ul>
<li>如在存储方式上 HDFS的存储是以文件为单位，每个文件64MB到128MB不等。而MongoDB则是细颗粒化的、以文档为单位的存储。</li>
<li>HDFS不支持索引的概念，对数据的操作局限于扫描性质的读，MongoDB则支持基于二级索引的快速检索。</li>
<li>MongoDB可以支持常见的增删改查场景，而HDFS一般只是一次写入后就很难进行修改。</li>
<li>从响应时间上来说，HDFS一般是分钟级别而MongoDB对手请求的响应时间通常以毫秒作为单位。</li>
</ul>
<h3 id="一个日志的例子"><a href="#一个日志的例子" class="headerlink" title="一个日志的例子"></a>一个日志的例子</h3><p>如果说刚才的比较有些抽象，我们可以结合一个实际一点的例子来理解。</p>
<p>比如说，一个比较经典的案例可能是日志记录管理。在HDFS里面你可能会用日期范围来命名文件，如7月1日，7月2日等等，每个文件是个日志文本文件，可能会有几万到几十万行日志。</p>
<p>而在MongoDB里面，我们可以采用一个JSON的格式，每一条日志就是一个JSON document。我们可以对某几个关心的字段建索引，如时间戳，错误类型等。</p>
<p>我们来考虑一些场景，加入我们相对7月份所有日志做一些全量的统计，比如每个页面的所有点击量，那么这个HDFS和MongoDB都可以正常处理。</p>
<p>如果有一天你的经理告诉你：他想知道网站上每天有多少404错误在发生，这个时候如果你用HDFS，就还是需要通过全量扫描所有行，而MongoDB则可以通过索引，很快地找到所有的404日志，可能花数秒钟就可以解答你经理的问题。</p>
<p>又比如说，如果你希望对每个日志项加一个自定义的属性，在进行一些预处理后，MongoDB就会比较容易地支持到。而一般来说，HDFS是不支持更新类型操作的。</p>
<p>好的，我们了解了MongoDB为什么可以替换HDFS并且为什么有这个必要来做这个事情，下面我们就来看看Spark和MongoDB怎么玩！</p>
<h2 id="Spark-MongoDB"><a href="#Spark-MongoDB" class="headerlink" title="Spark + MongoDB"></a>Spark + MongoDB</h2><p>Spark的工作流程可以概括为三部曲：创建并发任务，对数据进行transformation操作，如map， filter，union，intersect等，然后执行运算，如reduce，count，或者简单地收集结果。</p>
<p><img src="https://yerias.github.io/mongodb/mongo-3.png" alt=""></p>
<p>这里是Spark和MongoDB部署的一个典型架构。</p>
<p>Spark任务一般由Spark的driver节点发起，经过Spark Master进行资源调度分发。比如这里我们有4个Spark worker节点，这些节点上的几个executor 计算进程就会同时开始工作。一般一个core就对应一个executor。</p>
<p>每个executor会独立的去MongoDB取来原始数据，直接套用Spark提供的分析算法或者使用自定义流程来处理数据，计算完后把相应结果写回到MongoDB。</p>
<p>我们需要提到的是：在这里，所有和MongoDB的交互都是通过一个叫做Mongo-Spark的连接器来完成的。</p>
<p><img src="https://yerias.github.io/mongodb/mongo-4.png" alt=""></p>
<p>另一种常见的架构是结合MongoDB和HDFS的。Hadoop在非结构化数据处理的场景下要比MongoDB的普及率高。所以我们可以看到不少用户会已经将数据存放在HDFS上。这个时候你可以直接在HDFS上面架Spark来跑，Spark从HDFS取来原始数据进行计算，而MongoDB在这个场景下是用来保存处理结果。为什么要这么麻烦？几个原因：</p>
<ul>
<li>Spark处理结果数量可能会很大，比如说，个性化推荐可能会产生数百万至数千万条记录，需要一个能够支持每秒万级写入能力的数据库</li>
<li>处理结果可以直接用来驱动前台APP，如用户打开页面时获取后台已经为他准备好的推荐列表。</li>
</ul>
<h3 id="Mongo-Spark-Connector-连接器"><a href="#Mongo-Spark-Connector-连接器" class="headerlink" title="Mongo Spark Connector 连接器"></a>Mongo Spark Connector 连接器</h3><p>在这里我们在介绍下MongoDB官方提供的 <a href="https://github.com/mongodb/mongo-spark" target="_blank" rel="noopener">Mongo Spark连接器</a> 。目前有3个连接器可用，包括社区第三方开发的和之前Mongo Hadoop连接器等，这个Mongo-Spark是最新的，也是我们推荐的连接方案。</p>
<p><img src="https://yerias.github.io/mongodb/mongo-5.png" alt=""></p>
<p>这个连接器是专门为Spark打造的，支持双向数据，读出和写入。但是最关键的是 条件下推 ，也就是说：如果你在Spark端指定了查询或者限制条件的情况下，这个条件会被下推到MongoDB去执行，这样可以保证从MongoDB取出来、经过网络传输到Spark计算节点的数据确实都是用得着的。没有下推支持的话，每次操作很可能需要从MongoDB读取全量的数据，性能体验将会很糟糕。拿刚才的日志例子来说，如果我们只想对404错误日志进行分析，看那些错误都是哪些页面，以及每天错误页面数量的变化，如果有条件下推，那么我们可以给MongoDB一个限定条件：错误代码=404， 这个条件会在MongoDB服务器端执行，这样我们只需要通过网络传输可能只是全部日志的0.1%的数据，而不是没有条件下推情况下的全部数据。</p>
<p>另外，这个最新的连接器还支持和Spark计算节点Co-Lo 部署。就是说在同一个节点上同时部署Spark实例和MongoDB实例。这样做可以减少数据在网络上的传输带来的资源消耗及时延。当然，这种部署方式需要注意内存资源和CPU资源的隔离。隔离的方式可以通过<a href="http://lib.csdn.net/base/linux" target="_blank" rel="noopener">Linux</a>的cgroups。</p>
<h2 id="Spark-MongoDB-成功案例"><a href="#Spark-MongoDB-成功案例" class="headerlink" title="Spark + MongoDB 成功案例"></a>Spark + MongoDB 成功案例</h2><p>目前已经有很多案例在不同的应用场景中使用Spark+MongoDB。</p>
<p>法国航空是法国最大的航空公司，为了提高客户体验，在最近施行的360度客户视图中，使用Spark对已经收集在MongoDB里面的客户数据进行分类及行为分析，并把结果（如客户的类别、标签等信息）写回到MongoDB内每一个客户的文档结构里。</p>
<p>Stratio是美国硅谷一家著名的金融大数据公司。他们最近在一家在31个国家有分支机构的跨国银行实施了一个实时监控平台。该银行希望通过对日志的监控和分析来保证客户服务的响应时间以及实时监测一些可能的违规或者金融欺诈行为。在这个应用内， 他们使用了：</p>
<p>Stratio是美国硅谷一家著名的金融大数据公司。他们最近在一家在31个国家有分支机构的跨国银行实施了一个实时监控平台。该银行希望通过对日志的监控和分析来保证客户服务的响应时间以及实时监测一些可能的违规或者金融欺诈行为。在这个应用内， 他们使用了：</p>
<ul>
<li>Apache Flume 来收集log</li>
<li>Spark来处理实时的log</li>
<li>MongoDB来存储收集的log以及Spark分析的结果，如Key Performance Indicators等</li>
</ul>
<p>东方航空最近刚完成一个Spark运价的POC<a href="http://lib.csdn.net/base/softwaretest" target="_blank" rel="noopener">测试</a>。</p>
<h3 id="东方航空的挑战"><a href="#东方航空的挑战" class="headerlink" title="东方航空的挑战"></a>东方航空的挑战</h3><p>东方航空作为国内的3大行之一，每天有1000多个航班，服务26万多乘客。过去，顾客在网站上订购机票，平均资料库查询200次就会下单订购机票，但是现在平均要查询1.2万次才会发生一次订购行为，同样的订单量，查询量却成长百倍。按照50%直销率这个目标计算，东航的运价系统要支持每天16亿的运价请求。</p>
<h3 id="思路：空间换时间"><a href="#思路：空间换时间" class="headerlink" title="思路：空间换时间"></a>思路：空间换时间</h3><p>当前的运价是通过实时计算的，按照现在的计算能力，需要对已有系统进行100多倍的扩容。另一个常用的思路，就是采用空间换时间的方式。与其对每一次的运价请求进行耗时300ms的运算，不如事先把所有可能的票价查询组合穷举出来并进行批量计算，然后把结果存入MongoDB里面。当需要查询运价时，直接按照 出发+目的地+日期的方式做一个快速的DB查询，响应时间应该可以做到几十毫秒。</p>
<p>那为什么要用MongoDB？因为我们要处理的数据量庞大无比。按照1000多个航班，365天，26个仓位，100多渠道以及数个不同的航程类型，我们要实时存取的运价记录有数十亿条之多。这个已经远远超出常规RDBMS可以承受的范围。</p>
<p>MongoDB基于内存缓存的数据管理方式决定了对并发读写的响应可以做到很低延迟，水平扩展的方式可以通过多台节点同时并发处理海量请求。</p>
<p>事实上，全球最大的航空分销商，管理者全世界95%航空库存的Amadeus也正是使用MongoDB作为其1000多亿运价缓存的存储方案。</p>
<h3 id="Spark-MongoDB-方案"><a href="#Spark-MongoDB-方案" class="headerlink" title="Spark + MongoDB 方案"></a>Spark + MongoDB 方案</h3><p>我们知道MongoDB可以用来做我们海量运价数据的存储方案，在大规模并行计算方案上，就可以用到崭新的Spark技术。</p>
<p><img src="https://yerias.github.io/mongodb/mongo-6.png" alt=""></p>
<p>这里是一个运价系统的架构图。 左边是发起航班查询请求的客户端，首先会有API服务器进行预处理。一般航班请求会分为库存查询和运价查询。库存查询会直接到东航已有的库存系统（Seat Inventory），同样是实现在MongoDB上面的。在确定库存后根据库存结果再从Fare Cache系统内查询相应的运价。</p>
<p>Spark集群则是另外一套计算集群，通过Spark MongoDB连接套件和MongoDB Fare Cache集群连接。Spark 计算任务会定期触发（如每天一次或者每4小时一次），这个任务会对所有的可能的运价组合进行全量计算，然后存入MongoDB，以供查询使用。右半边则把原来实时运算的集群换成了Spark+MongoDB。Spark负责批量计算一年内所有航班所有仓位的所有价格，并以高并发的形式存储到MongoDB里面。每秒钟处理的运价可以达到数万条。</p>
<p>当来自客户端的运价查询达到服务端以后，服务端直接就向MongoDB发出按照日期，出发到达机场为条件的mongo查询。</p>
<h3 id="批处理计算流程"><a href="#批处理计算流程" class="headerlink" title="批处理计算流程"></a>批处理计算流程</h3><p><img src="https://yerias.github.io/mongodb/mongo-7.png" alt=""></p>
<p>这里是Spark计算任务的流程图。需要计算的任务，也就是所有日期航班仓位的组合，事先已经存放到MongoDB里面。</p>
<p>任务递交到master，然后预先加载所需参考数据，broadcast就是把这些在内存里的数据复制到每一个Spark计算节点的JVM，然后所有计算节点多线程并发执行，从Mongodb里取出需要计算的仓位，调用东航自己的运价逻辑，得出结果以后，并保存回MongoDB。</p>
<h3 id="Spark-任务入口程序"><a href="#Spark-任务入口程序" class="headerlink" title="Spark 任务入口程序"></a>Spark 任务入口程序</h3><p>Spark和MongoDB的连接使用非常简单，下面就是一个代码示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">// initialization dependencies including base prices, pricing rules and some reference data</span><br><span class="line">Map dependencies = MyDependencyManager.loadDependencies();</span><br><span class="line">// broadcasting dependencies</span><br><span class="line">javaSparkContext.broadcast(dependencies);</span><br><span class="line"></span><br><span class="line">// create job rdd</span><br><span class="line">cabinsRDD = MongoSpark.load(javaSparkContext).withPipeline(pipeline)</span><br><span class="line"></span><br><span class="line">// for each cabin, date, airport pair, calculate the price</span><br><span class="line">cabinsRDD.map(function calc_price);</span><br><span class="line"></span><br><span class="line">// collect the result, which will cause the data to be stored into MongoDB</span><br><span class="line">cabinsRDD.collect()</span><br><span class="line">cabinsRDD.saveToMongo()</span><br></pre></td></tr></table></figure>

<h3 id="处理能力和响应时间比较"><a href="#处理能力和响应时间比较" class="headerlink" title="处理能力和响应时间比较"></a>处理能力和响应时间比较</h3><p>这里是一个在东航POC的简单测试结果。从吞吐量的角度，新的API服务器单节点就可以处理3400个并发的运价请求。在显著提高了并发的同时，响应延迟则降低了10几倍，平均10ms就可以返回运价结果。按照这个性能，6台 API服务器就可以应付将来每天16亿的运价查询。</p>
<p><img src="https://yerias.github.io/mongodb/mongo-8.png" alt=""></p>
<h2 id="Spark-＋-MongoDB-演示"><a href="#Spark-＋-MongoDB-演示" class="headerlink" title="Spark ＋ MongoDB 演示"></a>Spark ＋ MongoDB 演示</h2><p>接下来是一个简单的Spark+MongoDB演示。</p>
<h3 id="安装-Spark"><a href="#安装-Spark" class="headerlink" title="安装 Spark"></a>安装 Spark</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># curl -OL http://d3kbcqa49mib13.cloudfront.net/spark-1.6.0-bin-hadoop2.6.tgz</span><br><span class="line"># mkdir -p ~/spark</span><br><span class="line"># tar -xvf spark-1.6.0-bin-hadoop2.6.tgz -C ~/spark --strip-components=1</span><br></pre></td></tr></table></figure>

<h3 id="测试连接器"><a href="#测试连接器" class="headerlink" title="测试连接器"></a>测试连接器</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cd ～／spark</span><br><span class="line"># ./bin/spark-shell \</span><br><span class="line">--conf &quot;spark.mongodb.input.uri=mongodb://127.0.0.1/flights.av&quot; \</span><br><span class="line">--conf &quot;spark.mongodb.output.uri=mongodb://127.0.0.1/flights.output&quot; \</span><br><span class="line">--packages org.mongodb.spark:mongo-spark-connector_2.10:1.0.0</span><br><span class="line"></span><br><span class="line">import com.mongodb.spark._</span><br><span class="line">import org.bson.Document</span><br><span class="line"></span><br><span class="line">MongoSpark.load(sc).take(10).foreach(println)</span><br></pre></td></tr></table></figure>

<h3 id="简单分组统计"><a href="#简单分组统计" class="headerlink" title="简单分组统计"></a>简单分组统计</h3><p>数据： 365天，所有航班库存信息，500万文档</p>
<p>任务： 按航班统计一年内所有余票量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MongoSpark.load(sc)</span><br><span class="line">     .map(doc=&gt;(doc.getString(&quot;flight&quot;) ,doc.getLong(&quot;seats&quot;)))</span><br><span class="line">     .reduceByKey((x,y)=&gt;(x+y))</span><br><span class="line">      .take(10)</span><br><span class="line">     .foreach(println)</span><br></pre></td></tr></table></figure>

<h3 id="简单分组统计加条件过滤"><a href="#简单分组统计加条件过滤" class="headerlink" title="简单分组统计加条件过滤"></a>简单分组统计加条件过滤</h3><p>数据： 365天，所有航班库存信息，500万文档</p>
<p>任务： 按航班统计一年内所有库存，但是只处理昆明出发的航班</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import org.bson.Document</span><br><span class="line"></span><br><span class="line">MongoSpark.load(sc)</span><br><span class="line">          .withPipeline(Seq(Document.parse(&quot;&#123; $match: &#123; orig :  &apos;KMG&apos;  &#125; &#125;&quot;)))</span><br><span class="line">    .map(doc=&gt;(doc.getString(&quot;flight&quot;) ,doc.getLong(&quot;seats&quot;)))</span><br><span class="line">    .reduceByKey((x,y)=&gt;(x+y))</span><br><span class="line">    .take(10)</span><br><span class="line">    .foreach(println)</span><br></pre></td></tr></table></figure>

<h3 id="性能优化事项"><a href="#性能优化事项" class="headerlink" title="性能优化事项"></a>性能优化事项</h3><ul>
<li>使用合适的chunksize (MB)<br>Total data size / chunksize = chunks = RDD partitions = spark tasks</li>
<li>不要将所有CPU核分配给Spark<br>预留1-2个core给操作系统及其他管理进程</li>
<li>同机部署<br>适当情况可以同机部署Spark+MongoDB，利用本地IO提高性能</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>上面只是一些简单的演示，实际上Spark + MongoDB的使用可以通过Spark的很多种形式来使用。我们来总结一下Spark ＋ Mongo的应用场景。在座的同学可能很多人已经使用了MongoDB，也有些人已经使用了Hadoop。我们可以从两个角度来考虑这个事情：</p>
<ul>
<li>对那些已经使用MongoDB的用户，如果你希望在你的MongoDB驱动的应用上提供个性化功能，比如说像Yahoo一样为你找感兴趣的新闻，能够在你的MongoDB数据上利用到Spark强大的机器学习或者流处理，你就可以考虑在MongoDB集群上部署Spark来实现这些功能。</li>
<li>如果你已经使用Hadoop而且数据已经在HDFS里面，你可以考虑使用Spark来实现更加实时更加快速的分析型需求，并且如果你的分析结果有数据量大、格式多变以及这些结果数据要及时提供给前台APP使用的需求，那么MongoDB可以用来作为你分析结果的一个存储方案。</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Tunan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yerias.github.io/2020/07/20/mongodb/1/">http://yerias.github.io/2020/07/20/mongodb/1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/MongoDB/">MongoDB</a></div><div class="social-share" data-disabled="facebook,twitter,douban,linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/07/20/mongodb/2/"><i class="fa fa-chevron-left">  </i><span>Spark连接MongoDB使用教程(转载)</span></a></div><div class="next-post pull-right"><a href="/2020/07/19/error/9/"><span>Spark程序夯住，日志信息打印极少，原因是没有够分配的系统资源</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2020 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="footer_custom_text">大家好，我是图南，很高兴认识你们！</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>