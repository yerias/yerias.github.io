<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="浅谈Spark RDD"><meta name="keywords" content="Spark"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>浅谈Spark RDD | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/xiaoqi.jpg"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><div class="follow-button"><a href="#">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">210</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">38</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">34</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">HADOOP</a><a class="author-info-links__name text-center" href="https://cwiki.apache.org/confluence/display/Hive/" target="_blank" rel="noopener">HIVE</a><a class="author-info-links__name text-center" href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">CDH</a><a class="author-info-links__name text-center" href="http://flume.apache.org/" target="_blank" rel="noopener">FLUME</a><a class="author-info-links__name text-center" href="https://azkaban.github.io/" target="_blank" rel="noopener">AZKABAN</a><a class="author-info-links__name text-center" href="https://www.maxinhong.com/" target="_blank" rel="noopener">叶梨子</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/qingyunzong" target="_blank" rel="noopener">老铁</a><a class="author-info-links__name text-center" href="https://blog.csdn.net/jim8973/" target="_blank" rel="noopener">飞哥</a><a class="author-info-links__name text-center" href="https://vinxikk.github.io/" target="_blank" rel="noopener">vinx</a><a class="author-info-links__name text-center" href="http://dongxicheng.org/" target="_blank" rel="noopener">懂西成(Hadoop技术内幕作者)</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/xing901022/" target="_blank" rel="noopener">xingoo</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/itboys/tag/" target="_blank" rel="noopener">大葱拌豆腐</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/shishanyuan" target="_blank" rel="noopener">郭景瞻(图解Spark作者)</a><a class="author-info-links__name text-center" href="https://segmentfault.com/u/wishdaren_5c243b920a3eb" target="_blank" rel="noopener">Wish大人</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/huxi2b/" target="_blank" rel="noopener">huxi_2b(kafka)</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">浅谈Spark RDD</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-11-10</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Spark/">Spark</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1.8k</span><span class="post-meta__separator">|</span><span>Reading time: 6 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>记得去年的一次面试，面试官让我谈谈对RDD的理解，RDD这东西叫做弹性分布式数据集，是Spark的核心概念</p>
<p>它是分布式的，可以分布在多台机器上，进行计算</p>
<p>它是弹性的，计算过程中内存不够时它会和磁盘进行数据交换</p>
<p>它表示已被分区，不可变的并能够被并行操作的数据集合</p>
<p>这些特性，想必大家都能说出来，我也是这么回答的，觉得问题很简单，可面试官回复我：仅看过几篇文章的人，也能有和你同样的回答，你对于RDD有独到的理解吗？</p>
<p>当时理解的太浅，回去后，就补习了一番</p>
<hr>
<p>今天就来谈一谈对RDD的理解</p>
<p>先看一下官方在源码中对RDD的注释：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">* Internally, each RDD is characterized by five main properties:</span><br><span class="line"> *   </span><br><span class="line"> *  - A list of partitions  一个分区的列表</span><br><span class="line"> *  - A function <span class="keyword">for</span> computing each split  一个计算每个分区的函数，即RDD的分片函数</span><br><span class="line"> *  - A list of dependencies on other RDDs    RDD之间的依赖关系 </span><br><span class="line"> *  - Optionally, a Partitioner <span class="keyword">for</span> key-<span class="function">value <span class="title">RDDs</span> <span class="params">(e.g. to say that the RDD is hash-partitioned)</span>    可选：一个Partitioner</span></span><br><span class="line"><span class="function"> *  - Optionally, a list of preferred locations to compute each split <span class="title">on</span> <span class="params">(e.g. block locations <span class="keyword">for</span></span></span></span><br><span class="line"><span class="function"><span class="params"> *    an HDFS file)</span>    可选：一个列表，存储存取每个Partition的优先位置</span></span><br><span class="line"><span class="function"> *</span></span><br><span class="line"><span class="function"> * All of the scheduling and execution in Spark is done based on these methods, allowing each RDD</span></span><br><span class="line"><span class="function"> * to implement its own way of computing itself. Indeed, users can implement custom <span class="title">RDDs</span> <span class="params">(e.g. <span class="keyword">for</span></span></span></span><br><span class="line"><span class="function"><span class="params"> * reading data from a new storage system)</span> by overriding these functions. Please refer to the</span></span><br><span class="line"><span class="function"> * [[http:<span class="comment">//www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf Spark paper]] for more details</span></span></span><br><span class="line"><span class="function"> * on RDD internals.</span></span><br><span class="line"><span class="function"> */</span></span><br></pre></td></tr></table></figure>



<p>通过官方的注释，我们可以了解到，每个RDD都有五个重要的属性</p>
<ol>
<li>一个分区的列表。我们可以理解为是一组分片，分片就是数据集的基本组成单位，对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户创建RDD时，可以指定RDD的分片个数，若没有指定，则采用默认（默认值：程序所分配的CPU Cores数目），下图描述了分区存储的计算模型，每个分配的存储是有BlockManager实现的。每个分区都会被逻辑映射成BlockManager的一个block，而这个block会被一个Task负责计算。</li>
</ol>
<p><img src="https://yerias.github.io/spark_img/20170919074522_68778.png" alt=""></p>
<ol start="2">
<li><p>一个计算每个分区的函数。即RDD的计算分片compute函数，Spark中的RDD计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复核，不需要保存每次计算的结果。</p>
</li>
<li><p>RDD之间的依赖关系。这里要说一下RDD的依赖关系</p>
<p>RDD和它的依赖的parent RDD(s)的关系有两种不同的类型，即宽依赖和窄依赖</p>
<p> ● 窄依赖指的是每一个parent RDD的partition最多被子RDD的一个partition使用</p>
<p> ● 宽依赖是指多个子RDD的partition会依赖同一个parent RDD的partition</p>
<p>  接下来，从不同类型的转换来进一步理解RDD的窄依赖和宽依赖的区别，如下图所示。</p>
<p><img src="https://yerias.github.io/spark_img/20170920031301_87301.png" alt=""></p>
<p>对于map和filter形式的转换来说，它们只是将Partition的数据根据转换的规则进行转化，并不涉及其他的处理，可以简单地认为只是将数据从一个形式转换到另一个形式。</p>
<p>对于union，只是将多个RDD合并成一个，parent RDD的Partition(s)不会有任何的变化，可以认为只是把parent RDD的Partition(s)简单进行复制与合并。</p>
<p>对于join，如果每个Partition仅仅和已知的、特定的Partition进行join，那么这个依赖关系也是窄依赖。对于这种有规则的数据的join，并不会引入昂贵的Shuffle。</p>
<p>对于窄依赖，由于RDD每个Partition依赖固定数量的parent RDD(s)的Partition(s)，因此可以通过一个计算任务来处理这些Partition，并且这些Partition相互独立，这些计算任务也就可以并行执行了。</p>
<p>对于groupByKey，子RDD的所有Partition(s)会依赖于parent RDD的所有Partition(s)，子RDD的Partition是parent RDD的所有Partition Shuffle的结果，因此这两个RDD是不能通过一个计算任务来完成的。同样，对于需要parent RDD的所有Partition进行join的转换，也是需要Shuffle，这类join的依赖就是宽依赖而不是前面提到的窄依赖了。</p>
<p>不同的操作依据其特性，可能会产生不同的依赖。例如map、filter操作会产生 narrow dependency 。reduceBykey操作会产生 wide / shuffle dependency。</p>
<p>通俗点来说，RDD的每个Partition，仅仅依赖于父RDD中的一个Partition，这才是窄。 就这么简单！</p>
<p>反正，子Rdd的partition和父Rdd的Partition如果是一对一就是窄依赖，这样理解就好区分了 　　　　　　　　　 </p>
<p>捋一下这里的源码：</p>
<p>所有的依赖都要实现</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">trait Dependency[T]：</span><br><span class="line">abstract class Dependency[T] extends Serializable &#123;</span><br><span class="line">    def rdd: RDD[T]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中rdd就是依赖的parent RDD。</p>
<p>对于窄依赖的实现（有两种）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">abstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] &#123;</span><br><span class="line">    <span class="comment">//返回子RDD的partitionId依赖的所有的parent RDD的Partition(s)</span></span><br><span class="line">    <span class="function">def <span class="title">getParents</span><span class="params">(partitionId: Int)</span>: Seq[Int]</span></span><br><span class="line"><span class="function">    override def rdd: RDD[T] </span>= _rdd</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>窄依赖是有两种具体实现，分别如下：</p>
<p>一种是一对一的依赖，即OneToOneDependency：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) &#123;</span><br><span class="line">    <span class="function">override def <span class="title">getParents</span><span class="params">(partitionId: Int)</span> </span>= List(partitionId)</span><br><span class="line">    通过getParents的实现不难看出，RDD仅仅依赖于parent RDD相同ID的Partition。</span><br></pre></td></tr></table></figure>

<p>还有一个是范围的依赖，即RangeDependency，它仅仅被org.apache.spark.rdd.UnionRDD使用。UnionRDD是把多个RDD合成一个RDD，这些RDD是被拼接而成，即每个parent RDD的Partition的相对顺序不会变，只不过每个parent RDD在UnionRDD中的Partition的起始位置不同。因此它的getPartents如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">override def <span class="title">getParents</span><span class="params">(partitionId: Int)</span> </span>= &#123;</span><br><span class="line">    <span class="keyword">if</span>(partitionId &gt;= outStart &amp;&amp; partitionId &lt; outStart + length) &#123;</span><br><span class="line">       List(partitionId - outStart + inStart)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       Nil</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中，inStart是parent RDD中Partition的起始位置，outStart是在UnionRDD中的起始位置，length就是parent RDD中Partition的数量。</p>
<p>对于宽依赖的实现（只有一种）</p>
<p>宽依赖的实现只有一种：ShuffleDependency。子RDD依赖于parent RDD的所有Partition，因此需要Shuffle过程：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">class ShuffleDependency[K, V, C](</span><br><span class="line">    <span class="meta">@transient</span> _rdd: RDD[_ &lt;: Product2[K, V]],</span><br><span class="line">    val partitioner: Partitioner,</span><br><span class="line">    val serializer: Option[Serializer] = None,</span><br><span class="line">    val keyOrdering: Option[Ordering[K]] = None,</span><br><span class="line">    val aggregator: Option[Aggregator[K, V, C]] = None,</span><br><span class="line">    val mapSideCombine: Boolean = <span class="keyword">false</span>)</span><br><span class="line">extends Dependency[Product2[K, V]] &#123;</span><br><span class="line"> </span><br><span class="line">override def rdd = _rdd.asInstanceOf[RDD[Product2[K, V]]]</span><br><span class="line"><span class="comment">//获取新的shuffleId</span></span><br><span class="line">val shuffleId: Int = _rdd.context.newShuffleId()</span><br><span class="line"><span class="comment">//向ShuffleManager注册Shuffle的信息</span></span><br><span class="line">val shuffleHandle: ShuffleHandle =</span><br><span class="line">_rdd.context.env.shuffleManager.registerShuffle(</span><br><span class="line">    shuffleId, _rdd.partitions.size, <span class="keyword">this</span>)</span><br><span class="line">    _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(<span class="keyword">this</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意：宽依赖支持两种Shuffle Manager。</p>
<p>即org.apache.spark.shuffle.hash.HashShuffleManager（基于Hash的Shuffle机制）和org.apache.spark.shuffle.sort.SortShuffleManager（基于排序的Shuffle机制）。</p>
</li>
<li><p>可选：KV RDD可重分区。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于key-value的RDD才会有Partitioner，非key-value的RDD的Partitioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</p>
</li>
<li><p>可选：移动计算，存储存取每个Partition的优先位置。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在块的位置。</p>
</li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Tunan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yerias.github.io/2020/11/10/spark/38/">http://yerias.github.io/2020/11/10/spark/38/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="social-share" data-disabled="facebook,twitter,douban,linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/11/16/spark/39/"><i class="fa fa-chevron-left">  </i><span>Spark Streaming 双流Join</span></a></div><div class="next-post pull-right"><a href="/2020/11/06/spark/37/"><span>Spark核心概念RDD</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2021 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="footer_custom_text">大家好，我是图南，很高兴认识你们！</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>