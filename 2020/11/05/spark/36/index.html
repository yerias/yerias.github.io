<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="SparkSQL 优化jdbc外部数据源的读写"><meta name="keywords" content="Spark"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>SparkSQL 优化jdbc外部数据源的读写 | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#目录"><span class="toc-number">1.</span> <span class="toc-text">目录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#jdbc-参数解读"><span class="toc-number">2.</span> <span class="toc-text">jdbc 参数解读</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#源码"><span class="toc-number">3.</span> <span class="toc-text">源码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#jdbc-读取并发度优化"><span class="toc-number">4.</span> <span class="toc-text">jdbc 读取并发度优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#数据读取分区的原理"><span class="toc-number">4.1.</span> <span class="toc-text">数据读取分区的原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#jdbc-写并发度优化"><span class="toc-number">5.</span> <span class="toc-text">jdbc 写并发度优化</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/xiaoqi.jpg"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><div class="follow-button"><a href="#">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">264</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">39</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">35</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">HADOOP</a><a class="author-info-links__name text-center" href="https://cwiki.apache.org/confluence/display/Hive/" target="_blank" rel="noopener">HIVE</a><a class="author-info-links__name text-center" href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">CDH</a><a class="author-info-links__name text-center" href="http://flume.apache.org/" target="_blank" rel="noopener">FLUME</a><a class="author-info-links__name text-center" href="https://azkaban.github.io/" target="_blank" rel="noopener">AZKABAN</a><a class="author-info-links__name text-center" href="https://www.maxinhong.com/" target="_blank" rel="noopener">叶梨子</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/qingyunzong" target="_blank" rel="noopener">老铁</a><a class="author-info-links__name text-center" href="https://blog.csdn.net/jim8973/" target="_blank" rel="noopener">飞哥</a><a class="author-info-links__name text-center" href="https://vinxikk.github.io/" target="_blank" rel="noopener">vinx</a><a class="author-info-links__name text-center" href="http://dongxicheng.org/" target="_blank" rel="noopener">懂西成(Hadoop技术内幕作者)</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/xing901022/" target="_blank" rel="noopener">xingoo</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/itboys/tag/" target="_blank" rel="noopener">大葱拌豆腐</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/shishanyuan" target="_blank" rel="noopener">郭景瞻(图解Spark作者)</a><a class="author-info-links__name text-center" href="https://segmentfault.com/u/wishdaren_5c243b920a3eb" target="_blank" rel="noopener">Wish大人</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/huxi2b/" target="_blank" rel="noopener">huxi_2b(kafka)</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">SparkSQL 优化jdbc外部数据源的读写</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-11-05</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Spark/">Spark</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">2.8k</span><span class="post-meta__separator">|</span><span>Reading time: 11 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>jdbc 参数解读</li>
<li>源码</li>
<li>jdbc 读并发度优化</li>
<li>jdbc 写并发度优化</li>
</ol>
<h2 id="jdbc-参数解读"><a href="#jdbc-参数解读" class="headerlink" title="jdbc 参数解读"></a>jdbc 参数解读</h2><p>Spark SQL还包括一个可以使用JDBC从其他数据库读取数据的数据源。与使用JdbcRDD相比，应优先使用此功能。这是因为结果作为DataFrame返回，它们可以在Spark SQL中轻松处理或与其他数据源连接。JDBC数据源也更易于使用Java或Python，因为它不需要用户提供ClassTag。</p>
<p>可以使用Data Sources API将远程数据库中的表加载为DataFrame或Spark SQL临时视图。用户可以在数据源选项中指定JDBC连接属性。user和password通常作为用于登录数据源的连接属性。除连接属性外，Spark还支持以下不区分大小写的选项：</p>
<table>
<thead>
<tr>
<th align="left">属性名称</th>
<th align="left">解释</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>url</code></td>
<td align="left">要连接的JDBC URL</td>
</tr>
<tr>
<td align="left"><code>dbtable</code></td>
<td align="left">读取或写入的JDBC表</td>
</tr>
<tr>
<td align="left"><code>query</code></td>
<td align="left">指定查询语句</td>
</tr>
<tr>
<td align="left"><code>driver</code></td>
<td align="left">用于连接到该URL的JDBC驱动类名</td>
</tr>
<tr>
<td align="left"><code>partitionColumn, lowerBound, upperBound</code></td>
<td align="left">如果指定了这些选项，则必须全部指定。另外， <code>numPartitions</code>必须指定</td>
</tr>
<tr>
<td align="left"><code>numPartitions</code></td>
<td align="left">表读写中可用于并行处理的最大分区数。这也确定了并发JDBC连接的最大数量。如果要写入的分区数超过此限制，我们可以通过<code>coalesce(numPartitions)</code>在写入之前进行调用将其降低到此限制</td>
</tr>
<tr>
<td align="left"><code>queryTimeout</code></td>
<td align="left">默认为<code>0</code>，查询超时时间</td>
</tr>
<tr>
<td align="left"><code>fetchsize</code></td>
<td align="left">JDBC的获取大小，它确定每次要获取多少行。这可以帮助提高JDBC驱动程序的性能</td>
</tr>
<tr>
<td align="left"><code>batchsize</code></td>
<td align="left">默认为1000，JDBC批处理大小，这可以帮助提高JDBC驱动程序的性能。</td>
</tr>
<tr>
<td align="left"><code>isolationLevel</code></td>
<td align="left">事务隔离级别，适用于当前连接。它可以是一个<code>NONE</code>，<code>READ_COMMITTED</code>，<code>READ_UNCOMMITTED</code>，<code>REPEATABLE_READ</code>，或<code>SERIALIZABLE</code>，对应于由JDBC的连接对象定义，缺省值为标准事务隔离级别<code>READ_UNCOMMITTED</code>。此选项仅适用于写作。</td>
</tr>
<tr>
<td align="left"><code>sessionInitStatement</code></td>
<td align="left">在向远程数据库打开每个数据库会话之后，在开始读取数据之前，此选项将执行自定义SQL语句，使用它来实现会话初始化代码。</td>
</tr>
<tr>
<td align="left"><code>truncate</code></td>
<td align="left"><strong>这是与JDBC writer相关的选项。当<code>SaveMode.Overwrite</code>启用时，就会清空目标表的内容，而不是删除和重建其现有的表。默认为<code>false</code></strong></td>
</tr>
<tr>
<td align="left"><code>pushDownPredicate</code></td>
<td align="left">用于启用或禁用谓词下推到JDBC数据源的选项。默认值为true，在这种情况下，Spark将尽可能将过滤器下推到JDBC数据源。</td>
</tr>
</tbody></table>
<p>函数示例：</p>
<figure class="highlight kotlin"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> jdbcDF = sparkSession.sqlContext.read.format(<span class="string">"jdbc"</span>)</span><br><span class="line">  .option(<span class="string">"url"</span>, url)</span><br><span class="line">  .option(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">  .option(<span class="string">"dbtable"</span>, <span class="string">"table"</span>)</span><br><span class="line">  .option(<span class="string">"user"</span>, <span class="string">"user"</span>)</span><br><span class="line">  .option(<span class="string">"partitionColumn"</span>, <span class="string">"id"</span>)</span><br><span class="line">  .option(<span class="string">"lowerBound"</span>, <span class="number">1</span>)</span><br><span class="line">  .option(<span class="string">"upperBound"</span>, <span class="number">10000</span>)</span><br><span class="line">  .option(<span class="string">"fetchsize"</span>, <span class="number">100</span>)   <span class="comment">//经测试，fetchsize和batchsize的大小对读写性能并没有变化</span></span><br><span class="line">  .option(<span class="string">"xxx"</span>, <span class="string">"xxx"</span>)</span><br><span class="line">  .load()</span><br></pre></td></tr></table></figure>

<p>从函数可以看出，option模式其实是一种开放接口，spark会根据具体的参数，做出相应的行为。</p>
<h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h2><ul>
<li><strong>SparkSession</strong></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Returns a [[DataFrameReader]] that can be used to read non-streaming data in as a</span></span><br><span class="line"><span class="comment">   * `DataFrame`.</span></span><br><span class="line"><span class="comment">   * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">   *   sparkSession.read.parquet("/path/to/file.parquet")</span></span><br><span class="line"><span class="comment">   *   sparkSession.read.schema(schema).json("/path/to/file.json")</span></span><br><span class="line"><span class="comment">   * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@since</span> 2.0.0</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  def read: DataFrameReader = <span class="keyword">new</span> DataFrameReader(self)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>DataFrameReader</strong></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="comment">// ...省略代码...</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   *所有的数据由RDD的一个分区处理，如果你这个表很大，很可能会出现OOM</span></span><br><span class="line"><span class="comment">   *可以使用DataFrameDF.rdd.partitions.size方法查看</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function">def <span class="title">jdbc</span><span class="params">(url: String, table: String, properties: Properties)</span>: DataFrame </span>= &#123;</span><br><span class="line">    assertNoSpecifiedSchema(<span class="string">"jdbc"</span>)</span><br><span class="line">    <span class="keyword">this</span>.extraOptions ++= properties.asScala</span><br><span class="line">    <span class="keyword">this</span>.extraOptions += (JDBCOptions.JDBC_URL -&gt; url, JDBCOptions.JDBC_TABLE_NAME -&gt; table)</span><br><span class="line">    format(<span class="string">"jdbc"</span>).load()</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> url 数据库url</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> table 表名</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> columnName 分区字段名</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> lowerBound  `columnName`的最小值,用于分区步长</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> upperBound  `columnName`的最大值,用于分区步长.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> numPartitions 分区数量 </span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> connectionProperties 其他参数</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@since</span> 1.4.0</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function">def <span class="title">jdbc</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">      url: String,</span></span></span><br><span class="line"><span class="function"><span class="params">      table: String,</span></span></span><br><span class="line"><span class="function"><span class="params">      columnName: String,</span></span></span><br><span class="line"><span class="function"><span class="params">      lowerBound: Long,</span></span></span><br><span class="line"><span class="function"><span class="params">      upperBound: Long,</span></span></span><br><span class="line"><span class="function"><span class="params">      numPartitions: Int,</span></span></span><br><span class="line"><span class="function"><span class="params">      connectionProperties: Properties)</span>: DataFrame </span>= &#123;</span><br><span class="line">    <span class="keyword">this</span>.extraOptions ++= Map(</span><br><span class="line">      JDBCOptions.JDBC_PARTITION_COLUMN -&gt; columnName,</span><br><span class="line">      JDBCOptions.JDBC_LOWER_BOUND -&gt; lowerBound.toString,</span><br><span class="line">      JDBCOptions.JDBC_UPPER_BOUND -&gt; upperBound.toString,</span><br><span class="line">      JDBCOptions.JDBC_NUM_PARTITIONS -&gt; numPartitions.toString)</span><br><span class="line">    jdbc(url, table, connectionProperties)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> predicates 每个分区的where条件</span></span><br><span class="line"><span class="comment">   * 比如："id &lt;= 1000", "score &gt; 1000 and score &lt;= 2000"</span></span><br><span class="line"><span class="comment">   * 将会分成两个分区</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@since</span> 1.4.0</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function">def <span class="title">jdbc</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">      url: String,</span></span></span><br><span class="line"><span class="function"><span class="params">      table: String,</span></span></span><br><span class="line"><span class="function"><span class="params">      predicates: Array[String],</span></span></span><br><span class="line"><span class="function"><span class="params">      connectionProperties: Properties)</span>: DataFrame </span>= &#123;</span><br><span class="line">    assertNoSpecifiedSchema(<span class="string">"jdbc"</span>)</span><br><span class="line">    val params = extraOptions.toMap ++ connectionProperties.asScala.toMap</span><br><span class="line">    val options = <span class="keyword">new</span> JDBCOptions(url, table, params)</span><br><span class="line">    val parts: Array[Partition] = predicates.zipWithIndex.map &#123; <span class="keyword">case</span> (part, i) =&gt;</span><br><span class="line">      JDBCPartition(part, i) : Partition</span><br><span class="line">    &#125;</span><br><span class="line">    val relation = JDBCRelation(parts, options)(sparkSession)</span><br><span class="line">    sparkSession.baseRelationToDataFrame(relation)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<h2 id="jdbc-读取并发度优化"><a href="#jdbc-读取并发度优化" class="headerlink" title="jdbc 读取并发度优化"></a>jdbc 读取并发度优化</h2><p>很多人在spark中使用默认提供的jdbc方法时，在数据库数据较大时经常发现任务 hang 住，其实是单线程任务过重导致，这时候需要提高读取的并发度。 </p>
<ol>
<li><p>单partition(无并发)</p>
<p>调用函数</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">def <span class="title">jdbc</span><span class="params">(url: String, table: String, properties: Properties)</span>: DataFrame</span></span><br></pre></td></tr></table></figure>

<p>使用:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val url = <span class="string">"jdbc:mysql://mysqlHost:3306/database"</span></span><br><span class="line">val tableName = <span class="string">"table"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置连接用户&amp;密码</span></span><br><span class="line">val prop = <span class="keyword">new</span> java.util.Properties</span><br><span class="line">prop.setProperty(<span class="string">"user"</span>,<span class="string">"username"</span>)</span><br><span class="line">prop.setProperty(<span class="string">"password"</span>,<span class="string">"pwd"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 取得该表数据</span></span><br><span class="line">val jdbcDF = sqlContext.read.jdbc(url,tableName,prop)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 一些操作</span></span><br><span class="line">....</span><br></pre></td></tr></table></figure>

<p>查看并发度</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">jdbcDF.rdd.partitions.size # 结果返回 1</span><br></pre></td></tr></table></figure>

<p>该操作的并发度为1，你所有的数据都会在一个partition中进行操作，意味着无论你给的资源有多少，只有一个task会执行任务，执行效率可想而之，并且在稍微大点的表中进行操作分分钟就会OOM。</p>
<p>更直观的说法是，达到千万级别的表就不要使用该操作，<code>count</code>操作就要等一万年，no zuo no die ,don’t to try !</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">WARN TaskSetManager: Lost task <span class="number">0.0</span> in stage <span class="number">6.0</span> (TID <span class="number">56</span>, spark047219):</span><br><span class="line"> java.lang.OutOfMemoryError: GC overhead limit exceeded</span><br><span class="line">at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:<span class="number">3380</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据 id (整型)字段分区</p>
<p>调用函数</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">def <span class="title">jdbc</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">url: String,</span></span></span><br><span class="line"><span class="function"><span class="params">table: String,</span></span></span><br><span class="line"><span class="function"><span class="params">columnName: String,    # 根据该字段分区，需要为整形，比如id等</span></span></span><br><span class="line"><span class="function"><span class="params">lowerBound: Long,      # 分区的下界</span></span></span><br><span class="line"><span class="function"><span class="params">upperBound: Long,      # 分区的上界</span></span></span><br><span class="line"><span class="function"><span class="params">numPartitions: Int,    # 分区的个数</span></span></span><br><span class="line"><span class="function"><span class="params">connectionProperties: Properties)</span>: DataFrame</span></span><br></pre></td></tr></table></figure>

<p>使用:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val url = <span class="string">"jdbc:mysql://mysqlHost:3306/database"</span></span><br><span class="line">val tableName = <span class="string">"table"</span></span><br><span class="line"></span><br><span class="line">val columnName = <span class="string">"colName"</span></span><br><span class="line">val lowerBound = <span class="number">1</span>,</span><br><span class="line">val upperBound = <span class="number">10000000</span>,</span><br><span class="line">val numPartitions = <span class="number">10</span>,</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置连接用户&amp;密码</span></span><br><span class="line">val prop = <span class="keyword">new</span> java.util.Properties</span><br><span class="line">prop.setProperty(<span class="string">"user"</span>,<span class="string">"username"</span>)</span><br><span class="line">prop.setProperty(<span class="string">"password"</span>,<span class="string">"pwd"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 取得该表数据</span></span><br><span class="line">val jdbcDF = sqlContext.read.jdbc(url,tableName,columnName,lowerBound,upperBound,numPartitions,prop)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 一些操作</span></span><br><span class="line">....</span><br></pre></td></tr></table></figure>

<p>查看并发度</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">jdbcDF.rdd.partitions.size # 结果返回 10</span><br></pre></td></tr></table></figure>

<p>该操作将字段 <code>colName</code> 中1-10000000条数据分到10个partition中，使用很方便，缺点也很明显，只能使用整形数据字段作为分区关键字。</p>
<p>3000w数据的表 <code>count</code> 跨集群操作只要2s。</p>
</li>
<li><p>根据时间字段分区</p>
<p>调用函数</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">jdbc(</span><br><span class="line">  url: String,</span><br><span class="line">  table: String,</span><br><span class="line">  predicates: Array[String],</span><br><span class="line">  connectionProperties: Properties): DataFrame</span><br></pre></td></tr></table></figure>

<p>下面以使用最多的时间字段分区为例:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val url = <span class="string">"jdbc:mysql://mysqlHost:3306/database"</span></span><br><span class="line">val tableName = <span class="string">"table"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置连接用户&amp;密码</span></span><br><span class="line">val prop = <span class="keyword">new</span> java.util.Properties</span><br><span class="line">prop.setProperty(<span class="string">"user"</span>,<span class="string">"username"</span>)</span><br><span class="line">prop.setProperty(<span class="string">"password"</span>,<span class="string">"pwd"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 将9月16-12月15三个月的数据取出，按时间分为6个partition</span></span><br><span class="line"><span class="comment">* 为了减少事例代码，这里的时间都是写死的</span></span><br><span class="line"><span class="comment">* modified_time 为时间字段</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line">val predicates =</span><br><span class="line">    Array(</span><br><span class="line">      <span class="string">"2015-09-16"</span> -&gt; <span class="string">"2015-09-30"</span>,</span><br><span class="line">      <span class="string">"2015-10-01"</span> -&gt; <span class="string">"2015-10-15"</span>,</span><br><span class="line">      <span class="string">"2015-10-16"</span> -&gt; <span class="string">"2015-10-31"</span>,</span><br><span class="line">      <span class="string">"2015-11-01"</span> -&gt; <span class="string">"2015-11-14"</span>,</span><br><span class="line">      <span class="string">"2015-11-15"</span> -&gt; <span class="string">"2015-11-30"</span>,</span><br><span class="line">      <span class="string">"2015-12-01"</span> -&gt; <span class="string">"2015-12-15"</span></span><br><span class="line">    ).map &#123;</span><br><span class="line">      <span class="keyword">case</span> (start, end) =&gt;</span><br><span class="line">        s<span class="string">"cast(modified_time as date) &gt;= date '$start' "</span> + s<span class="string">"AND cast(modified_time as date) &lt;= date '$end'"</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 取得该表数据</span></span><br><span class="line">val jdbcDF = sqlContext.read.jdbc(url,tableName,predicates,prop)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 一些操作</span></span><br></pre></td></tr></table></figure>

<p>查看并发度</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">jdbcDF.rdd.partitions.size # 结果返回 6</span><br></pre></td></tr></table></figure>

<p>该操作的每个分区数据都由该段时间的分区组成，这种方式适合各种场景，较为推荐。</p>
</li>
<li><p>id 取模方式分区</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">sqlContext.read.jdbc(url,tableName, <span class="string">"id%200"</span>, <span class="number">1</span>, <span class="number">1000000</span>,<span class="number">400</span>,prop)</span><br></pre></td></tr></table></figure>

<p>根据<code>numPartitions</code>确定合理的模值，可以尽量做到数据的连续，且写法简单，但是由于在ID字段上使用了函数计算，所以索引将失效，此时需要配合其他包含索引的where条件加以辅助，才能使查询性能最大化。</p>
</li>
<li><p>自定义处理方式</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">def getPredicates = &#123;    </span><br><span class="line">    <span class="comment">//1.获取表total数据。</span></span><br><span class="line">    <span class="comment">//2.按numPartitions均分，获得offset，可以确保每个分片的数据一致</span></span><br><span class="line">    <span class="comment">//3.获取每个分片内的最大最小ID，组装成条件数组</span></span><br><span class="line">     </span><br><span class="line">     。。。实现细节省略</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sqlContext.read.jdbc(url,table, getPredicates,connectionProperties)</span><br></pre></td></tr></table></figure>

<p>通过自由组装方式，可以达到精确控制，但是实现成本较高。</p>
</li>
</ol>
<h3 id="数据读取分区的原理"><a href="#数据读取分区的原理" class="headerlink" title="数据读取分区的原理"></a>数据读取分区的原理</h3><p>无论使用哪种JDBC API，spark拉取数据最终都是以<code>select语句</code>来执行的，所以在自定义分区条件或者指定的long型column时，都需要结合表的索引来综合考虑，才能以更高性能并发读取数据库数据。</p>
<p>API中的<code>columnName</code>其实只会作为where条件进行简单的拼接，所以数据库中支持的语法，都可以使用。<code>tableName</code>的原理也一样，仅会作为from 后的内容进行拼接，所以也可以写一个子句传入<code>tableName</code>中，但依然要在保证性能的前提下。</p>
<p>不仅仅是<code>取模操作</code>，数据库语法支持的任何函数，都可以在API中传入使用，关键在于性能是否达到预期。</p>
<p>JDBC的读取性能受很多条件影响，需要根据不同的数据库，表，索引，数据量，spark集群的executor情况等综合考虑，线上环境的操作，建议进行读写分离，即读备库，写主库。</p>
<p><code>注意:</code> 高并发度可以大幅度提高读取以及处理数据的速度，但是如果设置过高(大量的partition同时读取)也可能会将数据源数据库弄挂。</p>
<h2 id="jdbc-写并发度优化"><a href="#jdbc-写并发度优化" class="headerlink" title="jdbc 写并发度优化"></a>jdbc 写并发度优化</h2><ol>
<li><p>jdbc 方式</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">object BatchInsertMySQL &#123;</span><br><span class="line">  <span class="function"><span class="keyword">case</span> class <span class="title">Person</span><span class="params">(name: String, age: Int)</span></span></span><br><span class="line"><span class="function">  def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建sparkSession对象</span></span><br><span class="line">    val conf = <span class="keyword">new</span> SparkConf()</span><br><span class="line">      .setAppName(<span class="string">"BatchInsertMySQL"</span>)</span><br><span class="line">    val spark: SparkSession =  SparkSession.builder()</span><br><span class="line">      .config(conf)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// MySQL连接参数</span></span><br><span class="line">    val url = JDBCUtils.url</span><br><span class="line">    val user = JDBCUtils.user</span><br><span class="line">    val pwd = JDBCUtils.password</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建Properties对象，设置连接mysql的用户名和密码</span></span><br><span class="line">    val properties: Properties = <span class="keyword">new</span> Properties()</span><br><span class="line"></span><br><span class="line">    properties.setProperty(<span class="string">"user"</span>, user) <span class="comment">// 用户名</span></span><br><span class="line">    properties.setProperty(<span class="string">"password"</span>, pwd) <span class="comment">// 密码</span></span><br><span class="line">    properties.setProperty(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">    properties.setProperty(<span class="string">"numPartitions"</span>,<span class="string">"10"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取mysql中的表数据</span></span><br><span class="line">    val testDF: DataFrame = spark.read.jdbc(url, <span class="string">"test"</span>, properties)</span><br><span class="line">     println(<span class="string">"testDF的分区数：  "</span> + testDF.rdd.partitions.size)</span><br><span class="line">   testDF.createOrReplaceTempView(<span class="string">"test"</span>)</span><br><span class="line">   testDF.sqlContext.cacheTable(<span class="string">"test"</span>)</span><br><span class="line">   testDF.printSchema()</span><br><span class="line"></span><br><span class="line">    val result =</span><br><span class="line">      s<span class="string">""</span><span class="string">"-- SQL代码</span></span><br><span class="line"><span class="string">               "</span><span class="string">""</span>.stripMargin</span><br><span class="line"></span><br><span class="line">    val resultBatch = spark.sql(result).as[Person]</span><br><span class="line">    println(<span class="string">"resultBatch的分区数： "</span> + resultBatch.rdd.partitions.size)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 批量写入MySQL</span></span><br><span class="line">    <span class="comment">// 此处最好对处理的结果进行一次重分区</span></span><br><span class="line">    <span class="comment">// 由于数据量特别大，会造成每个分区数据特别多</span></span><br><span class="line">    resultBatch.repartition(<span class="number">400</span>).foreachPartition(record =&gt; &#123;</span><br><span class="line"></span><br><span class="line">      val list = <span class="keyword">new</span> ListBuffer[Person]</span><br><span class="line">      record.foreach(person =&gt; &#123;</span><br><span class="line">        val name = Person.name</span><br><span class="line">        val age = Person.age</span><br><span class="line">        list.append(Person(name,age))</span><br><span class="line">      &#125;)</span><br><span class="line">      upsertDateMatch(list) <span class="comment">//执行批量插入数据</span></span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">// 批量插入MySQL的方法</span></span><br><span class="line">    <span class="function">def <span class="title">upsertPerson</span><span class="params">(list: ListBuffer[Person])</span>: Unit </span>= &#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">var</span> connect: Connection = <span class="keyword">null</span></span><br><span class="line">      <span class="keyword">var</span> pstmt: PreparedStatement = <span class="keyword">null</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        connect = JDBCUtils.getConnection()</span><br><span class="line">        <span class="comment">// 禁用自动提交</span></span><br><span class="line">        connect.setAutoCommit(<span class="keyword">false</span>)</span><br><span class="line"></span><br><span class="line">        val sql = <span class="string">"REPLACE INTO `person`(name, age)"</span> +</span><br><span class="line">          <span class="string">" VALUES(?, ?)"</span></span><br><span class="line"></span><br><span class="line">        pstmt = connect.prepareStatement(sql)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">var</span> batchIndex = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> (person &lt;- list) &#123;</span><br><span class="line">          pstmt.setString(<span class="number">1</span>, person.name)</span><br><span class="line">          pstmt.setString(<span class="number">2</span>, person.age)</span><br><span class="line">          <span class="comment">// 加入批次</span></span><br><span class="line">          pstmt.addBatch()</span><br><span class="line">          batchIndex +=<span class="number">1</span></span><br><span class="line">          <span class="comment">// 控制提交的数量,</span></span><br><span class="line">          <span class="comment">// MySQL的批量写入尽量限制提交批次的数据量，否则会把MySQL写挂！！！</span></span><br><span class="line">          <span class="keyword">if</span>(batchIndex % <span class="number">1000</span> == <span class="number">0</span> &amp;&amp; batchIndex !=<span class="number">0</span>)&#123;</span><br><span class="line">            pstmt.executeBatch()</span><br><span class="line">            pstmt.clearBatch()</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 提交批次</span></span><br><span class="line">        pstmt.executeBatch()</span><br><span class="line">        connect.commit()</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> e: Exception =&gt;</span><br><span class="line">          e.printStackTrace()</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        JDBCUtils.closeConnection(connect, pstmt)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>df 方式</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">sqlDF.coalesce(<span class="number">10</span>)	<span class="comment">// 并行度</span></span><br><span class="line">    .write			</span><br><span class="line">    .mode(SaveMode.Overwrite)		<span class="comment">//覆盖模式</span></span><br><span class="line">    .format(<span class="string">"jdbc"</span>)</span><br><span class="line">    .option(<span class="string">"url"</span>, url)</span><br><span class="line">    .option(<span class="string">"dbtable"</span>, s<span class="string">"$db.$target"</span>)</span><br><span class="line">    .option(<span class="string">"user"</span>, user)</span><br><span class="line">    .option(<span class="string">"password"</span>, password)</span><br><span class="line">    .option(<span class="string">"driver"</span>,driver)</span><br><span class="line">    .option(<span class="string">"batchsize"</span>,<span class="string">"2000"</span>)		<span class="comment">// 每个批次写入的数据量</span></span><br><span class="line">    .option(<span class="string">"truncate"</span>,<span class="keyword">true</span>)		<span class="comment">// SaveMode.Overwrite 不删除而是清空表</span></span><br><span class="line">    .save()</span><br></pre></td></tr></table></figure>

</li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Tunan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yerias.github.io/2020/11/05/spark/36/">http://yerias.github.io/2020/11/05/spark/36/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="social-share" data-disabled="facebook,twitter,douban,linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/11/06/spark/37/"><i class="fa fa-chevron-left">  </i><span>Spark核心概念RDD</span></a></div><div class="next-post pull-right"><a href="/2020/08/10/hbase/6/"><span>region move&amp;region spilt&amp;region merge</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2021 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="footer_custom_text">大家好，我是图南，很高兴认识你们！</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>