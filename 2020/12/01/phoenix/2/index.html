<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="phoenix安装部署&amp;基本操作&amp;Spark读写Phoenix"><meta name="keywords" content="Phoenix"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>phoenix安装部署&amp;基本操作&amp;Spark读写Phoenix | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#目录"><span class="toc-number">1.</span> <span class="toc-text">目录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Phoenix"><span class="toc-number">2.</span> <span class="toc-text">Phoenix</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#特点"><span class="toc-number">2.1.</span> <span class="toc-text">特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#架构"><span class="toc-number">2.2.</span> <span class="toc-text">架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#作用"><span class="toc-number">2.3.</span> <span class="toc-text">作用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Phoenix安装部署"><span class="toc-number">3.</span> <span class="toc-text">Phoenix安装部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#下载与HBase版本兼容的Phoenix"><span class="toc-number">3.1.</span> <span class="toc-text">下载与HBase版本兼容的Phoenix</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#上传jar包到-opt-software"><span class="toc-number">3.2.</span> <span class="toc-text">上传jar包到/opt/software/</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#复制server和client这俩个包拷贝到各个节点的hbase-lib"><span class="toc-number">3.3.</span> <span class="toc-text">复制server和client这俩个包拷贝到各个节点的hbase/lib</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#在root权限下给-etc-profile-下添加如下内容"><span class="toc-number">3.4.</span> <span class="toc-text">在root权限下给/etc/profile 下添加如下内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#开启schema对应namespace"><span class="toc-number">3.5.</span> <span class="toc-text">开启schema对应namespace</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#重启HBase，重新连接启动Phoenix"><span class="toc-number">3.6.</span> <span class="toc-text">重启HBase，重新连接启动Phoenix</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#基本操作（常用命令）"><span class="toc-number">4.</span> <span class="toc-text">基本操作（常用命令）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#操作视图"><span class="toc-number">4.1.</span> <span class="toc-text">操作视图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#操作表"><span class="toc-number">4.2.</span> <span class="toc-text">操作表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#预分区"><span class="toc-number">4.3.</span> <span class="toc-text">预分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Phoenix表映射"><span class="toc-number">4.4.</span> <span class="toc-text">Phoenix表映射</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用spark对phoenix的读写"><span class="toc-number">5.</span> <span class="toc-text">使用spark对phoenix的读写</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#读数据"><span class="toc-number">5.1.</span> <span class="toc-text">读数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#创建RDD"><span class="toc-number">5.2.</span> <span class="toc-text">创建RDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#写数据"><span class="toc-number">5.3.</span> <span class="toc-text">写数据</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/xiaoqi.jpg"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><div class="follow-button"><a href="#">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">214</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">38</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">34</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">HADOOP</a><a class="author-info-links__name text-center" href="https://cwiki.apache.org/confluence/display/Hive/" target="_blank" rel="noopener">HIVE</a><a class="author-info-links__name text-center" href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">CDH</a><a class="author-info-links__name text-center" href="http://flume.apache.org/" target="_blank" rel="noopener">FLUME</a><a class="author-info-links__name text-center" href="https://azkaban.github.io/" target="_blank" rel="noopener">AZKABAN</a><a class="author-info-links__name text-center" href="https://www.maxinhong.com/" target="_blank" rel="noopener">叶梨子</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/qingyunzong" target="_blank" rel="noopener">老铁</a><a class="author-info-links__name text-center" href="https://blog.csdn.net/jim8973/" target="_blank" rel="noopener">飞哥</a><a class="author-info-links__name text-center" href="https://vinxikk.github.io/" target="_blank" rel="noopener">vinx</a><a class="author-info-links__name text-center" href="http://dongxicheng.org/" target="_blank" rel="noopener">懂西成(Hadoop技术内幕作者)</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/xing901022/" target="_blank" rel="noopener">xingoo</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/itboys/tag/" target="_blank" rel="noopener">大葱拌豆腐</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/shishanyuan" target="_blank" rel="noopener">郭景瞻(图解Spark作者)</a><a class="author-info-links__name text-center" href="https://segmentfault.com/u/wishdaren_5c243b920a3eb" target="_blank" rel="noopener">Wish大人</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/huxi2b/" target="_blank" rel="noopener">huxi_2b(kafka)</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">phoenix安装部署&amp;基本操作&amp;Spark读写Phoenix</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-12-01</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Phoenix/">Phoenix</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">4.4k</span><span class="post-meta__separator">|</span><span>Reading time: 19 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li><p>Phoenix特点</p>
</li>
<li><p>Phoenix安装部署</p>
</li>
<li><p>基本操作（常用命令）</p>
</li>
<li><p>Phoenix表映射</p>
</li>
<li><p>使用Spark对Phoenix的读写</p>
</li>
</ol>
<h2 id="Phoenix"><a href="#Phoenix" class="headerlink" title="Phoenix"></a>Phoenix</h2><p>Phoenix是HBase的开源SQL皮肤。可以使用标准JDBC API代替HBase客户端API来创建表，插入数据和查询HBase数据。</p>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ol>
<li><p>容易集成：如Spark，Hive，Pig，Flume和Map Reduce。</p>
</li>
<li><p>性能好：直接使用HBase API以及协处理器和自定义过滤器，可以为小型查询提供毫秒级的<a href="http://phoenix.apache.org/performance.html" target="_blank" rel="noopener">性能</a>，或者为数千万行提供数秒的性能。</p>
</li>
<li><p>操作简单：DML命令以及通过DDL命令创建表和版本化增量更改。</p>
</li>
<li><p>安全功能: <a href="https://issues.apache.org/jira/browse/PHOENIX-672" target="_blank" rel="noopener">支持GRANT和REVOKE</a> 。</p>
</li>
<li><p>完美支持Hbase二级索引创建。</p>
</li>
</ol>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><img src="https://yerias.github.io/phoenix/1247221-20190703080345081-71998722.png" alt=""></p>
<h3 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h3><p><img src="https://yerias.github.io/phoenix/1247221-20190703080323534-1178982359.png" alt=""></p>
<h2 id="Phoenix安装部署"><a href="#Phoenix安装部署" class="headerlink" title="Phoenix安装部署"></a>Phoenix安装部署</h2><p><a href="https://www.jianshu.com/p/be4a03f77e3d" target="_blank" rel="noopener">CDH集成phoenix</a>   <a href="https://my.oschina.net/hblt147/blog/3016196" target="_blank" rel="noopener">https://my.oschina.net/hblt147/blog/3016196</a></p>
<p>官方网址： <a href="http://phoenix.apache.org/index.html" target="_blank" rel="noopener">http://phoenix.apache.org/index.html</a></p>
<p><a href="https://archive.apache.org/dist/phoenix/?C=M;O=A" target="_blank" rel="noopener">https://archive.apache.org/dist/phoenix/?C=M;O=A</a></p>
<h3 id="下载与HBase版本兼容的Phoenix"><a href="#下载与HBase版本兼容的Phoenix" class="headerlink" title="下载与HBase版本兼容的Phoenix"></a>下载与HBase版本兼容的Phoenix</h3><p>下载地址：<a href="http://archive.apache.org/dist/phoenix/" target="_blank" rel="noopener">http://archive.apache.org/dist/phoenix/</a></p>
<h3 id="上传jar包到-opt-software"><a href="#上传jar包到-opt-software" class="headerlink" title="上传jar包到/opt/software/"></a>上传jar包到/opt/software/</h3><p>解压到/opt/module 改名为phoenix</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">[kris@hadoop101</span> <span class="string">module]$ tar -zxvf /opt/software/apache-phoenix-4.14.1-HBase-1.3-bin.tar.gz -C /opt/module </span></span><br><span class="line"></span><br><span class="line"><span class="meta">[kris@hadoop101</span> <span class="string">module]$ mv apache-phoenix-4.14.1-HBase-1.3-bin phoenix</span></span><br></pre></td></tr></table></figure>

<h3 id="复制server和client这俩个包拷贝到各个节点的hbase-lib"><a href="#复制server和client这俩个包拷贝到各个节点的hbase-lib" class="headerlink" title="复制server和client这俩个包拷贝到各个节点的hbase/lib"></a>复制server和client这俩个包拷贝到各个节点的hbase/lib</h3><p>在phoenix目录下</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">[kris@hadoop101</span> <span class="string">module]$ cd /opt/module/phoenix/</span></span><br></pre></td></tr></table></figure>

<p>向每个节点发送server jar</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">[kris@hadoop101</span> <span class="string">phoenix]$ cp phoenix-4.14.1-HBase-1.3-server.jar /opt/module/hbase-1.3.1/lib/</span></span><br><span class="line"><span class="meta">[kris@hadoop101</span> <span class="string">phoenix]$ scp phoenix-4.14.1-HBase-1.3-server.jar hadoop102:/opt/module/hbase-1.3.1/lib/</span></span><br><span class="line"><span class="meta">[kris@hadoop101</span> <span class="string">phoenix]$ scp phoenix-4.14.1-HBase-1.3-server.jar hadoop103:/opt/module/hbase-1.3.1/lib/</span></span><br></pre></td></tr></table></figure>

<p>向每个节点发送client jar</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">[kris@hadoop101</span> <span class="string">phoenix]$ cp phoenix-4.14.1-HBase-1.3-client.jar /opt/module/hbase-1.3.1/lib/</span></span><br><span class="line"><span class="meta">[kris@hadoop101</span> <span class="string">phoenix]$ scp phoenix-4.14.1-HBase-1.3-client.jar hadoop102:/opt/module/hbase-1.3.1/lib/</span></span><br><span class="line"><span class="meta">[kris@hadoop101</span> <span class="string">phoenix]$ scp phoenix-4.14.1-HBase-1.3-client.jar hadoop103:/opt/module/hbase-1.3.1/lib/</span></span><br></pre></td></tr></table></figure>

<h3 id="在root权限下给-etc-profile-下添加如下内容"><a href="#在root权限下给-etc-profile-下添加如下内容" class="headerlink" title="在root权限下给/etc/profile 下添加如下内容"></a>在root权限下给/etc/profile 下添加如下内容</h3><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">#phoenix</span></span><br><span class="line">    <span class="attr">export</span> <span class="string">PHOENIX_HOME=/opt/module/phoenix</span></span><br><span class="line">    <span class="attr">export</span> <span class="string">PHOENIX_CLASSPATH=$PHOENIX_HOME</span></span><br><span class="line">    <span class="attr">export</span> <span class="string">PATH=$PATH:$PHOENIX_HOME/bin</span></span><br></pre></td></tr></table></figure>

<h3 id="开启schema对应namespace"><a href="#开启schema对应namespace" class="headerlink" title="开启schema对应namespace"></a><strong>开启schema对应namespace</strong></h3><p><strong><a href="http://phoenix.apache.org/namspace_mapping.html" target="_blank" rel="noopener">http://phoenix.apache.org/namspace_mapping.html</a></strong></p>
<p><strong>在Phoenix中是没有Database的概念的，所有的表都在同一个命名空间。当然，Phoenix4.8开始支持多个命名空间了</strong>，如果要用自定义的namespace，Phoenix中与之对应的是schema的概念，但是默认是关闭的，需要单独配置。</p>
<ol>
<li><p>在<code>hbase/conf/hbase-site.xml</code>、<code>phoenix/bin/hbase-site.xml</code>两个文件中增加以下代码：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>phoenix.schema.isNamespaceMappingEnabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>phoenix.schema.mapSystemTablesToNamespace<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>如果HBase是分布式，则需要<strong>将文件分发到其他节点</strong>（最好是将该文件也复制到<code>phoenix/bin/</code>保证客户端与服务端的一致性）</p>
</li>
<li><p>Phoenix其他相关配置参照：<a href="https://phoenix.apache.org/tuning.html" target="_blank" rel="noopener">https://phoenix.apache.org/tuning.html</a></p>
</li>
</ol>
<h3 id="重启HBase，重新连接启动Phoenix"><a href="#重启HBase，重新连接启动Phoenix" class="headerlink" title="重启HBase，重新连接启动Phoenix"></a>重启HBase，重新连接启动Phoenix</h3><p> <strong>./sqlline.py hadoop101:2181</strong></p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">[kris@hadoop101</span> <span class="string">module]$ cd phoenix/</span></span><br><span class="line"><span class="meta">[kris@hadoop101</span> <span class="string">phoenix]$ bin/sqlline.py hadoop101,hadoop102,hadoop103:2181</span></span><br><span class="line"><span class="attr">Setting</span> <span class="string">property: [incremental, false]</span></span><br><span class="line"><span class="attr">Setting</span> <span class="string">property: [isolation, TRANSACTION_READ_COMMITTED]</span></span><br><span class="line"><span class="attr">issuing</span>: <span class="string">!connect jdbc:phoenix:hadoop101,hadoop102,hadoop103:2181 none none org.apache.phoenix.jdbc.PhoenixDriver</span></span><br><span class="line"><span class="attr">Connecting</span> <span class="string">to jdbc:phoenix:hadoop101,hadoop102,hadoop103:2181</span></span><br><span class="line"><span class="attr">SLF4J</span>: <span class="string">Class path contains multiple SLF4J bindings.</span></span><br><span class="line"><span class="attr">SLF4J</span>: <span class="string">Found binding in [jar:file:/opt/module/phoenix/phoenix-4.14.1-HBase-1.3-client.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span></span><br><span class="line"><span class="attr">SLF4J</span>: <span class="string">Found binding in [jar:file:/opt/module/hadoop-2.7.2/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span></span><br><span class="line"><span class="attr">SLF4J</span>: <span class="string">See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span></span><br><span class="line"><span class="meta">19/07/12</span> <span class="string">23:57:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span></span><br><span class="line"><span class="attr">Connected</span> <span class="string">to: Phoenix (version 4.14)</span></span><br><span class="line"><span class="attr">Driver</span>: <span class="string">PhoenixEmbeddedDriver (version 4.14)</span></span><br><span class="line"><span class="attr">Autocommit</span> <span class="string">status: true</span></span><br><span class="line"><span class="attr">Transaction</span> <span class="string">isolation: TRANSACTION_READ_COMMITTED</span></span><br><span class="line"><span class="attr">Building</span> <span class="string">list of tables and columns for tab-completion (set fastconnect to true to skip)...</span></span><br><span class="line"><span class="meta">133/133</span> <span class="string">(100%) Done</span></span><br><span class="line"><span class="attr">Done</span></span><br><span class="line"><span class="attr">sqlline</span> <span class="string">version 1.2.0</span></span><br><span class="line"><span class="attr">0</span>: <span class="string">jdbc:phoenix:hadoop101,hadoop102,hadoop103&gt;</span></span><br></pre></td></tr></table></figure>



<h2 id="基本操作（常用命令）"><a href="#基本操作（常用命令）" class="headerlink" title="基本操作（常用命令）"></a>基本操作（常用命令）</h2><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="comment">----HBase是大小写敏感，Phoenix操作时需要添加双引号，如果不添加双引号的话会统一转换成大写</span></span><br><span class="line">//显示所有表:  </span><br><span class="line">!table  或者 !tables</span><br><span class="line">//列出所有列</span><br><span class="line">$sqlline&gt; !columns test.stu</span><br><span class="line">//创建schema（相当于数据库）</span><br><span class="line">$sqlline&gt; create schema test;</span><br><span class="line"><span class="keyword">use</span> <span class="keyword">test</span>;</span><br><span class="line"></span><br><span class="line">//删除表结构</span><br><span class="line">$sqlline&gt; drop table stu;</span><br><span class="line"></span><br><span class="line">//创建表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu(<span class="keyword">id</span> <span class="built_in">integer</span> primary <span class="keyword">key</span> ,<span class="keyword">name</span> <span class="built_in">varchar</span>,age <span class="built_in">integer</span>);</span><br><span class="line">//创建表并指定列族</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu2(<span class="keyword">id</span> <span class="built_in">integer</span> primary <span class="keyword">key</span> ,<span class="string">"cf1"</span>.name <span class="built_in">varchar</span>,<span class="string">"cf1"</span>.age <span class="built_in">integer</span>) ;</span><br><span class="line"></span><br><span class="line">//插入数据和更新数据</span><br><span class="line">        在Phoenix中是没有<span class="keyword">Insert</span>语句的，取而代之的是<span class="keyword">Upsert</span>语句。<span class="keyword">Upsert</span>有两种用法，分别是:<span class="keyword">UPSERT</span> <span class="keyword">INTO</span>和 <span class="keyword">UPSERT</span> <span class="keyword">SELECT</span></span><br><span class="line">　　　　<span class="keyword">UPSERT</span> <span class="keyword">INTO</span>类似于<span class="keyword">insert</span> <span class="keyword">into</span>的语句，旨在单条插入外部数据</span><br><span class="line">　　　　<span class="keyword">UPSERT</span> <span class="keyword">SELECT</span>类似于Hive中的<span class="keyword">insert</span> <span class="keyword">select</span>语句，旨在批量插入其他表的数据。</span><br><span class="line">　　　　<span class="keyword">UPSERT</span> <span class="keyword">INTO</span> stu (<span class="keyword">id</span>,<span class="keyword">name</span>,age) <span class="keyword">SELECT</span> <span class="keyword">id</span>,<span class="keyword">name</span>,age <span class="keyword">FROM</span> sty2 <span class="keyword">WHERE</span> <span class="keyword">id</span> &lt; <span class="number">400</span>;</span><br><span class="line"></span><br><span class="line">upsert into stu2 <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'tom'</span>,<span class="number">12</span>);</span><br><span class="line">upsert into stu(id,name,age) <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'alex'</span>,<span class="number">22</span>);</span><br><span class="line">注意：在phoenix中插入语句并不会像传统数据库一样存在重复数据。因为Phoenix是构建在HBase之上的，也就是必须存在一个主键。</span><br><span class="line">由于HBase的主键设计，相同rowkey的内容可以直接覆盖，这就变相的更新了数据。</span><br><span class="line"></span><br><span class="line">//删除数据</span><br><span class="line">$sqlline&gt; delete from stu where id = 1 ; </span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> stu;</span><br><span class="line">//删除表  删除表和其他的数据库类似。不同的是可以加上CASCADE关键字，用于删除表的同时删除基于该表的所有视图。</span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> my_schema.my_table;</span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> my_schema.my_table <span class="keyword">CASCADE</span>;</span><br><span class="line">//条件查询</span><br><span class="line">Phoenix作为SQL On HBase引擎必不可少的就是SQL查询语句了，他能兼容大部分的SQL查询语句，比如UNION ALL GROUP BY ORDER BY LIMIT</span><br><span class="line">$sqlline&gt; select * from stu where name like 'a%' ;</span><br></pre></td></tr></table></figure>

<h3 id="操作视图"><a href="#操作视图" class="headerlink" title="操作视图"></a>操作视图</h3><ol>
<li><p>创建视图</p>
<p><img src="https://yerias.github.io/phoenix/1247221-20191014172200201-2024849451.png" alt=""></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> test_view <span class="keyword">AS</span> <span class="keyword">SELECT</span> *  <span class="keyword">FROM</span> <span class="keyword">test</span> <span class="keyword">where</span> description <span class="keyword">in</span> (<span class="string">'S1'</span>,<span class="string">'S2'</span>,<span class="string">'S3'</span>)</span><br><span class="line">除此之外，我们还能在视图上创建视图</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> test_view1 <span class="keyword">AS</span> <span class="keyword">SELECT</span> *  <span class="keyword">FROM</span> test_view <span class="keyword">where</span> description != <span class="string">'S1'</span>; //视图没办法只获取一部分数据的数据的,只能<span class="keyword">select</span> *</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除视图：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">VIEW</span> my_view</span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">VIEW</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> my_schema.my_view</span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">VIEW</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> my_schema.my_view <span class="keyword">CASCADE</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="操作表"><a href="#操作表" class="headerlink" title="操作表"></a>操作表</h3><ol>
<li><p>创建表：</p>
<p><img src="https://yerias.github.io/phoenix/1247221-20191014165455873-758172404.png" alt=""></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> us_population (</span><br><span class="line">State <span class="built_in">CHAR</span>(<span class="number">2</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">City <span class="built_in">VARCHAR</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">Population <span class="built_in">BIGINT</span></span><br><span class="line"><span class="keyword">CONSTRAINT</span> my_pk PRIMARY <span class="keyword">KEY</span> (state, city));</span><br></pre></td></tr></table></figure>

<p>在phoenix中，默认情况下，表名等会自动转换为大写，若要小写，使用双引号，如”us_population”。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">插入记录：</span><br><span class="line">upsert into us_population <span class="keyword">values</span>(<span class="string">'NY'</span>,<span class="string">'NewYork'</span>,<span class="number">8143197</span>);</span><br><span class="line"></span><br><span class="line">查询记录：</span><br><span class="line">0: jdbc:phoenix:hadoop101,hadoop102,hadoop103&gt; select * from us_population;</span><br><span class="line">+<span class="comment">--------+----------+-------------+</span></span><br><span class="line">| STATE  |   CITY   | POPULATION  |</span><br><span class="line">+<span class="comment">--------+----------+-------------+</span></span><br><span class="line">| NY     | NewYork  | 8142197     |</span><br><span class="line">+<span class="comment">--------+----------+-------------+</span></span><br><span class="line">1 row selected (0.083 seconds)</span><br><span class="line">0: jdbc:phoenix:hadoop101,hadoop102,hadoop103&gt; select * from us_population where state='NY';</span><br><span class="line">+<span class="comment">--------+----------+-------------+</span></span><br><span class="line">| STATE  |   CITY   | POPULATION  |</span><br><span class="line">+<span class="comment">--------+----------+-------------+</span></span><br><span class="line">| NY     | NewYork  | 8142197     |</span><br><span class="line">+<span class="comment">--------+----------+-------------+</span></span><br><span class="line">删除记录</span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> us_population wherestate=<span class="string">'NY'</span>;</span><br><span class="line">删除表</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> us_population;</span><br><span class="line">退出命令行</span><br><span class="line">!quit</span><br><span class="line"></span><br><span class="line">0: jdbc:phoenix:hadoop101,hadoop102,hadoop103&gt; !describe test</span><br><span class="line">+<span class="comment">------------+--------------+-------------+--------------+------------+------------+--------------+----------------+-+</span></span><br><span class="line">| TABLE_CAT  | TABLE_SCHEM  | TABLE_NAME  | COLUMN_NAME  | DATA_TYPE  | TYPE_NAME  | COLUMN_SIZE  | BUFFER_LENGTH  | |</span><br><span class="line">+<span class="comment">------------+--------------+-------------+--------------+------------+------------+--------------+----------------+-+</span></span><br><span class="line">+<span class="comment">------------+--------------+-------------+--------------+------------+------------+--------------+----------------+-+</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="预分区"><a href="#预分区" class="headerlink" title="预分区"></a>预分区</h3><ol>
<li><p>SALT_BUCKETS（加盐）</p>
<p>Salting能够通过预分区(pre-splitting)数据到多个region中来显著提升读写性能。</p>
<p>Salting 翻译成中文是加盐的意思，本质是在hbase中，rowkey的byte数组的第一个字节位置设定一个系统生成的byte值，这个byte值是由主键生成rowkey的byte数组做一个哈希<a href="http://lib.csdn.net/base/datastructure" target="_blank" rel="noopener">算法</a>，计算得来的。</p>
<p>Salting之后可以把数据分布到不同的region上，这样有利于phoenix并发的读写操作。关于SaltedTable的说明在 <a href="http://phoenix.apache.org/salted.html" target="_blank" rel="noopener">http://phoenix.apache.org/salted.html</a></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">test</span> (host <span class="built_in">VARCHAR</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> PRIMARY <span class="keyword">KEY</span>, description <span class="built_in">VARCHAR</span>) SALT_BUCKETS=<span class="number">16</span>;</span><br></pre></td></tr></table></figure>

<p>SALT_BUCKETS的值范围在（1 ~ 256）；</p>
<p>salted table可以自动在每一个rowkey前面加上一个字节，这样对于一段连续的rowkeys，它们在表中实际存储时，就被自动地分布到不同的region中去了。当指定要读写该段区间内的数据时，也就避免了读写操作都集中在同一个region上。</p>
<p>简而言之，如果我们用Phoenix创建了一个saltedtable，那么向该表中写入数据时，原始的rowkey的前面会被自动地加上一个byte（不同的rowkey会被分配不同的byte），使得连续的rowkeys也能被均匀地分布到多个regions。  在每条rowkey前面加了一个Byte，这里显示为了16进制。也正是因为添加了一个Byte，所以SALT_BUCKETS的值范围在必须再1 ~ 256之间</p>
<p>在使用SALT_BUCKETS的时候需要注意以下两点：</p>
<ul>
<li>创建salted table后，应该使用Phoenix SQL来读写数据，而不要混合使用Phoenix SQL和HBase API</li>
<li>如果通过Phoenix创建了一个salted table，那么只有通过Phoenix SQL插入数据才能使得被插入的原始rowkey前面被自动加上一个byte，通过HBase shell插入数据无法prefix原始的rowkey</li>
</ul>
</li>
<li><p>Pre-split（预分区）</p>
<p>Salting能够自动的设置表预分区，但是你得去控制表是如何分区的，所以在建phoenix表时，可以精确的指定要根据什么值来做预分区，比如：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">test</span> (host <span class="built_in">VARCHAR</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> PRIMARY <span class="keyword">KEY</span>, description <span class="built_in">VARCHAR</span>) <span class="keyword">SPLIT</span> <span class="keyword">ON</span> (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用多列族</p>
<p>列族包含相关的数据都在独立的文件中，在Phoenix设置多个列族可以提高查询性能。例如：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">test</span> (<span class="keyword">key</span> <span class="built_in">VARCHAR</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> PRIMARY <span class="keyword">KEY</span>, cf1.name <span class="built_in">VARCHAR</span>,cf1.age <span class="built_in">VARCHAR</span>, cf2.score <span class="built_in">VARCHAR</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用压缩</p>
<p>在数据量大的表上使用压缩算法来提高性能。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">test</span> (host <span class="built_in">VARCHAR</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> PRIMARY <span class="keyword">KEY</span>, description <span class="built_in">VARCHAR</span>) COMPRESSION=<span class="string">'Snappy'</span>;</span><br><span class="line"><span class="comment"># 在hbase中查看详情:</span></span><br><span class="line"><span class="keyword">describe</span> <span class="string">'TEST:TEST'</span></span><br></pre></td></tr></table></figure>



</li>
</ol>
<h3 id="Phoenix表映射"><a href="#Phoenix表映射" class="headerlink" title="Phoenix表映射"></a>Phoenix表映射</h3><ol>
<li><p>Phoenix和Hbase表的关系</p>
<p>默认情况下，直接在hbase中创建的表，通过phoenix是查看不到的。</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># hbase命令行中查看所有表：</span></span><br><span class="line"><span class="meta">[kris@hadoop101</span> <span class="string">bin]$ hbase shell</span></span><br><span class="line"></span><br><span class="line"><span class="meta">hbase(main)</span>:<span class="string">001:0&gt; list</span></span><br><span class="line"><span class="attr">phoenix命令行中查看所有表：</span></span><br><span class="line"><span class="attr">0</span>: <span class="string">jdbc:phoenix:hadoop101,hadoop102,hadoop103&gt; !tables</span></span><br></pre></td></tr></table></figure>

<p>如果要在phoenix中操作直接在hbase中创建的表，则需要在phoenix中进行表的映射。</p>
<p>映射方式有两种：<strong>视图映射和表映射</strong></p>
<p>Hbase命令行中创建表test，Hbase 中test的表结构如下，两个列簇name、company.</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">[kris@hadoop101</span> <span class="string">bin]$ hbase shell</span></span><br><span class="line"></span><br><span class="line"><span class="meta">hbase(main)</span>:<span class="string">002:0&gt; create 'test','name','company'</span></span><br><span class="line"><span class="attr">0</span> <span class="string">row(s) in 1.3380 seconds</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>视图映射</p>
<p>Phoenix创建的视图是只读的，所以只能用来做查询，无法通过视图对源数据进行修改等操作。</p>
<p>在phoenix中创建视图test 表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">0: jdbc:phoenix:hadoop101,hadoop102,hadoop103&gt; create view "test"(empid varchar primary key,"name"."firstname" varchar,"name"."lastname" varchar,"company"."name" varchar,"company"."address" varchar);</span><br><span class="line">0: jdbc:phoenix:hadoop101,hadoop102,hadoop103&gt; select * from "test";</span><br><span class="line">+<span class="comment">--------+------------+-----------+-------+----------+</span></span><br><span class="line">| EMPID  | firstname  | lastname  | name  | address  |</span><br><span class="line">+<span class="comment">--------+------------+-----------+-------+----------+</span></span><br><span class="line">+<span class="comment">--------+------------+-----------+-------+----------+</span></span><br></pre></td></tr></table></figure>

<p>删除视图</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">0: jdbc:phoenix:hadoop101,hadoop102,hadoop103&gt; drop view "test";</span><br></pre></td></tr></table></figure>
</li>
<li><p>表映射</p>
<p>使用Apache Phoenix创建对HBase的表映射，有两种方法：</p>
<ul>
<li><p>当HBase中已经存在表时，可以<strong>以类似创建视图的方式创建关联表</strong>，只需要将create view改为create table即可（phoenix 4.10 版本后，对列映射做了优化，采用一套新的机制，不在基于列名方式映射到 hbase，必须要表映射，需要禁用列映射规则（会降低查询性能））。</p>
<p><code>column_encoded_bytes=0</code></p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">0: jdbc:phoenix:hadoop101,hadoop102,hadoop103&gt; create table "test"(empid varchar primary key,"name"."firstname" varchar,"name"."lastname" varchar,"company"."name" varchar,"company"."address" varchar) column_encoded_bytes=0;</span><br></pre></td></tr></table></figure>
</li>
<li><p>当HBase中不存在表时，可以直接使用<strong>create table指令创建需要的表,系统将会自动在Phoenix和HBase中创建person_infomation的表</strong>，并会根据指令内的参数对表结构进行初始化。</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">0: jdbc:phoenix:hadoop101,hadoop102,hadoop103&gt; create table "test"(empid varchar primary key,"name"."firstname" varchar,"name"."lastname" varchar,"company"."name" varchar,"company"."address" varchar);</span><br></pre></td></tr></table></figure>
</li>
<li><p>注意: 设置schema的时候注意加上双引号，不然的话phoenix会默认大写，导致找不到指定的表</p>
</li>
</ul>
<p>CDH的Phoenix不支持这样创建表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MY_SCHEMA.MYTABLE (k <span class="built_in">BIGINT</span> PRIMARY <span class="keyword">KEY</span>, v <span class="built_in">VARCHAR</span>);</span><br></pre></td></tr></table></figure>

<p>若想在某个SCHEMA下创建表，需先使用USE SCHEMA语句切换到该SCHEMA下，再执行建表语句。</p>
</li>
<li><p>Phoenix中自增Id</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1、创建一个自增序列seq，缓存大小设置为10</span></span><br><span class="line">$sqlline&gt; create sequence seq  cache 10;</span><br><span class="line"><span class="comment"># 查看序列详情</span></span><br><span class="line">$sqlline&gt; select * from system."SEQUENCE";</span><br><span class="line"></span><br><span class="line">upsert into stu <span class="keyword">values</span>(<span class="keyword">next</span> <span class="keyword">value</span> <span class="keyword">for</span> seq,<span class="string">'aa'</span>,<span class="number">20</span>); //id从1开始,若之前有数据会被覆盖掉;</span><br><span class="line">//id超过10还会自增,但是若断开连接!quit,重新连接会导致自增id不连续;查看当前自增序列状态，发现当前值CURRENT_VALUE已经改变</span><br></pre></td></tr></table></figure>



</li>
</ol>
<h2 id="使用spark对phoenix的读写"><a href="#使用spark对phoenix的读写" class="headerlink" title="使用spark对phoenix的读写"></a>使用spark对phoenix的读写</h2><p> <a href="http://phoenix.apache.org/phoenix_spark.html" target="_blank" rel="noopener">http://phoenix.apache.org/phoenix_spark.html</a></p>
<p>IDEA环境依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-HBase-1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-spark<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-HBase-1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>hbase-site.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span>     </span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span>     </span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop101:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span>               </span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span>   </span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="comment">&lt;!-- 0.98后的新变动，之前版本没有.port,默认端口为60000(可省略) --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>16000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101,hadoop102,hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 参照zk的zoo.cfg文件中的dataDir值 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span>   </span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/zookeeper-3.4.10/zkData<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- phoenix regionserver 配置参数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.wal.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.region.server.rpc.scheduler.factory.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rpc.controllerfactory.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- phoenix master 配置参数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.loadbalancer.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.phoenix.hbase.index.balancer.IndexLoadBalancer<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.coprocessor.master.classes<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.phoenix.hbase.index.master.IndexMasterObserver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>phoenix.schema.isNamespaceMappingEnabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>phoenix.schema.mapSystemTablesToNamespace<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="读数据"><a href="#读数据" class="headerlink" title="读数据"></a>读数据</h3><ol>
<li><p>方式1(简单好用): </p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestSparkPhoenix</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().master(<span class="string">"local[*]"</span>).appName(<span class="string">"phoenix-test"</span>).getOrCreate()</span><br><span class="line">    <span class="keyword">var</span> df = spark</span><br><span class="line">      .read</span><br><span class="line">      .format(<span class="string">"org.apache.phoenix.spark"</span>)</span><br><span class="line">      .option(<span class="string">"table"</span>, <span class="string">"fruits"</span>)</span><br><span class="line">      .option(<span class="string">"zkUrl"</span>, <span class="string">"hadoop101,hadoop102,hadoop103:2181"</span>)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    df.show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>方式2:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">package com.easylife.phoenix</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration</span><br><span class="line">import org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;</span><br><span class="line">//否则编辑器识别不出语法，也不会自动import。</span><br><span class="line">import org.apache.phoenix.spark._</span><br><span class="line"></span><br><span class="line">object TestSparkPhoenix &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    //读</span><br><span class="line">    val spark = SparkSession.builder().master("local[*]").appName("phoenix-test").getOrCreate()</span><br><span class="line"></span><br><span class="line">    val configuration = new Configuration()</span><br><span class="line">    configuration.set(<span class="string">"hbase.zookeeper.quorum"</span>,<span class="string">"hadoop101,hadoop102,hadoop103:2181"</span>)</span><br><span class="line">    val df1: DataFrame = spark.sqlContext.phoenixTableAsDataFrame(<span class="string">"fruits"</span>,<span class="built_in">Array</span>(<span class="string">"id"</span>, <span class="string">"info.name"</span>), conf = configuration )</span><br><span class="line">    df1.show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="创建RDD"><a href="#创建RDD" class="headerlink" title="创建RDD"></a>创建RDD</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">//否则编辑器识别不出语法，也不会自动import。</span></span><br><span class="line"><span class="keyword">import</span> org.apache.phoenix.spark._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//使用Zookeeper URL创建RDD</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"phoenix-test"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line">    <span class="keyword">val</span> fruitRDD: <span class="type">RDD</span>[<span class="type">Map</span>[<span class="type">String</span>, <span class="type">AnyRef</span>]] = sc.phoenixTableAsRDD(<span class="string">"fruits"</span>,<span class="type">Array</span>(<span class="string">"id"</span>, <span class="string">"name"</span>), zkUrl = <span class="type">Some</span>(<span class="string">"hadoop101,hadoop102,hadoop103"</span>))</span><br><span class="line">    println(fruitRDD.count())</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="写数据"><a href="#写数据" class="headerlink" title="写数据"></a>写数据</h3><p>不加””, Phoenix中会自动转换为大写;</p>
<ol>
<li><p>方式1(保存RDD到Phoenix):</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">//先在Phoenix中创建表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> fruits(</span><br><span class="line"><span class="string">"id"</span> <span class="built_in">varchar</span> primary <span class="keyword">key</span>, </span><br><span class="line"><span class="string">"info"</span>.<span class="string">"color"</span> <span class="built_in">varchar</span>, </span><br><span class="line"><span class="string">"info"</span>.<span class="string">"name"</span> <span class="built_in">varchar</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="comment">//否则编辑器识别不出语法，也不会自动import。</span></span><br><span class="line"><span class="keyword">import</span> org.apache.phoenix.spark._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestSparkPhoenix</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="string">"local"</span>, <span class="string">"phoenix-test"</span>)</span><br><span class="line">    <span class="keyword">val</span> dateSet = <span class="type">List</span>((<span class="string">"1005"</span>, <span class="string">"white"</span>, <span class="string">"water"</span>), (<span class="string">"1006"</span>, <span class="string">"red"</span>, <span class="string">"watermelon"</span>))</span><br><span class="line">    sc.parallelize(dateSet)</span><br><span class="line">      .saveToPhoenix(<span class="string">"fruits"</span>, <span class="type">Seq</span>(<span class="string">"id"</span>, <span class="string">"color"</span>,<span class="string">"name"</span>),</span><br><span class="line">        zkUrl = <span class="type">Some</span>(<span class="string">"hadoop101,hadoop102,hadoop103:2181"</span>))</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在Phoenix中查看数据， <strong>通过Spark操作Phoenix是需要区分大小写的</strong>。这点非常重要</p>
<p>在Hbase中查看数据</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">scan</span> <span class="string">"FRUITS"</span></span><br></pre></td></tr></table></figure>

<p><strong>使用RDD的saveToPhoenix函数时必须严格按照Phoenix的Column名的大小写来输入：查看源码得</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveToPhoenix</span></span>(tableName: <span class="type">String</span>, cols: <span class="type">Seq</span>[<span class="type">String</span>],</span><br><span class="line">                    conf: <span class="type">Configuration</span> = <span class="keyword">new</span> <span class="type">Configuration</span>, zkUrl: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span>, tenantId: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span>)</span><br><span class="line">                    : <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Create a configuration object to use for saving</span></span><br><span class="line">    <span class="meta">@transient</span> <span class="keyword">val</span> outConfig = <span class="type">ConfigurationUtil</span>.getOutputConfiguration(tableName, cols, zkUrl, tenantId, <span class="type">Some</span>(conf))</span><br></pre></td></tr></table></figure>

<p>RDD保存时直接将存入的column数组传进来, Phoenix的API将Column原原本本作为输出的Column名，所以使用RDD的saveToPhoenix函数时必须严格按照Phoenix的Column名的大小写来输入。</p>
</li>
<li><p>方式2(使用DataFrame保存到phoenix):</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建表：CREATE TABLE STUDENT (ID INTEGER NOT NULL PRIMARY KEY, "cf1".name VARCHAR, "cf1".age INTEGER, "cf1".score DOUBLE);</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SaveMode</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="comment">//否则编辑器识别不出语法，也不会自动import。</span></span><br><span class="line"><span class="keyword">import</span> org.apache.phoenix.spark._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestSparkPhoenix</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().master(<span class="string">"local[*]"</span>).appName(<span class="string">"phoenix-test"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> dataSet = <span class="type">List</span>(<span class="type">Student</span>(<span class="number">1</span>,<span class="string">"kris"</span>,<span class="number">18</span>,<span class="number">95</span>),<span class="type">Student</span>(<span class="number">2</span>,<span class="string">"smile"</span>,<span class="number">19</span>,<span class="number">80</span>),<span class="type">Student</span>(<span class="number">3</span>,<span class="string">"alice"</span>,<span class="number">19</span>,<span class="number">100</span>))</span><br><span class="line">    <span class="keyword">val</span> df = spark.sqlContext.createDataFrame(dataSet)</span><br><span class="line">    df.write</span><br><span class="line">      .format(<span class="string">"org.apache.phoenix.spark"</span>)</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">      .options(<span class="type">Map</span>(<span class="string">"table"</span> -&gt; <span class="string">"STUDENT"</span>, <span class="string">"zkUrl"</span> -&gt; <span class="string">"hadoop101,hadoop102,hadoop103:2181"</span>))</span><br><span class="line">      .save()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params"><span class="type">ID</span>:<span class="type">Int</span>, <span class="type">Name</span>:<span class="type">String</span>,<span class="type">Age</span>: <span class="type">Int</span>,<span class="type">Score</span>:<span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">0</span>: jdbc:phoenix:hadoop101:<span class="number">2181</span>&gt; select * from student;</span><br><span class="line">+-----+--------+------+--------+</span><br><span class="line">| <span class="type">ID</span>  |  <span class="type">NAME</span>  | <span class="type">AGE</span>  | <span class="type">SCORE</span>  |</span><br><span class="line">+-----+--------+------+--------+</span><br><span class="line">| <span class="number">1</span>   | kris   | <span class="number">18</span>   | <span class="number">95.0</span>   |</span><br><span class="line">| <span class="number">2</span>   | smile  | <span class="number">19</span>   | <span class="number">80.0</span>   |</span><br><span class="line">| <span class="number">3</span>   | alice  | <span class="number">19</span>   | <span class="number">100.0</span>  |</span><br><span class="line">+-----+--------+------+--------+</span><br></pre></td></tr></table></figure>

<p>这种方式借助了org.apache.phoenix.spark里面的隐式函数</p>
<p>DataFrame保存时的列信息经过SchemaUtil.normalizeIdentifier(x)转化, 仅仅只是将字符串里面的引号去掉，然后转成大写。<br>不管我们的DataFrame的列是什么格式，最终都会转成大写。<br>然后Phoenix里面的列可能不是大写的，所以就可能出现列名是对的，但是大小写对应不上。</p>
<p>结论：</p>
<ul>
<li>在使用RDD保存数据到Phoenix的时候，要<strong>严格按照Phoenix列名的大小写来输入</strong></li>
<li>使用DataFrame保存的时候，<strong>对数据源的列名大小写无要求。但是必须保证Phoenix的表列名必须是大写的</strong></li>
<li>HBase建表的时候，我们<strong>建议您对表名和列都使用大写</strong></li>
<li>使用Phoenix创建表的时候，除非是已经存在了HBase的表，否则无需要建表的时候对列带引号,这样sql中即使是小写的列也会保存为大写</li>
</ul>
<p>注意: <code>phoenixTableAsDataFrame()</code>是<code>org.apache.phoenix.spark.SparkSqlContextFunctions</code>中的方法，<code>saveToPhoenix()</code>是<code>org.apache.phoenix.spark.DataFrameFunctions</code>中的方法，在<code>phoenix-spark-4.10.0-HBase-1.2.jar</code>中。使用这两个方法时必须 <code>import org.apache.phoenix.spark._</code>，否则编辑器识别不出语法，也不会自动<code>import</code>。</p>
</li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Tunan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yerias.github.io/2020/12/01/phoenix/2/">http://yerias.github.io/2020/12/01/phoenix/2/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Phoenix/">Phoenix</a></div><div class="social-share" data-disabled="facebook,twitter,douban,linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/12/03/clickhouse/3/"><i class="fa fa-chevron-left">  </i><span>ClickHouse表引擎的作用&amp;表引擎分类&amp;Log系列表引擎</span></a></div><div class="next-post pull-right"><a href="/2020/11/26/phoenix/1/"><span>Phoenix配置&amp;二级索引&amp;优化</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2021 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="footer_custom_text">大家好，我是图南，很高兴认识你们！</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>