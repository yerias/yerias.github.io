<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="数据列自动推导&amp;数据错误执行模式&amp;UDAF&amp;UDTF&amp;解读Spark SQL执行计划优化"><meta name="keywords" content="Spark"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>数据列自动推导&amp;数据错误执行模式&amp;UDAF&amp;UDTF&amp;解读Spark SQL执行计划优化 | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#目录"><span class="toc-number">1.</span> <span class="toc-text">目录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据列自动推导"><span class="toc-number">2.</span> <span class="toc-text">数据列自动推导</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据错误执行模式"><span class="toc-number">3.</span> <span class="toc-text">数据错误执行模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#UDAF"><span class="toc-number">4.</span> <span class="toc-text">UDAF</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#UDTF"><span class="toc-number">5.</span> <span class="toc-text">UDTF</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#解读Spark-SQL执行计划优化"><span class="toc-number">6.</span> <span class="toc-text">解读Spark SQL执行计划优化</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/xiaoqi.jpg"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><div class="follow-button"><a href="#">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">122</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">29</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">25</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">HADOOP</a><a class="author-info-links__name text-center" href="https://cwiki.apache.org/confluence/display/Hive/" target="_blank" rel="noopener">HIVE</a><a class="author-info-links__name text-center" href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">CDH</a><a class="author-info-links__name text-center" href="http://flume.apache.org/" target="_blank" rel="noopener">FLUME</a><a class="author-info-links__name text-center" href="https://azkaban.github.io/" target="_blank" rel="noopener">AZKABAN</a><a class="author-info-links__name text-center" href="https://www.maxinhong.com/" target="_blank" rel="noopener">叶梨子</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/qingyunzong" target="_blank" rel="noopener">老铁</a><a class="author-info-links__name text-center" href="https://blog.csdn.net/jim8973/" target="_blank" rel="noopener">飞哥</a><a class="author-info-links__name text-center" href="https://vinxikk.github.io/" target="_blank" rel="noopener">vinx</a><a class="author-info-links__name text-center" href="http://dongxicheng.org/" target="_blank" rel="noopener">懂西成</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/xing901022/" target="_blank" rel="noopener">xingoo</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">数据列自动推导&amp;数据错误执行模式&amp;UDAF&amp;UDTF&amp;解读Spark SQL执行计划优化</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-16</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Spark/">Spark</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1.4k</span><span class="post-meta__separator">|</span><span>Reading time: 6 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>数据列自动推导</li>
<li>数据错误执行模式</li>
<li>UDAF</li>
<li>UDTF</li>
<li>解读Spark SQL执行计划优化</li>
</ol>
<h2 id="数据列自动推导"><a href="#数据列自动推导" class="headerlink" title="数据列自动推导"></a>数据列自动推导</h2><ol>
<li><p>源数据</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">a|b|c</span><br><span class="line"><span class="number">1</span>|<span class="number">2</span>|<span class="number">3</span></span><br><span class="line"><span class="number">4</span>|tunan|<span class="number">6</span></span><br><span class="line"><span class="number">7</span>|<span class="number">8</span>|<span class="number">9.0</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>代码处理</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">    .builder()</span><br><span class="line">    .master(<span class="string">"local[2]"</span>)</span><br><span class="line">    .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    .getOrCreate()</span><br><span class="line">   </span><br><span class="line">  <span class="keyword">val</span> csvDF: <span class="type">DataFrame</span> = spark.read</span><br><span class="line">    .format(<span class="string">"csv"</span>)</span><br><span class="line">    .option(<span class="string">"header"</span>,<span class="string">"true"</span>)</span><br><span class="line">    .option(<span class="string">"sep"</span>,<span class="string">"|"</span>)</span><br><span class="line">    .option(<span class="string">"interSchema"</span>,<span class="string">"true"</span>)</span><br><span class="line">    .load(<span class="string">"tunan-spark-sql/data/test.csv"</span>)</span><br><span class="line">   </span><br><span class="line">  csvDF.printSchema()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>打印数据Schema信息</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- a: integer (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- b: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- c: double (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="数据错误执行模式"><a href="#数据错误执行模式" class="headerlink" title="数据错误执行模式"></a>数据错误执行模式</h2><p>在Spark中，读取数据时，遇到错误数据或者脏数据时，我们可以使用option设置mode，区分将错误数据是默认处理<code>PERMISSIVE</code>，还是丢弃数据<code>DROPMALFORMED</code>，还是快速失败<code>FAILFAST</code>，这些方法可以在ParseMode.scala</p>
<ol>
<li><p>源数据</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">"a"</span>:<span class="number">1</span>,<span class="string">"b"</span>:<span class="number">2</span>,<span class="string">"c"</span>:<span class="number">3</span>&#125;</span><br><span class="line">&#123;<span class="string">"a"</span>:<span class="number">4</span>,:<span class="number">5</span>,<span class="string">"c"</span>:<span class="number">6</span>&#125;</span><br><span class="line">&#123;<span class="string">"a"</span>:<span class="number">7</span>,<span class="string">"b"</span>:<span class="number">8</span>,<span class="string">"c"</span>:<span class="number">9</span>&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>读数据</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> jsonDF: <span class="type">DataFrame</span> = spark.read.json(<span class="string">"tunan-spark-sql/data/test.json"</span>)</span><br><span class="line">jsonDF.show()</span><br></pre></td></tr></table></figure>

<p>结果:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+----------------+----+----+----+</span><br><span class="line">| _corrupt_record|   a|   b|   c|</span><br><span class="line">+----------------+----+----+----+</span><br><span class="line">|            <span class="literal">null</span>|   <span class="number">1</span>|   <span class="number">2</span>|   <span class="number">3</span>|</span><br><span class="line">|&#123;<span class="string">"a"</span>:<span class="number">4</span>,:<span class="number">5</span>,<span class="string">"c"</span>:<span class="number">6</span>&#125;|<span class="literal">null</span>|<span class="literal">null</span>|<span class="literal">null</span>|</span><br><span class="line">|            <span class="literal">null</span>|   <span class="number">7</span>|   <span class="number">8</span>|   <span class="number">9</span>|</span><br><span class="line">+----------------+----+----+----+</span><br></pre></td></tr></table></figure>

<p>如果没有在option中设置mode选项，默认为<code>PERMISSIVE</code>，通过_corrupt_record列打印出错误信息</p>
</li>
<li><p>使用option设置mode为<code>DROPMALFORMED</code>，如果碰到错误的数据，则自动丢弃</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> jsonDF: <span class="type">DataFrame</span> = spark.read.option(<span class="string">"mode"</span>,<span class="string">"DROPMALFORMED"</span>).json(<span class="string">"tunan-spark-sql/data/test.json"</span>)</span><br><span class="line">jsonDF.show()</span><br></pre></td></tr></table></figure>

<p>结果:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+---+---+---+</span><br><span class="line">|  a|  b|  c|</span><br><span class="line">+---+---+---+</span><br><span class="line">|  <span class="number">1</span>|  <span class="number">2</span>|  <span class="number">3</span>|</span><br><span class="line">|  <span class="number">7</span>|  <span class="number">8</span>|  <span class="number">9</span>|</span><br><span class="line">+---+---+---+</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h2><ol>
<li><p>自定义一个UDAF的class或者object，作为具体的逻辑实现，需要继承<code>UserDefinedAggregateFunction</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AgeAvgUDAF</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span></span>&#123;</span><br><span class="line">    <span class="comment">//输入类型</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"input"</span>,<span class="type">DoubleType</span>,<span class="literal">true</span>)::<span class="type">Nil</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//聚合内部中的buffer类型</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"sums"</span>,<span class="type">DoubleType</span>,<span class="literal">true</span>)::</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"num"</span>,<span class="type">LongType</span>,<span class="literal">true</span>)::<span class="type">Nil</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//输入数据类型</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//输入数据类型是否和输出数据类型相等</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//聚合内部buffer的初始化</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      buffer(<span class="number">0</span>) = <span class="number">0.0</span></span><br><span class="line">      buffer(<span class="number">1</span>) = <span class="number">0</span>L</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分区内更新聚合buffer</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      buffer.update(<span class="number">0</span>,buffer.getDouble(<span class="number">0</span>)+input.getDouble(<span class="number">0</span>))</span><br><span class="line">      buffer.update(<span class="number">1</span>,buffer.getLong(<span class="number">1</span>)+<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分区间合并</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      buffer1.update(<span class="number">0</span>,buffer1.getDouble(<span class="number">0</span>)+buffer2.getDouble(<span class="number">0</span>))</span><br><span class="line">      buffer1.update(<span class="number">1</span>,buffer1.getLong(<span class="number">1</span>)+buffer2.getLong(<span class="number">1</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//最终计算</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">      buffer.getDouble(<span class="number">0</span>)/buffer.getLong(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>注册并使用UDAF</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">   <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">     .builder()</span><br><span class="line">     .master(<span class="string">"local[2]"</span>)</span><br><span class="line">     .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">     .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义数据源</span></span><br><span class="line">   <span class="keyword">val</span> list = <span class="keyword">new</span> util.<span class="type">ArrayList</span>[<span class="type">Row</span>]()</span><br><span class="line">   list.add(<span class="type">Row</span>(<span class="string">"zhangsan"</span>,<span class="number">18</span>,<span class="string">"男"</span>))</span><br><span class="line">   list.add(<span class="type">Row</span>(<span class="string">"lisi"</span>,<span class="number">20</span>,<span class="string">"男"</span>))</span><br><span class="line">   list.add(<span class="type">Row</span>(<span class="string">"wangwu"</span>,<span class="number">26</span>,<span class="string">"女"</span>))</span><br><span class="line">   list.add(<span class="type">Row</span>(<span class="string">"翠翠"</span>,<span class="number">18</span>,<span class="string">"女"</span>))</span><br><span class="line">   list.add(<span class="type">Row</span>(<span class="string">"闰土"</span>,<span class="number">8</span>,<span class="string">"男"</span>))</span><br><span class="line"></span><br><span class="line">   <span class="comment">// 自定义Schema</span></span><br><span class="line">   <span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">     <span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">StringType</span>, <span class="literal">true</span>)::</span><br><span class="line">     <span class="type">StructField</span>(<span class="string">"age"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>)::</span><br><span class="line">     <span class="type">StructField</span>(<span class="string">"sex"</span>, <span class="type">StringType</span>, <span class="literal">true</span>)::<span class="type">Nil</span></span><br><span class="line">   )</span><br><span class="line">   </span><br><span class="line"><span class="comment">//创建df</span></span><br><span class="line">   <span class="keyword">val</span> df = spark.createDataFrame(list, schema)</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//创建视图</span></span><br><span class="line">   df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//注册UDAF</span></span><br><span class="line">   spark.udf.register(<span class="string">"age_avg_udaf"</span>,<span class="type">AgeAvgUDAF</span>)</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//使用UDAF</span></span><br><span class="line">   spark.sql(<span class="string">"select sex,age_avg_udaf(age) as ave_age from people group by sex"</span>).show()</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>结果展示</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+---+---------+</span><br><span class="line">|sex|  ave_age|</span><br><span class="line">+---+---------+</span><br><span class="line">| 男|    <span class="number">15.33</span>|</span><br><span class="line">| 女|     <span class="number">22.0</span>|</span><br><span class="line">+---+---------+</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="UDTF"><a href="#UDTF" class="headerlink" title="UDTF"></a>UDTF</h2><p>UDTF还在研究，先搞个简单的案例</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ExplodeUDTF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">"local[2]"</span>)</span><br><span class="line">      .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 自定义schema  </span></span><br><span class="line">    <span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"teacher"</span>, <span class="type">StringType</span>, <span class="literal">true</span>) ::</span><br><span class="line">        <span class="type">StructField</span>(<span class="string">"sources"</span>, <span class="type">StringType</span>, <span class="literal">true</span>) :: <span class="type">Nil</span></span><br><span class="line">    )</span><br><span class="line">	<span class="comment">// 自定义数据源</span></span><br><span class="line">    <span class="keyword">val</span> list = <span class="keyword">new</span> util.<span class="type">ArrayList</span>[<span class="type">Row</span>]()</span><br><span class="line">    list.add(<span class="type">Row</span>(<span class="string">"tunan"</span>, <span class="string">"hive,spark,flink"</span>))</span><br><span class="line">    list.add(<span class="type">Row</span>(<span class="string">"xiaoqi"</span>, <span class="string">"cdh,kafka,hbase"</span>))</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 创建临时视图</span></span><br><span class="line">    <span class="keyword">val</span> df = spark.createDataFrame(list, schema)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// 使用flatMap拆分</span></span><br><span class="line">    df.flatMap(x =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> line = <span class="keyword">new</span> <span class="type">ListBuffer</span>[(<span class="type">String</span>, <span class="type">String</span>)]()</span><br><span class="line">      <span class="keyword">val</span> sources = x.getString(<span class="number">1</span>).split(<span class="string">","</span>)</span><br><span class="line">      <span class="keyword">for</span> (source &lt;- sources)&#123;</span><br><span class="line">        line.append((x.getString(<span class="number">0</span>),source))</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//返回</span></span><br><span class="line">      line</span><br><span class="line">    &#125;).toDF(<span class="string">"teacher"</span>,<span class="string">"source"</span>).show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结果</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">+-------+------+</span><br><span class="line">|teacher|source|</span><br><span class="line">+-------+------+</span><br><span class="line">|  tunan|  hive|</span><br><span class="line">|  tunan| spark|</span><br><span class="line">|  tunan| flink|</span><br><span class="line">| xiaoqi|   cdh|</span><br><span class="line">| xiaoqi| kafka|</span><br><span class="line">| xiaoqi| hbase|</span><br><span class="line">+-------+------+</span><br></pre></td></tr></table></figure>

<h2 id="解读Spark-SQL执行计划优化"><a href="#解读Spark-SQL执行计划优化" class="headerlink" title="解读Spark SQL执行计划优化"></a>解读Spark SQL执行计划优化</h2><ol>
<li><p>建空表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> sqltest (<span class="keyword">key</span> <span class="keyword">string</span>,<span class="keyword">value</span> <span class="keyword">string</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行SQL</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">explain</span> <span class="keyword">extended</span> <span class="keyword">select</span> a.key*(<span class="number">3</span>*<span class="number">5</span>),b.value <span class="keyword">from</span> sqltest a <span class="keyword">join</span> sqltest b <span class="keyword">on</span> a.key=b.key <span class="keyword">and</span> a.key &gt;<span class="number">3</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>解读执行计划</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">// 解析逻辑计划，做些简单的解析</span><br><span class="line">== Parsed Logical Plan ==</span><br><span class="line">'Project [unresolvedalias(('a.key * (3 * 5)), None), 'b.value]</span><br><span class="line">+- 'Join Inner, (('a.key = 'b.key) &amp;&amp; ('a.key &gt; 3))</span><br><span class="line">   :- 'SubqueryAlias `a`</span><br><span class="line">   :  +- 'UnresolvedRelation `sqltest`</span><br><span class="line">   +- 'SubqueryAlias `b`</span><br><span class="line">      +- 'UnresolvedRelation `sqltest`</span><br><span class="line"></span><br><span class="line">// 分析逻辑计划，解析出了数据类型，拿到数据库和表，拿到了序列化方式                           </span><br><span class="line">== Analyzed Logical Plan ==</span><br><span class="line">(CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE)): double, value: string</span><br><span class="line">Project [(cast(key<span class="comment">#2 as double) * cast((3 * 5) as double)) AS (CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE))#6, value#5]</span></span><br><span class="line">+- Join Inner, ((key<span class="comment">#2 = key#4) &amp;&amp; (cast(key#2 as int) &gt; 3))</span></span><br><span class="line">   :- SubqueryAlias `a`</span><br><span class="line">   :  +- SubqueryAlias `default`.`sqltest`</span><br><span class="line">   :     +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key<span class="comment">#2, value#3]</span></span><br><span class="line">   +- SubqueryAlias `b`</span><br><span class="line">      +- SubqueryAlias `default`.`sqltest`</span><br><span class="line">         +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key<span class="comment">#4, value#5]</span></span><br><span class="line"></span><br><span class="line">// 优化逻辑计划，数值类型的运算直接拿到结果，解析过滤条件</span><br><span class="line">== Optimized Logical Plan ==</span><br><span class="line">Project [(cast(key<span class="comment">#2 as double) * 15.0) AS (CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE))#6, value#5]</span></span><br><span class="line">+- Join Inner, (key<span class="comment">#2 = key#4)</span></span><br><span class="line">   :- Project [key<span class="comment">#2]</span></span><br><span class="line">   :  +- Filter (isnotnull(key<span class="comment">#2) &amp;&amp; (cast(key#2 as int) &gt; 3))</span></span><br><span class="line">   :     +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key<span class="comment">#2, value#3]</span></span><br><span class="line">   +- Filter ((cast(key<span class="comment">#4 as int) &gt; 3) &amp;&amp; isnotnull(key#4))</span></span><br><span class="line">      +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key<span class="comment">#4, value#5]</span></span><br><span class="line"></span><br><span class="line">//物理计划，join方式为SortMergeJoin，数据使用hashpartitioning保存，扫描表的方式是HiveTableRelation</span><br><span class="line">== Physical Plan ==</span><br><span class="line">*(5) Project [(cast(key#2 as double) * 15.0) AS (CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE))#6, value#5]</span><br><span class="line">+- *(5) SortMergeJoin [key#2], [key#4], Inner</span><br><span class="line">   :- *(2) Sort [key#2 ASC NULLS FIRST], false, 0</span><br><span class="line">   :  +- Exchange hashpartitioning(key<span class="comment">#2, 200)</span></span><br><span class="line">   :     +- *(1) Filter (isnotnull(key#2) &amp;&amp; (cast(key#2 as int) &gt; 3))</span><br><span class="line">   :        +- Scan hive default.sqltest [key<span class="comment">#2], HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#2, value#3]</span></span><br><span class="line">   +- *(4) Sort [key#4 ASC NULLS FIRST], false, 0</span><br><span class="line">      +- Exchange hashpartitioning(key<span class="comment">#4, 200)</span></span><br><span class="line">         +- *(3) Filter ((cast(key#4 as int) &gt; 3) &amp;&amp; isnotnull(key#4))</span><br><span class="line">            +- Scan hive default.sqltest [key<span class="comment">#4, value#5], HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#4, value#5]</span></span><br></pre></td></tr></table></figure>

<p>可以简单的看做四步，分别是解析逻辑计划、分析逻辑计划、优化逻辑计划、物理执行计划</p>
</li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Tunan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yerias.github.io/2020/04/16/spark/17/">http://yerias.github.io/2020/04/16/spark/17/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="social-share" data-disabled="facebook,twitter,douban,linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/04/16/spark/18/"><i class="fa fa-chevron-left">  </i><span>自定义外部Text数据源</span></a></div><div class="next-post pull-right"><a href="/2020/04/15/jvm/1/"><span>JVM之运行时数据区</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2020 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="footer_custom_text">大家好，我是图南，很高兴认识你们！</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>