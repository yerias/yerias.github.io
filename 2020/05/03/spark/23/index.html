<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Spark 读写压缩文件的一次简单尝试"><meta name="keywords" content="Spark"><meta name="author" content="Tunan"><meta name="copyright" content="Tunan"><title>Spark 读写压缩文件的一次简单尝试 | TUNANのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#前言"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#读LZO文件写HDFS"><span class="toc-number">2.</span> <span class="toc-text">读LZO文件写HDFS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#读LZO文件写HDFS也为LZO文件"><span class="toc-number">3.</span> <span class="toc-text">读LZO文件写HDFS也为LZO文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#读LZO文件写MYSQL"><span class="toc-number">4.</span> <span class="toc-text">读LZO文件写MYSQL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#读LZO文件写HIVE"><span class="toc-number">5.</span> <span class="toc-text">读LZO文件写HIVE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#读LZO文件压缩写HIVE"><span class="toc-number">6.</span> <span class="toc-text">读LZO文件压缩写HIVE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#总结"><span class="toc-number">7.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/xiaoqi.jpg"></div><div class="author-info__name text-center">Tunan</div><div class="author-info__description text-center">BigData Developer</div><div class="follow-button"><a href="#">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">136</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">31</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">27</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">HADOOP</a><a class="author-info-links__name text-center" href="https://cwiki.apache.org/confluence/display/Hive/" target="_blank" rel="noopener">HIVE</a><a class="author-info-links__name text-center" href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">CDH</a><a class="author-info-links__name text-center" href="http://flume.apache.org/" target="_blank" rel="noopener">FLUME</a><a class="author-info-links__name text-center" href="https://azkaban.github.io/" target="_blank" rel="noopener">AZKABAN</a><a class="author-info-links__name text-center" href="https://www.maxinhong.com/" target="_blank" rel="noopener">叶梨子</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/qingyunzong" target="_blank" rel="noopener">老铁</a><a class="author-info-links__name text-center" href="https://blog.csdn.net/jim8973/" target="_blank" rel="noopener">飞哥</a><a class="author-info-links__name text-center" href="https://vinxikk.github.io/" target="_blank" rel="noopener">vinx</a><a class="author-info-links__name text-center" href="http://dongxicheng.org/" target="_blank" rel="noopener">懂西成(Hadoop技术内幕作者)</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/xing901022/" target="_blank" rel="noopener">xingoo</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/itboys/tag/" target="_blank" rel="noopener">大葱拌豆腐</a><a class="author-info-links__name text-center" href="https://www.cnblogs.com/shishanyuan" target="_blank" rel="noopener">郭景瞻(图解Spark作者)</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">TUNANのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">Spark 读写压缩文件的一次简单尝试</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-03</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Spark/">Spark</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1.4k</span><span class="post-meta__separator">|</span><span>Reading time: 5 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>我认为以节省存储空间为角度出发，Spark作业中的读写压缩文件是必不可少的话题，当然这在MR作业中也有体现和实际解决这种问题，现在我们就要在Spark中解决这种问题。</p>
<p>如果需要安装Lzo可以看我的其他<a href="https://yerias.github.io/2018/10/15/hadoop/13/">文章</a></p>
<p>源文件是一份access的原始数据</p>
<p><img src="https://yerias.github.io/spark_img/access.log.png" alt=""></p>
<p>我们在上传到服务器上的时候，使用lzop命令压缩该文件，得到压缩后的文件，上传HDFSF并对该文件创建索引，得到我们代码中需要处理的源文件</p>
<p><img src="https://yerias.github.io/spark_img/compress-1.png" alt=""></p>
<p>需要注意的是未压缩的文件是1G，压缩后为314M</p>
<p>下图是我们想要做的事情</p>
<p><img src="https://yerias.github.io/spark_img/format%E8%AF%BB%E5%86%99.jpg" alt=""></p>
<p><strong>各种压缩格式的性能比较</strong></p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>工具</th>
<th>算法</th>
<th>扩展名</th>
<th>是否支持分割</th>
<th>Hadoop编码/解码器</th>
<th>hadoop自带</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>N/A</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>No</td>
<td>org.apache.hadoop.io.compress.DefalutCodec</td>
<td>是</td>
</tr>
<tr>
<td>gzip</td>
<td>gzip</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>No</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
<td>是</td>
</tr>
<tr>
<td>bzip2</td>
<td>bzip2</td>
<td>bzip2</td>
<td>.bz2</td>
<td>yes</td>
<td>org.apache.hadoop.io.compress.Bzip2Codec</td>
<td>是</td>
</tr>
<tr>
<td>LZO</td>
<td>Lzop</td>
<td>LZO</td>
<td>.lzo</td>
<td>yes(建索引)</td>
<td>com.hadoop.compression.lzo.LzoCodec</td>
<td>是</td>
</tr>
<tr>
<td>LZ4</td>
<td>N/A</td>
<td>LZ4</td>
<td>.lz4</td>
<td>No</td>
<td>org.apache.hadoop.io.compress.Lz4Codec</td>
<td>否</td>
</tr>
<tr>
<td>Snappy</td>
<td>N/A</td>
<td>Snappy</td>
<td>.snappy</td>
<td>No</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
<td>否</td>
</tr>
</tbody></table>
<p>压缩比：Snappy&lt;LZ4&lt;LZO&lt;GZIP&lt;BZIP2</p>
<p><strong>各种压缩格式的性能优缺点</strong></p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td>lzo</td>
<td>优点：压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便<br/>缺点：不支持split</td>
</tr>
<tr>
<td>snappy</td>
<td>优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便<br/>缺点：压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）</td>
</tr>
<tr>
<td>gzip</td>
<td>优点：压缩速度快；支持hadoop native库<br/>缺点：不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令</td>
</tr>
<tr>
<td>bzip2</td>
<td>优点：支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便<br/>缺点：压缩/解压速度慢；不支持native</td>
</tr>
</tbody></table>
<h3 id="读LZO文件写HDFS"><a href="#读LZO文件写HDFS" class="headerlink" title="读LZO文件写HDFS"></a>读LZO文件写HDFS</h3><p>需要注意的是在Spark中读取HDFS上的压缩文件，需要使用newAPIHadoopFile接口，并且传入LzoTextInputFormat，这个依赖不好解决，仓库是twitter的，下载不了，最好的办法是去github下载源码，install到本地</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.hadoop.gplcompression<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-lzo<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.4.20<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>读取文件的具体代码</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> linesRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.newAPIHadoopFile(in, classOf[<span class="type">LzoTextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>])  .map(x =&gt; x._2.toString)</span><br></pre></td></tr></table></figure>

<p>现在我们已经读进来了，可以实现我们的业务逻辑，那么最后在Spark Core中如何写出去呢? 只需要使用saveAsTextFile</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">etl(linesRDD,hashMap)  .filter(x =&gt; x.responseSize != <span class="number">0</span>).saveAsTextFile(out)</span><br></pre></td></tr></table></figure>

<h3 id="读LZO文件写HDFS也为LZO文件"><a href="#读LZO文件写HDFS也为LZO文件" class="headerlink" title="读LZO文件写HDFS也为LZO文件"></a>读LZO文件写HDFS也为LZO文件</h3><p>读LZO文件上面的方法同样适用，而写为LZO文件只需要在saveAsTextFile中指定LzopCodec即可，适用于所有的格式压缩</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">etl(linesRDD,hashMap)  .filter(x =&gt; x.responseSize != <span class="number">0</span>).saveAsTextFile(out,classOf[<span class="type">LzopCodec</span>])</span><br></pre></td></tr></table></figure>

<h3 id="读LZO文件写MYSQL"><a href="#读LZO文件写MYSQL" class="headerlink" title="读LZO文件写MYSQL"></a>读LZO文件写MYSQL</h3><p>读写表和读写文件不同，读写表更适合用Spark SQL来实现，现在我们的表的字段非常多，不适用于使用tuple的实现方式，从而自定义外部数据源，在TableScan中也是实现与上面相同代码，就能将lzo文件分片读写</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">    logError(<span class="string">"进入buildScan方法"</span>)</span><br><span class="line">    <span class="comment">// 使用RDD拿到Lzo本文内容</span></span><br><span class="line">    <span class="keyword">val</span> lines = sqlContext.sparkContext.newAPIHadoopFile(path, classOf[<span class="type">LzoTextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>])</span><br><span class="line">    .map(_._2.toString)</span><br><span class="line">    <span class="comment">// 拿到响应的schema信息</span></span><br><span class="line">    <span class="keyword">val</span> fields = schema.fields</span><br><span class="line">    <span class="comment">// 拿到每行数据，做简单处理，返回RDD[Row]</span></span><br><span class="line">    lines.map(_.split(<span class="string">","</span>).map(_.trim)).map(_.zipWithIndex.map &#123;</span><br><span class="line">        <span class="keyword">case</span> (value, index) =&gt;</span><br><span class="line">        <span class="type">Utils</span>.caseTo(value, fields(index).dataType)</span><br><span class="line">    &#125;).map(x =&gt; <span class="type">Row</span>.fromSeq(x))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我在这里尝试了压缩写和普通写，都可以查询到数据</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">        properties.put(<span class="string">"user"</span>,<span class="string">"root"</span>)</span><br><span class="line">        properties.put(<span class="string">"password"</span>,<span class="string">"root"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> url = <span class="string">"jdbc:mysql://hadoop:3306/tunan?useUnicode=true&amp;characterEncoding=utf-8"</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!flat)&#123;</span><br><span class="line">            <span class="comment">// 保存到MySQL</span></span><br><span class="line">            result.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">              .jdbc(url,<span class="string">"domain_group"</span>,properties)</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="comment">// 保存到MySQL,压缩格式为lzo</span></span><br><span class="line">            result.write.format(<span class="string">"orc"</span>).option(<span class="string">"compression"</span>,<span class="string">"lzo"</span>).mode(<span class="string">"overwrite"</span>)</span><br><span class="line">              .jdbc(url,<span class="string">"domain_group"</span>,properties)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>

<p>查询：</p>
<p><img src="https://yerias.github.io/spark_img/compress-2.png" alt=""></p>
<h3 id="读LZO文件写HIVE"><a href="#读LZO文件写HIVE" class="headerlink" title="读LZO文件写HIVE"></a>读LZO文件写HIVE</h3><p>读的方式也是一样的，使用外部数据源的方式直接得到DataFrame，在写的时候指定了存储格式，而不指定压缩格式的话，会默认指定压缩格式为Snappy</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">textDf.write.format(<span class="string">"parquet"</span>).mode(<span class="string">"overwrite"</span>).saveAsTable(<span class="string">"store_format.parquet_tb"</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://yerias.github.io/spark_img/compress-3.png" alt=""></p>
<h3 id="读LZO文件压缩写HIVE"><a href="#读LZO文件压缩写HIVE" class="headerlink" title="读LZO文件压缩写HIVE"></a>读LZO文件压缩写HIVE</h3><p>读Lzo的方式如上，在这里我们尝试了几种格式的压缩写</p>
<ol>
<li><p>指定了存储格式为parquet，压缩格式为lzo</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">textDf.write.format(<span class="string">"parquet"</span>).option(<span class="string">"compression"</span>,<span class="string">"lzo"</span>).mode(<span class="string">"overwrite"</span>).saveAsTable(<span class="string">"store_format.parquet_tb"</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://yerias.github.io/spark_img/compress-4.png" alt=""></p>
<p>可以看得到压缩格式和存储格式发生了变化，我们继续查表</p>
<p><img src="https://yerias.github.io/spark_img/compress-5.png" alt=""></p>
</li>
<li><p>现在我们将压缩格式改为orc，存储格式还是使用lzo</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">textDf.write.format(<span class="string">"orc"</span>).option(<span class="string">"compression"</span>,<span class="string">"lzo"</span>).mode(<span class="string">"overwrite"</span>).saveAsTable(<span class="string">"store_format.parquet_tb"</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://yerias.github.io/spark_img/compress-6.png" alt=""></p>
<p>我们发现orc和lzo格式并不能一起使用，查出来的是无效的结果</p>
</li>
<li><p>继续使用压缩格式为orc，存储格式改为snappy</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">textDf.write.format(<span class="string">"orc"</span>).option(<span class="string">"compression"</span>,<span class="string">"snappy"</span>).mode(<span class="string">"overwrite"</span>).saveAsTable(<span class="string">"store_format.parquet_tb"</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://yerias.github.io/spark_img/compress-7.png" alt=""></p>
<p>查询结果：</p>
<p><img src="https://yerias.github.io/spark_img/compress-5.png" alt=""></p>
</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol>
<li>使用Lzo压缩的压缩比相比于Snappy较高</li>
<li>无论哪种压缩格式在MySQL中都是无效的</li>
<li>Parquet可以和Snappy或者Lzo搭配使用，ORC可以和Snappy或者Bzip搭配使用</li>
<li>ORC不能和Lzo搭配使用</li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Tunan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yerias.github.io/2020/05/03/spark/23/">http://yerias.github.io/2020/05/03/spark/23/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="social-share" data-disabled="facebook,twitter,douban,linkedin,diandian"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/05/03/presto/2/"><i class="fa fa-chevron-left">  </i><span>Presto部署</span></a></div><div class="next-post pull-right"><a href="/2020/05/02/spark/24/"><span>SS+Kafka提交服务器&amp;窗口函数&amp;SS调优</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(/img/soroush-golpoor-1497416-unsplash.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2020 By Tunan</div><div class="framework-info"><span>Driven - </span><a href="#"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="#"><span>Melody</span></a></div><div class="footer_custom_text">大家好，我是图南，很高兴认识你们！</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>